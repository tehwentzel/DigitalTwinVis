{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "050c878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7418d82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score,accuracy_score,precision_recall_fscore_support\n",
    "from Constants import *\n",
    "from Preprocessing import *\n",
    "from Models import *\n",
    "import copy\n",
    "from Utils import *\n",
    "from DeepSurvivalModels import *\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c427599a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(536, 62)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = DTDataset(use_smote=False)\n",
    "data.processed_df.T\n",
    "data.get_input_state(1).shape\n",
    "# data.processed_df#.shape, len(data.processed_df.index.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2937b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DSM(\n",
       "  (act): Tanh()\n",
       "  (shape): ParameterList(\n",
       "      (0): Parameter containing: [torch.float32 of size 3]\n",
       "      (1): Parameter containing: [torch.float32 of size 3]\n",
       "      (2): Parameter containing: [torch.float32 of size 3]\n",
       "      (3): Parameter containing: [torch.float32 of size 3]\n",
       "  )\n",
       "  (scale): ParameterList(\n",
       "      (0): Parameter containing: [torch.float32 of size 3]\n",
       "      (1): Parameter containing: [torch.float32 of size 3]\n",
       "      (2): Parameter containing: [torch.float32 of size 3]\n",
       "      (3): Parameter containing: [torch.float32 of size 3]\n",
       "  )\n",
       "  (gate): ModuleList(\n",
       "    (0-3): 4 x Sequential(\n",
       "      (0): Linear(in_features=103, out_features=3, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (scaleg): ModuleList(\n",
       "    (0-3): 4 x Sequential(\n",
       "      (0): Linear(in_features=103, out_features=3, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (shapeg): ModuleList(\n",
       "    (0-3): 4 x Sequential(\n",
       "      (0): Linear(in_features=103, out_features=3, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (embedding): Sequential(\n",
       "    (0): Linear(in_features=79, out_features=100, bias=False)\n",
       "    (1): Tanh()\n",
       "  )\n",
       "  (input_dropout): Dropout(p=0, inplace=False)\n",
       "  (embedding_dropout): Dropout(p=0.5, inplace=False)\n",
       "  (squish): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Utils import *\n",
    "model1,model2,model3,smodel3 = load_transition_models()\n",
    "smodel3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b29d259b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8., 0., 0.]) 147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]),\n",
       " {'pd1': tensor([[0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [6.5378e-01, 3.3400e-01, 1.2220e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [1.0692e-01, 8.8322e-01, 9.8608e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [4.5066e-01, 5.4254e-01, 6.8048e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [7.2752e-01, 2.7248e-01, 1.9548e-22],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [7.7904e-02, 9.1975e-01, 2.3486e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [2.3376e-02, 9.7389e-01, 2.7377e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [1.4404e-01, 8.4511e-01, 1.0851e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [9.0641e-01, 9.2574e-02, 1.0162e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01]], grad_fn=<CopySlices>),\n",
       "  'nd1': tensor([[0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [2.4202e-03, 9.9522e-01, 2.3607e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [4.0155e-03, 9.9205e-01, 3.9321e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [1.3572e-03, 9.9728e-01, 1.3625e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [5.3609e-33, 1.0000e+00, 5.3609e-33],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [6.2975e-04, 9.9874e-01, 6.3383e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [1.4531e-03, 9.9709e-01, 1.4556e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [3.9960e-03, 9.9204e-01, 3.9618e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [1.6883e-04, 9.9967e-01, 1.6593e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01]], grad_fn=<CopySlices>),\n",
       "  'nd2': tensor([[9.8130e-01, 1.8394e-02, 3.0103e-04],\n",
       "          [8.4328e-01, 1.5671e-01, 5.3298e-06],\n",
       "          [7.4203e-01, 2.5728e-01, 6.8109e-04],\n",
       "          [7.4908e-01, 2.4683e-01, 4.0813e-03],\n",
       "          [7.7093e-01, 2.2430e-01, 4.7723e-03],\n",
       "          [5.1221e-01, 4.8418e-01, 3.6074e-03],\n",
       "          [5.3750e-01, 4.6202e-01, 4.8001e-04],\n",
       "          [3.9753e-01, 5.9980e-01, 2.6660e-03],\n",
       "          [7.1889e-01, 2.7859e-01, 2.5187e-03],\n",
       "          [3.9861e-01, 5.9555e-01, 5.8383e-03],\n",
       "          [3.9455e-01, 6.0083e-01, 4.6153e-03],\n",
       "          [7.4597e-01, 2.5365e-01, 3.7563e-04],\n",
       "          [3.9753e-01, 5.9738e-01, 5.0922e-03],\n",
       "          [3.4412e-01, 6.5304e-01, 2.8352e-03],\n",
       "          [6.5742e-01, 3.3741e-01, 5.1778e-03],\n",
       "          [5.8397e-01, 4.1154e-01, 4.4958e-03],\n",
       "          [7.4178e-01, 2.5815e-01, 6.9111e-05],\n",
       "          [6.8987e-01, 3.0487e-01, 5.2540e-03],\n",
       "          [3.6970e-01, 6.2657e-01, 3.7252e-03],\n",
       "          [4.9761e-01, 4.9996e-01, 2.4263e-03],\n",
       "          [7.9082e-01, 2.0511e-01, 4.0699e-03],\n",
       "          [5.9940e-01, 3.9643e-01, 4.1668e-03],\n",
       "          [6.7224e-01, 3.2202e-01, 5.7383e-03],\n",
       "          [7.3378e-01, 2.6581e-01, 4.1576e-04],\n",
       "          [1.9571e-01, 8.0429e-01, 2.4263e-06],\n",
       "          [7.1419e-01, 2.8311e-01, 2.6987e-03],\n",
       "          [2.5820e-01, 7.4180e-01, 2.5768e-06],\n",
       "          [6.3838e-01, 3.6114e-01, 4.8612e-04],\n",
       "          [4.6434e-01, 5.3065e-01, 5.0066e-03],\n",
       "          [4.6983e-01, 5.2586e-01, 4.3084e-03],\n",
       "          [3.7894e-01, 6.1755e-01, 3.5118e-03],\n",
       "          [6.2411e-01, 3.6698e-01, 8.9121e-03],\n",
       "          [4.1003e-01, 5.8569e-01, 4.2800e-03],\n",
       "          [6.6125e-01, 3.3430e-01, 4.4561e-03],\n",
       "          [6.8569e-01, 3.1227e-01, 2.0444e-03],\n",
       "          [9.6573e-01, 3.3744e-02, 5.2725e-04],\n",
       "          [5.9778e-01, 3.9674e-01, 5.4827e-03],\n",
       "          [7.7064e-01, 2.2605e-01, 3.3013e-03],\n",
       "          [3.9426e-01, 6.0344e-01, 2.2917e-03],\n",
       "          [4.5624e-01, 5.4001e-01, 3.7524e-03],\n",
       "          [6.7899e-01, 3.1416e-01, 6.8501e-03],\n",
       "          [9.1603e-01, 8.3965e-02, 8.5255e-06],\n",
       "          [7.6297e-01, 2.3389e-01, 3.1395e-03],\n",
       "          [8.4838e-01, 1.5136e-01, 2.6131e-04],\n",
       "          [4.7422e-01, 5.2233e-01, 3.4575e-03],\n",
       "          [7.6502e-01, 2.3453e-01, 4.4455e-04],\n",
       "          [8.2658e-01, 1.7033e-01, 3.0865e-03],\n",
       "          [3.7670e-01, 6.2189e-01, 1.4111e-03],\n",
       "          [5.7215e-01, 4.2175e-01, 6.0984e-03],\n",
       "          [4.8292e-01, 5.1230e-01, 4.7819e-03],\n",
       "          [7.2372e-01, 2.7565e-01, 6.2551e-04],\n",
       "          [6.7999e-01, 3.1141e-01, 8.5974e-03],\n",
       "          [9.1360e-01, 8.4451e-02, 1.9493e-03],\n",
       "          [5.1165e-01, 4.8206e-01, 6.2988e-03],\n",
       "          [4.8335e-01, 5.1229e-01, 4.3579e-03],\n",
       "          [5.6015e-01, 4.3164e-01, 8.2106e-03],\n",
       "          [8.0353e-01, 1.9267e-01, 3.8058e-03],\n",
       "          [4.1701e-01, 5.7827e-01, 4.7242e-03],\n",
       "          [9.3535e-01, 6.4642e-02, 2.8986e-06],\n",
       "          [5.2459e-01, 4.6883e-01, 6.5816e-03],\n",
       "          [4.3945e-01, 5.5627e-01, 4.2803e-03],\n",
       "          [5.2450e-01, 4.7098e-01, 4.5247e-03],\n",
       "          [4.2625e-01, 5.6863e-01, 5.1189e-03],\n",
       "          [3.9869e-01, 5.9813e-01, 3.1758e-03],\n",
       "          [1.8225e-01, 8.1775e-01, 2.9974e-06],\n",
       "          [3.8075e-01, 6.1645e-01, 2.7978e-03],\n",
       "          [4.4744e-01, 5.4773e-01, 4.8231e-03],\n",
       "          [5.0087e-01, 4.9380e-01, 5.3246e-03],\n",
       "          [6.0474e-01, 3.8882e-01, 6.4334e-03],\n",
       "          [5.4002e-01, 4.5526e-01, 4.7274e-03],\n",
       "          [4.5328e-01, 5.4156e-01, 5.1683e-03],\n",
       "          [7.7153e-01, 2.2307e-01, 5.3960e-03],\n",
       "          [4.4431e-01, 5.5145e-01, 4.2473e-03],\n",
       "          [5.1999e-01, 4.7677e-01, 3.2454e-03],\n",
       "          [7.8700e-01, 2.1131e-01, 1.6976e-03],\n",
       "          [4.3005e-01, 5.6788e-01, 2.0719e-03],\n",
       "          [5.2854e-01, 4.7072e-01, 7.3148e-04],\n",
       "          [4.9486e-01, 4.9837e-01, 6.7661e-03],\n",
       "          [4.2598e-01, 5.7272e-01, 1.3049e-03],\n",
       "          [5.3819e-01, 4.5928e-01, 2.5309e-03],\n",
       "          [5.1058e-01, 4.8558e-01, 3.8325e-03],\n",
       "          [7.1421e-01, 2.8172e-01, 4.0634e-03],\n",
       "          [6.9653e-01, 2.9921e-01, 4.2583e-03],\n",
       "          [4.6503e-01, 5.3255e-01, 2.4151e-03],\n",
       "          [8.5784e-01, 1.4177e-01, 3.9154e-04],\n",
       "          [3.5233e-01, 6.4498e-01, 2.6906e-03],\n",
       "          [6.8233e-01, 3.1475e-01, 2.9228e-03],\n",
       "          [2.6067e-01, 7.3832e-01, 1.0026e-03],\n",
       "          [4.0530e-01, 5.9198e-01, 2.7150e-03],\n",
       "          [9.6671e-01, 3.2846e-02, 4.4359e-04],\n",
       "          [3.7823e-01, 6.1840e-01, 3.3648e-03],\n",
       "          [3.3895e-01, 6.5844e-01, 2.6121e-03],\n",
       "          [5.5910e-01, 4.3441e-01, 6.4904e-03],\n",
       "          [3.8772e-01, 6.0945e-01, 2.8311e-03],\n",
       "          [6.6281e-01, 3.3688e-01, 3.0861e-04],\n",
       "          [7.7882e-01, 2.1938e-01, 1.7960e-03],\n",
       "          [5.0879e-01, 4.8479e-01, 6.4250e-03],\n",
       "          [8.1870e-01, 1.8099e-01, 3.0986e-04],\n",
       "          [3.8876e-01, 6.0717e-01, 4.0670e-03],\n",
       "          [5.8182e-01, 4.1609e-01, 2.0876e-03],\n",
       "          [5.0090e-01, 4.9572e-01, 3.3800e-03],\n",
       "          [5.2426e-01, 4.6947e-01, 6.2649e-03],\n",
       "          [6.0865e-01, 3.8241e-01, 8.9425e-03],\n",
       "          [6.2736e-01, 3.7229e-01, 3.4616e-04],\n",
       "          [4.9065e-01, 5.0435e-01, 5.0075e-03],\n",
       "          [7.3218e-01, 2.5989e-01, 7.9352e-03],\n",
       "          [5.7932e-01, 4.1478e-01, 5.9078e-03],\n",
       "          [7.1597e-01, 2.8099e-01, 3.0322e-03],\n",
       "          [3.2543e-01, 6.7186e-01, 2.7083e-03],\n",
       "          [4.8489e-01, 5.1129e-01, 3.8243e-03],\n",
       "          [5.9658e-01, 3.9778e-01, 5.6438e-03],\n",
       "          [5.3852e-01, 4.5688e-01, 4.5990e-03],\n",
       "          [4.1872e-01, 5.7554e-01, 5.7425e-03],\n",
       "          [9.1693e-01, 8.2191e-02, 8.7498e-04],\n",
       "          [5.6424e-01, 4.2858e-01, 7.1865e-03],\n",
       "          [5.3183e-01, 4.6436e-01, 3.8057e-03],\n",
       "          [3.2847e-01, 6.6821e-01, 3.3216e-03],\n",
       "          [4.4230e-01, 5.5274e-01, 4.9600e-03],\n",
       "          [4.3369e-01, 5.6384e-01, 2.4630e-03],\n",
       "          [4.5820e-01, 5.3674e-01, 5.0614e-03],\n",
       "          [7.9216e-01, 2.0536e-01, 2.4822e-03],\n",
       "          [8.4953e-01, 1.4738e-01, 3.0987e-03],\n",
       "          [6.3506e-01, 3.5754e-01, 7.4040e-03],\n",
       "          [5.6241e-01, 4.3119e-01, 6.4066e-03],\n",
       "          [5.7185e-01, 4.2121e-01, 6.9367e-03],\n",
       "          [4.3047e-01, 5.6577e-01, 3.7631e-03],\n",
       "          [3.4954e-01, 6.4876e-01, 1.7008e-03],\n",
       "          [7.1291e-01, 2.8672e-01, 3.6829e-04],\n",
       "          [5.9484e-01, 4.0388e-01, 1.2812e-03],\n",
       "          [3.8816e-01, 6.0914e-01, 2.7040e-03],\n",
       "          [5.9625e-01, 4.0156e-01, 2.1817e-03],\n",
       "          [4.9755e-01, 4.9805e-01, 4.3957e-03],\n",
       "          [6.9719e-01, 2.9815e-01, 4.6611e-03],\n",
       "          [4.8821e-01, 5.0402e-01, 7.7687e-03],\n",
       "          [4.0274e-01, 5.9123e-01, 6.0243e-03],\n",
       "          [7.3136e-01, 2.6456e-01, 4.0877e-03],\n",
       "          [6.7859e-01, 3.1306e-01, 8.3519e-03],\n",
       "          [5.5600e-01, 4.3797e-01, 6.0218e-03],\n",
       "          [3.5783e-01, 6.3609e-01, 6.0825e-03],\n",
       "          [8.2095e-01, 1.7429e-01, 4.7593e-03],\n",
       "          [4.6750e-01, 5.2771e-01, 4.7835e-03],\n",
       "          [6.0760e-01, 3.8632e-01, 6.0793e-03],\n",
       "          [3.4829e-01, 6.5171e-01, 3.9677e-06],\n",
       "          [6.6706e-01, 3.3180e-01, 1.1455e-03],\n",
       "          [5.6630e-01, 4.2889e-01, 4.8165e-03],\n",
       "          [8.1956e-01, 1.7784e-01, 2.6014e-03],\n",
       "          [7.0691e-01, 2.8799e-01, 5.1023e-03]], grad_fn=<CopySlices>),\n",
       "  'pd2': tensor([[9.9999e-01, 5.7998e-06, 5.7962e-06],\n",
       "          [1.0000e+00, 1.4080e-10, 1.4079e-10],\n",
       "          [1.0000e+00, 2.0708e-06, 2.0695e-06],\n",
       "          [9.9982e-01, 8.8775e-05, 8.8438e-05],\n",
       "          [9.9975e-01, 1.2461e-04, 1.2419e-04],\n",
       "          [9.9991e-01, 4.6495e-05, 4.6428e-05],\n",
       "          [1.0000e+00, 8.6569e-07, 8.6544e-07],\n",
       "          [9.9994e-01, 2.7739e-05, 2.7693e-05],\n",
       "          [9.9994e-01, 2.8346e-05, 2.8303e-05],\n",
       "          [9.9972e-01, 1.3816e-04, 1.3766e-04],\n",
       "          [9.9982e-01, 8.9677e-05, 8.9381e-05],\n",
       "          [1.0000e+00, 6.4970e-07, 6.4953e-07],\n",
       "          [9.9980e-01, 9.9939e-05, 9.9683e-05],\n",
       "          [9.9993e-01, 3.3193e-05, 3.3138e-05],\n",
       "          [9.9975e-01, 1.2476e-04, 1.2427e-04],\n",
       "          [9.9982e-01, 9.0316e-05, 9.0082e-05],\n",
       "          [1.0000e+00, 3.1827e-08, 3.1826e-08],\n",
       "          [9.9978e-01, 1.1079e-04, 1.1055e-04],\n",
       "          [9.9988e-01, 6.0030e-05, 5.9904e-05],\n",
       "          [9.9995e-01, 2.3462e-05, 2.3422e-05],\n",
       "          [9.9976e-01, 1.2265e-04, 1.2224e-04],\n",
       "          [9.9986e-01, 7.2289e-05, 7.2129e-05],\n",
       "          [9.9970e-01, 1.4823e-04, 1.4761e-04],\n",
       "          [1.0000e+00, 1.1380e-06, 1.1374e-06],\n",
       "          [1.0000e+00, 3.1105e-11, 3.1105e-11],\n",
       "          [9.9992e-01, 3.9191e-05, 3.9108e-05],\n",
       "          [1.0000e+00, 3.2351e-11, 3.2351e-11],\n",
       "          [1.0000e+00, 1.0548e-06, 1.0544e-06],\n",
       "          [9.9983e-01, 8.6133e-05, 8.5890e-05],\n",
       "          [9.9986e-01, 6.9005e-05, 6.8862e-05],\n",
       "          [9.9990e-01, 5.1151e-05, 5.1063e-05],\n",
       "          [9.9933e-01, 3.3647e-04, 3.3460e-04],\n",
       "          [9.9985e-01, 7.6476e-05, 7.6305e-05],\n",
       "          [9.9981e-01, 9.6395e-05, 9.6077e-05],\n",
       "          [9.9996e-01, 1.9622e-05, 1.9603e-05],\n",
       "          [9.9998e-01, 9.7160e-06, 9.7058e-06],\n",
       "          [9.9975e-01, 1.2345e-04, 1.2304e-04],\n",
       "          [9.9987e-01, 6.6085e-05, 6.5891e-05],\n",
       "          [9.9995e-01, 2.2546e-05, 2.2498e-05],\n",
       "          [9.9989e-01, 5.2534e-05, 5.2428e-05],\n",
       "          [9.9957e-01, 2.1330e-04, 2.1249e-04],\n",
       "          [1.0000e+00, 6.0589e-10, 6.0588e-10],\n",
       "          [9.9988e-01, 5.9820e-05, 5.9734e-05],\n",
       "          [1.0000e+00, 4.9776e-07, 4.9761e-07],\n",
       "          [9.9991e-01, 4.3273e-05, 4.3208e-05],\n",
       "          [1.0000e+00, 1.2282e-06, 1.2277e-06],\n",
       "          [9.9987e-01, 6.3489e-05, 6.3318e-05],\n",
       "          [9.9998e-01, 7.7299e-06, 7.7239e-06],\n",
       "          [9.9971e-01, 1.4545e-04, 1.4499e-04],\n",
       "          [9.9982e-01, 8.7859e-05, 8.7612e-05],\n",
       "          [1.0000e+00, 2.0387e-06, 2.0384e-06],\n",
       "          [9.9931e-01, 3.4781e-04, 3.4581e-04],\n",
       "          [9.9990e-01, 4.9938e-05, 4.9814e-05],\n",
       "          [9.9967e-01, 1.6302e-04, 1.6240e-04],\n",
       "          [9.9986e-01, 7.0104e-05, 6.9960e-05],\n",
       "          [9.9949e-01, 2.5374e-04, 2.5224e-04],\n",
       "          [9.9984e-01, 8.2024e-05, 8.1832e-05],\n",
       "          [9.9979e-01, 1.0736e-04, 1.0705e-04],\n",
       "          [1.0000e+00, 1.2676e-10, 1.2676e-10],\n",
       "          [9.9971e-01, 1.4711e-04, 1.4666e-04],\n",
       "          [9.9986e-01, 7.1602e-05, 7.1439e-05],\n",
       "          [9.9982e-01, 8.7961e-05, 8.7719e-05],\n",
       "          [9.9979e-01, 1.0596e-04, 1.0554e-04],\n",
       "          [9.9992e-01, 3.8834e-05, 3.8771e-05],\n",
       "          [1.0000e+00, 5.8329e-11, 5.8329e-11],\n",
       "          [9.9993e-01, 3.4197e-05, 3.4140e-05],\n",
       "          [9.9981e-01, 9.5464e-05, 9.5183e-05],\n",
       "          [9.9979e-01, 1.0525e-04, 1.0486e-04],\n",
       "          [9.9968e-01, 1.6012e-04, 1.5970e-04],\n",
       "          [9.9982e-01, 9.1952e-05, 9.1631e-05],\n",
       "          [9.9981e-01, 9.2923e-05, 9.2606e-05],\n",
       "          [9.9963e-01, 1.8378e-04, 1.8305e-04],\n",
       "          [9.9985e-01, 7.3491e-05, 7.3333e-05],\n",
       "          [9.9991e-01, 4.4616e-05, 4.4519e-05],\n",
       "          [9.9997e-01, 1.6043e-05, 1.6013e-05],\n",
       "          [9.9996e-01, 1.9200e-05, 1.9171e-05],\n",
       "          [9.9999e-01, 2.6160e-06, 2.6143e-06],\n",
       "          [9.9966e-01, 1.7008e-04, 1.6936e-04],\n",
       "          [9.9998e-01, 8.8458e-06, 8.8380e-06],\n",
       "          [9.9995e-01, 2.2980e-05, 2.2946e-05],\n",
       "          [9.9988e-01, 5.7720e-05, 5.7583e-05],\n",
       "          [9.9984e-01, 7.9091e-05, 7.8814e-05],\n",
       "          [9.9981e-01, 9.5267e-05, 9.4941e-05],\n",
       "          [9.9996e-01, 2.0785e-05, 2.0751e-05],\n",
       "          [1.0000e+00, 1.2035e-06, 1.2029e-06],\n",
       "          [9.9994e-01, 3.0197e-05, 3.0145e-05],\n",
       "          [9.9993e-01, 3.7042e-05, 3.6978e-05],\n",
       "          [9.9999e-01, 4.9261e-06, 4.9228e-06],\n",
       "          [9.9994e-01, 2.8812e-05, 2.8776e-05],\n",
       "          [9.9998e-01, 7.6301e-06, 7.6211e-06],\n",
       "          [9.9990e-01, 4.8441e-05, 4.8329e-05],\n",
       "          [9.9994e-01, 2.8852e-05, 2.8803e-05],\n",
       "          [9.9966e-01, 1.7099e-04, 1.7013e-04],\n",
       "          [9.9994e-01, 3.1368e-05, 3.1323e-05],\n",
       "          [1.0000e+00, 5.7585e-07, 5.7575e-07],\n",
       "          [9.9996e-01, 1.8012e-05, 1.7982e-05],\n",
       "          [9.9965e-01, 1.7292e-04, 1.7216e-04],\n",
       "          [1.0000e+00, 5.9871e-07, 5.9865e-07],\n",
       "          [9.9986e-01, 7.0234e-05, 7.0080e-05],\n",
       "          [9.9995e-01, 2.3075e-05, 2.3034e-05],\n",
       "          [9.9990e-01, 5.1027e-05, 5.0922e-05],\n",
       "          [9.9970e-01, 1.4852e-04, 1.4801e-04],\n",
       "          [9.9934e-01, 3.3028e-04, 3.2844e-04],\n",
       "          [1.0000e+00, 5.0630e-07, 5.0624e-07],\n",
       "          [9.9981e-01, 9.6746e-05, 9.6529e-05],\n",
       "          [9.9939e-01, 3.0429e-04, 3.0285e-04],\n",
       "          [9.9977e-01, 1.1586e-04, 1.1559e-04],\n",
       "          [9.9992e-01, 3.8266e-05, 3.8206e-05],\n",
       "          [9.9993e-01, 3.2773e-05, 3.2712e-05],\n",
       "          [9.9988e-01, 5.9533e-05, 5.9424e-05],\n",
       "          [9.9974e-01, 1.3158e-04, 1.3101e-04],\n",
       "          [9.9983e-01, 8.7350e-05, 8.7140e-05],\n",
       "          [9.9975e-01, 1.2513e-04, 1.2477e-04],\n",
       "          [9.9998e-01, 9.9374e-06, 9.9216e-06],\n",
       "          [9.9966e-01, 1.6808e-04, 1.6744e-04],\n",
       "          [9.9988e-01, 5.9401e-05, 5.9265e-05],\n",
       "          [9.9989e-01, 5.4942e-05, 5.4824e-05],\n",
       "          [9.9982e-01, 9.0228e-05, 9.0015e-05],\n",
       "          [9.9995e-01, 2.4280e-05, 2.4234e-05],\n",
       "          [9.9980e-01, 9.9274e-05, 9.8953e-05],\n",
       "          [9.9993e-01, 3.6431e-05, 3.6337e-05],\n",
       "          [9.9986e-01, 7.1074e-05, 7.0897e-05],\n",
       "          [9.9954e-01, 2.3048e-04, 2.2939e-04],\n",
       "          [9.9966e-01, 1.7254e-04, 1.7197e-04],\n",
       "          [9.9960e-01, 2.0152e-04, 2.0084e-04],\n",
       "          [9.9989e-01, 5.2935e-05, 5.2842e-05],\n",
       "          [9.9997e-01, 1.4245e-05, 1.4223e-05],\n",
       "          [1.0000e+00, 6.2075e-07, 6.2061e-07],\n",
       "          [9.9998e-01, 7.4965e-06, 7.4887e-06],\n",
       "          [9.9994e-01, 2.9912e-05, 2.9867e-05],\n",
       "          [9.9995e-01, 2.4191e-05, 2.4159e-05],\n",
       "          [9.9986e-01, 7.1765e-05, 7.1600e-05],\n",
       "          [9.9978e-01, 1.1043e-04, 1.1003e-04],\n",
       "          [9.9952e-01, 2.4177e-04, 2.4057e-04],\n",
       "          [9.9970e-01, 1.5243e-04, 1.5191e-04],\n",
       "          [9.9981e-01, 9.6199e-05, 9.5847e-05],\n",
       "          [9.9946e-01, 2.7209e-04, 2.7078e-04],\n",
       "          [9.9974e-01, 1.2925e-04, 1.2880e-04],\n",
       "          [9.9970e-01, 1.5124e-04, 1.5069e-04],\n",
       "          [9.9971e-01, 1.4348e-04, 1.4296e-04],\n",
       "          [9.9985e-01, 7.5073e-05, 7.4775e-05],\n",
       "          [9.9970e-01, 1.5119e-04, 1.5046e-04],\n",
       "          [1.0000e+00, 6.6300e-11, 6.6300e-11],\n",
       "          [9.9999e-01, 6.9663e-06, 6.9597e-06],\n",
       "          [9.9983e-01, 8.7422e-05, 8.7165e-05],\n",
       "          [9.9991e-01, 4.5204e-05, 4.5103e-05],\n",
       "          [9.9977e-01, 1.1747e-04, 1.1717e-04]], grad_fn=<CopySlices>),\n",
       "  'mod': tensor([[1.0000e+00, 4.9459e-33, 4.9459e-33, 4.9459e-33, 4.9459e-33, 4.9459e-33],\n",
       "          [1.0000e+00, 4.0775e-20, 4.0775e-20, 4.0775e-20, 4.0775e-20, 4.0775e-20],\n",
       "          [1.0000e+00, 7.5609e-20, 7.5609e-20, 7.5609e-20, 7.5609e-20, 7.5609e-20],\n",
       "          [1.0000e+00, 5.1180e-20, 5.1180e-20, 5.1180e-20, 5.1180e-20, 5.1180e-20],\n",
       "          [1.0000e+00, 5.4615e-20, 5.4615e-20, 5.4615e-20, 5.4615e-20, 5.4615e-20],\n",
       "          [1.0000e+00, 7.2504e-20, 7.2504e-20, 7.2504e-20, 7.2504e-20, 7.2504e-20],\n",
       "          [1.0000e+00, 4.7301e-20, 4.7301e-20, 4.7301e-20, 4.7301e-20, 4.7301e-20],\n",
       "          [1.0000e+00, 6.3521e-20, 6.3521e-20, 6.3521e-20, 6.3521e-20, 6.3521e-20],\n",
       "          [1.0000e+00, 7.2699e-20, 7.2699e-20, 7.2699e-20, 7.2699e-20, 7.2699e-20],\n",
       "          [1.0000e+00, 8.0317e-20, 8.0317e-20, 8.0317e-20, 8.0317e-20, 8.0317e-20],\n",
       "          [1.0000e+00, 7.3892e-20, 7.3892e-20, 7.3892e-20, 7.3892e-20, 7.3892e-20],\n",
       "          [1.0000e+00, 7.8688e-20, 7.8688e-20, 7.8688e-20, 7.8688e-20, 7.8688e-20],\n",
       "          [1.0000e+00, 7.9696e-20, 7.9696e-20, 7.9696e-20, 7.9696e-20, 7.9696e-20],\n",
       "          [1.0000e+00, 6.4759e-20, 6.4759e-20, 6.4759e-20, 6.4759e-20, 6.4759e-20],\n",
       "          [1.0000e+00, 4.8488e-20, 4.8488e-20, 4.8488e-20, 4.8488e-20, 4.8488e-20],\n",
       "          [1.0000e+00, 1.0459e-19, 1.0459e-19, 1.0459e-19, 1.0459e-19, 1.0459e-19],\n",
       "          [1.0000e+00, 5.0035e-20, 5.0035e-20, 5.0035e-20, 5.0035e-20, 5.0035e-20],\n",
       "          [1.0000e+00, 6.0005e-20, 6.0005e-20, 6.0005e-20, 6.0005e-20, 6.0005e-20],\n",
       "          [1.0000e+00, 7.8837e-20, 7.8837e-20, 7.8837e-20, 7.8837e-20, 7.8837e-20],\n",
       "          [1.0000e+00, 8.3055e-20, 8.3055e-20, 8.3055e-20, 8.3055e-20, 8.3055e-20],\n",
       "          [1.0000e+00, 6.6357e-20, 6.6357e-20, 6.6357e-20, 6.6357e-20, 6.6357e-20],\n",
       "          [1.0000e+00, 1.0588e-19, 1.0588e-19, 1.0588e-19, 1.0588e-19, 1.0588e-19],\n",
       "          [1.0000e+00, 5.7578e-20, 5.7578e-20, 5.7578e-20, 5.7578e-20, 5.7578e-20],\n",
       "          [1.0000e+00, 3.6225e-20, 3.6225e-20, 3.6225e-20, 3.6225e-20, 3.6225e-20],\n",
       "          [1.0000e+00, 1.7362e-31, 1.7362e-31, 1.7362e-31, 1.7362e-31, 1.7362e-31],\n",
       "          [1.0000e+00, 1.1973e-19, 1.1973e-19, 1.1973e-19, 1.1973e-19, 1.1973e-19],\n",
       "          [1.0000e+00, 2.5357e-31, 2.5357e-31, 2.5357e-31, 2.5357e-31, 2.5357e-31],\n",
       "          [1.0000e+00, 7.9566e-20, 7.9566e-20, 7.9566e-20, 7.9566e-20, 7.9566e-20],\n",
       "          [1.0000e+00, 6.5419e-20, 6.5419e-20, 6.5419e-20, 6.5419e-20, 6.5419e-20],\n",
       "          [1.0000e+00, 7.1364e-20, 7.1364e-20, 7.1364e-20, 7.1364e-20, 7.1364e-20],\n",
       "          [1.0000e+00, 5.6633e-20, 5.6633e-20, 5.6633e-20, 5.6633e-20, 5.6633e-20],\n",
       "          [1.0000e+00, 6.4062e-20, 6.4062e-20, 6.4062e-20, 6.4062e-20, 6.4062e-20],\n",
       "          [1.0000e+00, 7.7718e-20, 7.7718e-20, 7.7718e-20, 7.7718e-20, 7.7718e-20],\n",
       "          [1.0000e+00, 5.0149e-20, 5.0149e-20, 5.0149e-20, 5.0149e-20, 5.0149e-20],\n",
       "          [9.5510e-01, 8.8384e-03, 8.5235e-03, 9.1952e-03, 8.5933e-03, 9.7463e-03],\n",
       "          [1.0000e+00, 4.4786e-33, 4.4786e-33, 4.4786e-33, 4.4786e-33, 4.4786e-33],\n",
       "          [1.0000e+00, 5.2528e-20, 5.2528e-20, 5.2528e-20, 5.2528e-20, 5.2528e-20],\n",
       "          [1.0000e+00, 5.6898e-20, 5.6898e-20, 5.6898e-20, 5.6898e-20, 5.6898e-20],\n",
       "          [1.0000e+00, 8.8033e-20, 8.8033e-20, 8.8033e-20, 8.8033e-20, 8.8033e-20],\n",
       "          [1.0000e+00, 6.9578e-20, 6.9578e-20, 6.9578e-20, 6.9578e-20, 6.9578e-20],\n",
       "          [1.0000e+00, 5.7103e-20, 5.7103e-20, 5.7103e-20, 5.7103e-20, 5.7103e-20],\n",
       "          [1.0000e+00, 2.6798e-20, 2.6798e-20, 2.6798e-20, 2.6798e-20, 2.6798e-20],\n",
       "          [9.4682e-01, 1.0687e-02, 1.0239e-02, 1.0945e-02, 1.0322e-02, 1.0984e-02],\n",
       "          [1.0000e+00, 1.2000e-19, 1.2000e-19, 1.2000e-19, 1.2000e-19, 1.2000e-19],\n",
       "          [1.0000e+00, 7.7077e-20, 7.7077e-20, 7.7077e-20, 7.7077e-20, 7.7077e-20],\n",
       "          [1.0000e+00, 6.3328e-20, 6.3328e-20, 6.3328e-20, 6.3328e-20, 6.3328e-20],\n",
       "          [1.0000e+00, 5.4383e-20, 5.4383e-20, 5.4383e-20, 5.4383e-20, 5.4383e-20],\n",
       "          [1.0000e+00, 1.1220e-19, 1.1220e-19, 1.1220e-19, 1.1220e-19, 1.1220e-19],\n",
       "          [1.0000e+00, 5.8553e-20, 5.8553e-20, 5.8553e-20, 5.8553e-20, 5.8553e-20],\n",
       "          [1.0000e+00, 6.7004e-20, 6.7004e-20, 6.7004e-20, 6.7004e-20, 6.7004e-20],\n",
       "          [9.7549e-01, 4.8171e-03, 4.7481e-03, 5.0368e-03, 4.7613e-03, 5.1456e-03],\n",
       "          [1.0000e+00, 5.9389e-20, 5.9389e-20, 5.9389e-20, 5.9389e-20, 5.9389e-20],\n",
       "          [1.0000e+00, 6.7018e-20, 6.7018e-20, 6.7018e-20, 6.7018e-20, 6.7018e-20],\n",
       "          [1.0000e+00, 7.3915e-20, 7.3915e-20, 7.3915e-20, 7.3915e-20, 7.3915e-20],\n",
       "          [1.0000e+00, 7.1188e-20, 7.1188e-20, 7.1188e-20, 7.1188e-20, 7.1188e-20],\n",
       "          [1.0000e+00, 6.5447e-20, 6.5447e-20, 6.5447e-20, 6.5447e-20, 6.5447e-20],\n",
       "          [1.0000e+00, 4.1354e-20, 4.1354e-20, 4.1354e-20, 4.1354e-20, 4.1354e-20],\n",
       "          [1.0000e+00, 8.0765e-20, 8.0765e-20, 8.0765e-20, 8.0765e-20, 8.0765e-20],\n",
       "          [1.0000e+00, 1.7219e-20, 1.7219e-20, 1.7219e-20, 1.7219e-20, 1.7219e-20],\n",
       "          [1.0000e+00, 5.9966e-20, 5.9966e-20, 5.9966e-20, 5.9966e-20, 5.9966e-20],\n",
       "          [1.0000e+00, 7.5547e-20, 7.5547e-20, 7.5547e-20, 7.5547e-20, 7.5547e-20],\n",
       "          [1.0000e+00, 6.0996e-20, 6.0996e-20, 6.0996e-20, 6.0996e-20, 6.0996e-20],\n",
       "          [1.0000e+00, 5.5181e-20, 5.5181e-20, 5.5181e-20, 5.5181e-20, 5.5181e-20],\n",
       "          [1.0000e+00, 6.9833e-20, 6.9833e-20, 6.9833e-20, 6.9833e-20, 6.9833e-20],\n",
       "          [1.0000e+00, 1.0938e-28, 1.0938e-28, 1.0938e-28, 1.0938e-28, 1.0938e-28],\n",
       "          [1.0000e+00, 6.6389e-20, 6.6389e-20, 6.6389e-20, 6.6389e-20, 6.6389e-20],\n",
       "          [1.0000e+00, 5.2015e-20, 5.2015e-20, 5.2015e-20, 5.2015e-20, 5.2015e-20],\n",
       "          [1.0000e+00, 5.1578e-20, 5.1578e-20, 5.1578e-20, 5.1578e-20, 5.1578e-20],\n",
       "          [1.0000e+00, 5.7613e-20, 5.7613e-20, 5.7613e-20, 5.7613e-20, 5.7613e-20],\n",
       "          [1.0000e+00, 5.9429e-20, 5.9429e-20, 5.9429e-20, 5.9429e-20, 5.9429e-20],\n",
       "          [1.0000e+00, 6.1614e-20, 6.1614e-20, 6.1614e-20, 6.1614e-20, 6.1614e-20],\n",
       "          [1.0000e+00, 7.8165e-20, 7.8165e-20, 7.8165e-20, 7.8165e-20, 7.8165e-20],\n",
       "          [1.0000e+00, 7.6834e-20, 7.6834e-20, 7.6834e-20, 7.6834e-20, 7.6834e-20],\n",
       "          [1.0000e+00, 9.7821e-20, 9.7821e-20, 9.7821e-20, 9.7821e-20, 9.7821e-20],\n",
       "          [1.0000e+00, 8.2234e-20, 8.2234e-20, 8.2234e-20, 8.2234e-20, 8.2234e-20],\n",
       "          [1.0000e+00, 8.0274e-20, 8.0274e-20, 8.0274e-20, 8.0274e-20, 8.0274e-20],\n",
       "          [1.0000e+00, 5.6654e-20, 5.6654e-20, 5.6654e-20, 5.6654e-20, 5.6654e-20],\n",
       "          [1.0000e+00, 7.1351e-20, 7.1351e-20, 7.1351e-20, 7.1351e-20, 7.1351e-20],\n",
       "          [1.0000e+00, 4.8314e-20, 4.8314e-20, 4.8314e-20, 4.8314e-20, 4.8314e-20],\n",
       "          [1.0000e+00, 7.0362e-20, 7.0362e-20, 7.0362e-20, 7.0362e-20, 7.0362e-20],\n",
       "          [1.0000e+00, 7.4385e-20, 7.4385e-20, 7.4385e-20, 7.4385e-20, 7.4385e-20],\n",
       "          [1.0000e+00, 8.7072e-20, 8.7072e-20, 8.7072e-20, 8.7072e-20, 8.7072e-20],\n",
       "          [1.0000e+00, 6.4010e-20, 6.4010e-20, 6.4010e-20, 6.4010e-20, 6.4010e-20],\n",
       "          [1.0000e+00, 7.7176e-20, 7.7176e-20, 7.7176e-20, 7.7176e-20, 7.7176e-20],\n",
       "          [1.0000e+00, 5.0524e-20, 5.0524e-20, 5.0524e-20, 5.0524e-20, 5.0524e-20],\n",
       "          [1.0000e+00, 7.2736e-20, 7.2736e-20, 7.2736e-20, 7.2736e-20, 7.2736e-20],\n",
       "          [1.0000e+00, 6.8235e-20, 6.8235e-20, 6.8235e-20, 6.8235e-20, 6.8235e-20],\n",
       "          [1.0000e+00, 1.1546e-19, 1.1546e-19, 1.1546e-19, 1.1546e-19, 1.1546e-19],\n",
       "          [1.0000e+00, 7.1672e-20, 7.1672e-20, 7.1672e-20, 7.1672e-20, 7.1672e-20],\n",
       "          [1.0000e+00, 3.9421e-33, 3.9421e-33, 3.9421e-33, 3.9421e-33, 3.9421e-33],\n",
       "          [1.0000e+00, 7.5387e-20, 7.5387e-20, 7.5387e-20, 7.5387e-20, 7.5387e-20],\n",
       "          [1.0000e+00, 7.2823e-20, 7.2823e-20, 7.2823e-20, 7.2823e-20, 7.2823e-20],\n",
       "          [1.0000e+00, 5.2530e-20, 5.2530e-20, 5.2530e-20, 5.2530e-20, 5.2530e-20],\n",
       "          [1.0000e+00, 7.7864e-20, 7.7864e-20, 7.7864e-20, 7.7864e-20, 7.7864e-20],\n",
       "          [1.0000e+00, 8.6859e-20, 8.6859e-20, 8.6859e-20, 8.6859e-20, 8.6859e-20],\n",
       "          [1.0000e+00, 1.1444e-19, 1.1444e-19, 1.1444e-19, 1.1444e-19, 1.1444e-19],\n",
       "          [1.0000e+00, 4.5012e-20, 4.5012e-20, 4.5012e-20, 4.5012e-20, 4.5012e-20],\n",
       "          [9.8970e-01, 2.0594e-03, 2.0129e-03, 2.0868e-03, 2.0197e-03, 2.1171e-03],\n",
       "          [1.0000e+00, 7.8285e-20, 7.8285e-20, 7.8285e-20, 7.8285e-20, 7.8285e-20],\n",
       "          [9.8075e-01, 3.8936e-03, 3.7138e-03, 3.9278e-03, 3.7685e-03, 3.9434e-03],\n",
       "          [1.0000e+00, 7.4711e-20, 7.4711e-20, 7.4711e-20, 7.4711e-20, 7.4711e-20],\n",
       "          [1.0000e+00, 5.2102e-20, 5.2102e-20, 5.2102e-20, 5.2102e-20, 5.2102e-20],\n",
       "          [1.0000e+00, 6.4362e-20, 6.4362e-20, 6.4362e-20, 6.4362e-20, 6.4362e-20],\n",
       "          [1.0000e+00, 9.4449e-20, 9.4449e-20, 9.4449e-20, 9.4449e-20, 9.4449e-20],\n",
       "          [1.0000e+00, 7.0755e-20, 7.0755e-20, 7.0755e-20, 7.0755e-20, 7.0755e-20],\n",
       "          [1.0000e+00, 5.6610e-20, 5.6610e-20, 5.6610e-20, 5.6610e-20, 5.6610e-20],\n",
       "          [1.0000e+00, 6.0948e-20, 6.0948e-20, 6.0948e-20, 6.0948e-20, 6.0948e-20],\n",
       "          [1.0000e+00, 7.3550e-20, 7.3550e-20, 7.3550e-20, 7.3550e-20, 7.3550e-20],\n",
       "          [1.0000e+00, 6.5177e-20, 6.5177e-20, 6.5177e-20, 6.5177e-20, 6.5177e-20],\n",
       "          [1.0000e+00, 7.4212e-20, 7.4212e-20, 7.4212e-20, 7.4212e-20, 7.4212e-20],\n",
       "          [9.4793e-01, 1.0531e-02, 9.9250e-03, 1.0675e-02, 1.0081e-02, 1.0861e-02],\n",
       "          [1.0000e+00, 7.4166e-20, 7.4166e-20, 7.4166e-20, 7.4166e-20, 7.4166e-20],\n",
       "          [1.0000e+00, 8.4188e-20, 8.4188e-20, 8.4188e-20, 8.4188e-20, 8.4188e-20],\n",
       "          [1.0000e+00, 8.6474e-20, 8.6474e-20, 8.6474e-20, 8.6474e-20, 8.6474e-20],\n",
       "          [1.0000e+00, 5.9226e-20, 5.9226e-20, 5.9226e-20, 5.9226e-20, 5.9226e-20],\n",
       "          [1.0000e+00, 6.2937e-20, 6.2937e-20, 6.2937e-20, 6.2937e-20, 6.2937e-20],\n",
       "          [1.0000e+00, 6.8871e-20, 6.8871e-20, 6.8871e-20, 6.8871e-20, 6.8871e-20],\n",
       "          [1.0000e+00, 7.8619e-20, 7.8619e-20, 7.8619e-20, 7.8619e-20, 7.8619e-20],\n",
       "          [1.0000e+00, 8.5710e-20, 8.5710e-20, 8.5710e-20, 8.5710e-20, 8.5710e-20],\n",
       "          [1.0000e+00, 5.4213e-20, 5.4213e-20, 5.4213e-20, 5.4213e-20, 5.4213e-20],\n",
       "          [1.0000e+00, 4.1280e-20, 4.1280e-20, 4.1280e-20, 4.1280e-20, 4.1280e-20],\n",
       "          [1.0000e+00, 6.2871e-20, 6.2871e-20, 6.2871e-20, 6.2871e-20, 6.2871e-20],\n",
       "          [1.0000e+00, 6.1448e-20, 6.1448e-20, 6.1448e-20, 6.1448e-20, 6.1448e-20],\n",
       "          [1.0000e+00, 7.6826e-20, 7.6826e-20, 7.6826e-20, 7.6826e-20, 7.6826e-20],\n",
       "          [1.0000e+00, 6.4511e-20, 6.4511e-20, 6.4511e-20, 6.4511e-20, 6.4511e-20],\n",
       "          [1.0000e+00, 7.9196e-20, 7.9196e-20, 7.9196e-20, 7.9196e-20, 7.9196e-20],\n",
       "          [1.0000e+00, 6.3663e-20, 6.3663e-20, 6.3663e-20, 6.3663e-20, 6.3663e-20],\n",
       "          [1.0000e+00, 7.3487e-20, 7.3487e-20, 7.3487e-20, 7.3487e-20, 7.3487e-20],\n",
       "          [1.0000e+00, 5.3573e-20, 5.3573e-20, 5.3573e-20, 5.3573e-20, 5.3573e-20],\n",
       "          [1.0000e+00, 1.0124e-19, 1.0124e-19, 1.0124e-19, 1.0124e-19, 1.0124e-19],\n",
       "          [1.0000e+00, 6.8562e-20, 6.8562e-20, 6.8562e-20, 6.8562e-20, 6.8562e-20],\n",
       "          [1.0000e+00, 6.8775e-20, 6.8775e-20, 6.8775e-20, 6.8775e-20, 6.8775e-20],\n",
       "          [1.0000e+00, 7.4660e-20, 7.4660e-20, 7.4660e-20, 7.4660e-20, 7.4660e-20],\n",
       "          [1.0000e+00, 5.7047e-20, 5.7047e-20, 5.7047e-20, 5.7047e-20, 5.7047e-20],\n",
       "          [1.0000e+00, 8.8111e-20, 8.8111e-20, 8.8111e-20, 8.8111e-20, 8.8111e-20],\n",
       "          [1.0000e+00, 7.0286e-20, 7.0286e-20, 7.0286e-20, 7.0286e-20, 7.0286e-20],\n",
       "          [1.0000e+00, 5.2553e-20, 5.2553e-20, 5.2553e-20, 5.2553e-20, 5.2553e-20],\n",
       "          [1.0000e+00, 5.4283e-20, 5.4283e-20, 5.4283e-20, 5.4283e-20, 5.4283e-20],\n",
       "          [1.0000e+00, 6.8654e-20, 6.8654e-20, 6.8654e-20, 6.8654e-20, 6.8654e-20],\n",
       "          [1.0000e+00, 5.3484e-20, 5.3484e-20, 5.3484e-20, 5.3484e-20, 5.3484e-20],\n",
       "          [1.0000e+00, 6.5502e-20, 6.5502e-20, 6.5502e-20, 6.5502e-20, 6.5502e-20],\n",
       "          [1.0000e+00, 5.9238e-20, 5.9238e-20, 5.9238e-20, 5.9238e-20, 5.9238e-20],\n",
       "          [9.9717e-01, 5.6427e-04, 5.5209e-04, 5.7575e-04, 5.5514e-04, 5.7986e-04],\n",
       "          [1.0000e+00, 5.7185e-20, 5.7185e-20, 5.7185e-20, 5.7185e-20, 5.7185e-20],\n",
       "          [1.0000e+00, 6.1799e-20, 6.1799e-20, 6.1799e-20, 6.1799e-20, 6.1799e-20],\n",
       "          [1.0000e+00, 5.7142e-20, 5.7142e-20, 5.7142e-20, 5.7142e-20, 5.7142e-20],\n",
       "          [1.0000e+00, 4.7676e-20, 4.7676e-20, 4.7676e-20, 4.7676e-20, 4.7676e-20]],\n",
       "         grad_fn=<CopySlices>),\n",
       "  'cc': tensor([[1.0623e-02, 9.7697e-01, 6.4750e-03, 5.9316e-03],\n",
       "          [6.8736e-05, 9.9983e-01, 5.0966e-05, 5.0665e-05],\n",
       "          [4.0725e-03, 9.9035e-01, 2.8647e-03, 2.7098e-03],\n",
       "          [2.2465e-02, 9.4729e-01, 1.6000e-02, 1.4241e-02],\n",
       "          [3.4862e-02, 9.2160e-01, 2.3117e-02, 2.0417e-02],\n",
       "          [2.7969e-02, 9.3696e-01, 1.8754e-02, 1.6320e-02],\n",
       "          [4.2248e-03, 9.9011e-01, 2.8822e-03, 2.7837e-03],\n",
       "          [2.6046e-02, 9.4137e-01, 1.7195e-02, 1.5384e-02],\n",
       "          [2.4766e-02, 9.4477e-01, 1.6244e-02, 1.4222e-02],\n",
       "          [4.3505e-02, 9.0112e-01, 2.9856e-02, 2.5519e-02],\n",
       "          [3.6962e-02, 9.1540e-01, 2.5479e-02, 2.2156e-02],\n",
       "          [4.2803e-03, 9.8993e-01, 2.9682e-03, 2.8243e-03],\n",
       "          [3.9700e-02, 9.1045e-01, 2.6715e-02, 2.3131e-02],\n",
       "          [2.7175e-02, 9.3885e-01, 1.8005e-02, 1.5971e-02],\n",
       "          [2.5638e-02, 9.3880e-01, 1.8692e-02, 1.6868e-02],\n",
       "          [3.7051e-02, 9.1538e-01, 2.5618e-02, 2.1951e-02],\n",
       "          [7.5734e-04, 9.9820e-01, 5.2506e-04, 5.1442e-04],\n",
       "          [3.5094e-02, 9.2363e-01, 2.1862e-02, 1.9418e-02],\n",
       "          [3.5838e-02, 9.1941e-01, 2.3978e-02, 2.0778e-02],\n",
       "          [1.8748e-02, 9.5847e-01, 1.1973e-02, 1.0806e-02],\n",
       "          [2.9476e-02, 9.3371e-01, 1.9421e-02, 1.7389e-02],\n",
       "          [3.5605e-02, 9.1875e-01, 2.4575e-02, 2.1067e-02],\n",
       "          [3.0344e-02, 9.2822e-01, 2.1993e-02, 1.9443e-02],\n",
       "          [4.5187e-03, 9.8955e-01, 3.0337e-03, 2.9002e-03],\n",
       "          [5.8424e-05, 9.9985e-01, 4.5569e-05, 4.5171e-05],\n",
       "          [2.1973e-02, 9.4896e-01, 1.5513e-02, 1.3558e-02],\n",
       "          [6.3575e-05, 9.9984e-01, 4.9569e-05, 4.9106e-05],\n",
       "          [2.8961e-03, 9.9289e-01, 2.1540e-03, 2.0580e-03],\n",
       "          [3.3802e-02, 9.2394e-01, 2.2518e-02, 1.9745e-02],\n",
       "          [3.2010e-02, 9.2750e-01, 2.1704e-02, 1.8786e-02],\n",
       "          [3.3552e-02, 9.2612e-01, 2.1420e-02, 1.8910e-02],\n",
       "          [4.9562e-02, 8.8732e-01, 3.3987e-02, 2.9126e-02],\n",
       "          [3.9756e-02, 9.1114e-01, 2.6365e-02, 2.2743e-02],\n",
       "          [2.7228e-02, 9.3685e-01, 1.8961e-02, 1.6958e-02],\n",
       "          [2.4039e-02, 9.4913e-01, 1.4058e-02, 1.2774e-02],\n",
       "          [1.0078e-02, 9.7768e-01, 6.3847e-03, 5.8581e-03],\n",
       "          [2.7884e-02, 9.3502e-01, 1.9638e-02, 1.7455e-02],\n",
       "          [2.9441e-02, 9.3403e-01, 1.9399e-02, 1.7133e-02],\n",
       "          [1.6710e-02, 9.6335e-01, 1.0424e-02, 9.5155e-03],\n",
       "          [2.9093e-02, 9.3395e-01, 1.9723e-02, 1.7234e-02],\n",
       "          [4.4844e-02, 9.0099e-01, 2.8952e-02, 2.5215e-02],\n",
       "          [2.6487e-04, 9.9939e-01, 1.7319e-04, 1.7146e-04],\n",
       "          [1.9977e-02, 9.5440e-01, 1.3361e-02, 1.2262e-02],\n",
       "          [3.4860e-03, 9.9195e-01, 2.3357e-03, 2.2325e-03],\n",
       "          [2.9927e-02, 9.3323e-01, 1.9681e-02, 1.7162e-02],\n",
       "          [5.7957e-03, 9.8692e-01, 3.7526e-03, 3.5334e-03],\n",
       "          [2.7453e-02, 9.3843e-01, 1.7927e-02, 1.6192e-02],\n",
       "          [1.4364e-02, 9.6823e-01, 9.1590e-03, 8.2480e-03],\n",
       "          [4.2107e-02, 9.0664e-01, 2.7314e-02, 2.3943e-02],\n",
       "          [3.2345e-02, 9.2474e-01, 2.2881e-02, 2.0036e-02],\n",
       "          [5.3586e-03, 9.8847e-01, 3.1605e-03, 3.0109e-03],\n",
       "          [5.1789e-02, 8.8293e-01, 3.5128e-02, 3.0155e-02],\n",
       "          [1.9067e-02, 9.5491e-01, 1.3770e-02, 1.2249e-02],\n",
       "          [4.5461e-02, 8.9644e-01, 3.1458e-02, 2.6639e-02],\n",
       "          [3.2240e-02, 9.2710e-01, 2.1798e-02, 1.8861e-02],\n",
       "          [3.8922e-02, 9.1036e-01, 2.7084e-02, 2.3634e-02],\n",
       "          [2.7815e-02, 9.3759e-01, 1.8210e-02, 1.6384e-02],\n",
       "          [3.6447e-02, 9.1748e-01, 2.4614e-02, 2.1460e-02],\n",
       "          [9.8512e-05, 9.9977e-01, 6.7391e-05, 6.6991e-05],\n",
       "          [4.2092e-02, 9.0705e-01, 2.7091e-02, 2.3763e-02],\n",
       "          [3.9792e-02, 9.1081e-01, 2.6523e-02, 2.2878e-02],\n",
       "          [3.0126e-02, 9.3196e-01, 1.9952e-02, 1.7967e-02],\n",
       "          [2.6996e-02, 9.3575e-01, 1.9754e-02, 1.7502e-02],\n",
       "          [2.6941e-02, 9.3840e-01, 1.8365e-02, 1.6299e-02],\n",
       "          [7.9121e-05, 9.9980e-01, 5.9871e-05, 5.9451e-05],\n",
       "          [3.0951e-02, 9.3191e-01, 1.9711e-02, 1.7432e-02],\n",
       "          [3.6064e-02, 9.1810e-01, 2.4454e-02, 2.1377e-02],\n",
       "          [2.4510e-02, 9.4230e-01, 1.7432e-02, 1.5757e-02],\n",
       "          [4.1055e-02, 9.0813e-01, 2.7137e-02, 2.3675e-02],\n",
       "          [3.0156e-02, 9.3062e-01, 2.0841e-02, 1.8388e-02],\n",
       "          [2.4630e-02, 9.4320e-01, 1.6997e-02, 1.5175e-02],\n",
       "          [4.1518e-02, 9.0690e-01, 2.7631e-02, 2.3948e-02],\n",
       "          [3.9297e-02, 9.1252e-01, 2.5822e-02, 2.2356e-02],\n",
       "          [2.9741e-02, 9.3173e-01, 2.0504e-02, 1.8028e-02],\n",
       "          [1.1016e-02, 9.7428e-01, 7.6378e-03, 7.0695e-03],\n",
       "          [1.5066e-02, 9.6456e-01, 1.0635e-02, 9.7407e-03],\n",
       "          [5.6740e-03, 9.8683e-01, 3.8720e-03, 3.6284e-03],\n",
       "          [3.5833e-02, 9.1963e-01, 2.3647e-02, 2.0888e-02],\n",
       "          [1.0565e-02, 9.7543e-01, 7.2286e-03, 6.7723e-03],\n",
       "          [1.8847e-02, 9.5583e-01, 1.3337e-02, 1.1991e-02],\n",
       "          [3.0103e-02, 9.3153e-01, 2.0433e-02, 1.7932e-02],\n",
       "          [3.0323e-02, 9.2944e-01, 2.1619e-02, 1.8621e-02],\n",
       "          [3.0287e-02, 9.3211e-01, 1.9777e-02, 1.7825e-02],\n",
       "          [1.9469e-02, 9.5680e-01, 1.2447e-02, 1.1280e-02],\n",
       "          [4.1982e-03, 9.9019e-01, 2.8646e-03, 2.7425e-03],\n",
       "          [2.7201e-02, 9.3874e-01, 1.8039e-02, 1.6017e-02],\n",
       "          [1.6780e-02, 9.6001e-01, 1.2175e-02, 1.1039e-02],\n",
       "          [1.0790e-02, 9.7640e-01, 6.6622e-03, 6.1457e-03],\n",
       "          [2.8905e-02, 9.3657e-01, 1.8318e-02, 1.6209e-02],\n",
       "          [9.8255e-03, 9.7777e-01, 6.4359e-03, 5.9726e-03],\n",
       "          [2.8347e-02, 9.3456e-01, 1.9617e-02, 1.7480e-02],\n",
       "          [2.6576e-02, 9.4001e-01, 1.7699e-02, 1.5718e-02],\n",
       "          [3.2649e-02, 9.2430e-01, 2.2681e-02, 2.0372e-02],\n",
       "          [2.6333e-02, 9.4080e-01, 1.7505e-02, 1.5361e-02],\n",
       "          [3.9265e-03, 9.9141e-01, 2.3983e-03, 2.2688e-03],\n",
       "          [1.1828e-02, 9.7338e-01, 7.6863e-03, 7.1084e-03],\n",
       "          [3.4272e-02, 9.2080e-01, 2.3730e-02, 2.1194e-02],\n",
       "          [3.3704e-03, 9.9271e-01, 1.9863e-03, 1.9285e-03],\n",
       "          [3.8360e-02, 9.1402e-01, 2.5555e-02, 2.2066e-02],\n",
       "          [1.3740e-02, 9.6815e-01, 9.3371e-03, 8.7765e-03],\n",
       "          [2.3269e-02, 9.4711e-01, 1.5594e-02, 1.4028e-02],\n",
       "          [4.3647e-02, 9.0479e-01, 2.7379e-02, 2.4183e-02],\n",
       "          [4.9420e-02, 8.8744e-01, 3.4018e-02, 2.9121e-02],\n",
       "          [3.0635e-03, 9.9321e-01, 1.9143e-03, 1.8153e-03],\n",
       "          [3.9983e-02, 9.0981e-01, 2.6978e-02, 2.3232e-02],\n",
       "          [4.7378e-02, 8.9219e-01, 3.2554e-02, 2.7875e-02],\n",
       "          [3.8302e-02, 9.1594e-01, 2.4332e-02, 2.1422e-02],\n",
       "          [2.7250e-02, 9.3899e-01, 1.7972e-02, 1.5792e-02],\n",
       "          [2.3287e-02, 9.4685e-01, 1.5693e-02, 1.4164e-02],\n",
       "          [2.9182e-02, 9.3346e-01, 1.9787e-02, 1.7572e-02],\n",
       "          [3.2714e-02, 9.2658e-01, 2.1320e-02, 1.9388e-02],\n",
       "          [4.0428e-02, 9.1020e-01, 2.6458e-02, 2.2913e-02],\n",
       "          [4.3436e-02, 9.0179e-01, 2.9573e-02, 2.5198e-02],\n",
       "          [1.1330e-02, 9.7365e-01, 7.8525e-03, 7.1709e-03],\n",
       "          [4.4273e-02, 9.0036e-01, 2.9502e-02, 2.5862e-02],\n",
       "          [2.6231e-02, 9.3867e-01, 1.8576e-02, 1.6524e-02],\n",
       "          [3.5026e-02, 9.2210e-01, 2.2849e-02, 2.0020e-02],\n",
       "          [3.8492e-02, 9.1370e-01, 2.5578e-02, 2.2229e-02],\n",
       "          [1.6817e-02, 9.6216e-01, 1.0979e-02, 1.0047e-02],\n",
       "          [2.8464e-02, 9.3642e-01, 1.8503e-02, 1.6613e-02],\n",
       "          [1.8342e-02, 9.5713e-01, 1.2849e-02, 1.1678e-02],\n",
       "          [2.7341e-02, 9.3897e-01, 1.7877e-02, 1.5808e-02],\n",
       "          [4.5746e-02, 8.9580e-01, 3.1421e-02, 2.7034e-02],\n",
       "          [4.5652e-02, 8.9675e-01, 3.1140e-02, 2.6458e-02],\n",
       "          [4.3575e-02, 9.0134e-01, 2.9707e-02, 2.5380e-02],\n",
       "          [3.5660e-02, 9.2013e-01, 2.3688e-02, 2.0526e-02],\n",
       "          [1.2104e-02, 9.7187e-01, 8.2805e-03, 7.7406e-03],\n",
       "          [4.9293e-03, 9.8897e-01, 3.1376e-03, 2.9656e-03],\n",
       "          [7.2043e-03, 9.8300e-01, 5.0842e-03, 4.7066e-03],\n",
       "          [2.6169e-02, 9.4118e-01, 1.7328e-02, 1.5324e-02],\n",
       "          [2.0736e-02, 9.5463e-01, 1.2897e-02, 1.1737e-02],\n",
       "          [3.2283e-02, 9.2704e-01, 2.1788e-02, 1.8894e-02],\n",
       "          [2.7207e-02, 9.3718e-01, 1.8819e-02, 1.6793e-02],\n",
       "          [4.0492e-02, 9.0537e-01, 2.9088e-02, 2.5048e-02],\n",
       "          [4.3518e-02, 9.0137e-01, 2.9812e-02, 2.5303e-02],\n",
       "          [2.8375e-02, 9.3447e-01, 1.9590e-02, 1.7560e-02],\n",
       "          [3.8841e-02, 9.0983e-01, 2.7296e-02, 2.4037e-02],\n",
       "          [3.0592e-02, 9.2843e-01, 2.1792e-02, 1.9186e-02],\n",
       "          [3.4191e-02, 9.1986e-01, 2.4598e-02, 2.1349e-02],\n",
       "          [3.8477e-02, 9.1421e-01, 2.5151e-02, 2.2167e-02],\n",
       "          [2.5773e-02, 9.3910e-01, 1.8569e-02, 1.6555e-02],\n",
       "          [2.9400e-02, 9.3039e-01, 2.1487e-02, 1.8725e-02],\n",
       "          [6.4984e-05, 9.9984e-01, 4.7697e-05, 4.7499e-05],\n",
       "          [1.1506e-02, 9.7404e-01, 7.4956e-03, 6.9563e-03],\n",
       "          [2.8167e-02, 9.3617e-01, 1.8832e-02, 1.6828e-02],\n",
       "          [2.2934e-02, 9.4859e-01, 1.4939e-02, 1.3541e-02],\n",
       "          [3.0063e-02, 9.2929e-01, 2.1653e-02, 1.8989e-02]],\n",
       "         grad_fn=<CopySlices>),\n",
       "  'dlt1': tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.0462e-01, 1.4526e-01, 2.0063e-01, 1.0213e-01, 1.0915e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.0104e-01, 1.6509e-01, 1.6652e-01, 1.2528e-01, 1.3181e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [9.2880e-02, 1.3465e-01, 1.2032e-01, 6.1232e-02, 8.8740e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [6.6841e-12, 3.5887e-10, 1.5162e-08, 3.1460e-12, 1.6253e-11],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [5.1650e-02, 8.2712e-02, 1.0176e-01, 5.9118e-02, 5.0126e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [6.0092e-02, 1.1064e-01, 1.1208e-01, 9.4492e-02, 8.1720e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.0279e-01, 1.4553e-01, 1.6049e-01, 1.2810e-01, 1.2113e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [4.6450e-02, 6.9729e-02, 9.1878e-02, 3.6674e-02, 4.9577e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
       "         grad_fn=<CopySlices>),\n",
       "  'dlt2': tensor([[0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.]], grad_fn=<CopySlices>)})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def temporal_loss(timestoevents,weights=None,maxtime=48,threshold=True):\n",
    "    #list of expected times to events, usualy in order of Const.temporal_outcomes\n",
    "    #basically longer = better, we count > maxtime (weeks) as no event\n",
    "    if weights is None: \n",
    "        weights = [1 for i in range(len(timestoevents))]\n",
    "    scores =  [(w*maxtime/t)for w,t in zip(weights,timestoevents)]\n",
    "    if threshold:\n",
    "        scores = [s*torch.lt(t,maxtime) for s,t in zip(scores,timestoevents)]\n",
    "    scores = torch.stack(scores).sum(axis=0)\n",
    "    return scores\n",
    "\n",
    "def outcome_loss(ypred,weights=None):\n",
    "    #default weights is bad\n",
    "    if weights is None: \n",
    "        print('using default outcome loss weights, which is probably wrong since bad stuff should be negative')\n",
    "        weights = [1 for i in range(ypred.shape[1])]\n",
    "    l = torch.mul(ypred[:,0],weights[0])\n",
    "    for i,weight in enumerate(weights[1:]):\n",
    "        #weights with negative values will invert the outcome so e.g. Regional control becomes no regional control\n",
    "        #so the penaly is correct\n",
    "        newloss = torch.mul(ypred[:,i+1],weight)\n",
    "        l = torch.add(l,newloss)\n",
    "    return l\n",
    "\n",
    "def calc_optimal_decisions(dataset,ids,m1,m2,m3,sm3,\n",
    "                           weights=[0,0.5,.5,0], #weight for OS, FT, AS, and LRC as binary probabilities\n",
    "                           tweights=[1,1,1,1], #weight for OS, LRC, FDM, and event (any + FT or AS at 6m) as time to event in weeks\n",
    "                           outcome_loss_func=None,\n",
    "                           threshold_temporal_loss = False,\n",
    "                           maxtime=48,\n",
    "                           get_transitions=True):\n",
    "    m1.eval()\n",
    "    m2.eval()\n",
    "    m3.eval()\n",
    "    sm3.eval()\n",
    "    device = m1.get_device()\n",
    "    data = dataset.processed_df.copy().loc[ids]\n",
    "    \n",
    "    def get_dlt(state):\n",
    "        if state == 2:\n",
    "            return data[Const.dlt2].copy()\n",
    "        d = data[Const.dlt1].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_pd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.primary_disease_states2].copy()\n",
    "        d = data[Const.primary_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_nd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.nodal_disease_states2].copy()\n",
    "        d = data[Const.nodal_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_cc(state):\n",
    "        res = data[Const.ccs].copy()\n",
    "        if state == 1:\n",
    "            res.values[:,:] = np.zeros(res.values.shape)\n",
    "        return res\n",
    "    \n",
    "    def get_mod(state):\n",
    "        res = data[Const.modifications].copy()\n",
    "        #this should have an ic condition but we don't use it anumore anywa\n",
    "        return res\n",
    "        \n",
    "    def formatdf(d):\n",
    "        d = df_to_torch(d).to(device)\n",
    "        return d\n",
    "    \n",
    "    \n",
    "    outcomedf = data[Const.outcomes]\n",
    "    baseline = dataset.get_state('baseline').loc[ids]\n",
    "    baseline_input = formatdf(baseline)\n",
    "\n",
    "        \n",
    "    if outcome_loss_func is None:\n",
    "        outcome_loss_func = outcome_loss\n",
    "    \n",
    "    cat = lambda x: torch.cat([xx.to(device) for xx in x],axis=1).to(device)\n",
    "    format_transition = lambda x: x.to(device)\n",
    "    def get_outcome(d1,d2,d3):\n",
    "        d1 = torch.full((len(ids),1),d1).type(torch.FloatTensor)\n",
    "        d2 = torch.full((len(ids),1),d2).type(torch.FloatTensor)\n",
    "        d3 = torch.full((len(ids),1),d3).type(torch.FloatTensor)\n",
    "        \n",
    "        tinput1 = cat([baseline_input,d1])\n",
    "        ytransition = m1(tinput1)\n",
    "        [ypd1,ynd1,ymod,ydlt1] = [format_transition(xx) for xx in ytransition['predictions']]\n",
    "        d1_thresh = torch.gt(d1,.5).view(-1,1).to(device)\n",
    "        ypd1[:,0:2] = ypd1[:,0:2]*d1_thresh\n",
    "        ynd1[:,0:2] = ynd1[:,0:2]*d1_thresh\n",
    "        \n",
    "        tinput2 = cat([baseline_input,ypd1,ynd1,ymod,ydlt1,d1,d2])\n",
    "        ytransition2 = m2(tinput2)\n",
    "        [ypd2,ynd2,ycc,ydlt2] = [format_transition(xx) for xx in ytransition2['predictions']]\n",
    "        \n",
    "        input3 = cat([baseline_input, ypd2, ynd2, ycc, ydlt2, d1, d2,d3])\n",
    "        outcome = m3(input3)['predictions']\n",
    "        temporal_outcomes = sm3.time_to_event(input3,n_samples=1)\n",
    "        \n",
    "        transitions = {\n",
    "            'pd1': ypd1,\n",
    "            'nd1': ynd1,\n",
    "            'nd2': ynd2,\n",
    "            'pd2': ypd2,\n",
    "            'mod': ymod,\n",
    "            'cc': ycc,\n",
    "            'dlt1': ydlt1,\n",
    "            'dlt2': ydlt2,\n",
    "        }\n",
    "        return outcome, temporal_outcomes, transitions\n",
    "\n",
    "    losses = []\n",
    "    loss_order = []\n",
    "    transitions = {}\n",
    "    for d1 in [0,1]:\n",
    "        for d2 in [0,1]:\n",
    "            for d3 in [0,1]:\n",
    "                outcomes, tte, transition_entry = get_outcome(d1,d2,d3)\n",
    "                loss = outcome_loss_func(outcomes,weights)\n",
    "                tloss = temporal_loss(tte,tweights,maxtime=maxtime,threshold=threshold_temporal_loss)\n",
    "                loss += tloss\n",
    "                losses.append(loss)\n",
    "                loss_order.append([d1,d2,d3])\n",
    "                transitions[str(d1)+str(d2)+str(d3)] = transition_entry\n",
    "    losses = torch.stack(losses,axis=1)\n",
    "    optimal_decisions = [loss_order[i] for i in torch.argmin(losses,axis=1)]\n",
    "    result = torch.tensor(optimal_decisions).type(torch.FloatTensor)\n",
    "    print(result.sum(axis=0),result.shape[0])\n",
    "    if get_transitions:\n",
    "        opt_transitions = {k: torch.zeros(v.shape).type(torch.FloatTensor) for k,v in transitions['000'].items()}\n",
    "        for i,od in enumerate(optimal_decisions):\n",
    "            key = ''.join([str(o) for o in od])\n",
    "            entry = transitions[key]\n",
    "            for kk,vv in entry.items():\n",
    "                opt_transitions[kk][i,:] = vv[i,:]\n",
    "        return result, opt_transitions\n",
    "    return result\n",
    "\n",
    "test, testtest = get_tt_split()\n",
    "calc_optimal_decisions(DTDataset(),\n",
    "                       testtest,model1,model2,model3,smodel3,\n",
    "                       threshold_temporal_loss=False,\n",
    "                       maxtime=48,\n",
    "                       weights=[0,0,0,0],\n",
    "                       tweights=[2,0.1,0,0],\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "122ee514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([25.,  0.,  0.]) 389\n",
      "tensor([8., 0., 0.]) 147\n",
      "torch.Size([3, 536, 87])\n",
      "tensor([0.0544, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 0 _____\n",
      "val reward 0.325850248336792\n",
      "imitation reward 1.7759063243865967\n",
      "distance losses 2.728210210800171 0.6751828193664551\n",
      "distributions [0.0030181524343788624, 0.004276015795767307, 0.0045722066424787045]\n",
      "[{'decision': 0, 'optimal_auc': 0.5620503597122302, 'imitation_auc': 0.6645992366412214, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.5002717391304348, 'optimal_acc': 1.0, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.5696122059758424, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.0544, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 1 _____\n",
      "val reward 0.3722558319568634\n",
      "imitation reward 1.297990083694458\n",
      "distance losses 2.4456264972686768 0.5394548177719116\n",
      "distributions [0.0014792433939874172, 0.0009013219969347119, 0.0009147615055553615]\n",
      "[{'decision': 0, 'optimal_auc': 0.21492805755395683, 'imitation_auc': 0.5066793893129771, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.6614130434782608, 'optimal_acc': 1.0, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.6090273363000636, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.0544, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 2 _____\n",
      "val reward 0.32839250564575195\n",
      "imitation reward 1.3940527439117432\n",
      "distance losses 2.542198419570923 0.5635664463043213\n",
      "distributions [0.0030754003673791885, 0.0005834712064824998, 0.0006977081065997481]\n",
      "[{'decision': 0, 'optimal_auc': 0.19694244604316544, 'imitation_auc': 0.46183206106870234, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.6630434782608696, 'optimal_acc': 1.0, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7155117609663064, 'optimal_acc': 1.0, 'imitation_acc': 0.5782312925170068}]\n",
      "tensor([0.0544, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 3 _____\n",
      "val reward 0.30285245180130005\n",
      "imitation reward 1.1871767044067383\n",
      "distance losses 2.387005567550659 0.6128950715065002\n",
      "distributions [0.004899625200778246, 9.97216411633417e-05, 0.00011617340351222083]\n",
      "[{'decision': 0, 'optimal_auc': 0.22302158273381295, 'imitation_auc': 0.5162213740458016, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.6491847826086956, 'optimal_acc': 1.0, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7828989192625556, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.0544, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 4 _____\n",
      "val reward 0.24283435940742493\n",
      "imitation reward 1.2153080701828003\n",
      "distance losses 2.4535160064697266 0.42780202627182007\n",
      "distributions [0.016890374943614006, 4.545688352664001e-05, 5.126729956828058e-05]\n",
      "[{'decision': 0, 'optimal_auc': 0.36780575539568344, 'imitation_auc': 0.5109732824427481, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.6603260869565217, 'optimal_acc': 1.0, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8156389065479974, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.0544, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 5 _____\n",
      "val reward 0.21481694281101227\n",
      "imitation reward 1.121403694152832\n",
      "distance losses 2.515554904937744 0.42282819747924805\n",
      "distributions [0.07116417586803436, 4.0088027162710205e-05, 4.473105582292192e-05]\n",
      "[{'decision': 0, 'optimal_auc': 0.5881294964028777, 'imitation_auc': 0.5267175572519084, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.6942934782608695, 'optimal_acc': 1.0, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8013350286077559, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.0544, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 6 _____\n",
      "val reward 0.26144587993621826\n",
      "imitation reward 1.1654247045516968\n",
      "distance losses 2.5682637691497803 0.45782485604286194\n",
      "distributions [0.15732385218143463, 1.9157070710207336e-05, 2.1590363758150488e-05]\n",
      "[{'decision': 0, 'optimal_auc': 0.6537769784172662, 'imitation_auc': 0.5443702290076335, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.6970108695652174, 'optimal_acc': 1.0, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.789574062301335, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.0544, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 7 _____\n",
      "val reward 0.22315694391727448\n",
      "imitation reward 1.1342672109603882\n",
      "distance losses 2.4279048442840576 0.533222496509552\n",
      "distributions [0.10341496020555496, 5.319198862707708e-06, 6.148055945232045e-06]\n",
      "[{'decision': 0, 'optimal_auc': 0.6735611510791367, 'imitation_auc': 0.5415076335877862, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7144021739130435, 'optimal_acc': 1.0, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.803560076287349, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.0544, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 8 _____\n",
      "val reward 0.2067657709121704\n",
      "imitation reward 1.1590652465820312\n",
      "distance losses 2.581289052963257 0.5387816429138184\n",
      "distributions [0.042631033807992935, 1.5058321878314018e-06, 1.7665843188297004e-06]\n",
      "[{'decision': 0, 'optimal_auc': 0.7194244604316546, 'imitation_auc': 0.549618320610687, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7192934782608695, 'optimal_acc': 1.0, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8064208518753972, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.0544, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 9 _____\n",
      "val reward 0.21477317810058594\n",
      "imitation reward 1.1236218214035034\n",
      "distance losses 2.625795602798462 0.6023893356323242\n",
      "distributions [0.026909206062555313, 5.736997650274134e-07, 7.046218684081396e-07]\n",
      "[{'decision': 0, 'optimal_auc': 0.7428057553956835, 'imitation_auc': 0.5663167938931297, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7184782608695652, 'optimal_acc': 1.0, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8108709472345836, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.0544, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 10 _____\n",
      "val reward 0.2077416181564331\n",
      "imitation reward 1.1105365753173828\n",
      "distance losses 2.7033579349517822 0.5127290487289429\n",
      "distributions [0.03198660537600517, 2.9272112556100183e-07, 3.934783876502479e-07]\n",
      "[{'decision': 0, 'optimal_auc': 0.7589928057553956, 'imitation_auc': 0.5586832061068702, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7198369565217391, 'optimal_acc': 1.0, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.813731722822632, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.0544, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 11 _____\n",
      "val reward 0.19852453470230103\n",
      "imitation reward 1.1149855852127075\n",
      "distance losses 2.7265336513519287 0.5397408604621887\n",
      "distributions [0.05511217191815376, 1.7541356100991834e-07, 2.6489792048778327e-07]\n",
      "[{'decision': 0, 'optimal_auc': 0.7733812949640287, 'imitation_auc': 0.5734732824427481, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7122282608695651, 'optimal_acc': 1.0, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8134138588684043, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0544, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 12 _____\n",
      "val reward 0.21059641242027283\n",
      "imitation reward 1.0823335647583008\n",
      "distance losses 2.7554006576538086 0.35311341285705566\n",
      "distributions [0.10066709667444229, 1.4174489137985802e-07, 2.383086439294857e-07]\n",
      "[{'decision': 0, 'optimal_auc': 0.7589928057553957, 'imitation_auc': 0.5706106870229007, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.6980978260869565, 'optimal_acc': 1.0, 'imitation_acc': 0.7891156462585034}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.819135410044501, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.0544, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 13 _____\n",
      "val reward 0.2146233320236206\n",
      "imitation reward 1.0837063789367676\n",
      "distance losses 2.5318968296051025 0.5357825756072998\n",
      "distributions [0.11034917831420898, 1.2370111335258116e-07, 2.1511141312657855e-07]\n",
      "[{'decision': 0, 'optimal_auc': 0.7571942446043165, 'imitation_auc': 0.5543893129770993, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.689945652173913, 'optimal_acc': 1.0, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8219961856325493, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.0544, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 14 _____\n",
      "val reward 0.19796495139598846\n",
      "imitation reward 1.1004630327224731\n",
      "distance losses 2.6692516803741455 0.5414073467254639\n",
      "distributions [0.07280854135751724, 9.715790127984292e-08, 1.721077751426492e-07]\n",
      "[{'decision': 0, 'optimal_auc': 0.7535971223021584, 'imitation_auc': 0.5744274809160306, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.6956521739130433, 'optimal_acc': 1.0, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8223140495867769, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.0544, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 15 _____\n",
      "val reward 0.19126459956169128\n",
      "imitation reward 1.1185789108276367\n",
      "distance losses 2.6791791915893555 0.5871112942695618\n",
      "distributions [0.0535447858273983, 9.507260756436153e-08, 1.7666071983057918e-07]\n",
      "[{'decision': 0, 'optimal_auc': 0.7670863309352518, 'imitation_auc': 0.6025763358778626, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7084239130434783, 'optimal_acc': 1.0, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8264462809917356, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.0544, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 16 _____\n",
      "val reward 0.18508176505565643\n",
      "imitation reward 1.081757664680481\n",
      "distance losses 2.5946145057678223 0.6152755618095398\n",
      "distributions [0.04974497854709625, 1.1237943908781745e-07, 2.128427922798437e-07]\n",
      "[{'decision': 0, 'optimal_auc': 0.8309352517985611, 'imitation_auc': 0.6149809160305343, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7307065217391304, 'optimal_acc': 1.0, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8328035600762873, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.0544, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 17 _____\n",
      "val reward 0.18026338517665863\n",
      "imitation reward 1.111591100692749\n",
      "distance losses 2.5329458713531494 0.4744625985622406\n",
      "distributions [0.05772363021969795, 1.518945822454043e-07, 2.736043995810178e-07]\n",
      "[{'decision': 0, 'optimal_auc': 0.8426258992805755, 'imitation_auc': 0.6197519083969465, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7347826086956522, 'optimal_acc': 1.0, 'imitation_acc': 0.7551020408163265}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.84170375079466, 'optimal_acc': 1.0, 'imitation_acc': 0.8299319727891157}]\n",
      "tensor([0.0544, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 18 _____\n",
      "val reward 0.1759408563375473\n",
      "imitation reward 1.164323091506958\n",
      "distance losses 2.5062716007232666 0.5697649717330933\n",
      "distributions [0.05144983530044556, 1.2116058201172564e-07, 2.1909640679496079e-07]\n",
      "[{'decision': 0, 'optimal_auc': 0.8516187050359711, 'imitation_auc': 0.6130725190839694, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7304347826086957, 'optimal_acc': 1.0, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8417037507946599, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.0544, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 19 _____\n",
      "val reward 0.17364883422851562\n",
      "imitation reward 1.2373440265655518\n",
      "distance losses 2.597726345062256 0.4530809223651886\n",
      "distributions [0.06193114444613457, 1.274749479307502e-07, 2.3490869693887362e-07]\n",
      "[{'decision': 0, 'optimal_auc': 0.8489208633093526, 'imitation_auc': 0.5830152671755725, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7203804347826087, 'optimal_acc': 1.0, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8404322949777495, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.0544, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 20 _____\n",
      "val reward 0.18722295761108398\n",
      "imitation reward 1.0701913833618164\n",
      "distance losses 2.5280961990356445 0.46817389130592346\n",
      "distributions [0.09784453362226486, 2.424712022275344e-07, 4.309264625135256e-07]\n",
      "[{'decision': 0, 'optimal_auc': 0.8579136690647482, 'imitation_auc': 0.5586832061068703, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7244565217391306, 'optimal_acc': 1.0, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.829942784488239, 'optimal_acc': 1.0, 'imitation_acc': 0.8299319727891157}]\n",
      "tensor([0.0544, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 21 _____\n",
      "val reward 0.18881160020828247\n",
      "imitation reward 1.096114158630371\n",
      "distance losses 2.4059946537017822 0.5753152966499329\n",
      "distributions [0.10029356926679611, 3.716087348948349e-07, 6.151323646008677e-07]\n",
      "[{'decision': 0, 'optimal_auc': 0.8732014388489209, 'imitation_auc': 0.5543893129770993, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.8707482993197279}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7195652173913044, 'optimal_acc': 1.0, 'imitation_acc': 0.7551020408163265}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8286713286713288, 'optimal_acc': 1.0, 'imitation_acc': 0.8095238095238095}]\n",
      "tensor([0.0544, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 22 _____\n",
      "val reward 0.1753704696893692\n",
      "imitation reward 1.1990035772323608\n",
      "distance losses 2.7149457931518555 0.6183377504348755\n",
      "distributions [0.061413634568452835, 3.3600394999666605e-07, 5.103951821183728e-07]\n",
      "[{'decision': 0, 'optimal_auc': 0.8821942446043166, 'imitation_auc': 0.5920801526717556, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7168478260869565, 'optimal_acc': 1.0, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.821360457724094, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.0544, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 23 _____\n",
      "val reward 0.17323020100593567\n",
      "imitation reward 1.13640296459198\n",
      "distance losses 2.7275686264038086 0.537670373916626\n",
      "distributions [0.06352406740188599, 6.132971748229465e-07, 8.682932843839808e-07]\n",
      "[{'decision': 0, 'optimal_auc': 0.881294964028777, 'imitation_auc': 0.6245229007633587, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7277173913043479, 'optimal_acc': 1.0, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8153210425937699, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0544, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 24 _____\n",
      "val reward 0.18063783645629883\n",
      "imitation reward 1.1303930282592773\n",
      "distance losses 2.7410621643066406 0.42590248584747314\n",
      "distributions [0.09573030471801758, 1.322642447121325e-06, 1.7915821217684424e-06]\n",
      "[{'decision': 0, 'optimal_auc': 0.8875899280575539, 'imitation_auc': 0.6397900763358778, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.8707482993197279}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7366847826086956, 'optimal_acc': 1.0, 'imitation_acc': 0.7414965986394558}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8296249205340115, 'optimal_acc': 1.0, 'imitation_acc': 0.8163265306122449}]\n",
      "tensor([0.0544, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 25 _____\n",
      "val reward 0.17772386968135834\n",
      "imitation reward 1.1936062574386597\n",
      "distance losses 2.6197683811187744 0.467324435710907\n",
      "distributions [0.10334429144859314, 1.898229470498336e-06, 2.4810176455503097e-06]\n",
      "[{'decision': 0, 'optimal_auc': 0.8938848920863309, 'imitation_auc': 0.6192748091603053, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.8707482993197279}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7478260869565218, 'optimal_acc': 1.0, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8331214240305149, 'optimal_acc': 1.0, 'imitation_acc': 0.7891156462585034}]\n",
      "tensor([0.0544, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 26 _____\n",
      "val reward 0.1549067497253418\n",
      "imitation reward 1.048067331314087\n",
      "distance losses 2.6974799633026123 0.4176192581653595\n",
      "distributions [0.05394160374999046, 1.4814540918450803e-06, 1.7367035525239771e-06]\n",
      "[{'decision': 0, 'optimal_auc': 0.89568345323741, 'imitation_auc': 0.620706106870229, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7570652173913044, 'optimal_acc': 1.0, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8283534647171011, 'optimal_acc': 1.0, 'imitation_acc': 0.8299319727891157}]\n",
      "tensor([0.0544, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 27 _____\n",
      "val reward 0.1561318039894104\n",
      "imitation reward 1.1160727739334106\n",
      "distance losses 2.7233974933624268 0.4891243278980255\n",
      "distributions [0.0368223637342453, 1.4693332559545524e-06, 1.6189695770663093e-06]\n",
      "[{'decision': 0, 'optimal_auc': 0.8992805755395683, 'imitation_auc': 0.6302480916030535, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7551630434782608, 'optimal_acc': 1.0, 'imitation_acc': 0.7959183673469388}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8242212333121424, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.0544, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 28 _____\n",
      "val reward 0.15407779812812805\n",
      "imitation reward 1.0693647861480713\n",
      "distance losses 2.7029194831848145 0.5410069823265076\n",
      "distributions [0.05362241342663765, 3.1625597785023274e-06, 3.4529584809206426e-06]\n",
      "[{'decision': 0, 'optimal_auc': 0.8983812949640287, 'imitation_auc': 0.6197519083969466, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7024456521739131, 'optimal_acc': 1.0, 'imitation_acc': 0.7551020408163265}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8022886204704387, 'optimal_acc': 1.0, 'imitation_acc': 0.8163265306122449}]\n",
      "tensor([0.0544, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 29 _____\n",
      "val reward 0.16903626918792725\n",
      "imitation reward 1.2691376209259033\n",
      "distance losses 2.4072134494781494 0.4671403467655182\n",
      "distributions [0.09167610108852386, 6.866584499221062e-06, 7.738130079815164e-06]\n",
      "[{'decision': 0, 'optimal_auc': 0.8884892086330936, 'imitation_auc': 0.5963740458015268, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.8299319727891157}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.702445652173913, 'optimal_acc': 1.0, 'imitation_acc': 0.7687074829931972}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7883026064844247, 'optimal_acc': 1.0, 'imitation_acc': 0.7959183673469388}]\n",
      "tensor([0.0544, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 30 _____\n",
      "val reward 0.17026200890541077\n",
      "imitation reward 1.2751195430755615\n",
      "distance losses 2.6931111812591553 0.6275227665901184\n",
      "distributions [0.08826594054698944, 9.29680754779838e-06, 1.0278103218297474e-05]\n",
      "[{'decision': 0, 'optimal_auc': 0.8920863309352518, 'imitation_auc': 0.6016221374045801, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.8299319727891157}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7008152173913043, 'optimal_acc': 1.0, 'imitation_acc': 0.7687074829931972}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7822631913541004, 'optimal_acc': 1.0, 'imitation_acc': 0.8027210884353742}]\n",
      "tensor([0.0544, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 31 _____\n",
      "val reward 0.16417020559310913\n",
      "imitation reward 1.1112947463989258\n",
      "distance losses 2.7647547721862793 0.48385635018348694\n",
      "distributions [0.06386870890855789, 9.177876563626342e-06, 9.183422662317753e-06]\n",
      "[{'decision': 0, 'optimal_auc': 0.8947841726618704, 'imitation_auc': 0.6354961832061069, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.8843537414965986}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.669836956521739, 'optimal_acc': 1.0, 'imitation_acc': 0.7551020408163265}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7739987285441831, 'optimal_acc': 1.0, 'imitation_acc': 0.7959183673469388}]\n",
      "tensor([0.0544, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 32 _____\n",
      "val reward 0.16253362596035004\n",
      "imitation reward 1.1150704622268677\n",
      "distance losses 2.6664938926696777 0.5934960842132568\n",
      "distributions [0.06135105714201927, 1.059255373547785e-05, 9.931194654200226e-06]\n",
      "[{'decision': 0, 'optimal_auc': 0.8974820143884892, 'imitation_auc': 0.650763358778626, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.652445652173913, 'optimal_acc': 1.0, 'imitation_acc': 0.7619047619047619}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7711379529561347, 'optimal_acc': 1.0, 'imitation_acc': 0.8163265306122449}]\n",
      "tensor([0.0544, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 33 _____\n",
      "val reward 0.16784441471099854\n",
      "imitation reward 1.113494873046875\n",
      "distance losses 2.778205633163452 0.4518381655216217\n",
      "distributions [0.08036669343709946, 1.6039210095186718e-05, 1.5083841390151065e-05]\n",
      "[{'decision': 0, 'optimal_auc': 0.8929856115107915, 'imitation_auc': 0.6550572519083969, 'optimal_acc': 0.9523809523809523, 'imitation_acc': 0.8775510204081632}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.65, 'optimal_acc': 1.0, 'imitation_acc': 0.7414965986394558}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7914812460267007, 'optimal_acc': 1.0, 'imitation_acc': 0.8027210884353742}]\n",
      "tensor([0.0544, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 34 _____\n",
      "val reward 0.17822007834911346\n",
      "imitation reward 1.1767783164978027\n",
      "distance losses 2.6221749782562256 0.5941585302352905\n",
      "distributions [0.09423629194498062, 2.0734602003358305e-05, 2.1578467567451298e-05]\n",
      "[{'decision': 0, 'optimal_auc': 0.8821942446043165, 'imitation_auc': 0.6526717557251908, 'optimal_acc': 0.9319727891156463, 'imitation_acc': 0.8367346938775511}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.691304347826087, 'optimal_acc': 1.0, 'imitation_acc': 0.7551020408163265}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8061029879211697, 'optimal_acc': 1.0, 'imitation_acc': 0.8027210884353742}]\n",
      "tensor([0.0544, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 35 _____\n",
      "val reward 0.1827671378850937\n",
      "imitation reward 1.1250972747802734\n",
      "distance losses 2.4091525077819824 0.4809192419052124\n",
      "distributions [0.10746128112077713, 2.2156722479849122e-05, 2.379699253651779e-05]\n",
      "[{'decision': 0, 'optimal_auc': 0.8929856115107913, 'imitation_auc': 0.655057251908397, 'optimal_acc': 0.9115646258503401, 'imitation_acc': 0.8367346938775511}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7054347826086956, 'optimal_acc': 1.0, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8045136681500318, 'optimal_acc': 1.0, 'imitation_acc': 0.7891156462585034}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0544, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 36 _____\n",
      "val reward 0.16212324798107147\n",
      "imitation reward 1.1287782192230225\n",
      "distance losses 2.800011157989502 0.49350109696388245\n",
      "distributions [0.0813124030828476, 1.9080920537817292e-05, 2.0029459847137332e-05]\n",
      "[{'decision': 0, 'optimal_auc': 0.9082733812949639, 'imitation_auc': 0.6698473282442747, 'optimal_acc': 0.9319727891156463, 'imitation_acc': 0.8775510204081632}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.695108695652174, 'optimal_acc': 1.0, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7949777495232041, 'optimal_acc': 1.0, 'imitation_acc': 0.8299319727891157}]\n",
      "tensor([0.0544, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 37 _____\n",
      "val reward 0.15071257948875427\n",
      "imitation reward 1.1395437717437744\n",
      "distance losses 2.7426280975341797 0.5628157258033752\n",
      "distributions [0.0689985454082489, 2.113919072144199e-05, 2.207921352237463e-05]\n",
      "[{'decision': 0, 'optimal_auc': 0.9100719424460433, 'imitation_auc': 0.6808206106870229, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.8843537414965986}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7135869565217393, 'optimal_acc': 1.0, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7914812460267007, 'optimal_acc': 1.0, 'imitation_acc': 0.8299319727891157}]\n",
      "++++++++++Final+++++++++++\n",
      "best tensor(1.0928, grad_fn=<AddBackward0>)\n",
      "[{'decision': 0, 'optimal_auc': 0.89568345323741, 'imitation_auc': 0.620706106870229, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7570652173913044, 'optimal_acc': 1.0, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8283534647171011, 'optimal_acc': 1.0, 'imitation_acc': 0.8299319727891157}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionAttentionModel(\n",
       "  (input_dropout): Dropout(p=0.25, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=100, out_features=1000, bias=True)\n",
       "  )\n",
       "  (batchnorm): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       "  (relu): Softplus(beta=1, threshold=20)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (softmax): Softmax(dim=1)\n",
       "  (final_opt_layer): Linear(in_features=1000, out_features=100, bias=True)\n",
       "  (final_imitation_layer): Linear(in_features=1000, out_features=100, bias=True)\n",
       "  (final_layer): Linear(in_features=1000, out_features=6, bias=True)\n",
       "  (resize_layer): Linear(in_features=91, out_features=100, bias=True)\n",
       "  (attentions): ModuleList(\n",
       "    (0): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (norms): ModuleList(\n",
       "    (0): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (activation): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_unique_sequence(array):\n",
    "    #converts a row of boolean values to a unique number e.g. [1,1,0] => 11, [0,0,1] => 100\n",
    "    uniqueify = lambda r: torch.sum(torch.stack([i*(10**ii) for ii,i in enumerate(r)]))\n",
    "    return torch_apply_along_axis(uniqueify,array)\n",
    "\n",
    "def train_decision_model_triplet(\n",
    "    tmodel1,\n",
    "    tmodel2,\n",
    "    tmodel3,\n",
    "    smodel3,\n",
    "    use_default_split=True,\n",
    "    use_bagging_split=False,\n",
    "    use_attention=True,\n",
    "    lr=.001,\n",
    "    epochs=10000,\n",
    "    patience=5,\n",
    "    weights=[0,.5,.5,0], #realtive weight of survival, feeding tube, aspiration, andl lrc\n",
    "    tweights=[1,1,1,0], #weight for OS, LRC, FDM, and event (any + FT or AS at 6m) as time to event in weeks\n",
    "    opt_weights=[1,1,1], #weights for policy model for optimal decisions\n",
    "    imitation_weights=[.5,1,1],#weights of imitation decisions, because ic overtrains too quickly\n",
    "    imitation_weight=1,\n",
    "    reward_weight=1,\n",
    "    imitation_triplet_weight=2,\n",
    "    reward_triplet_weight = 2,\n",
    "    shufflecol_chance = 0.1,\n",
    "    split=.7,\n",
    "    resample_training=False,\n",
    "    save_path='../data/models/',\n",
    "    file_suffix='',\n",
    "    verbose=True,\n",
    "    use_gpu=False,\n",
    "    **model_kwargs,\n",
    "):\n",
    "    tmodel1.eval()\n",
    "    tmodel2.eval()\n",
    "    tmodel3.eval()\n",
    "\n",
    "    train_ids, test_ids = get_tt_split(use_default_split=use_default_split,use_bagging_split=use_bagging_split,resample_training=resample_training)\n",
    "    true_ids = train_ids + test_ids #for saving memory without upsampling\n",
    "\n",
    "    dataset = DTDataset()\n",
    "    data = dataset.processed_df.copy()\n",
    "    \n",
    "    def get_dlt(state):\n",
    "        if state == 2:\n",
    "            return data[Const.dlt2].copy()\n",
    "        d = data[Const.dlt1].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_pd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.primary_disease_states2].copy()\n",
    "        d = data[Const.primary_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_nd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.nodal_disease_states2].copy()\n",
    "        d = data[Const.nodal_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_cc(state):\n",
    "        res = data[Const.ccs].copy()\n",
    "        if state == 1:\n",
    "            res.values[:,:] = np.zeros(res.values.shape)\n",
    "        return res\n",
    "    \n",
    "    def get_mod(state):\n",
    "        res = data[Const.modifications].copy()\n",
    "        #this should have an ic condition but we don't use it anumore anywa\n",
    "        return res\n",
    "        \n",
    "    outcomedf = data[Const.outcomes]\n",
    "    baseline = dataset.get_state('baseline')\n",
    "    \n",
    "    def formatdf(d,dids=train_ids):\n",
    "        d = df_to_torch(d.loc[dids]).to(model.get_device())\n",
    "        return d\n",
    "    \n",
    "    def makegrad(v):\n",
    "        if not v.requires_grad:\n",
    "            v.requires_grad=True\n",
    "        return v\n",
    "    \n",
    "    if use_attention:\n",
    "        model = DecisionAttentionModel(baseline.shape[1],**model_kwargs)\n",
    "    else:\n",
    "        model_kwargs = {k:v for k,v in model_kwargs.items() if 'attention' not in k and 'embed' not in k}\n",
    "        model = DecisionModel(baseline.shape[1],**model_kwargs)\n",
    "        \n",
    "    device = 'cpu'\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "        \n",
    "    model.set_device(device)\n",
    "\n",
    "    tmodel1.set_device(device)\n",
    "    tmodel2.set_device(device)\n",
    "    tmodel3.set_device(device)\n",
    "    smodel3.set_device(device)\n",
    "    hashcode = str(hash(','.join([str(i) for i in train_ids])))\n",
    "    \n",
    "    save_file = save_path + 'model_' + model.identifier +'_hash' + hashcode + file_suffix + '.tar'\n",
    "    model.fit_normalizer(df_to_torch(baseline.loc[train_ids]))\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "    \n",
    "    \n",
    "    optimal_train,transitions_train = calc_optimal_decisions(dataset,train_ids,tmodel1,tmodel2,tmodel3,smodel3,\n",
    "                                           weights=weights,tweights=tweights,\n",
    "                                          )\n",
    "    optimal_test,transitions_test = calc_optimal_decisions(dataset,test_ids,tmodel1,tmodel2,tmodel3,smodel3,\n",
    "                                          weights=weights,tweights=tweights,\n",
    "                                         )\n",
    "    optimal_train = optimal_train.to(model.get_device())\n",
    "    optimal_test = optimal_test.to(model.get_device())\n",
    "    mse = torch.nn.MSELoss()\n",
    "    bce = torch.nn.BCELoss()\n",
    "    \n",
    "    def compare_decisions(d1,d2,d3,ids):\n",
    "#         ypred = np.concatenate([dd.cpu().detach().numpy().reshape(-1,1) for dd in [d1,d2,d3]],axis=1)\n",
    "        ytrue = df_to_torch(outcomedf.loc[ids])\n",
    "        dloss = bce(d1.view(-1),ytrue[:,0])\n",
    "        dloss += bce(d2.view(-1),ytrue[:,1])\n",
    "        dloss += bce(d3,view(-1),ytrue[:,2])\n",
    "        return dloss\n",
    "        \n",
    "    def remove_decisions(df):\n",
    "        cols = [c for c in df.columns if c not in Const.decisions ]\n",
    "        ddf = df[cols]\n",
    "        return ddf\n",
    "    \n",
    "    makeinput = lambda step,dids: df_to_torch(remove_decisions(dataset.get_input_state(step=step,ids=dids)))\n",
    "    threshold = lambda x: torch.gt(x,torch.rand(x.shape[0])).type(torch.FloatTensor)\n",
    "\n",
    "    randchoice = lambda x: x[torch.randint(len(x),(1,))[0]]\n",
    "    tloss_func = torch.nn.TripletMarginLoss()\n",
    "    def get_tloss(row,step,yt,x,imitation=True):\n",
    "        if yt[:,step].std() < .001:\n",
    "            return torch.tensor([0]).to(device)\n",
    "        positive_idx= torch.nonzero(yt[:,step] == yt[row,step])\n",
    "        if len(positive_idx) <= 1:\n",
    "            print('no losses','n positive',len(positive_idx),'yt',yt[row,step],'row',row,'step',step,'imitation',imitation,end='\\r')\n",
    "            return torch.tensor([0]).to(device)\n",
    "        positive_idx = torch.stack([ii for ii in positive_idx if ii != row]).view(-1)\n",
    "        negative_idx = torch.tensor([ii for ii in range(x.shape[0]) if ii not in positive_idx and ii != row])\n",
    "        if len(positive_idx) < 1 or len(negative_idx) < 1:\n",
    "            print('no losses','n positive',len(positive_idx),'n negative',len(negative_idx),'yt',yt[row,step],'row',row,'step',step,'imitation',imitation,end='\\r')\n",
    "            return torch.tensor([0]).to(device)\n",
    "        positive = x[randchoice(positive_idx)]\n",
    "        negative = x[randchoice(negative_idx)]\n",
    "        anchor = x[row]\n",
    "        if use_attention:\n",
    "            [anchor_embedding,pos_embedding,neg_embedding] = [model.get_embedding(xx.view(1,-1),position=step,use_saved_memory=True) for xx in [anchor,positive,negative]]\n",
    "        else:    \n",
    "            [anchor_embedding,pos_embedding,neg_embedding] = [model.get_embedding(xx.view(1,-1),position=step,concatenate=False)[int(imitation)] for xx in [anchor,positive,negative]]\n",
    "        tloss = tloss_func(anchor_embedding,pos_embedding,neg_embedding)\n",
    "        return tloss\n",
    "    #save the inputs from the whole dataset for future reference\n",
    "    if use_attention:\n",
    "        full_data = []\n",
    "        for mstep in [0,1,2]:\n",
    "            full_data_step = [baseline, get_dlt(min(mstep,1)),\n",
    "                         get_dlt(mstep),get_pd(mstep),get_nd(mstep),get_cc(mstep),get_mod(mstep)]\n",
    "            full_data_step = torch.cat([formatdf(fd,true_ids) for fd in full_data_step],axis=1)\n",
    "            full_data.append(full_data_step)\n",
    "        full_data = torch.stack(full_data)\n",
    "        model.save_memory(full_data)\n",
    "        print(full_data.shape)\n",
    "        \n",
    "    def step(train=True):\n",
    "        if train:\n",
    "            model.train(True)\n",
    "            tmodel1.train(True)\n",
    "            tmodel2.train(True)\n",
    "            tmodel3.train(True)\n",
    "            optimizer.zero_grad()\n",
    "            ids = train_ids\n",
    "            y_opt = makegrad(optimal_train)\n",
    "            transition_dict = {k: torch.clone(v).detach() for k,v in transitions_train.items()}\n",
    "        else:\n",
    "            ids = test_ids\n",
    "            model.eval()\n",
    "            tmodel1.eval()\n",
    "            tmodel2.eval()\n",
    "            tmodel3.eval()\n",
    "            y_opt = makegrad(optimal_test)\n",
    "            print(y_opt.mean(axis=0))\n",
    "            transition_dict = {k: torch.clone(v).detach() for k,v in transitions_test.items()}\n",
    "        model.set_device(device)\n",
    "        ytrain = df_to_torch(outcomedf.loc[ids]).to(device)\n",
    "        #imitation losses and decision 1\n",
    "        xxtrained = [baseline, get_dlt(0),get_dlt(0),get_pd(0),get_nd(0),get_cc(0),get_mod(0)]\n",
    "        xxtrain = [formatdf(xx,ids) for xx in xxtrained]\n",
    "        xxtrain = torch.cat(xxtrain,axis=1).to(device)\n",
    "        o1 = model(xxtrain,position=0,use_saved_memory= (not train))\n",
    "        decision1_imitation = o1[:,3]\n",
    "        decision1_opt = o1[:,0]\n",
    "    \n",
    "        imitation_loss1 = bce(decision1_imitation,ytrain[:,0])\n",
    "        imitation_loss1 = torch.mul(imitation_loss1,imitation_weights[0])\n",
    "        opt_loss1 = bce(decision1_opt,y_opt[:,0])\n",
    "        opt_loss1 = torch.mul(opt_loss1,opt_weights[0])\n",
    "        \n",
    "        x1_imitation = [baseline, get_dlt(1),get_dlt(0),get_pd(1),get_nd(1),get_cc(1),get_mod(1)]\n",
    "        x1_imitation = [formatdf(xx1,ids) for xx1 in x1_imitation]\n",
    "        x1_imitation = torch.cat(x1_imitation,axis=1).to(device)\n",
    "        \n",
    "        o2 = model(x1_imitation,position=1,use_saved_memory= (not train))\n",
    "            \n",
    "        decision2_imitation = o2[:,4]\n",
    "            \n",
    "        imitation_loss2 =  bce(decision2_imitation,ytrain[:,1])\n",
    "        imitation_loss2 = torch.mul(imitation_loss2,imitation_weights[1])\n",
    "        \n",
    "        \n",
    "        x2_imitation = [baseline, get_dlt(1),get_dlt(2),get_pd(2),get_nd(2),get_cc(2),get_mod(2)]\n",
    "        \n",
    "        x2_imitation = [formatdf(xx2,ids) for xx2 in x2_imitation]\n",
    "        x2_imitation = torch.cat(x2_imitation,axis=1).to(device)\n",
    "        \n",
    "        \n",
    "        o3 = model(x2_imitation,position=2,use_saved_memory= (not train))\n",
    "        \n",
    "        decision3_imitation = o3[:,5]\n",
    "        \n",
    "        imitation_loss3 = bce(decision3_imitation,ytrain[:,2])\n",
    "        imitation_loss3 = torch.mul(imitation_loss3,imitation_weights[2])\n",
    "        \n",
    "        opt_input2 = [\n",
    "            formatdf(baseline,ids), \n",
    "            transition_dict['dlt1'],\n",
    "            formatdf(get_dlt(0),ids),\n",
    "            transition_dict['pd1'],\n",
    "            transition_dict['nd1'], \n",
    "            formatdf(get_cc(0),ids),\n",
    "            transition_dict['mod']\n",
    "                 ]\n",
    "        opt_input2 = [o.to(device) for o in opt_input2]\n",
    "\n",
    "        opt_input2 = torch.cat(opt_input2,axis=1).to(device)\n",
    "        decision2_opt = model(opt_input2,position=1,use_saved_memory= (not train))[:,1]\n",
    "        \n",
    "        opt_loss2 = bce(decision2_opt,y_opt[:,1])\n",
    "        opt_loss2 = torch.mul(opt_loss2,opt_weights[1])\n",
    "        \n",
    "        opt_input3 = [\n",
    "            formatdf(baseline,ids),\n",
    "            transition_dict['dlt1'],\n",
    "            transition_dict['dlt2'],\n",
    "            transition_dict['pd2'],\n",
    "            transition_dict['nd2'],\n",
    "            transition_dict['cc'],\n",
    "            transition_dict['mod'],\n",
    "        ]\n",
    "        opt_input3 = [o.to(device) for o in opt_input3]\n",
    "        opt_input3 = torch.cat(opt_input3,axis=1).to(device)\n",
    "        decision3_opt = model(opt_input3,position=2,use_saved_memory= (not train))[:,2]\n",
    "        \n",
    "        opt_loss3 = bce(decision3_opt,y_opt[:,2])\n",
    "        opt_loss3 = torch.mul(opt_loss3,opt_weights[2])\n",
    "        \n",
    "        iloss = torch.add(torch.add(imitation_loss1,imitation_loss2),imitation_loss3)\n",
    "        iloss = torch.mul(iloss,imitation_weight)\n",
    "        \n",
    "        reward_loss = torch.add(torch.add(opt_loss1,opt_loss2),opt_loss3)\n",
    "        reward_loss =torch.mul(reward_loss,reward_weight)\n",
    "        \n",
    "        loss = torch.add(iloss,reward_loss)\n",
    "        \n",
    "        imitation_tloss = torch.FloatTensor([0]).to(device)\n",
    "        opt_tloss = torch.FloatTensor([0]).to(device)\n",
    "        n_rows = xxtrain.shape[0]\n",
    "        if reward_triplet_weight + imitation_triplet_weight > 0.0001:\n",
    "            for i in range(n_rows):\n",
    "                \n",
    "                if imitation_triplet_weight > .0001:\n",
    "                    imitation_tloss += get_tloss(i,0,ytrain,xxtrain,True)\n",
    "                    imitation_tloss += get_tloss(i,1,ytrain,x1_imitation,True)\n",
    "                    imitation_tloss += get_tloss(i,2,ytrain,x2_imitation,True)\n",
    "                if reward_triplet_weight > .0001:\n",
    "                    opt_tloss += get_tloss(i,0,y_opt,xxtrain,False)\n",
    "                    opt_tloss += get_tloss(i,1,y_opt,opt_input2,False)\n",
    "                    opt_tloss += get_tloss(i,2,y_opt,opt_input3,False)\n",
    "            loss += torch.mul(imitation_tloss[0],imitation_triplet_weight/n_rows)\n",
    "            loss += torch.mul(opt_tloss[0],reward_triplet_weight/n_rows)\n",
    "        losses = [iloss,reward_loss,imitation_tloss*imitation_triplet_weight/n_rows,opt_tloss*reward_triplet_weight/n_rows]\n",
    "        \n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            return losses\n",
    "        else:\n",
    "            scores = []\n",
    "            distributions = [decision1_opt.mean().item(),decision2_opt.mean().item(),decision3_opt.mean().item()]\n",
    "            imitation = [decision1_imitation,decision2_imitation,decision3_imitation]\n",
    "            optimal = [decision1_opt,decision2_opt,decision3_opt]\n",
    "            for i,decision_im in enumerate(imitation):\n",
    "                deci = decision_im.cpu().detach().numpy()\n",
    "                deci0 = (deci > .5).astype(int)\n",
    "                iout = ytrain[:,i].cpu().detach().numpy()\n",
    "                acci = accuracy_score(iout,deci0)\n",
    "                try:\n",
    "                    auci = roc_auc_score(iout,deci)\n",
    "                except:\n",
    "                    auci = -1\n",
    "                \n",
    "                deco = optimal[i].cpu().detach().numpy()\n",
    "                deci0 = (deco > .5).astype(int)\n",
    "                oout = y_opt[:,i].cpu().detach().numpy()\n",
    "                acco = accuracy_score(oout,deci0)\n",
    "                try:\n",
    "                    auco = roc_auc_score(oout,deco)\n",
    "                except:\n",
    "                    auco=-1\n",
    "                scores.append({'decision': i,'optimal_auc': auco,'imitation_auc': auci,'optimal_acc': acco,'imitation_acc': acci})\n",
    "            return losses, scores, distributions\n",
    "        \n",
    "    best_val_loss = torch.tensor(1000000000.0)\n",
    "    steps_since_improvement = 0\n",
    "    best_val_score = {}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        losses = step(True)\n",
    "        val_losses,val_metrics,val_distributions = step(False)\n",
    "        vl = val_losses[0] + val_losses[1]\n",
    "        for vm in val_metrics:\n",
    "            vl += (-((vm['optimal_auc']*reward_weight) + (vm['imitation_auc']*imitation_weight)))/10\n",
    "        if verbose:\n",
    "            print('______epoch',str(epoch),'_____')\n",
    "            print('val reward',val_losses[1].item())\n",
    "            print('imitation reward', val_losses[0].item())\n",
    "            print('distance losses',val_losses[2].item(),val_losses[-1].item())\n",
    "            print('distributions',val_distributions)\n",
    "            print(val_metrics)\n",
    "        if vl < best_val_loss:\n",
    "            best_val_loss = vl\n",
    "            best_val_score = val_metrics\n",
    "            best_val_distributions = val_distributions\n",
    "            steps_since_improvement = 0\n",
    "            torch.save(model.state_dict(),save_file)\n",
    "        else:\n",
    "            steps_since_improvement += 1\n",
    "        if steps_since_improvement > patience:\n",
    "            break\n",
    "    print('++++++++++Final+++++++++++')\n",
    "    print('best',best_val_loss)\n",
    "    print(best_val_score)\n",
    "    model.load_state_dict(torch.load(save_file))\n",
    "    model.eval()\n",
    "    return model, best_val_score, best_val_loss, best_val_distributions\n",
    "\n",
    "from Models import *\n",
    "args = {\n",
    "    'hidden_layers': [1000], \n",
    "    'opt_layer_size': 20, \n",
    "    'imitation_layer_size': 20, \n",
    "    'dropout': 0.25, \n",
    "    'input_dropout': 0.25, \n",
    "    'shufflecol_chance': 0.5\n",
    "}\n",
    "\n",
    "#1.8424\n",
    "decision_model, decision_score, decision_loss, _ = train_decision_model_triplet(\n",
    "    model1,model2,model3,smodel3,\n",
    "    lr=.01,\n",
    "    imitation_weight=1,\n",
    "    reward_weight=1,\n",
    "    patience=10,\n",
    "    imitation_triplet_weight=1,\n",
    "    reward_triplet_weight =1,\n",
    "    verbose=True,\n",
    "    weights=[0,0,0,0], #realtive weight of survival, feeding tube, aspiration, andl lrc\n",
    "    tweights=[2,.1,0,0], #weight for OS, LRC, FDM, and event (any + FT or AS at 6m) as time to event in weeks\n",
    "    use_attention=True,\n",
    "    **args)\n",
    "decision_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73f37f8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decision</th>\n",
       "      <th>optimal_auc</th>\n",
       "      <th>imitation_auc</th>\n",
       "      <th>optimal_acc</th>\n",
       "      <th>imitation_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.895683</td>\n",
       "      <td>0.620706</td>\n",
       "      <td>0.945578</td>\n",
       "      <td>0.891156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.757065</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.775510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.828353</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.829932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   decision  optimal_auc  imitation_auc  optimal_acc  imitation_acc\n",
       "0         0     0.895683       0.620706     0.945578       0.891156\n",
       "1         1    -1.000000       0.757065     1.000000       0.775510\n",
       "2         2    -1.000000       0.828353     1.000000       0.829932"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_model.set_device('cpu')\n",
    "torch.save(decision_model,'../resources/decision_model.pt')\n",
    "pd.DataFrame(decision_score).to_csv('../results/policy_model_score.csv')\n",
    "pd.DataFrame(decision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b25922a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([65.,  0.,  0.]) 389\n",
      "tensor([26.,  0.,  0.]) 147\n",
      "torch.Size([3, 536, 87])\n",
      "tensor([0.1769, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 0 _____\n",
      "val reward 0.8488877415657043\n",
      "imitation reward 2.0121195316314697\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.008012257516384125, 0.0033295094035565853, 0.0008333181613124907]\n",
      "[{'decision': 0, 'optimal_auc': 0.6433566433566433, 'imitation_auc': 0.5854007633587787, 'optimal_acc': 0.8231292517006803, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.6309782608695651, 'optimal_acc': 1.0, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7094723458359822, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.1769, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 1 _____\n",
      "val reward 0.5268533825874329\n",
      "imitation reward 1.3480204343795776\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.089411661028862, 0.0002860285749193281, 5.930369297857396e-05]\n",
      "[{'decision': 0, 'optimal_auc': 0.3750794659885569, 'imitation_auc': 0.46564885496183206, 'optimal_acc': 0.8231292517006803, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.6630434782608696, 'optimal_acc': 1.0, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.4904640813731723, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.1769, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 2 _____\n",
      "val reward 0.6665776968002319\n",
      "imitation reward 1.3214225769042969\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.4699949324131012, 5.0018814363284037e-05, 1.089552097255364e-05]\n",
      "[{'decision': 0, 'optimal_auc': 0.42339478703115063, 'imitation_auc': 0.4179389312977099, 'optimal_acc': 0.5986394557823129, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.725, 'optimal_acc': 1.0, 'imitation_acc': 0.7414965986394558}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.694850603941513, 'optimal_acc': 1.0, 'imitation_acc': 0.8027210884353742}]\n",
      "tensor([0.1769, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 3 _____\n",
      "val reward 0.49326831102371216\n",
      "imitation reward 1.2191617488861084\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.08352340757846832, 7.89203022577567e-06, 8.82338667906879e-07]\n",
      "[{'decision': 0, 'optimal_auc': 0.6983471074380165, 'imitation_auc': 0.5052480916030535, 'optimal_acc': 0.8231292517006803, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7255434782608695, 'optimal_acc': 1.0, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7809917355371901, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.1769, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 4 _____\n",
      "val reward 0.5890281200408936\n",
      "imitation reward 1.2426385879516602\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.03623984381556511, 2.4119049157889094e-06, 2.0991795679492498e-07]\n",
      "[{'decision': 0, 'optimal_auc': 0.777177368086459, 'imitation_auc': 0.5715648854961831, 'optimal_acc': 0.8231292517006803, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7285326086956522, 'optimal_acc': 1.0, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7739987285441831, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.1769, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 5 _____\n",
      "val reward 0.5040984153747559\n",
      "imitation reward 1.086816668510437\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.07210345566272736, 2.2459880710812286e-06, 2.2463939330918947e-07]\n",
      "[{'decision': 0, 'optimal_auc': 0.7072472981563891, 'imitation_auc': 0.6068702290076335, 'optimal_acc': 0.8231292517006803, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7258152173913044, 'optimal_acc': 1.0, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7657342657342657, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.1769, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 6 _____\n",
      "val reward 0.4693891406059265\n",
      "imitation reward 1.0637032985687256\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.2512543797492981, 2.2469164377980633e-06, 3.103539825133339e-07]\n",
      "[{'decision': 0, 'optimal_auc': 0.6401780038143675, 'imitation_auc': 0.6245229007633588, 'optimal_acc': 0.8231292517006803, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7228260869565217, 'optimal_acc': 1.0, 'imitation_acc': 0.7551020408163265}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7689129052765417, 'optimal_acc': 1.0, 'imitation_acc': 0.8095238095238095}]\n",
      "tensor([0.1769, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 7 _____\n",
      "val reward 0.5457435846328735\n",
      "imitation reward 1.061959147453308\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.36955592036247253, 1.8797683196680737e-06, 2.96375446851016e-07]\n",
      "[{'decision': 0, 'optimal_auc': 0.6331849968213604, 'imitation_auc': 0.6350190839694657, 'optimal_acc': 0.8231292517006803, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7203804347826087, 'optimal_acc': 1.0, 'imitation_acc': 0.7551020408163265}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7800381436745073, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.1769, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 8 _____\n",
      "val reward 0.4346011281013489\n",
      "imitation reward 1.0758790969848633\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.15597492456436157, 1.3254591522127157e-06, 1.6659548407460534e-07]\n",
      "[{'decision': 0, 'optimal_auc': 0.7148760330578513, 'imitation_auc': 0.6350190839694656, 'optimal_acc': 0.8231292517006803, 'imitation_acc': 0.8775510204081632}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7157608695652173, 'optimal_acc': 1.0, 'imitation_acc': 0.7619047619047619}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7781309599491417, 'optimal_acc': 1.0, 'imitation_acc': 0.8027210884353742}]\n",
      "tensor([0.1769, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 9 _____\n",
      "val reward 0.4634825587272644\n",
      "imitation reward 1.1050522327423096\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.07790891081094742, 7.032365374470828e-07, 7.071369623190549e-08]\n",
      "[{'decision': 0, 'optimal_auc': 0.7774952320406866, 'imitation_auc': 0.6354961832061069, 'optimal_acc': 0.8231292517006803, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7119565217391305, 'optimal_acc': 1.0, 'imitation_acc': 0.7619047619047619}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.78099173553719, 'optimal_acc': 1.0, 'imitation_acc': 0.8027210884353742}]\n",
      "tensor([0.1769, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 10 _____\n",
      "val reward 0.4523351192474365\n",
      "imitation reward 1.1534987688064575\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.0729348286986351, 5.04846013882343e-07, 5.2933476268890445e-08]\n",
      "[{'decision': 0, 'optimal_auc': 0.8083280356007629, 'imitation_auc': 0.6378816793893131, 'optimal_acc': 0.8231292517006803, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7152173913043478, 'optimal_acc': 1.0, 'imitation_acc': 0.7619047619047619}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.786395422759059, 'optimal_acc': 1.0, 'imitation_acc': 0.8095238095238095}]\n",
      "tensor([0.1769, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 11 _____\n",
      "val reward 0.39409953355789185\n",
      "imitation reward 1.1324260234832764\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.11226455867290497, 5.720874582948454e-07, 8.148151664499892e-08]\n",
      "[{'decision': 0, 'optimal_auc': 0.8178639542275905, 'imitation_auc': 0.6345419847328244, 'optimal_acc': 0.8231292517006803, 'imitation_acc': 0.8843537414965986}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7135869565217392, 'optimal_acc': 1.0, 'imitation_acc': 0.7619047619047619}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7873490146217419, 'optimal_acc': 1.0, 'imitation_acc': 0.7891156462585034}]\n",
      "tensor([0.1769, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 12 _____\n",
      "val reward 0.3674570322036743\n",
      "imitation reward 1.1490429639816284\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.2088186889886856, 5.078309186501428e-07, 9.029338343680138e-08]\n",
      "[{'decision': 0, 'optimal_auc': 0.8258105530832804, 'imitation_auc': 0.622614503816794, 'optimal_acc': 0.8571428571428571, 'imitation_acc': 0.8775510204081632}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7108695652173913, 'optimal_acc': 1.0, 'imitation_acc': 0.7619047619047619}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7813095994914177, 'optimal_acc': 1.0, 'imitation_acc': 0.7891156462585034}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1769, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 13 _____\n",
      "val reward 0.39634063839912415\n",
      "imitation reward 1.1749008893966675\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.2865397334098816, 3.9219514746946516e-07, 7.78546151991577e-08]\n",
      "[{'decision': 0, 'optimal_auc': 0.8302606484424666, 'imitation_auc': 0.6192748091603053, 'optimal_acc': 0.8163265306122449, 'imitation_acc': 0.8775510204081632}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7133152173913044, 'optimal_acc': 1.0, 'imitation_acc': 0.7551020408163265}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7835346471710107, 'optimal_acc': 1.0, 'imitation_acc': 0.8027210884353742}]\n",
      "tensor([0.1769, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 14 _____\n",
      "val reward 0.3510810136795044\n",
      "imitation reward 1.1551833152770996\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.21866436302661896, 3.9559540709888097e-07, 8.445369559240135e-08]\n",
      "[{'decision': 0, 'optimal_auc': 0.8445645263827082, 'imitation_auc': 0.6230916030534351, 'optimal_acc': 0.8299319727891157, 'imitation_acc': 0.8775510204081632}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.714945652173913, 'optimal_acc': 1.0, 'imitation_acc': 0.7551020408163265}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7803560076287348, 'optimal_acc': 1.0, 'imitation_acc': 0.7891156462585034}]\n",
      "tensor([0.1769, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 15 _____\n",
      "val reward 0.34577131271362305\n",
      "imitation reward 1.1318604946136475\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.14887091517448425, 4.0807313439472637e-07, 8.980054388985081e-08]\n",
      "[{'decision': 0, 'optimal_auc': 0.8547361729179911, 'imitation_auc': 0.6278625954198472, 'optimal_acc': 0.8435374149659864, 'imitation_acc': 0.8775510204081632}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7078804347826086, 'optimal_acc': 1.0, 'imitation_acc': 0.7482993197278912}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.786395422759059, 'optimal_acc': 1.0, 'imitation_acc': 0.7891156462585034}]\n",
      "tensor([0.1769, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 16 _____\n",
      "val reward 0.3611208200454712\n",
      "imitation reward 1.1296212673187256\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.12167441099882126, 4.2704763814072066e-07, 9.427608915757446e-08]\n",
      "[{'decision': 0, 'optimal_auc': 0.8575969485060394, 'imitation_auc': 0.6321564885496184, 'optimal_acc': 0.8435374149659864, 'imitation_acc': 0.8843537414965986}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7078804347826086, 'optimal_acc': 1.0, 'imitation_acc': 0.7619047619047619}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7892561983471074, 'optimal_acc': 1.0, 'imitation_acc': 0.7959183673469388}]\n",
      "tensor([0.1769, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 17 _____\n",
      "val reward 0.3505924344062805\n",
      "imitation reward 1.1258440017700195\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.13247229158878326, 5.589775469161395e-07, 1.2114168157495442e-07]\n",
      "[{'decision': 0, 'optimal_auc': 0.8668150031786396, 'imitation_auc': 0.6354961832061069, 'optimal_acc': 0.8367346938775511, 'imitation_acc': 0.8843537414965986}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7125, 'optimal_acc': 1.0, 'imitation_acc': 0.7687074829931972}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.794977749523204, 'optimal_acc': 1.0, 'imitation_acc': 0.7959183673469388}]\n",
      "tensor([0.1769, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 18 _____\n",
      "val reward 0.33159565925598145\n",
      "imitation reward 1.119023323059082\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.17437529563903809, 6.624327966164856e-07, 1.4114888813310245e-07]\n",
      "[{'decision': 0, 'optimal_auc': 0.8760330578512397, 'imitation_auc': 0.6331106870229007, 'optimal_acc': 0.8367346938775511, 'imitation_acc': 0.8775510204081632}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7230978260869565, 'optimal_acc': 1.0, 'imitation_acc': 0.7687074829931972}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7987921169739352, 'optimal_acc': 1.0, 'imitation_acc': 0.7891156462585034}]\n",
      "tensor([0.1769, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 19 _____\n",
      "val reward 0.33691224455833435\n",
      "imitation reward 1.103203296661377\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.22291915118694305, 6.092549256209168e-07, 1.1793698462270186e-07]\n",
      "[{'decision': 0, 'optimal_auc': 0.8795295613477432, 'imitation_auc': 0.6312022900763359, 'optimal_acc': 0.8299319727891157, 'imitation_acc': 0.8843537414965986}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7345108695652174, 'optimal_acc': 1.0, 'imitation_acc': 0.7619047619047619}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7940241576605213, 'optimal_acc': 1.0, 'imitation_acc': 0.7959183673469388}]\n",
      "tensor([0.1769, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 20 _____\n",
      "val reward 0.3416356146335602\n",
      "imitation reward 1.0967810153961182\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.22979207336902618, 3.852538270621153e-07, 6.444124522886341e-08]\n",
      "[{'decision': 0, 'optimal_auc': 0.8792116973935156, 'imitation_auc': 0.6397900763358779, 'optimal_acc': 0.8231292517006803, 'imitation_acc': 0.8775510204081632}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7483695652173913, 'optimal_acc': 1.0, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8010171646535283, 'optimal_acc': 1.0, 'imitation_acc': 0.7959183673469388}]\n",
      "tensor([0.1769, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 21 _____\n",
      "val reward 0.33513355255126953\n",
      "imitation reward 1.1226500272750854\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.18558578193187714, 1.2468036914015102e-07, 1.8605222251721898e-08]\n",
      "[{'decision': 0, 'optimal_auc': 0.87730451366815, 'imitation_auc': 0.6479007633587786, 'optimal_acc': 0.8435374149659864, 'imitation_acc': 0.8775510204081632}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7527173913043478, 'optimal_acc': 1.0, 'imitation_acc': 0.7687074829931972}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8029243483788938, 'optimal_acc': 1.0, 'imitation_acc': 0.8027210884353742}]\n",
      "tensor([0.1769, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 22 _____\n",
      "val reward 0.34687554836273193\n",
      "imitation reward 1.13468337059021\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.1460246592760086, 8.245372384863003e-08, 1.197243193473696e-08]\n",
      "[{'decision': 0, 'optimal_auc': 0.8839796567069295, 'imitation_auc': 0.6531488549618321, 'optimal_acc': 0.8503401360544217, 'imitation_acc': 0.8843537414965986}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7581521739130435, 'optimal_acc': 1.0, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8048315321042594, 'optimal_acc': 1.0, 'imitation_acc': 0.7959183673469388}]\n",
      "tensor([0.1769, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 23 _____\n",
      "val reward 0.34521034359931946\n",
      "imitation reward 1.1400524377822876\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.1461578756570816, 9.028848069192463e-08, 1.367429813825538e-08]\n",
      "[{'decision': 0, 'optimal_auc': 0.8814367450731088, 'imitation_auc': 0.6521946564885496, 'optimal_acc': 0.8503401360544217, 'imitation_acc': 0.8707482993197279}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7644021739130434, 'optimal_acc': 1.0, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.805149396058487, 'optimal_acc': 1.0, 'imitation_acc': 0.8163265306122449}]\n",
      "tensor([0.1769, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 24 _____\n",
      "val reward 0.33397865295410156\n",
      "imitation reward 1.1372673511505127\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.17412224411964417, 1.3668410758782557e-07, 2.0207794548809943e-08]\n",
      "[{'decision': 0, 'optimal_auc': 0.8769866497139225, 'imitation_auc': 0.6383587786259541, 'optimal_acc': 0.8775510204081632, 'imitation_acc': 0.8639455782312925}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7690217391304347, 'optimal_acc': 1.0, 'imitation_acc': 0.7551020408163265}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8038779402415765, 'optimal_acc': 1.0, 'imitation_acc': 0.7959183673469388}]\n",
      "tensor([0.1769, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 25 _____\n",
      "val reward 0.3424661159515381\n",
      "imitation reward 1.1546518802642822\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.2155582159757614, 1.8001796320277208e-07, 2.2700580970536066e-08]\n",
      "[{'decision': 0, 'optimal_auc': 0.8779402415766051, 'imitation_auc': 0.6254770992366412, 'optimal_acc': 0.8435374149659864, 'imitation_acc': 0.8639455782312925}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7684782608695652, 'optimal_acc': 1.0, 'imitation_acc': 0.7551020408163265}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8013350286077559, 'optimal_acc': 1.0, 'imitation_acc': 0.7959183673469388}]\n",
      "tensor([0.1769, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 26 _____\n",
      "val reward 0.34261131286621094\n",
      "imitation reward 1.1742727756500244\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.21927593648433685, 1.6793252655133983e-07, 1.988354725313002e-08]\n",
      "[{'decision': 0, 'optimal_auc': 0.8814367450731088, 'imitation_auc': 0.618797709923664, 'optimal_acc': 0.8503401360544217, 'imitation_acc': 0.8639455782312925}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7627717391304347, 'optimal_acc': 1.0, 'imitation_acc': 0.7687074829931972}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.789574062301335, 'optimal_acc': 1.0, 'imitation_acc': 0.7959183673469388}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1769, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 27 _____\n",
      "val reward 0.3324897885322571\n",
      "imitation reward 1.2066798210144043\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.19053611159324646, 1.1477845163199163e-07, 1.4610249898794336e-08]\n",
      "[{'decision': 0, 'optimal_auc': 0.8855689764780674, 'imitation_auc': 0.623091603053435, 'optimal_acc': 0.8639455782312925, 'imitation_acc': 0.8571428571428571}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.752445652173913, 'optimal_acc': 1.0, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7819453273998728, 'optimal_acc': 1.0, 'imitation_acc': 0.8163265306122449}]\n",
      "tensor([0.1769, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 28 _____\n",
      "val reward 0.336943656206131\n",
      "imitation reward 1.2627991437911987\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.15760163962841034, 6.795051632479954e-08, 1.0446188802859524e-08]\n",
      "[{'decision': 0, 'optimal_auc': 0.8906547997457088, 'imitation_auc': 0.6245229007633587, 'optimal_acc': 0.8775510204081632, 'imitation_acc': 0.8639455782312925}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7432065217391305, 'optimal_acc': 1.0, 'imitation_acc': 0.7687074829931972}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7838525111252383, 'optimal_acc': 1.0, 'imitation_acc': 0.8163265306122449}]\n",
      "tensor([0.1769, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 29 _____\n",
      "val reward 0.3386375606060028\n",
      "imitation reward 1.3285082578659058\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.15478159487247467, 6.141621611277515e-08, 1.0419610951828417e-08]\n",
      "[{'decision': 0, 'optimal_auc': 0.8935155753337571, 'imitation_auc': 0.6221374045801527, 'optimal_acc': 0.8707482993197279, 'imitation_acc': 0.8775510204081632}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7377717391304348, 'optimal_acc': 1.0, 'imitation_acc': 0.7687074829931972}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7886204704386522, 'optimal_acc': 1.0, 'imitation_acc': 0.8095238095238095}]\n",
      "tensor([0.1769, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 30 _____\n",
      "val reward 0.3361169695854187\n",
      "imitation reward 1.3267285823822021\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.1680002212524414, 9.388676858179679e-08, 1.4433332751195849e-08]\n",
      "[{'decision': 0, 'optimal_auc': 0.8900190718372536, 'imitation_auc': 0.6211832061068702, 'optimal_acc': 0.8571428571428571, 'imitation_acc': 0.8571428571428571}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7361413043478261, 'optimal_acc': 1.0, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7816274634456453, 'optimal_acc': 1.0, 'imitation_acc': 0.7959183673469388}]\n",
      "tensor([0.1769, 0.0000, 0.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 31 _____\n",
      "val reward 0.34805411100387573\n",
      "imitation reward 1.371893048286438\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.21076877415180206, 9.60619175316424e-08, 1.37953204415453e-08]\n",
      "[{'decision': 0, 'optimal_auc': 0.8849332485696122, 'imitation_auc': 0.6111641221374046, 'optimal_acc': 0.8571428571428571, 'imitation_acc': 0.8435374149659864}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7399456521739131, 'optimal_acc': 1.0, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7848061029879212, 'optimal_acc': 1.0, 'imitation_acc': 0.8027210884353742}]\n",
      "++++++++++Final+++++++++++\n",
      "best tensor(1.3316, grad_fn=<AddBackward0>)\n",
      "[{'decision': 0, 'optimal_auc': 0.8792116973935156, 'imitation_auc': 0.6397900763358779, 'optimal_acc': 0.8231292517006803, 'imitation_acc': 0.8775510204081632}, {'decision': 1, 'optimal_auc': -1, 'imitation_auc': 0.7483695652173913, 'optimal_acc': 1.0, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8010171646535283, 'optimal_acc': 1.0, 'imitation_acc': 0.7959183673469388}]\n"
     ]
    }
   ],
   "source": [
    "#1.8424\n",
    "decision_model2, decision_score2, decision_loss2, _ = train_decision_model_triplet(\n",
    "    model1,model2,model3,smodel3,\n",
    "    lr=.01,\n",
    "    imitation_weight=1,\n",
    "    reward_weight=1,\n",
    "    patience=10,\n",
    "    imitation_triplet_weight=0,\n",
    "    reward_triplet_weight =0,\n",
    "    verbose=True,\n",
    "    weights=[0,0,0,0], #realtive weight of survival, feeding tube, aspiration, andl lrc\n",
    "    tweights=[1,1,0,0], #weight for OS, LRC, FDM, and event (any + FT or AS at 6m) as time to event in weeks\n",
    "    use_attention=True,\n",
    "    **args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a7faad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decision_model(\n",
    "    tmodel1,\n",
    "    tmodel2,\n",
    "    tmodel3,\n",
    "    smodel3,\n",
    "    use_default_split=True,\n",
    "    use_bagging_split=False,\n",
    "    lr=.0001,\n",
    "    epochs=10000,\n",
    "    patience=50,\n",
    "    weights=[0,.5,.5,0], #realtive weight of survival, feeding tube, aspiration, andl lrc\n",
    "    tweights=[1,1,1,0]\n",
    "    imitation_weights=[.5,1,1],#weights of imitation decisions, because ic overtrains too quickly\n",
    "    imitation_weight=0.1,\n",
    "    shufflecol_chance = 0.1,\n",
    "    reward_weight=1,\n",
    "    imitation_triplet_weight=0,\n",
    "    reward_triplet_weight = 0,\n",
    "    split=.7,\n",
    "    resample_training=False,\n",
    "    save_path='../data/models/',\n",
    "    file_suffix='',\n",
    "    use_gpu=True,\n",
    "    use_attention=True,\n",
    "    verbose=True,\n",
    "    threshold_decisions=True,#convert decisiosn to binary in simulation, usually breaks it\n",
    "    use_smote=False,\n",
    "    validate_with_memory=True,\n",
    "    **model_kwargs):\n",
    "    #outdated method of doing stuff, haven't updated with new loss functions idk\n",
    "    tmodel1.eval()\n",
    "    tmodel2.eval()\n",
    "    tmodel3.eval()\n",
    "    smodel3.eval()\n",
    "    train_ids, test_ids = get_tt_split(use_default_split=use_default_split,use_bagging_split=use_bagging_split,resample_training=resample_training)\n",
    "    true_ids = train_ids + test_ids #for saving memory without upsampling\n",
    "    if use_smote:\n",
    "        dataset = DTDataset(use_smote=True,smote_ids = train_ids)\n",
    "        train_ids = [i for i in dataset.processed_df.index.values if i not in test_ids]\n",
    "    else:\n",
    "        dataset = DTDataset()\n",
    "    data = dataset.processed_df.copy()\n",
    "    \n",
    "    def get_dlt(state):\n",
    "        if state == 2:\n",
    "            return data[Const.dlt2].copy()\n",
    "        d = data[Const.dlt1].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_pd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.primary_disease_states2].copy()\n",
    "        d = data[Const.primary_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_nd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.nodal_disease_states2].copy()\n",
    "        d = data[Const.nodal_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_cc(state):\n",
    "        res = data[Const.ccs].copy()\n",
    "        if state == 1:\n",
    "            res.values[:,:] = np.zeros(res.values.shape)\n",
    "        return res\n",
    "    \n",
    "    def get_mod(state):\n",
    "        res = data[Const.modifications].copy()\n",
    "        #this should have an ic condition but we don't use it anumore anywa\n",
    "        return res\n",
    "        \n",
    "    outcomedf = data[Const.outcomes]\n",
    "    baseline = dataset.get_state('baseline')\n",
    "    \n",
    "    def formatdf(d,dids=train_ids):\n",
    "        d = df_to_torch(d.loc[dids]).to(model.get_device())\n",
    "        return d\n",
    "    \n",
    "    def makegrad(v):\n",
    "        if not v.requires_grad:\n",
    "            v.requires_grad=True\n",
    "        return v\n",
    "    \n",
    "    if use_attention:\n",
    "        model = DecisionAttentionModel(baseline.shape[1],**model_kwargs)\n",
    "    else:\n",
    "        model_kwargs = {k:v for k,v in model_kwargs.items() if 'attention' not in k and 'embed' not in k}\n",
    "        model = DecisionModel(baseline.shape[1],**model_kwargs)\n",
    "\n",
    "    device = 'cpu'\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "        \n",
    "    model.set_device(device)\n",
    "\n",
    "    tmodel1.set_device(device)\n",
    "    tmodel2.set_device(device)\n",
    "    tmodel3.set_device(device)\n",
    "    smodel3.set_device(device)\n",
    "    \n",
    "    hashcode = str(hash(','.join([str(i) for i in train_ids])))\n",
    "    \n",
    "    save_file = save_path + 'model_' + model.identifier +'_hash' + hashcode + file_suffix + '.tar'\n",
    "    model.fit_normalizer(df_to_torch(baseline.loc[train_ids]).to(model.get_device()))\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "    \n",
    "    mse = torch.nn.MSELoss()\n",
    "    bce = torch.nn.BCELoss()\n",
    "    device = model.get_device()\n",
    "    def compare_decisions(d1,d2,d3,ids):\n",
    "#         ypred = np.concatenate([dd.cpu().detach().numpy().reshape(-1,1) for dd in [d1,d2,d3]],axis=1)\n",
    "        ytrue = df_to_torch(outcomedf.loc[ids])\n",
    "        dloss = bce(d1.view(-1),ytrue[:,0])\n",
    "        dloss += bce(d2.view(-1),ytrue[:,1])\n",
    "        dloss += bce(d3,view(-1),ytrue[:,2])\n",
    "        return dloss\n",
    "        \n",
    "    def remove_decisions(df):\n",
    "        cols = [c for c in df.columns if c not in Const.decisions ]\n",
    "        ddf = df[cols]\n",
    "        return ddf\n",
    "    \n",
    "    makeinput = lambda step,dids: df_to_torch(remove_decisions(dataset.get_input_state(step=step,ids=dids))).to(device)\n",
    "    thresh = lambda x: torch.sigmoid(100000000*(x - .5))\n",
    "\n",
    "    optimal_train,transitions_train = calc_optimal_decisions(dataset,train_ids,tmodel1,tmodel2,tmodel3,smodel3,\n",
    "                                           weights=weights,tweights=tweights,\n",
    "                                          )\n",
    "    optimal_test,transitions_test = calc_optimal_decisions(dataset,test_ids,tmodel1,tmodel2,tmodel3,smodel3,\n",
    "                                          weights=weights,tweights=tweights,\n",
    "                                         )\n",
    "    optimal_train = optimal_train.to(model.get_device())\n",
    "    optimal_test = optimal_test.to(model.get_device())\n",
    "    \n",
    "    #save the inputs from the whole dataset for future reference\n",
    "    if use_attention:\n",
    "        full_data = []\n",
    "        for mstep in [0,1,2]:\n",
    "            full_data_step = [baseline, get_dlt(min(mstep,1)),\n",
    "                         get_dlt(mstep),get_pd(mstep),get_nd(mstep),get_cc(mstep),get_mod(mstep)]\n",
    "            full_data_step = torch.cat([formatdf(fd,true_ids) for fd in full_data_step],axis=1)\n",
    "            full_data.append(full_data_step)\n",
    "        full_data = torch.stack(full_data)\n",
    "        model.save_memory(full_data.to(device))\n",
    "        print(full_data.shape)\n",
    "        \n",
    "    randchoice = lambda x: x[torch.randint(len(x),(1,))[0]]\n",
    "    tloss_func = torch.nn.TripletMarginLoss()\n",
    "    def get_tloss(row,step,yt,x,imitation=True):\n",
    "        if yt[:,step].std() < .001:\n",
    "            return torch.tensor([0]).to(device)\n",
    "        positive_idx= torch.nonzero(yt[:,step] == yt[row,step])\n",
    "        positive_idx = torch.stack([ii for ii in positive_idx if ii != row]).view(-1)\n",
    "        negative_idx = torch.tensor([ii for ii in range(x.shape[0]) if ii not in positive_idx and ii != row])\n",
    "        if len(positive_idx) < 1 or len(negative_idx) < 1:\n",
    "            print('no losses','n positive',len(positive_idx),'n negative',len(negative_idx),'yt',yt[row,step],'row',row,'step',step,'imitation',imitation,end='\\r')\n",
    "            return torch.tensor([0]).to(device)\n",
    "        positive = x[randchoice(positive_idx)]\n",
    "        negative = x[randchoice(negative_idx)]\n",
    "        anchor = x[row]\n",
    "        if use_attention:\n",
    "            [anchor_embedding,pos_embedding,neg_embedding] = [model.get_embedding(xx.view(1,-1),position=step,use_saved_memory=True) for xx in [anchor,positive,negative]]\n",
    "        else:    \n",
    "            [anchor_embedding,pos_embedding,neg_embedding] = [model.get_embedding(xx.view(1,-1),position=step,concatenate=False)[int(imitation)] for xx in [anchor,positive,negative]]\n",
    "        tloss = tloss_func(anchor_embedding,pos_embedding,neg_embedding)\n",
    "        return tloss\n",
    "    \n",
    "    def step(train=True):\n",
    "        if train:\n",
    "            model.train(True)\n",
    "            tmodel1.train(True)\n",
    "            tmodel2.train(True)\n",
    "            tmodel3.train(True)\n",
    "            optimizer.zero_grad()\n",
    "            ids = train_ids\n",
    "            y_opt = makegrad(optimal_train)\n",
    "        else:\n",
    "            ids = test_ids\n",
    "            model.eval()\n",
    "            tmodel1.eval()\n",
    "            tmodel2.eval()\n",
    "            tmodel3.eval()\n",
    "            y_opt = makegrad(optimal_test)\n",
    "            \n",
    "            \n",
    "        ytrain = df_to_torch(outcomedf.loc[ids]).to(device)\n",
    "        #imitation losses and decision 1\n",
    "        xxtrained = [baseline, get_dlt(0),get_dlt(0),get_pd(0),get_nd(0),get_cc(0),get_mod(0)]\n",
    "        xxtrain = torch.cat([formatdf(xx,ids) for xx in xxtrained],axis=1).to(device)\n",
    "        \n",
    "        use_memory = (not train) and validate_with_memory\n",
    "\n",
    "        o1 = model(xxtrain,position=0,use_saved_memory = use_memory)\n",
    "\n",
    "        decision1_imitation = o1[:,3]\n",
    "        \n",
    "        decision1_opt = o1[:,0]\n",
    "        if threshold_decisions:\n",
    "            decision1_opt = thresh(decision1_opt)\n",
    "\n",
    "        imitation_loss1 = bce(decision1_imitation,ytrain[:,0])\n",
    "        imitation_loss1 = torch.mul(imitation_loss1,imitation_weights[0])\n",
    "        \n",
    "        x1_imitation = [baseline, get_dlt(1),get_dlt(0),get_pd(1),get_nd(1),get_cc(1),get_mod(1)]\n",
    "        x1_imitation = [formatdf(xx1,ids) for xx1 in x1_imitation]\n",
    "        x1_imitation = torch.cat(x1_imitation,axis=1).to(device)\n",
    "        decision2_imitation = model(x1_imitation,position=1,use_saved_memory = use_memory)[:,4]\n",
    "\n",
    "        imitation_loss2 =  bce(decision2_imitation,ytrain[:,1])\n",
    "        imitation_loss2 = torch.mul(imitation_loss2,imitation_weights[1])\n",
    "        \n",
    "        x2_imitation = [baseline, get_dlt(1),get_dlt(2),get_pd(2),get_nd(2),get_cc(2),get_mod(2)]\n",
    "        \n",
    "        x2_imitation = [formatdf(xx2,ids) for xx2 in x2_imitation]\n",
    "        x2_imitation = torch.cat(x2_imitation,axis=1)\n",
    "        decision3_imitation = model(x2_imitation,position=2,use_saved_memory = use_memory)[:,5]\n",
    "\n",
    "        imitation_loss3 = bce(decision3_imitation,ytrain[:,2])\n",
    "        imitation_loss3 = torch.mul(imitation_loss3,imitation_weights[2])\n",
    "        \n",
    "        #reward decisions\n",
    "        xx1 = makeinput(1,ids)\n",
    "        xx2 = makeinput(2,ids)\n",
    "        xx3 = makeinput(3,ids)\n",
    "\n",
    "        xx1 = makegrad(xx1)\n",
    "        xx2 = makegrad(xx2)\n",
    "        xx3 = makegrad(xx3)\n",
    "        baseline_train_base = formatdf(baseline,ids)\n",
    "            \n",
    "        baseline_train = torch.clone(baseline_train_base)\n",
    "\n",
    "        \n",
    "        xi1 = torch.cat([xx1,decision1_opt.view(-1,1)],axis=1)\n",
    "        print(train,tmodel1.training,tmodel1.dropout.training)\n",
    "        [ypd1, ynd1, ymod, ydlt1] = tmodel1(xi1)['predictions']\n",
    "        print(train,tmodel1.training,tmodel1.dropout.training)\n",
    "        d1_thresh = torch.gt(decision1_opt.view(-1,1),.5).to(ypd1.device)\n",
    "        d1_scale = torch.cat([d1_thresh,d1_thresh,torch.ones(d1_thresh.view(-1,1).shape).to(ypd1.device)],dim=1)\n",
    "        ypd1= torch.mul(ypd1,d1_scale)\n",
    "        ynd1= torch.mul(ynd1,d1_scale)\n",
    "        \n",
    "        x1 = [baseline_train,ydlt1,formatdf(get_dlt(0),ids),ypd1,ynd1,formatdf(get_cc(1),ids),ymod]\n",
    "        x1= torch.cat([xx1.to(model.get_device()) for xx1 in x1],axis=1)\n",
    "        \n",
    "        decision2_opt = model(x1,position=1,use_saved_memory = use_memory)[:,1] \n",
    "        if threshold_decisions:\n",
    "            decision2_opt = thresh(decision2_opt)\n",
    "            \n",
    "        xi2 = torch.cat([xx2,decision1_opt.view(-1,1),decision2_opt.view(-1,1)],axis=1)\n",
    "        [ypd2,ynd2,ycc,ydlt2] = tmodel2(xi2)['predictions']\n",
    "\n",
    "        x2 = [baseline_train,ydlt1,ydlt2,ypd2,ynd2,ycc,ymod]\n",
    "        x2 = torch.cat([xx2.to(model.get_device()) for xx2 in x2],axis=1)\n",
    "        decision3_opt = model(x2,position=2,use_saved_memory = use_memory)[:,2]\n",
    "        \n",
    "        if threshold_decisions:\n",
    "            decision3_opt = thresh(decision3_opt)\n",
    "            \n",
    "        xi3 = torch.cat([xx3,decision1_opt.view(-1,1),decision2_opt.view(-1,1),decision3_opt.view(-1,1)],axis=1)\n",
    "        \n",
    "        outcomes = tmodel3(xi3)['predictions']\n",
    "        survival = smodel3.time_to_event(xi3,n_samples=1)\n",
    "        if not train and verbose:\n",
    "            print(torch.mean(outcomes,dim=0))\n",
    "            \n",
    "        reward_loss = torch.mean(outcome_loss(outcomes,weights) + temporal_loss(survival,tweights))\n",
    "        loss = torch.add(imitation_loss1,imitation_loss2)\n",
    "        loss = torch.add(loss,imitation_loss3)\n",
    "        loss = torch.mul(loss,imitation_weight/3)\n",
    "        loss = torch.add(loss,torch.mul(reward_loss,reward_weight))\n",
    "        \n",
    "        imitation_tloss = torch.FloatTensor([0]).to(device)\n",
    "        opt_tloss = torch.FloatTensor([0]).to(device)\n",
    "        n_rows = x1.shape[0]\n",
    "        if reward_triplet_weight + imitation_triplet_weight > 0.0001:\n",
    "            for i in range(n_rows):\n",
    "                #skip if we're using an attention model idk\n",
    "                if not use_attention and imitation_triplet_weight > .0001:\n",
    "                    imitation_tloss += get_tloss(i,0,ytrain,xxtrain,True)\n",
    "                    imitation_tloss += get_tloss(i,1,ytrain,x1_imitation,True)\n",
    "                    imitation_tloss += get_tloss(i,2,ytrain,x2_imitation,True)\n",
    "                if reward_triplet_weight > .0001:\n",
    "                    opt_tloss += get_tloss(i,0,y_opt,xxtrain,False)\n",
    "                    opt_tloss += get_tloss(i,1,y_opt,x1,False)\n",
    "                    opt_tloss += get_tloss(i,2,y_opt,x2,False)\n",
    "            loss += torch.mul(imitation_tloss[0],imitation_triplet_weight/n_rows)\n",
    "            loss += torch.mul(opt_tloss[0],reward_triplet_weight/n_rows)\n",
    "        \n",
    "        losses = [imitation_loss1+imitation_loss2+imitation_loss3,reward_loss,imitation_tloss,opt_tloss]\n",
    "\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            return losses\n",
    "        else:\n",
    "            scores = []\n",
    "            distributions = [decision1_opt.mean().item(),decision2_opt.mean().item(),decision3_opt.mean().item()]\n",
    "            imitation = [decision1_imitation,decision2_imitation,decision3_imitation]\n",
    "            optimal = [decision1_opt,decision2_opt,decision3_opt]\n",
    "            for i,decision_im in enumerate(imitation):\n",
    "                deci = decision_im.cpu().detach().numpy()\n",
    "                deci0 = (deci > .5).astype(int)\n",
    "                iout = ytrain[:,i].cpu().detach().numpy()\n",
    "                acci = accuracy_score(iout,deci0)\n",
    "                try:\n",
    "                    auci = roc_auc_score(iout,deci)\n",
    "                except:\n",
    "                    auci = -1\n",
    "                \n",
    "                deco = optimal[i].cpu().detach().numpy()\n",
    "                deci0 = (deco > .5).astype(int)\n",
    "                oout = y_opt[:,i].cpu().detach().numpy()\n",
    "                acco = accuracy_score(oout,deci0)\n",
    "                try:\n",
    "                    auco = roc_auc_score(oout,deco)\n",
    "                except:\n",
    "                    auco=-1\n",
    "                scores.append({'decision': i,'optimal_auc': auco,'imitation_auc': auci,'optimal_acc': acco,'imitation_acc': acci})\n",
    "            return losses, scores, distributions\n",
    "        \n",
    "    best_val_loss = torch.tensor(1000000000.0)\n",
    "    steps_since_improvement = 0\n",
    "    best_val_score = {}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        losses = step(True)\n",
    "        val_losses,val_metrics,val_distributions = step(False)\n",
    "        vl = val_losses[0] + val_losses[1]\n",
    "        for vm in val_metrics:\n",
    "            vl += (-((vm['optimal_auc']*reward_weight) + (vm['imitation_auc']*imitation_weight)))/10\n",
    "        if verbose:\n",
    "            print('______epoch',str(epoch),'_____')\n",
    "            print('val reward',val_losses[1].item())\n",
    "            print('imitation reward', val_losses[0].item())\n",
    "            if len(val_losses) > 2:\n",
    "                print('distance losses',val_losses[2].item(),val_losses[-1].item())\n",
    "            print('distributions',val_distributions)\n",
    "            print(val_metrics)\n",
    "        if vl < best_val_loss:\n",
    "            best_val_loss = vl\n",
    "            best_val_score = val_metrics\n",
    "            best_val_distributions = val_distributions\n",
    "            steps_since_improvement = 0\n",
    "            torch.save(model.state_dict(),save_file)\n",
    "        else:\n",
    "            steps_since_improvement += 1\n",
    "        if steps_since_improvement > patience:\n",
    "            break\n",
    "    print('++++++++++Final+++++++++++')\n",
    "    print('best',best_val_loss)\n",
    "    print(best_val_score)\n",
    "    model.load_state_dict(torch.load(save_file))\n",
    "    model.eval()\n",
    "    return model, best_val_score, best_val_loss, best_val_distributions\n",
    "\n",
    "from Models import *\n",
    "# args = {\n",
    "#     'hidden_layers': [50,50], \n",
    "#     'attention_heads': [2,2],\n",
    "#     'embed_size': 120, \n",
    "#     'dropout': 0.5, \n",
    "#     'input_dropout': 0.2, \n",
    "#     'shufflecol_chance':  0.2,\n",
    "# }\n",
    "args = {\n",
    "    'hidden_layers': [500], \n",
    "    'opt_layer_size': 20, \n",
    "    'imitation_layer_size': 20, \n",
    "    'dropout': 0.25, \n",
    "    'input_dropout': 0.25, \n",
    "    'shufflecol_chance': 0.5\n",
    "}\n",
    "from Models import *\n",
    "decision_model, _, _, _ = train_decision_model(\n",
    "    model1,model2,model3,smodel3,\n",
    "    lr=.001,\n",
    "    use_attention=True,\n",
    "    imitation_weight=1,\n",
    "    imitation_triplet_weight=0,\n",
    "    reward_triplet_weight=0,\n",
    "    reward_weight=2,\n",
    "    validate_with_memory=True,\n",
    "    use_smote=False,\n",
    "    **args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
