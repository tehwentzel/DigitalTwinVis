{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050c878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "7418d82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score,accuracy_score,precision_recall_fscore_support\n",
    "from Constants import *\n",
    "from Preprocessing import *\n",
    "from Models import *\n",
    "import copy\n",
    "from Utils import *\n",
    "from DeepSurvivalModels import *\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c427599a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(536, 62)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = DTDataset(use_smote=False)\n",
    "data.processed_df.T\n",
    "data.get_input_state(1).shape\n",
    "# data.processed_df#.shape, len(data.processed_df.index.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "f2937b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DSM(\n",
       "  (act): Tanh()\n",
       "  (shape): ParameterList(\n",
       "      (0): Parameter containing: [torch.float32 of size 6]\n",
       "      (1): Parameter containing: [torch.float32 of size 6]\n",
       "      (2): Parameter containing: [torch.float32 of size 6]\n",
       "      (3): Parameter containing: [torch.float32 of size 6]\n",
       "  )\n",
       "  (scale): ParameterList(\n",
       "      (0): Parameter containing: [torch.float32 of size 6]\n",
       "      (1): Parameter containing: [torch.float32 of size 6]\n",
       "      (2): Parameter containing: [torch.float32 of size 6]\n",
       "      (3): Parameter containing: [torch.float32 of size 6]\n",
       "  )\n",
       "  (gate): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=103, out_features=6, bias=False)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=103, out_features=6, bias=False)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=103, out_features=6, bias=False)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Linear(in_features=103, out_features=6, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (scaleg): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=103, out_features=6, bias=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=103, out_features=6, bias=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=103, out_features=6, bias=True)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Linear(in_features=103, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (shapeg): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=103, out_features=6, bias=False)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=103, out_features=6, bias=False)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=103, out_features=6, bias=False)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Linear(in_features=103, out_features=6, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (embedding): Sequential(\n",
       "    (0): Linear(in_features=79, out_features=100, bias=False)\n",
       "    (1): Tanh()\n",
       "  )\n",
       "  (input_dropout): Dropout(p=0, inplace=False)\n",
       "  (embedding_dropout): Dropout(p=0.5, inplace=False)\n",
       "  (squish): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Utils import *\n",
    "model1,model2,model3,smodel3 = load_transition_models()\n",
    "smodel3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "b29d259b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([19., 79.,  1.]) 147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 0.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 1., 0.]]),\n",
       " {'pd1': tensor([[9.9906e-01, 9.3773e-04, 2.1684e-26],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [7.4482e-01, 2.4814e-01, 7.0376e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [9.8208e-01, 1.7920e-02, 8.6600e-26],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [7.7330e-01, 2.0924e-01, 1.7461e-02],\n",
       "          [6.0094e-01, 3.8780e-01, 1.1257e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [7.7998e-02, 9.1913e-01, 2.8742e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [4.5066e-01, 5.4254e-01, 6.8048e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [9.5384e-02, 8.9334e-01, 1.1275e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [2.4838e-03, 9.9701e-01, 5.0394e-04],\n",
       "          [8.7578e-02, 9.0313e-01, 9.2939e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [3.6667e-01, 6.1371e-01, 1.9622e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [8.6300e-01, 1.3389e-01, 3.1060e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [2.6831e-01, 7.1474e-01, 1.6952e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [1.2367e-01, 8.7044e-01, 5.8930e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [1.4404e-01, 8.4511e-01, 1.0851e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [1.7139e-01, 8.1631e-01, 1.2301e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [5.6724e-02, 9.3835e-01, 4.9307e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [4.2038e-01, 5.6089e-01, 1.8735e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [4.0001e-01, 5.8889e-01, 1.1095e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01]], grad_fn=<CopySlices>),\n",
       "  'nd1': tensor([[1.4860e-38, 1.0000e+00, 1.4860e-38],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [1.8943e-03, 9.9627e-01, 1.8403e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [1.7654e-38, 1.0000e+00, 1.7654e-38],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [5.6885e-03, 9.8871e-01, 5.5982e-03],\n",
       "          [2.7378e-03, 9.9455e-01, 2.7086e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [6.1012e-04, 9.9879e-01, 5.9927e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [1.3572e-03, 9.9728e-01, 1.3625e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [6.8287e-03, 9.8644e-01, 6.7290e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [9.1299e-04, 9.9817e-01, 9.1219e-04],\n",
       "          [4.5776e-03, 9.9087e-01, 4.5565e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [5.6558e-03, 9.8868e-01, 5.6598e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [7.5785e-04, 9.9850e-01, 7.3755e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [6.2516e-03, 9.8758e-01, 6.1648e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [1.1743e-03, 9.9766e-01, 1.1655e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [3.9960e-03, 9.9204e-01, 3.9618e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [4.4603e-03, 9.9116e-01, 4.3815e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [3.7327e-03, 9.9261e-01, 3.6594e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [4.9181e-03, 9.9018e-01, 4.8989e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [2.9339e-03, 9.9419e-01, 2.8776e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01]], grad_fn=<CopySlices>),\n",
       "  'nd2': tensor([[9.8386e-01, 1.5858e-02, 2.8070e-04],\n",
       "          [8.4328e-01, 1.5671e-01, 5.3298e-06],\n",
       "          [7.4203e-01, 2.5728e-01, 6.8109e-04],\n",
       "          [7.4908e-01, 2.4683e-01, 4.0813e-03],\n",
       "          [7.7093e-01, 2.2430e-01, 4.7723e-03],\n",
       "          [4.5771e-01, 5.3868e-01, 3.6057e-03],\n",
       "          [5.3750e-01, 4.6202e-01, 4.8001e-04],\n",
       "          [3.2533e-01, 6.7206e-01, 2.6114e-03],\n",
       "          [7.1889e-01, 2.7859e-01, 2.5187e-03],\n",
       "          [3.1903e-01, 6.7553e-01, 5.4378e-03],\n",
       "          [3.1246e-01, 6.8299e-01, 4.5515e-03],\n",
       "          [7.4597e-01, 2.5365e-01, 3.7563e-04],\n",
       "          [3.9753e-01, 5.9738e-01, 5.0922e-03],\n",
       "          [2.9082e-01, 7.0640e-01, 2.7839e-03],\n",
       "          [6.5742e-01, 3.3741e-01, 5.1778e-03],\n",
       "          [5.2470e-01, 4.7038e-01, 4.9246e-03],\n",
       "          [6.7628e-01, 3.2364e-01, 7.3570e-05],\n",
       "          [6.0739e-01, 3.8720e-01, 5.4126e-03],\n",
       "          [3.6970e-01, 6.2657e-01, 3.7252e-03],\n",
       "          [4.9761e-01, 4.9996e-01, 2.4263e-03],\n",
       "          [7.9082e-01, 2.0511e-01, 4.0699e-03],\n",
       "          [5.4428e-01, 4.5131e-01, 4.4083e-03],\n",
       "          [6.7224e-01, 3.2202e-01, 5.7383e-03],\n",
       "          [6.9160e-01, 3.0801e-01, 3.9784e-04],\n",
       "          [1.5971e-01, 8.4029e-01, 1.9486e-06],\n",
       "          [6.6420e-01, 3.3279e-01, 3.0084e-03],\n",
       "          [2.0657e-01, 7.9342e-01, 2.1137e-06],\n",
       "          [6.7173e-01, 3.2777e-01, 4.9918e-04],\n",
       "          [4.0163e-01, 5.9361e-01, 4.7629e-03],\n",
       "          [4.1012e-01, 5.8568e-01, 4.1929e-03],\n",
       "          [3.1307e-01, 6.8352e-01, 3.4114e-03],\n",
       "          [5.4061e-01, 4.5032e-01, 9.0714e-03],\n",
       "          [4.1003e-01, 5.8569e-01, 4.2800e-03],\n",
       "          [6.6125e-01, 3.3430e-01, 4.4561e-03],\n",
       "          [6.1869e-01, 3.7998e-01, 1.3284e-03],\n",
       "          [9.7124e-01, 2.8398e-02, 3.6692e-04],\n",
       "          [5.1631e-01, 4.7800e-01, 5.6880e-03],\n",
       "          [7.4963e-01, 2.4528e-01, 5.0932e-03],\n",
       "          [3.6792e-01, 6.2867e-01, 3.4081e-03],\n",
       "          [3.9175e-01, 6.0482e-01, 3.4301e-03],\n",
       "          [5.9049e-01, 4.0256e-01, 6.9512e-03],\n",
       "          [9.1603e-01, 8.3965e-02, 8.5255e-06],\n",
       "          [7.5597e-01, 2.4130e-01, 2.7305e-03],\n",
       "          [8.3705e-01, 1.6259e-01, 3.6379e-04],\n",
       "          [4.1972e-01, 5.7704e-01, 3.2415e-03],\n",
       "          [7.1296e-01, 2.8656e-01, 4.8027e-04],\n",
       "          [8.2658e-01, 1.7033e-01, 3.0865e-03],\n",
       "          [3.7670e-01, 6.2189e-01, 1.4111e-03],\n",
       "          [4.8267e-01, 5.1065e-01, 6.6832e-03],\n",
       "          [4.8292e-01, 5.1230e-01, 4.7819e-03],\n",
       "          [7.2372e-01, 2.7565e-01, 6.2551e-04],\n",
       "          [6.7999e-01, 3.1141e-01, 8.5974e-03],\n",
       "          [9.1360e-01, 8.4451e-02, 1.9493e-03],\n",
       "          [4.3909e-01, 5.5478e-01, 6.1312e-03],\n",
       "          [4.2285e-01, 5.7286e-01, 4.2875e-03],\n",
       "          [5.4032e-01, 4.5024e-01, 9.4466e-03],\n",
       "          [8.0353e-01, 1.9267e-01, 3.8058e-03],\n",
       "          [4.1701e-01, 5.7827e-01, 4.7242e-03],\n",
       "          [9.3535e-01, 6.4642e-02, 2.8986e-06],\n",
       "          [4.3554e-01, 5.5838e-01, 6.0785e-03],\n",
       "          [3.7254e-01, 6.2307e-01, 4.3936e-03],\n",
       "          [5.2450e-01, 4.7098e-01, 4.5247e-03],\n",
       "          [3.3065e-01, 6.6520e-01, 4.1470e-03],\n",
       "          [3.9869e-01, 5.9813e-01, 3.1758e-03],\n",
       "          [1.2534e-01, 8.7466e-01, 2.7052e-06],\n",
       "          [3.0144e-01, 6.9582e-01, 2.7408e-03],\n",
       "          [3.4016e-01, 6.5555e-01, 4.2910e-03],\n",
       "          [4.2343e-01, 5.7140e-01, 5.1675e-03],\n",
       "          [5.2151e-01, 4.7130e-01, 7.1866e-03],\n",
       "          [5.4002e-01, 4.5526e-01, 4.7274e-03],\n",
       "          [3.8607e-01, 6.0908e-01, 4.8551e-03],\n",
       "          [7.7153e-01, 2.2307e-01, 5.3960e-03],\n",
       "          [3.8032e-01, 6.1531e-01, 4.3675e-03],\n",
       "          [4.6109e-01, 5.3541e-01, 3.5012e-03],\n",
       "          [7.8700e-01, 2.1131e-01, 1.6976e-03],\n",
       "          [4.3005e-01, 5.6788e-01, 2.0719e-03],\n",
       "          [5.0434e-01, 4.9465e-01, 1.0109e-03],\n",
       "          [5.1940e-01, 4.7268e-01, 7.9233e-03],\n",
       "          [4.2598e-01, 5.7272e-01, 1.3049e-03],\n",
       "          [5.3819e-01, 4.5928e-01, 2.5309e-03],\n",
       "          [4.3333e-01, 5.6316e-01, 3.5115e-03],\n",
       "          [7.1421e-01, 2.8172e-01, 4.0634e-03],\n",
       "          [6.7262e-01, 3.2102e-01, 6.3586e-03],\n",
       "          [4.6503e-01, 5.3255e-01, 2.4151e-03],\n",
       "          [8.8920e-01, 1.1046e-01, 3.4043e-04],\n",
       "          [2.8876e-01, 7.0880e-01, 2.4372e-03],\n",
       "          [6.0035e-01, 3.9691e-01, 2.7375e-03],\n",
       "          [2.0244e-01, 7.9666e-01, 8.9372e-04],\n",
       "          [3.4377e-01, 6.5362e-01, 2.6037e-03],\n",
       "          [9.6671e-01, 3.2846e-02, 4.4359e-04],\n",
       "          [3.7823e-01, 6.1840e-01, 3.3648e-03],\n",
       "          [2.7646e-01, 7.2119e-01, 2.3512e-03],\n",
       "          [5.6122e-01, 4.3144e-01, 7.3387e-03],\n",
       "          [3.4361e-01, 6.5378e-01, 2.6124e-03],\n",
       "          [6.6281e-01, 3.3688e-01, 3.0861e-04],\n",
       "          [7.6382e-01, 2.3429e-01, 1.8922e-03],\n",
       "          [4.2164e-01, 5.7215e-01, 6.2073e-03],\n",
       "          [8.0611e-01, 1.9356e-01, 3.3043e-04],\n",
       "          [3.8876e-01, 6.0717e-01, 4.0670e-03],\n",
       "          [5.8754e-01, 4.1069e-01, 1.7674e-03],\n",
       "          [5.0090e-01, 4.9572e-01, 3.3800e-03],\n",
       "          [4.3001e-01, 5.6390e-01, 6.0950e-03],\n",
       "          [5.2255e-01, 4.6849e-01, 8.9570e-03],\n",
       "          [6.2736e-01, 3.7229e-01, 3.4616e-04],\n",
       "          [4.0506e-01, 5.8957e-01, 5.3634e-03],\n",
       "          [7.3218e-01, 2.5989e-01, 7.9352e-03],\n",
       "          [5.0079e-01, 4.9353e-01, 5.6740e-03],\n",
       "          [7.1597e-01, 2.8099e-01, 3.0322e-03],\n",
       "          [3.2543e-01, 6.7186e-01, 2.7083e-03],\n",
       "          [4.2073e-01, 5.7504e-01, 4.2223e-03],\n",
       "          [5.5815e-01, 4.3625e-01, 5.6087e-03],\n",
       "          [4.3920e-01, 5.5563e-01, 5.1660e-03],\n",
       "          [3.4084e-01, 6.5366e-01, 5.4921e-03],\n",
       "          [9.1693e-01, 8.2191e-02, 8.7498e-04],\n",
       "          [4.7362e-01, 5.1897e-01, 7.4192e-03],\n",
       "          [5.3183e-01, 4.6436e-01, 3.8057e-03],\n",
       "          [3.2847e-01, 6.6821e-01, 3.3216e-03],\n",
       "          [4.4230e-01, 5.5274e-01, 4.9600e-03],\n",
       "          [4.3369e-01, 5.6384e-01, 2.4630e-03],\n",
       "          [4.9127e-01, 5.0062e-01, 8.1133e-03],\n",
       "          [7.9216e-01, 2.0536e-01, 2.4822e-03],\n",
       "          [8.4953e-01, 1.4738e-01, 3.0987e-03],\n",
       "          [6.3506e-01, 3.5754e-01, 7.4040e-03],\n",
       "          [4.8151e-01, 5.1179e-01, 6.6983e-03],\n",
       "          [4.8100e-01, 5.1272e-01, 6.2804e-03],\n",
       "          [3.6948e-01, 6.2654e-01, 3.9869e-03],\n",
       "          [3.2455e-01, 6.7330e-01, 2.1469e-03],\n",
       "          [6.6099e-01, 3.3863e-01, 3.7570e-04],\n",
       "          [5.9484e-01, 4.0388e-01, 1.2812e-03],\n",
       "          [3.8816e-01, 6.0914e-01, 2.7040e-03],\n",
       "          [5.9625e-01, 4.0156e-01, 2.1817e-03],\n",
       "          [4.2668e-01, 5.6920e-01, 4.1110e-03],\n",
       "          [6.6110e-01, 3.3416e-01, 4.7383e-03],\n",
       "          [3.9772e-01, 5.9583e-01, 6.4511e-03],\n",
       "          [3.2602e-01, 6.6834e-01, 5.6367e-03],\n",
       "          [7.3136e-01, 2.6456e-01, 4.0877e-03],\n",
       "          [6.7859e-01, 3.1306e-01, 8.3519e-03],\n",
       "          [4.6523e-01, 5.2860e-01, 6.1708e-03],\n",
       "          [2.7646e-01, 7.1882e-01, 4.7212e-03],\n",
       "          [8.2095e-01, 1.7429e-01, 4.7593e-03],\n",
       "          [4.6750e-01, 5.2771e-01, 4.7835e-03],\n",
       "          [6.0760e-01, 3.8632e-01, 6.0793e-03],\n",
       "          [2.0784e-01, 7.9216e-01, 3.6109e-06],\n",
       "          [5.7638e-01, 4.2245e-01, 1.1644e-03],\n",
       "          [5.6237e-01, 4.3094e-01, 6.6901e-03],\n",
       "          [8.1956e-01, 1.7784e-01, 2.6014e-03],\n",
       "          [6.3514e-01, 3.5965e-01, 5.2059e-03]], grad_fn=<CopySlices>),\n",
       "  'pd2': tensor([[9.9999e-01, 6.7606e-06, 6.7568e-06],\n",
       "          [1.0000e+00, 1.4080e-10, 1.4079e-10],\n",
       "          [1.0000e+00, 2.0708e-06, 2.0695e-06],\n",
       "          [9.9982e-01, 8.8775e-05, 8.8438e-05],\n",
       "          [9.9975e-01, 1.2461e-04, 1.2419e-04],\n",
       "          [9.9990e-01, 5.2425e-05, 5.2337e-05],\n",
       "          [1.0000e+00, 8.6569e-07, 8.6544e-07],\n",
       "          [9.9993e-01, 3.2696e-05, 3.2629e-05],\n",
       "          [9.9994e-01, 2.8346e-05, 2.8303e-05],\n",
       "          [9.9970e-01, 1.5232e-04, 1.5162e-04],\n",
       "          [9.9978e-01, 1.0936e-04, 1.0889e-04],\n",
       "          [1.0000e+00, 6.4970e-07, 6.4953e-07],\n",
       "          [9.9980e-01, 9.9939e-05, 9.9683e-05],\n",
       "          [9.9992e-01, 4.0809e-05, 4.0726e-05],\n",
       "          [9.9975e-01, 1.2476e-04, 1.2427e-04],\n",
       "          [9.9978e-01, 1.1235e-04, 1.1197e-04],\n",
       "          [1.0000e+00, 3.4940e-08, 3.4938e-08],\n",
       "          [9.9976e-01, 1.2112e-04, 1.2078e-04],\n",
       "          [9.9988e-01, 6.0030e-05, 5.9904e-05],\n",
       "          [9.9995e-01, 2.3462e-05, 2.3422e-05],\n",
       "          [9.9976e-01, 1.2265e-04, 1.2224e-04],\n",
       "          [9.9983e-01, 8.3217e-05, 8.2990e-05],\n",
       "          [9.9970e-01, 1.4823e-04, 1.4761e-04],\n",
       "          [1.0000e+00, 1.0970e-06, 1.0963e-06],\n",
       "          [1.0000e+00, 2.4952e-11, 2.4952e-11],\n",
       "          [9.9991e-01, 4.6907e-05, 4.6784e-05],\n",
       "          [1.0000e+00, 2.7363e-11, 2.7363e-11],\n",
       "          [1.0000e+00, 1.2857e-06, 1.2853e-06],\n",
       "          [9.9982e-01, 9.0996e-05, 9.0700e-05],\n",
       "          [9.9985e-01, 7.7361e-05, 7.7177e-05],\n",
       "          [9.9988e-01, 5.7762e-05, 5.7635e-05],\n",
       "          [9.9926e-01, 3.7270e-04, 3.7019e-04],\n",
       "          [9.9985e-01, 7.6476e-05, 7.6305e-05],\n",
       "          [9.9981e-01, 9.6395e-05, 9.6077e-05],\n",
       "          [9.9999e-01, 7.1581e-06, 7.1520e-06],\n",
       "          [9.9999e-01, 6.2443e-06, 6.2396e-06],\n",
       "          [9.9971e-01, 1.4507e-04, 1.4443e-04],\n",
       "          [9.9966e-01, 1.6943e-04, 1.6877e-04],\n",
       "          [9.9989e-01, 5.2734e-05, 5.2579e-05],\n",
       "          [9.9989e-01, 5.2851e-05, 5.2731e-05],\n",
       "          [9.9955e-01, 2.2571e-04, 2.2467e-04],\n",
       "          [1.0000e+00, 6.0589e-10, 6.0588e-10],\n",
       "          [9.9991e-01, 4.3763e-05, 4.3683e-05],\n",
       "          [1.0000e+00, 9.4017e-07, 9.3988e-07],\n",
       "          [9.9991e-01, 4.3157e-05, 4.3084e-05],\n",
       "          [1.0000e+00, 1.4304e-06, 1.4298e-06],\n",
       "          [9.9987e-01, 6.3489e-05, 6.3318e-05],\n",
       "          [9.9998e-01, 7.7299e-06, 7.7239e-06],\n",
       "          [9.9961e-01, 1.9556e-04, 1.9473e-04],\n",
       "          [9.9982e-01, 8.7859e-05, 8.7612e-05],\n",
       "          [1.0000e+00, 2.0387e-06, 2.0384e-06],\n",
       "          [9.9931e-01, 3.4781e-04, 3.4581e-04],\n",
       "          [9.9990e-01, 4.9938e-05, 4.9814e-05],\n",
       "          [9.9964e-01, 1.7812e-04, 1.7728e-04],\n",
       "          [9.9984e-01, 7.9891e-05, 7.9700e-05],\n",
       "          [9.9928e-01, 3.5878e-04, 3.5655e-04],\n",
       "          [9.9984e-01, 8.2024e-05, 8.1832e-05],\n",
       "          [9.9979e-01, 1.0736e-04, 1.0705e-04],\n",
       "          [1.0000e+00, 1.2676e-10, 1.2676e-10],\n",
       "          [9.9970e-01, 1.5019e-04, 1.4963e-04],\n",
       "          [9.9983e-01, 8.6472e-05, 8.6224e-05],\n",
       "          [9.9982e-01, 8.7961e-05, 8.7719e-05],\n",
       "          [9.9983e-01, 8.3934e-05, 8.3634e-05],\n",
       "          [9.9992e-01, 3.8834e-05, 3.8771e-05],\n",
       "          [1.0000e+00, 6.7054e-11, 6.7054e-11],\n",
       "          [9.9992e-01, 4.0441e-05, 4.0356e-05],\n",
       "          [9.9981e-01, 9.3593e-05, 9.3285e-05],\n",
       "          [9.9977e-01, 1.1418e-04, 1.1369e-04],\n",
       "          [9.9957e-01, 2.1488e-04, 2.1406e-04],\n",
       "          [9.9982e-01, 9.1952e-05, 9.1631e-05],\n",
       "          [9.9981e-01, 9.5005e-05, 9.4651e-05],\n",
       "          [9.9963e-01, 1.8378e-04, 1.8305e-04],\n",
       "          [9.9982e-01, 8.9495e-05, 8.9258e-05],\n",
       "          [9.9989e-01, 5.5700e-05, 5.5545e-05],\n",
       "          [9.9997e-01, 1.6043e-05, 1.6013e-05],\n",
       "          [9.9996e-01, 1.9200e-05, 1.9171e-05],\n",
       "          [9.9999e-01, 5.2601e-06, 5.2555e-06],\n",
       "          [9.9946e-01, 2.7139e-04, 2.6990e-04],\n",
       "          [9.9998e-01, 8.8458e-06, 8.8380e-06],\n",
       "          [9.9995e-01, 2.2980e-05, 2.2946e-05],\n",
       "          [9.9989e-01, 5.6619e-05, 5.6463e-05],\n",
       "          [9.9984e-01, 7.9091e-05, 7.8814e-05],\n",
       "          [9.9956e-01, 2.2053e-04, 2.1940e-04],\n",
       "          [9.9996e-01, 2.0785e-05, 2.0751e-05],\n",
       "          [1.0000e+00, 1.3370e-06, 1.3365e-06],\n",
       "          [9.9994e-01, 3.0372e-05, 3.0314e-05],\n",
       "          [9.9993e-01, 3.3355e-05, 3.3294e-05],\n",
       "          [9.9999e-01, 5.1263e-06, 5.1222e-06],\n",
       "          [9.9994e-01, 3.1001e-05, 3.0956e-05],\n",
       "          [9.9998e-01, 7.6301e-06, 7.6211e-06],\n",
       "          [9.9990e-01, 4.8441e-05, 4.8329e-05],\n",
       "          [9.9994e-01, 2.8906e-05, 2.8851e-05],\n",
       "          [9.9952e-01, 2.4155e-04, 2.4019e-04],\n",
       "          [9.9994e-01, 3.1050e-05, 3.1000e-05],\n",
       "          [1.0000e+00, 5.7585e-07, 5.7575e-07],\n",
       "          [9.9995e-01, 2.3768e-05, 2.3725e-05],\n",
       "          [9.9963e-01, 1.8686e-04, 1.8590e-04],\n",
       "          [1.0000e+00, 6.0160e-07, 6.0152e-07],\n",
       "          [9.9986e-01, 7.0234e-05, 7.0080e-05],\n",
       "          [9.9997e-01, 1.6445e-05, 1.6417e-05],\n",
       "          [9.9990e-01, 5.1027e-05, 5.0922e-05],\n",
       "          [9.9965e-01, 1.7323e-04, 1.7244e-04],\n",
       "          [9.9929e-01, 3.5871e-04, 3.5632e-04],\n",
       "          [1.0000e+00, 5.0630e-07, 5.0624e-07],\n",
       "          [9.9975e-01, 1.2691e-04, 1.2654e-04],\n",
       "          [9.9939e-01, 3.0429e-04, 3.0285e-04],\n",
       "          [9.9976e-01, 1.2017e-04, 1.1982e-04],\n",
       "          [9.9992e-01, 3.8266e-05, 3.8206e-05],\n",
       "          [9.9993e-01, 3.2773e-05, 3.2712e-05],\n",
       "          [9.9984e-01, 7.9101e-05, 7.8897e-05],\n",
       "          [9.9972e-01, 1.3831e-04, 1.3767e-04],\n",
       "          [9.9975e-01, 1.2348e-04, 1.2308e-04],\n",
       "          [9.9972e-01, 1.3885e-04, 1.3832e-04],\n",
       "          [9.9998e-01, 9.9374e-06, 9.9216e-06],\n",
       "          [9.9959e-01, 2.0455e-04, 2.0352e-04],\n",
       "          [9.9988e-01, 5.9401e-05, 5.9265e-05],\n",
       "          [9.9989e-01, 5.4942e-05, 5.4824e-05],\n",
       "          [9.9982e-01, 9.0228e-05, 9.0015e-05],\n",
       "          [9.9995e-01, 2.4280e-05, 2.4234e-05],\n",
       "          [9.9943e-01, 2.8493e-04, 2.8340e-04],\n",
       "          [9.9993e-01, 3.6431e-05, 3.6337e-05],\n",
       "          [9.9986e-01, 7.1074e-05, 7.0897e-05],\n",
       "          [9.9954e-01, 2.3048e-04, 2.2939e-04],\n",
       "          [9.9959e-01, 2.0688e-04, 2.0598e-04],\n",
       "          [9.9964e-01, 1.7803e-04, 1.7735e-04],\n",
       "          [9.9987e-01, 6.7152e-05, 6.6998e-05],\n",
       "          [9.9995e-01, 2.6536e-05, 2.6481e-05],\n",
       "          [1.0000e+00, 6.8108e-07, 6.8089e-07],\n",
       "          [9.9998e-01, 7.4965e-06, 7.4887e-06],\n",
       "          [9.9994e-01, 2.9912e-05, 2.9867e-05],\n",
       "          [9.9995e-01, 2.4191e-05, 2.4159e-05],\n",
       "          [9.9985e-01, 7.4356e-05, 7.4160e-05],\n",
       "          [9.9977e-01, 1.1357e-04, 1.1320e-04],\n",
       "          [9.9961e-01, 1.9338e-04, 1.9246e-04],\n",
       "          [9.9967e-01, 1.6602e-04, 1.6533e-04],\n",
       "          [9.9981e-01, 9.6199e-05, 9.5847e-05],\n",
       "          [9.9946e-01, 2.7209e-04, 2.7078e-04],\n",
       "          [9.9970e-01, 1.5071e-04, 1.5001e-04],\n",
       "          [9.9976e-01, 1.1976e-04, 1.1934e-04],\n",
       "          [9.9971e-01, 1.4348e-04, 1.4296e-04],\n",
       "          [9.9985e-01, 7.5073e-05, 7.4775e-05],\n",
       "          [9.9970e-01, 1.5119e-04, 1.5046e-04],\n",
       "          [1.0000e+00, 7.6450e-11, 7.6450e-11],\n",
       "          [9.9999e-01, 7.3751e-06, 7.3671e-06],\n",
       "          [9.9961e-01, 1.9299e-04, 1.9215e-04],\n",
       "          [9.9991e-01, 4.5204e-05, 4.5103e-05],\n",
       "          [9.9976e-01, 1.2254e-04, 1.2216e-04]], grad_fn=<CopySlices>),\n",
       "  'mod': tensor([[1.0000e+00, 1.6408e-30, 1.6408e-30, 1.6408e-30, 1.6408e-30, 1.6408e-30],\n",
       "          [1.0000e+00, 4.0775e-20, 4.0775e-20, 4.0775e-20, 4.0775e-20, 4.0775e-20],\n",
       "          [1.0000e+00, 7.5609e-20, 7.5609e-20, 7.5609e-20, 7.5609e-20, 7.5609e-20],\n",
       "          [1.0000e+00, 5.1180e-20, 5.1180e-20, 5.1180e-20, 5.1180e-20, 5.1180e-20],\n",
       "          [1.0000e+00, 5.4615e-20, 5.4615e-20, 5.4615e-20, 5.4615e-20, 5.4615e-20],\n",
       "          [1.0000e+00, 7.2504e-20, 7.2504e-20, 7.2504e-20, 7.2504e-20, 7.2504e-20],\n",
       "          [1.0000e+00, 4.7301e-20, 4.7301e-20, 4.7301e-20, 4.7301e-20, 4.7301e-20],\n",
       "          [1.0000e+00, 6.3521e-20, 6.3521e-20, 6.3521e-20, 6.3521e-20, 6.3521e-20],\n",
       "          [1.0000e+00, 7.2699e-20, 7.2699e-20, 7.2699e-20, 7.2699e-20, 7.2699e-20],\n",
       "          [1.0000e+00, 8.0317e-20, 8.0317e-20, 8.0317e-20, 8.0317e-20, 8.0317e-20],\n",
       "          [1.0000e+00, 7.3892e-20, 7.3892e-20, 7.3892e-20, 7.3892e-20, 7.3892e-20],\n",
       "          [1.0000e+00, 7.8688e-20, 7.8688e-20, 7.8688e-20, 7.8688e-20, 7.8688e-20],\n",
       "          [1.0000e+00, 7.9696e-20, 7.9696e-20, 7.9696e-20, 7.9696e-20, 7.9696e-20],\n",
       "          [1.0000e+00, 6.4759e-20, 6.4759e-20, 6.4759e-20, 6.4759e-20, 6.4759e-20],\n",
       "          [1.0000e+00, 4.8488e-20, 4.8488e-20, 4.8488e-20, 4.8488e-20, 4.8488e-20],\n",
       "          [1.0000e+00, 1.0459e-19, 1.0459e-19, 1.0459e-19, 1.0459e-19, 1.0459e-19],\n",
       "          [1.0000e+00, 5.0035e-20, 5.0035e-20, 5.0035e-20, 5.0035e-20, 5.0035e-20],\n",
       "          [1.0000e+00, 6.0005e-20, 6.0005e-20, 6.0005e-20, 6.0005e-20, 6.0005e-20],\n",
       "          [1.0000e+00, 7.8837e-20, 7.8837e-20, 7.8837e-20, 7.8837e-20, 7.8837e-20],\n",
       "          [1.0000e+00, 8.3055e-20, 8.3055e-20, 8.3055e-20, 8.3055e-20, 8.3055e-20],\n",
       "          [1.0000e+00, 6.6357e-20, 6.6357e-20, 6.6357e-20, 6.6357e-20, 6.6357e-20],\n",
       "          [1.0000e+00, 1.0588e-19, 1.0588e-19, 1.0588e-19, 1.0588e-19, 1.0588e-19],\n",
       "          [1.0000e+00, 5.7578e-20, 5.7578e-20, 5.7578e-20, 5.7578e-20, 5.7578e-20],\n",
       "          [1.0000e+00, 3.6225e-20, 3.6225e-20, 3.6225e-20, 3.6225e-20, 3.6225e-20],\n",
       "          [1.0000e+00, 1.7362e-31, 1.7362e-31, 1.7362e-31, 1.7362e-31, 1.7362e-31],\n",
       "          [1.0000e+00, 1.1973e-19, 1.1973e-19, 1.1973e-19, 1.1973e-19, 1.1973e-19],\n",
       "          [1.0000e+00, 2.5357e-31, 2.5357e-31, 2.5357e-31, 2.5357e-31, 2.5357e-31],\n",
       "          [9.7427e-01, 5.0502e-03, 4.9584e-03, 5.2764e-03, 4.9968e-03, 5.4493e-03],\n",
       "          [1.0000e+00, 6.5419e-20, 6.5419e-20, 6.5419e-20, 6.5419e-20, 6.5419e-20],\n",
       "          [1.0000e+00, 7.1364e-20, 7.1364e-20, 7.1364e-20, 7.1364e-20, 7.1364e-20],\n",
       "          [1.0000e+00, 5.6633e-20, 5.6633e-20, 5.6633e-20, 5.6633e-20, 5.6633e-20],\n",
       "          [1.0000e+00, 6.4062e-20, 6.4062e-20, 6.4062e-20, 6.4062e-20, 6.4062e-20],\n",
       "          [1.0000e+00, 7.7718e-20, 7.7718e-20, 7.7718e-20, 7.7718e-20, 7.7718e-20],\n",
       "          [1.0000e+00, 5.0149e-20, 5.0149e-20, 5.0149e-20, 5.0149e-20, 5.0149e-20],\n",
       "          [1.0000e+00, 8.3438e-20, 8.3438e-20, 8.3438e-20, 8.3438e-20, 8.3438e-20],\n",
       "          [1.0000e+00, 1.5212e-30, 1.5212e-30, 1.5212e-30, 1.5212e-30, 1.5212e-30],\n",
       "          [1.0000e+00, 5.2528e-20, 5.2528e-20, 5.2528e-20, 5.2528e-20, 5.2528e-20],\n",
       "          [9.3050e-01, 1.3856e-02, 1.3179e-02, 1.4281e-02, 1.3270e-02, 1.4914e-02],\n",
       "          [9.6413e-01, 6.9720e-03, 6.8104e-03, 7.5363e-03, 6.9331e-03, 7.6133e-03],\n",
       "          [1.0000e+00, 6.9578e-20, 6.9578e-20, 6.9578e-20, 6.9578e-20, 6.9578e-20],\n",
       "          [1.0000e+00, 5.7103e-20, 5.7103e-20, 5.7103e-20, 5.7103e-20, 5.7103e-20],\n",
       "          [1.0000e+00, 2.6798e-20, 2.6798e-20, 2.6798e-20, 2.6798e-20, 2.6798e-20],\n",
       "          [1.0000e+00, 5.9085e-20, 5.9085e-20, 5.9085e-20, 5.9085e-20, 5.9085e-20],\n",
       "          [9.8561e-01, 2.8594e-03, 2.7880e-03, 2.9470e-03, 2.8176e-03, 2.9783e-03],\n",
       "          [1.0000e+00, 7.7077e-20, 7.7077e-20, 7.7077e-20, 7.7077e-20, 7.7077e-20],\n",
       "          [1.0000e+00, 6.3328e-20, 6.3328e-20, 6.3328e-20, 6.3328e-20, 6.3328e-20],\n",
       "          [1.0000e+00, 5.4383e-20, 5.4383e-20, 5.4383e-20, 5.4383e-20, 5.4383e-20],\n",
       "          [1.0000e+00, 1.1220e-19, 1.1220e-19, 1.1220e-19, 1.1220e-19, 1.1220e-19],\n",
       "          [1.0000e+00, 5.8553e-20, 5.8553e-20, 5.8553e-20, 5.8553e-20, 5.8553e-20],\n",
       "          [1.0000e+00, 6.7004e-20, 6.7004e-20, 6.7004e-20, 6.7004e-20, 6.7004e-20],\n",
       "          [9.7549e-01, 4.8171e-03, 4.7481e-03, 5.0368e-03, 4.7613e-03, 5.1456e-03],\n",
       "          [1.0000e+00, 5.9389e-20, 5.9389e-20, 5.9389e-20, 5.9389e-20, 5.9389e-20],\n",
       "          [1.0000e+00, 6.7018e-20, 6.7018e-20, 6.7018e-20, 6.7018e-20, 6.7018e-20],\n",
       "          [1.0000e+00, 7.3915e-20, 7.3915e-20, 7.3915e-20, 7.3915e-20, 7.3915e-20],\n",
       "          [1.0000e+00, 7.1188e-20, 7.1188e-20, 7.1188e-20, 7.1188e-20, 7.1188e-20],\n",
       "          [9.3254e-01, 1.3505e-02, 1.2936e-02, 1.3924e-02, 1.3096e-02, 1.3998e-02],\n",
       "          [1.0000e+00, 4.1354e-20, 4.1354e-20, 4.1354e-20, 4.1354e-20, 4.1354e-20],\n",
       "          [1.0000e+00, 8.0765e-20, 8.0765e-20, 8.0765e-20, 8.0765e-20, 8.0765e-20],\n",
       "          [1.0000e+00, 1.7219e-20, 1.7219e-20, 1.7219e-20, 1.7219e-20, 1.7219e-20],\n",
       "          [1.0000e+00, 5.9966e-20, 5.9966e-20, 5.9966e-20, 5.9966e-20, 5.9966e-20],\n",
       "          [1.0000e+00, 7.5547e-20, 7.5547e-20, 7.5547e-20, 7.5547e-20, 7.5547e-20],\n",
       "          [1.0000e+00, 6.0996e-20, 6.0996e-20, 6.0996e-20, 6.0996e-20, 6.0996e-20],\n",
       "          [1.0000e+00, 5.5181e-20, 5.5181e-20, 5.5181e-20, 5.5181e-20, 5.5181e-20],\n",
       "          [1.0000e+00, 6.9833e-20, 6.9833e-20, 6.9833e-20, 6.9833e-20, 6.9833e-20],\n",
       "          [1.0000e+00, 1.4216e-31, 1.4216e-31, 1.4216e-31, 1.4216e-31, 1.4216e-31],\n",
       "          [1.0000e+00, 6.6389e-20, 6.6389e-20, 6.6389e-20, 6.6389e-20, 6.6389e-20],\n",
       "          [1.0000e+00, 5.2015e-20, 5.2015e-20, 5.2015e-20, 5.2015e-20, 5.2015e-20],\n",
       "          [1.0000e+00, 5.1578e-20, 5.1578e-20, 5.1578e-20, 5.1578e-20, 5.1578e-20],\n",
       "          [1.0000e+00, 5.7613e-20, 5.7613e-20, 5.7613e-20, 5.7613e-20, 5.7613e-20],\n",
       "          [1.0000e+00, 5.9429e-20, 5.9429e-20, 5.9429e-20, 5.9429e-20, 5.9429e-20],\n",
       "          [1.0000e+00, 6.1614e-20, 6.1614e-20, 6.1614e-20, 6.1614e-20, 6.1614e-20],\n",
       "          [1.0000e+00, 7.8165e-20, 7.8165e-20, 7.8165e-20, 7.8165e-20, 7.8165e-20],\n",
       "          [1.0000e+00, 7.6834e-20, 7.6834e-20, 7.6834e-20, 7.6834e-20, 7.6834e-20],\n",
       "          [1.0000e+00, 9.7821e-20, 9.7821e-20, 9.7821e-20, 9.7821e-20, 9.7821e-20],\n",
       "          [1.0000e+00, 8.2234e-20, 8.2234e-20, 8.2234e-20, 8.2234e-20, 8.2234e-20],\n",
       "          [1.0000e+00, 8.0274e-20, 8.0274e-20, 8.0274e-20, 8.0274e-20, 8.0274e-20],\n",
       "          [9.8679e-01, 2.6387e-03, 2.5746e-03, 2.7095e-03, 2.6012e-03, 2.6856e-03],\n",
       "          [9.4900e-01, 1.0137e-02, 9.7731e-03, 1.0601e-02, 9.8685e-03, 1.0616e-02],\n",
       "          [1.0000e+00, 4.8314e-20, 4.8314e-20, 4.8314e-20, 4.8314e-20, 4.8314e-20],\n",
       "          [1.0000e+00, 7.0362e-20, 7.0362e-20, 7.0362e-20, 7.0362e-20, 7.0362e-20],\n",
       "          [1.0000e+00, 7.4385e-20, 7.4385e-20, 7.4385e-20, 7.4385e-20, 7.4385e-20],\n",
       "          [1.0000e+00, 8.7072e-20, 8.7072e-20, 8.7072e-20, 8.7072e-20, 8.7072e-20],\n",
       "          [9.3738e-01, 1.2546e-02, 1.1971e-02, 1.2887e-02, 1.2118e-02, 1.3094e-02],\n",
       "          [1.0000e+00, 7.7176e-20, 7.7176e-20, 7.7176e-20, 7.7176e-20, 7.7176e-20],\n",
       "          [9.8771e-01, 2.3974e-03, 2.3833e-03, 2.5086e-03, 2.4053e-03, 2.5988e-03],\n",
       "          [1.0000e+00, 7.2736e-20, 7.2736e-20, 7.2736e-20, 7.2736e-20, 7.2736e-20],\n",
       "          [1.0000e+00, 6.8235e-20, 6.8235e-20, 6.8235e-20, 6.8235e-20, 6.8235e-20],\n",
       "          [1.0000e+00, 1.1546e-19, 1.1546e-19, 1.1546e-19, 1.1546e-19, 1.1546e-19],\n",
       "          [1.0000e+00, 7.1672e-20, 7.1672e-20, 7.1672e-20, 7.1672e-20, 7.1672e-20],\n",
       "          [1.0000e+00, 3.9421e-33, 3.9421e-33, 3.9421e-33, 3.9421e-33, 3.9421e-33],\n",
       "          [1.0000e+00, 7.5387e-20, 7.5387e-20, 7.5387e-20, 7.5387e-20, 7.5387e-20],\n",
       "          [1.0000e+00, 7.2823e-20, 7.2823e-20, 7.2823e-20, 7.2823e-20, 7.2823e-20],\n",
       "          [9.4059e-01, 1.1802e-02, 1.1301e-02, 1.2514e-02, 1.1446e-02, 1.2346e-02],\n",
       "          [1.0000e+00, 7.7864e-20, 7.7864e-20, 7.7864e-20, 7.7864e-20, 7.7864e-20],\n",
       "          [1.0000e+00, 8.6859e-20, 8.6859e-20, 8.6859e-20, 8.6859e-20, 8.6859e-20],\n",
       "          [9.7750e-01, 4.4884e-03, 4.3413e-03, 4.6096e-03, 4.3711e-03, 4.6869e-03],\n",
       "          [1.0000e+00, 4.5012e-20, 4.5012e-20, 4.5012e-20, 4.5012e-20, 4.5012e-20],\n",
       "          [1.0000e+00, 5.5797e-20, 5.5797e-20, 5.5797e-20, 5.5797e-20, 5.5797e-20],\n",
       "          [1.0000e+00, 7.8285e-20, 7.8285e-20, 7.8285e-20, 7.8285e-20, 7.8285e-20],\n",
       "          [1.0000e+00, 2.6661e-20, 2.6661e-20, 2.6661e-20, 2.6661e-20, 2.6661e-20],\n",
       "          [1.0000e+00, 7.4711e-20, 7.4711e-20, 7.4711e-20, 7.4711e-20, 7.4711e-20],\n",
       "          [1.0000e+00, 5.2102e-20, 5.2102e-20, 5.2102e-20, 5.2102e-20, 5.2102e-20],\n",
       "          [1.0000e+00, 6.4362e-20, 6.4362e-20, 6.4362e-20, 6.4362e-20, 6.4362e-20],\n",
       "          [1.0000e+00, 9.4449e-20, 9.4449e-20, 9.4449e-20, 9.4449e-20, 9.4449e-20],\n",
       "          [1.0000e+00, 7.0755e-20, 7.0755e-20, 7.0755e-20, 7.0755e-20, 7.0755e-20],\n",
       "          [1.0000e+00, 5.6610e-20, 5.6610e-20, 5.6610e-20, 5.6610e-20, 5.6610e-20],\n",
       "          [1.0000e+00, 6.0948e-20, 6.0948e-20, 6.0948e-20, 6.0948e-20, 6.0948e-20],\n",
       "          [1.0000e+00, 7.3550e-20, 7.3550e-20, 7.3550e-20, 7.3550e-20, 7.3550e-20],\n",
       "          [1.0000e+00, 6.5177e-20, 6.5177e-20, 6.5177e-20, 6.5177e-20, 6.5177e-20],\n",
       "          [1.0000e+00, 7.4212e-20, 7.4212e-20, 7.4212e-20, 7.4212e-20, 7.4212e-20],\n",
       "          [9.4793e-01, 1.0531e-02, 9.9250e-03, 1.0675e-02, 1.0081e-02, 1.0861e-02],\n",
       "          [1.0000e+00, 7.4166e-20, 7.4166e-20, 7.4166e-20, 7.4166e-20, 7.4166e-20],\n",
       "          [1.0000e+00, 8.4188e-20, 8.4188e-20, 8.4188e-20, 8.4188e-20, 8.4188e-20],\n",
       "          [1.0000e+00, 8.6474e-20, 8.6474e-20, 8.6474e-20, 8.6474e-20, 8.6474e-20],\n",
       "          [1.0000e+00, 5.9226e-20, 5.9226e-20, 5.9226e-20, 5.9226e-20, 5.9226e-20],\n",
       "          [1.0000e+00, 6.2937e-20, 6.2937e-20, 6.2937e-20, 6.2937e-20, 6.2937e-20],\n",
       "          [1.0000e+00, 6.8871e-20, 6.8871e-20, 6.8871e-20, 6.8871e-20, 6.8871e-20],\n",
       "          [1.0000e+00, 7.8619e-20, 7.8619e-20, 7.8619e-20, 7.8619e-20, 7.8619e-20],\n",
       "          [1.0000e+00, 8.5710e-20, 8.5710e-20, 8.5710e-20, 8.5710e-20, 8.5710e-20],\n",
       "          [9.4857e-01, 1.0251e-02, 9.8390e-03, 1.0636e-02, 1.0024e-02, 1.0684e-02],\n",
       "          [1.0000e+00, 4.1280e-20, 4.1280e-20, 4.1280e-20, 4.1280e-20, 4.1280e-20],\n",
       "          [1.0000e+00, 6.2871e-20, 6.2871e-20, 6.2871e-20, 6.2871e-20, 6.2871e-20],\n",
       "          [1.0000e+00, 6.1448e-20, 6.1448e-20, 6.1448e-20, 6.1448e-20, 6.1448e-20],\n",
       "          [1.0000e+00, 7.6826e-20, 7.6826e-20, 7.6826e-20, 7.6826e-20, 7.6826e-20],\n",
       "          [1.0000e+00, 6.4511e-20, 6.4511e-20, 6.4511e-20, 6.4511e-20, 6.4511e-20],\n",
       "          [1.0000e+00, 7.9196e-20, 7.9196e-20, 7.9196e-20, 7.9196e-20, 7.9196e-20],\n",
       "          [9.6392e-01, 7.1740e-03, 6.9354e-03, 7.5944e-03, 6.9978e-03, 7.3757e-03],\n",
       "          [1.0000e+00, 7.3487e-20, 7.3487e-20, 7.3487e-20, 7.3487e-20, 7.3487e-20],\n",
       "          [1.0000e+00, 5.3573e-20, 5.3573e-20, 5.3573e-20, 5.3573e-20, 5.3573e-20],\n",
       "          [1.0000e+00, 1.0124e-19, 1.0124e-19, 1.0124e-19, 1.0124e-19, 1.0124e-19],\n",
       "          [1.0000e+00, 6.8562e-20, 6.8562e-20, 6.8562e-20, 6.8562e-20, 6.8562e-20],\n",
       "          [1.0000e+00, 6.8775e-20, 6.8775e-20, 6.8775e-20, 6.8775e-20, 6.8775e-20],\n",
       "          [9.4306e-01, 1.1336e-02, 1.0921e-02, 1.1682e-02, 1.1077e-02, 1.1922e-02],\n",
       "          [1.0000e+00, 5.7047e-20, 5.7047e-20, 5.7047e-20, 5.7047e-20, 5.7047e-20],\n",
       "          [1.0000e+00, 8.8111e-20, 8.8111e-20, 8.8111e-20, 8.8111e-20, 8.8111e-20],\n",
       "          [1.0000e+00, 7.0286e-20, 7.0286e-20, 7.0286e-20, 7.0286e-20, 7.0286e-20],\n",
       "          [1.0000e+00, 5.2553e-20, 5.2553e-20, 5.2553e-20, 5.2553e-20, 5.2553e-20],\n",
       "          [1.0000e+00, 5.4283e-20, 5.4283e-20, 5.4283e-20, 5.4283e-20, 5.4283e-20],\n",
       "          [1.0000e+00, 6.8654e-20, 6.8654e-20, 6.8654e-20, 6.8654e-20, 6.8654e-20],\n",
       "          [1.0000e+00, 5.3484e-20, 5.3484e-20, 5.3484e-20, 5.3484e-20, 5.3484e-20],\n",
       "          [1.0000e+00, 6.5502e-20, 6.5502e-20, 6.5502e-20, 6.5502e-20, 6.5502e-20],\n",
       "          [1.0000e+00, 5.9238e-20, 5.9238e-20, 5.9238e-20, 5.9238e-20, 5.9238e-20],\n",
       "          [1.0000e+00, 4.2090e-20, 4.2090e-20, 4.2090e-20, 4.2090e-20, 4.2090e-20],\n",
       "          [1.0000e+00, 5.7185e-20, 5.7185e-20, 5.7185e-20, 5.7185e-20, 5.7185e-20],\n",
       "          [9.6407e-01, 7.0626e-03, 6.8776e-03, 7.5056e-03, 6.9977e-03, 7.4912e-03],\n",
       "          [1.0000e+00, 5.7142e-20, 5.7142e-20, 5.7142e-20, 5.7142e-20, 5.7142e-20],\n",
       "          [1.0000e+00, 4.7676e-20, 4.7676e-20, 4.7676e-20, 4.7676e-20, 4.7676e-20]],\n",
       "         grad_fn=<CopySlices>),\n",
       "  'cc': tensor([[9.9389e-03, 9.7852e-01, 5.9951e-03, 5.5493e-03],\n",
       "          [6.8736e-05, 9.9983e-01, 5.0966e-05, 5.0665e-05],\n",
       "          [4.0725e-03, 9.9035e-01, 2.8647e-03, 2.7098e-03],\n",
       "          [2.2465e-02, 9.4729e-01, 1.6000e-02, 1.4241e-02],\n",
       "          [3.4862e-02, 9.2160e-01, 2.3117e-02, 2.0417e-02],\n",
       "          [2.4548e-02, 9.4242e-01, 1.7736e-02, 1.5299e-02],\n",
       "          [4.2248e-03, 9.9011e-01, 2.8822e-03, 2.7837e-03],\n",
       "          [2.3355e-02, 9.4587e-01, 1.6293e-02, 1.4480e-02],\n",
       "          [2.4766e-02, 9.4477e-01, 1.6244e-02, 1.4222e-02],\n",
       "          [3.7710e-02, 9.1105e-01, 2.7743e-02, 2.3494e-02],\n",
       "          [3.3721e-02, 9.2033e-01, 2.4667e-02, 2.1287e-02],\n",
       "          [4.2803e-03, 9.8993e-01, 2.9682e-03, 2.8243e-03],\n",
       "          [3.9700e-02, 9.1045e-01, 2.6715e-02, 2.3131e-02],\n",
       "          [2.4123e-02, 9.4370e-01, 1.7123e-02, 1.5049e-02],\n",
       "          [2.5638e-02, 9.3880e-01, 1.8692e-02, 1.6868e-02],\n",
       "          [3.4505e-02, 9.1819e-01, 2.5607e-02, 2.1695e-02],\n",
       "          [6.9053e-04, 9.9834e-01, 4.9116e-04, 4.8099e-04],\n",
       "          [3.0598e-02, 9.3041e-01, 2.0667e-02, 1.8321e-02],\n",
       "          [3.5838e-02, 9.1941e-01, 2.3978e-02, 2.0778e-02],\n",
       "          [1.8748e-02, 9.5847e-01, 1.1973e-02, 1.0806e-02],\n",
       "          [2.9476e-02, 9.3371e-01, 1.9421e-02, 1.7389e-02],\n",
       "          [3.2191e-02, 9.2378e-01, 2.3777e-02, 2.0256e-02],\n",
       "          [3.0344e-02, 9.2822e-01, 2.1993e-02, 1.9443e-02],\n",
       "          [3.8278e-03, 9.9094e-01, 2.6764e-03, 2.5574e-03],\n",
       "          [4.6000e-05, 9.9988e-01, 3.7218e-05, 3.6917e-05],\n",
       "          [2.0281e-02, 9.5141e-01, 1.5160e-02, 1.3145e-02],\n",
       "          [5.1335e-05, 9.9987e-01, 4.1384e-05, 4.1028e-05],\n",
       "          [2.8644e-03, 9.9290e-01, 2.1580e-03, 2.0732e-03],\n",
       "          [2.9262e-02, 9.3226e-01, 2.0530e-02, 1.7947e-02],\n",
       "          [2.8028e-02, 9.3403e-01, 2.0428e-02, 1.7510e-02],\n",
       "          [2.9553e-02, 9.3220e-01, 2.0367e-02, 1.7882e-02],\n",
       "          [4.4794e-02, 8.9473e-01, 3.2593e-02, 2.7879e-02],\n",
       "          [3.9756e-02, 9.1114e-01, 2.6365e-02, 2.2743e-02],\n",
       "          [2.7228e-02, 9.3685e-01, 1.8961e-02, 1.6958e-02],\n",
       "          [1.5766e-02, 9.6509e-01, 1.0030e-02, 9.1155e-03],\n",
       "          [7.1765e-03, 9.8390e-01, 4.5942e-03, 4.3277e-03],\n",
       "          [2.5876e-02, 9.3813e-01, 1.9084e-02, 1.6907e-02],\n",
       "          [3.6412e-02, 9.1784e-01, 2.4215e-02, 2.1537e-02],\n",
       "          [2.0758e-02, 9.5375e-01, 1.3314e-02, 1.2181e-02],\n",
       "          [2.4429e-02, 9.4251e-01, 1.7689e-02, 1.5370e-02],\n",
       "          [3.9252e-02, 9.0942e-01, 2.7464e-02, 2.3866e-02],\n",
       "          [2.6487e-04, 9.9939e-01, 1.7319e-04, 1.7146e-04],\n",
       "          [1.6017e-02, 9.6238e-01, 1.1293e-02, 1.0315e-02],\n",
       "          [4.4219e-03, 9.8978e-01, 2.9594e-03, 2.8390e-03],\n",
       "          [2.5021e-02, 9.4188e-01, 1.7711e-02, 1.5392e-02],\n",
       "          [5.2562e-03, 9.8778e-01, 3.5887e-03, 3.3757e-03],\n",
       "          [2.7453e-02, 9.3843e-01, 1.7927e-02, 1.6192e-02],\n",
       "          [1.4364e-02, 9.6823e-01, 9.1590e-03, 8.2480e-03],\n",
       "          [4.1246e-02, 9.0592e-01, 2.8240e-02, 2.4594e-02],\n",
       "          [3.2345e-02, 9.2474e-01, 2.2881e-02, 2.0036e-02],\n",
       "          [5.3586e-03, 9.8847e-01, 3.1605e-03, 3.0109e-03],\n",
       "          [5.1789e-02, 8.8293e-01, 3.5128e-02, 3.0155e-02],\n",
       "          [1.9067e-02, 9.5491e-01, 1.3770e-02, 1.2249e-02],\n",
       "          [3.9774e-02, 9.0604e-01, 2.9439e-02, 2.4746e-02],\n",
       "          [2.8363e-02, 9.3328e-01, 2.0661e-02, 1.7694e-02],\n",
       "          [4.0592e-02, 9.0593e-01, 2.8303e-02, 2.5178e-02],\n",
       "          [2.7815e-02, 9.3759e-01, 1.8210e-02, 1.6384e-02],\n",
       "          [3.6447e-02, 9.1748e-01, 2.4614e-02, 2.1460e-02],\n",
       "          [9.8512e-05, 9.9977e-01, 6.7391e-05, 6.6991e-05],\n",
       "          [3.5908e-02, 9.1723e-01, 2.4995e-02, 2.1865e-02],\n",
       "          [3.5872e-02, 9.1667e-01, 2.5572e-02, 2.1881e-02],\n",
       "          [3.0126e-02, 9.3196e-01, 1.9952e-02, 1.7967e-02],\n",
       "          [2.0924e-02, 9.4870e-01, 1.6053e-02, 1.4319e-02],\n",
       "          [2.6941e-02, 9.3840e-01, 1.8365e-02, 1.6299e-02],\n",
       "          [7.9904e-05, 9.9980e-01, 6.1602e-05, 6.1057e-05],\n",
       "          [2.7317e-02, 9.3734e-01, 1.8815e-02, 1.6524e-02],\n",
       "          [3.0295e-02, 9.2833e-01, 2.2099e-02, 1.9277e-02],\n",
       "          [2.1727e-02, 9.4771e-01, 1.6066e-02, 1.4496e-02],\n",
       "          [3.9032e-02, 9.0951e-01, 2.7607e-02, 2.3853e-02],\n",
       "          [3.0156e-02, 9.3062e-01, 2.0841e-02, 1.8388e-02],\n",
       "          [2.1590e-02, 9.4933e-01, 1.5326e-02, 1.3753e-02],\n",
       "          [4.1518e-02, 9.0690e-01, 2.7631e-02, 2.3948e-02],\n",
       "          [3.5937e-02, 9.1741e-01, 2.5108e-02, 2.1542e-02],\n",
       "          [2.8034e-02, 9.3369e-01, 2.0450e-02, 1.7826e-02],\n",
       "          [1.1016e-02, 9.7428e-01, 7.6378e-03, 7.0695e-03],\n",
       "          [1.5066e-02, 9.6456e-01, 1.0635e-02, 9.7407e-03],\n",
       "          [6.8797e-03, 9.8408e-01, 4.6608e-03, 4.3757e-03],\n",
       "          [3.7371e-02, 9.1403e-01, 2.5653e-02, 2.2950e-02],\n",
       "          [1.0565e-02, 9.7543e-01, 7.2286e-03, 6.7723e-03],\n",
       "          [1.8847e-02, 9.5583e-01, 1.3337e-02, 1.1991e-02],\n",
       "          [2.4969e-02, 9.4128e-01, 1.8021e-02, 1.5727e-02],\n",
       "          [3.0323e-02, 9.2944e-01, 2.1619e-02, 1.8621e-02],\n",
       "          [3.5545e-02, 9.1897e-01, 2.3866e-02, 2.1624e-02],\n",
       "          [1.9469e-02, 9.5680e-01, 1.2447e-02, 1.1280e-02],\n",
       "          [3.9033e-03, 9.9084e-01, 2.6778e-03, 2.5787e-03],\n",
       "          [2.2770e-02, 9.4706e-01, 1.6009e-02, 1.4159e-02],\n",
       "          [1.3740e-02, 9.6632e-01, 1.0458e-02, 9.4868e-03],\n",
       "          [9.1562e-03, 9.7928e-01, 6.0278e-03, 5.5401e-03],\n",
       "          [2.4838e-02, 9.4326e-01, 1.6964e-02, 1.4935e-02],\n",
       "          [9.8255e-03, 9.7777e-01, 6.4359e-03, 5.9726e-03],\n",
       "          [2.8347e-02, 9.3456e-01, 1.9617e-02, 1.7480e-02],\n",
       "          [2.2277e-02, 9.4812e-01, 1.5705e-02, 1.3894e-02],\n",
       "          [3.3931e-02, 9.2039e-01, 2.3911e-02, 2.1769e-02],\n",
       "          [2.1793e-02, 9.4913e-01, 1.5521e-02, 1.3560e-02],\n",
       "          [3.9265e-03, 9.9141e-01, 2.3983e-03, 2.2688e-03],\n",
       "          [1.1535e-02, 9.7351e-01, 7.7293e-03, 7.2296e-03],\n",
       "          [3.0707e-02, 9.2715e-01, 2.2255e-02, 1.9885e-02],\n",
       "          [2.9911e-03, 9.9333e-01, 1.8738e-03, 1.8036e-03],\n",
       "          [3.8360e-02, 9.1402e-01, 2.5555e-02, 2.2066e-02],\n",
       "          [1.1916e-02, 9.7199e-01, 8.3127e-03, 7.7801e-03],\n",
       "          [2.3269e-02, 9.4711e-01, 1.5594e-02, 1.4028e-02],\n",
       "          [3.9570e-02, 9.1051e-01, 2.6584e-02, 2.3338e-02],\n",
       "          [4.4454e-02, 8.9545e-01, 3.2398e-02, 2.7695e-02],\n",
       "          [3.0635e-03, 9.9321e-01, 1.9143e-03, 1.8153e-03],\n",
       "          [3.6965e-02, 9.1363e-01, 2.6670e-02, 2.2736e-02],\n",
       "          [4.7378e-02, 8.9219e-01, 3.2554e-02, 2.7875e-02],\n",
       "          [3.2917e-02, 9.2452e-01, 2.2665e-02, 1.9901e-02],\n",
       "          [2.7250e-02, 9.3899e-01, 1.7972e-02, 1.5792e-02],\n",
       "          [2.3287e-02, 9.4685e-01, 1.5693e-02, 1.4164e-02],\n",
       "          [2.8714e-02, 9.3276e-01, 2.0462e-02, 1.8069e-02],\n",
       "          [2.8496e-02, 9.3458e-01, 1.9286e-02, 1.7638e-02],\n",
       "          [3.9468e-02, 9.0903e-01, 2.7776e-02, 2.3731e-02],\n",
       "          [3.7859e-02, 9.1109e-01, 2.7664e-02, 2.3384e-02],\n",
       "          [1.1330e-02, 9.7365e-01, 7.8525e-03, 7.1709e-03],\n",
       "          [4.0459e-02, 9.0554e-01, 2.8879e-02, 2.5122e-02],\n",
       "          [2.6231e-02, 9.3867e-01, 1.8576e-02, 1.6524e-02],\n",
       "          [3.5026e-02, 9.2210e-01, 2.2849e-02, 2.0020e-02],\n",
       "          [3.8492e-02, 9.1370e-01, 2.5578e-02, 2.2229e-02],\n",
       "          [1.6817e-02, 9.6216e-01, 1.0979e-02, 1.0047e-02],\n",
       "          [3.7715e-02, 9.1414e-01, 2.5346e-02, 2.2797e-02],\n",
       "          [1.8342e-02, 9.5713e-01, 1.2849e-02, 1.1678e-02],\n",
       "          [2.7341e-02, 9.3897e-01, 1.7877e-02, 1.5808e-02],\n",
       "          [4.5746e-02, 8.9580e-01, 3.1421e-02, 2.7034e-02],\n",
       "          [4.1579e-02, 9.0223e-01, 3.0519e-02, 2.5671e-02],\n",
       "          [3.5412e-02, 9.1701e-01, 2.5570e-02, 2.2005e-02],\n",
       "          [3.2921e-02, 9.2367e-01, 2.3363e-02, 2.0042e-02],\n",
       "          [1.4208e-02, 9.6694e-01, 9.7203e-03, 9.1344e-03],\n",
       "          [4.2969e-03, 9.9004e-01, 2.9157e-03, 2.7494e-03],\n",
       "          [7.2043e-03, 9.8300e-01, 5.0842e-03, 4.7066e-03],\n",
       "          [2.6169e-02, 9.4118e-01, 1.7328e-02, 1.5324e-02],\n",
       "          [2.0736e-02, 9.5463e-01, 1.2897e-02, 1.1737e-02],\n",
       "          [2.7393e-02, 9.3563e-01, 1.9860e-02, 1.7117e-02],\n",
       "          [2.6654e-02, 9.3857e-01, 1.8171e-02, 1.6610e-02],\n",
       "          [3.2233e-02, 9.2301e-01, 2.3965e-02, 2.0790e-02],\n",
       "          [3.8134e-02, 9.1069e-01, 2.7727e-02, 2.3451e-02],\n",
       "          [2.8375e-02, 9.3447e-01, 1.9590e-02, 1.7560e-02],\n",
       "          [3.8841e-02, 9.0983e-01, 2.7296e-02, 2.4037e-02],\n",
       "          [2.8517e-02, 9.3144e-01, 2.1343e-02, 1.8700e-02],\n",
       "          [2.6789e-02, 9.3551e-01, 2.0107e-02, 1.7589e-02],\n",
       "          [3.8477e-02, 9.1421e-01, 2.5151e-02, 2.2167e-02],\n",
       "          [2.5773e-02, 9.3910e-01, 1.8569e-02, 1.6555e-02],\n",
       "          [2.9400e-02, 9.3039e-01, 2.1487e-02, 1.8725e-02],\n",
       "          [6.5723e-05, 9.9983e-01, 5.0108e-05, 4.9834e-05],\n",
       "          [9.9206e-03, 9.7690e-01, 6.8317e-03, 6.3517e-03],\n",
       "          [3.4373e-02, 9.2111e-01, 2.3374e-02, 2.1142e-02],\n",
       "          [2.2934e-02, 9.4859e-01, 1.4939e-02, 1.3541e-02],\n",
       "          [2.7357e-02, 9.3410e-01, 2.0543e-02, 1.8003e-02]],\n",
       "         grad_fn=<CopySlices>),\n",
       "  'dlt1': tensor([[1.2511e-13, 2.5146e-11, 4.7221e-10, 1.1828e-14, 4.7815e-13],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [8.9422e-02, 1.4623e-01, 1.4872e-01, 8.2682e-02, 1.1969e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [8.1032e-14, 2.4479e-11, 4.0830e-10, 1.2520e-14, 3.7395e-13],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.4588e-01, 1.7067e-01, 1.8852e-01, 1.2124e-01, 1.4473e-01],\n",
       "          [1.2966e-01, 1.4542e-01, 1.3645e-01, 9.9954e-02, 1.1423e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [6.3851e-02, 1.1019e-01, 1.3202e-01, 6.2661e-02, 8.1944e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [9.2880e-02, 1.3465e-01, 1.2032e-01, 6.1232e-02, 8.8740e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.0524e-01, 1.8229e-01, 1.9647e-01, 1.4657e-01, 1.3877e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [3.9557e-02, 1.2131e-01, 1.0404e-01, 9.0946e-02, 7.0090e-02],\n",
       "          [9.8061e-02, 1.7145e-01, 1.8906e-01, 1.2718e-01, 1.2642e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.3083e-01, 1.6805e-01, 1.8318e-01, 1.2216e-01, 1.4084e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [7.5313e-02, 1.0039e-01, 1.5183e-01, 5.3073e-02, 9.5986e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.1885e-01, 1.6868e-01, 1.8438e-01, 1.3502e-01, 1.5176e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [6.3001e-02, 1.2077e-01, 1.2764e-01, 8.3531e-02, 9.2144e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.0279e-01, 1.4553e-01, 1.6049e-01, 1.2810e-01, 1.2113e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.0389e-01, 1.6813e-01, 1.8917e-01, 1.2470e-01, 1.2111e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.0364e-01, 1.5661e-01, 1.5451e-01, 1.1966e-01, 1.1295e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.2386e-01, 1.7837e-01, 1.8881e-01, 1.2221e-01, 1.3998e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [9.3306e-02, 1.3600e-01, 1.7121e-01, 1.1317e-01, 1.3712e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
       "         grad_fn=<CopySlices>),\n",
       "  'dlt2': tensor([[1.2220e-02, 9.5679e-03, 5.1572e-03, 1.2922e-02, 7.1978e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.6960e-02, 1.7289e-02, 1.3153e-02, 2.4891e-02, 1.6459e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.2530e-02, 1.6140e-02, 1.0507e-02, 1.9642e-02, 1.2524e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [3.6736e-02, 2.4713e-02, 2.0817e-02, 3.2109e-02, 2.3147e-02],\n",
       "          [3.4977e-02, 2.4212e-02, 1.9813e-02, 3.2864e-02, 2.1349e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.2771e-02, 1.6134e-02, 1.1959e-02, 2.1542e-02, 1.4119e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [3.6019e-02, 2.1465e-02, 1.9554e-02, 3.3437e-02, 2.2694e-02],\n",
       "          [2.2421e-03, 9.4574e-04, 4.7767e-04, 1.9261e-03, 8.3058e-04],\n",
       "          [3.9424e-02, 2.7129e-02, 1.7742e-02, 3.6759e-02, 2.3407e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [3.1493e-02, 1.9257e-02, 1.6215e-02, 2.8474e-02, 1.9643e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [7.7968e-03, 4.4736e-03, 2.3903e-03, 5.1385e-03, 3.2988e-03],\n",
       "          [1.0952e-04, 5.8633e-05, 3.3072e-05, 1.4697e-04, 4.5861e-05],\n",
       "          [2.5260e-02, 1.4884e-02, 1.3568e-02, 2.6216e-02, 1.6233e-02],\n",
       "          [1.1992e-04, 5.7464e-05, 3.6721e-05, 1.4312e-04, 4.6682e-05],\n",
       "          [7.2997e-03, 4.0866e-03, 2.9022e-03, 8.3175e-03, 3.5756e-03],\n",
       "          [3.0042e-02, 2.0512e-02, 1.6716e-02, 3.2448e-02, 2.0683e-02],\n",
       "          [2.9875e-02, 1.9819e-02, 1.5442e-02, 2.7617e-02, 1.8756e-02],\n",
       "          [2.6638e-02, 2.0516e-02, 1.4177e-02, 2.6087e-02, 1.6954e-02],\n",
       "          [5.2510e-02, 3.7924e-02, 3.0056e-02, 5.3557e-02, 3.7457e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.0856e-02, 9.0985e-03, 4.9037e-03, 1.3125e-02, 7.0967e-03],\n",
       "          [3.7100e-02, 2.6461e-02, 1.9057e-02, 3.4028e-02, 2.4893e-02],\n",
       "          [3.4799e-02, 2.7178e-02, 2.0663e-02, 3.9815e-02, 2.4058e-02],\n",
       "          [2.4873e-02, 2.0460e-02, 1.4045e-02, 2.7862e-02, 1.4254e-02],\n",
       "          [2.5783e-02, 1.7212e-02, 1.3376e-02, 2.3633e-02, 1.5452e-02],\n",
       "          [4.6968e-02, 3.3800e-02, 2.2016e-02, 4.1344e-02, 2.9201e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.4685e-02, 1.6454e-02, 1.2380e-02, 2.2515e-02, 1.5307e-02],\n",
       "          [8.1092e-03, 4.4968e-03, 3.3537e-03, 6.0927e-03, 3.4277e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [4.2040e-02, 3.0150e-02, 2.2758e-02, 4.1827e-02, 2.8885e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [4.0770e-02, 2.7109e-02, 2.3536e-02, 3.7088e-02, 2.4674e-02],\n",
       "          [3.0294e-02, 2.0149e-02, 1.5637e-02, 2.8009e-02, 1.9015e-02],\n",
       "          [4.9012e-02, 3.6838e-02, 2.8358e-02, 5.5742e-02, 3.1949e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [3.9612e-02, 3.0221e-02, 1.9337e-02, 3.5051e-02, 2.5278e-02],\n",
       "          [2.9539e-02, 2.0932e-02, 1.6584e-02, 2.7863e-02, 1.8978e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.9631e-02, 2.4170e-02, 1.6426e-02, 2.7022e-02, 1.9104e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.2212e-02, 1.6865e-02, 1.1842e-02, 2.0823e-02, 1.3725e-02],\n",
       "          [3.0914e-02, 2.3434e-02, 1.7845e-02, 3.0506e-02, 2.1103e-02],\n",
       "          [3.3928e-02, 2.5980e-02, 1.7907e-02, 3.1208e-02, 2.2190e-02],\n",
       "          [4.3732e-02, 3.0681e-02, 2.2631e-02, 4.2933e-02, 2.9629e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [3.1722e-02, 2.4472e-02, 1.6394e-02, 2.9827e-02, 1.9841e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [3.1519e-02, 2.0654e-02, 1.7322e-02, 3.0688e-02, 2.0681e-02],\n",
       "          [2.8894e-02, 1.8404e-02, 1.3366e-02, 2.4810e-02, 1.6397e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.1080e-02, 6.8540e-03, 4.6825e-03, 1.0840e-02, 5.8306e-03],\n",
       "          [4.4163e-02, 3.1858e-02, 2.5509e-02, 4.9270e-02, 2.7436e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.8748e-02, 1.9239e-02, 1.3273e-02, 2.3712e-02, 1.6005e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [3.8205e-02, 3.4699e-02, 2.3484e-02, 4.7393e-02, 2.7608e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [8.4209e-03, 4.9632e-03, 3.1355e-03, 7.6513e-03, 3.5508e-03],\n",
       "          [2.2586e-02, 1.5733e-02, 1.0868e-02, 1.9495e-02, 1.2912e-02],\n",
       "          [2.5077e-02, 1.6780e-02, 1.0855e-02, 2.3234e-02, 1.3898e-02],\n",
       "          [1.1411e-02, 7.2962e-03, 5.4770e-03, 1.0657e-02, 5.9485e-03],\n",
       "          [2.1327e-02, 1.4923e-02, 1.0604e-02, 1.9993e-02, 1.3282e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.2188e-02, 1.5443e-02, 1.0687e-02, 1.9208e-02, 1.2637e-02],\n",
       "          [4.0536e-02, 3.1712e-02, 2.5871e-02, 4.9858e-02, 2.6900e-02],\n",
       "          [2.1713e-02, 1.4459e-02, 1.1027e-02, 1.9713e-02, 1.3269e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.8076e-02, 1.3105e-02, 8.4121e-03, 2.2350e-02, 1.1205e-02],\n",
       "          [3.5974e-02, 2.7396e-02, 2.2154e-02, 3.8964e-02, 2.8578e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [3.8143e-02, 3.2438e-02, 2.0836e-02, 3.5801e-02, 2.5900e-02],\n",
       "          [5.1675e-02, 3.7659e-02, 2.9817e-02, 5.3011e-02, 3.7259e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [3.5334e-02, 2.3366e-02, 1.8935e-02, 3.3083e-02, 2.2723e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [3.7676e-02, 2.7510e-02, 1.7545e-02, 3.3989e-02, 2.3838e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [3.1755e-02, 1.9191e-02, 1.4799e-02, 2.9074e-02, 2.0201e-02],\n",
       "          [3.1928e-02, 2.8429e-02, 1.9834e-02, 3.7526e-02, 2.1661e-02],\n",
       "          [3.4646e-02, 2.3528e-02, 1.8430e-02, 3.2010e-02, 2.1906e-02],\n",
       "          [3.5762e-02, 2.3827e-02, 2.0528e-02, 3.1767e-02, 2.3166e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [4.3930e-02, 3.0993e-02, 2.2712e-02, 4.1602e-02, 2.8644e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [4.1897e-02, 3.3844e-02, 2.5750e-02, 4.5548e-02, 2.9571e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [4.4572e-02, 2.7796e-02, 2.4445e-02, 4.0296e-02, 2.7468e-02],\n",
       "          [3.7308e-02, 2.6967e-02, 2.1927e-02, 3.7710e-02, 2.7282e-02],\n",
       "          [2.7414e-02, 1.8738e-02, 1.5068e-02, 2.6190e-02, 1.7945e-02],\n",
       "          [1.9885e-02, 1.3415e-02, 1.0265e-02, 1.9721e-02, 1.0365e-02],\n",
       "          [6.6037e-03, 3.3969e-03, 2.6482e-03, 4.5047e-03, 2.7092e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.9257e-02, 1.9828e-02, 1.5206e-02, 2.6882e-02, 1.8029e-02],\n",
       "          [3.3566e-02, 2.6220e-02, 1.8444e-02, 3.6680e-02, 2.2217e-02],\n",
       "          [3.7490e-02, 2.8323e-02, 2.2700e-02, 3.8153e-02, 2.8249e-02],\n",
       "          [3.8630e-02, 2.5608e-02, 2.4088e-02, 3.9626e-02, 2.6879e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [3.7639e-02, 2.7905e-02, 1.9675e-02, 3.3469e-02, 2.5723e-02],\n",
       "          [3.2013e-02, 2.5019e-02, 1.9303e-02, 3.2010e-02, 2.4721e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.4060e-02, 9.1321e-03, 6.5300e-03, 1.0328e-02, 6.9054e-03],\n",
       "          [4.2271e-02, 2.8845e-02, 2.2631e-02, 4.3508e-02, 2.5982e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [3.6854e-02, 2.5700e-02, 1.5954e-02, 2.9198e-02, 2.1653e-02]],\n",
       "         grad_fn=<CopySlices>)})"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def temporal_loss(timestoevents,weights=None,maxtime=48,threshold=True):\n",
    "    #list of expected times to events, usualy in order of Const.temporal_outcomes\n",
    "    #basically longer = better, we count > maxtime (weeks) as no event\n",
    "    if weights is None: \n",
    "        weights = [1 for i in range(len(timestoevents))]\n",
    "    scores =  [(w*maxtime/t)for w,t in zip(weights,timestoevents)]\n",
    "    if threshold:\n",
    "        scores = [s*torch.lt(t,maxtime) for s,t in zip(scores,timestoevents)]\n",
    "    scores = torch.stack(scores).sum(axis=0)\n",
    "    return scores\n",
    "\n",
    "def outcome_loss(ypred,weights=None):\n",
    "    #default weights is bad\n",
    "    if weights is None: \n",
    "        print('using default outcome loss weights, which is probably wrong since bad stuff should be negative')\n",
    "        weights = [1 for i in range(ypred.shape[1])]\n",
    "    l = torch.mul(ypred[:,0],weights[0])\n",
    "    for i,weight in enumerate(weights[1:]):\n",
    "        #weights with negative values will invert the outcome so e.g. Regional control becomes no regional control\n",
    "        #so the penaly is correct\n",
    "        newloss = torch.mul(ypred[:,i+1],weight)\n",
    "        l = torch.add(l,newloss)\n",
    "    return l\n",
    "\n",
    "def calc_optimal_decisions(dataset,ids,m1,m2,m3,sm3,\n",
    "                           weights=[0,0.5,.5,0], #weight for OS, FT, AS, and LRC as binary probabilities\n",
    "                           tweights=[1,1,1,1], #weight for OS, LRC, FDM, and event (any + FT or AS at 6m) as time to event in weeks\n",
    "                           outcome_loss_func=None,\n",
    "                           threshold_temporal_loss = False,\n",
    "                           maxtime=48,\n",
    "                           get_transitions=True):\n",
    "    m1.eval()\n",
    "    m2.eval()\n",
    "    m3.eval()\n",
    "    sm3.eval()\n",
    "    device = m1.get_device()\n",
    "    data = dataset.processed_df.copy().loc[ids]\n",
    "    \n",
    "    def get_dlt(state):\n",
    "        if state == 2:\n",
    "            return data[Const.dlt2].copy()\n",
    "        d = data[Const.dlt1].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_pd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.primary_disease_states2].copy()\n",
    "        d = data[Const.primary_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_nd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.nodal_disease_states2].copy()\n",
    "        d = data[Const.nodal_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_cc(state):\n",
    "        res = data[Const.ccs].copy()\n",
    "        if state == 1:\n",
    "            res.values[:,:] = np.zeros(res.values.shape)\n",
    "        return res\n",
    "    \n",
    "    def get_mod(state):\n",
    "        res = data[Const.modifications].copy()\n",
    "        #this should have an ic condition but we don't use it anumore anywa\n",
    "        return res\n",
    "        \n",
    "    def formatdf(d):\n",
    "        d = df_to_torch(d).to(device)\n",
    "        return d\n",
    "    \n",
    "    \n",
    "    outcomedf = data[Const.outcomes]\n",
    "    baseline = dataset.get_state('baseline').loc[ids]\n",
    "    baseline_input = formatdf(baseline)\n",
    "\n",
    "        \n",
    "    if outcome_loss_func is None:\n",
    "        outcome_loss_func = outcome_loss\n",
    "    \n",
    "    cat = lambda x: torch.cat([xx.to(device) for xx in x],axis=1).to(device)\n",
    "    format_transition = lambda x: x.to(device)\n",
    "    def get_outcome(d1,d2,d3):\n",
    "        d1 = torch.full((len(ids),1),d1).type(torch.FloatTensor)\n",
    "        d2 = torch.full((len(ids),1),d2).type(torch.FloatTensor)\n",
    "        d3 = torch.full((len(ids),1),d3).type(torch.FloatTensor)\n",
    "        \n",
    "        tinput1 = cat([baseline_input,d1])\n",
    "        ytransition = m1(tinput1)\n",
    "        [ypd1,ynd1,ymod,ydlt1] = [format_transition(xx) for xx in ytransition['predictions']]\n",
    "        d1_thresh = torch.gt(d1,.5).view(-1,1).to(device)\n",
    "        ypd1[:,0:2] = ypd1[:,0:2]*d1_thresh\n",
    "        ynd1[:,0:2] = ynd1[:,0:2]*d1_thresh\n",
    "        \n",
    "        tinput2 = cat([baseline_input,ypd1,ynd1,ymod,ydlt1,d1,d2])\n",
    "        ytransition2 = m2(tinput2)\n",
    "        [ypd2,ynd2,ycc,ydlt2] = [format_transition(xx) for xx in ytransition2['predictions']]\n",
    "        \n",
    "        input3 = cat([baseline_input, ypd2, ynd2, ycc, ydlt2, d1, d2,d3])\n",
    "        outcome = m3(input3)['predictions']\n",
    "        temporal_outcomes = sm3.time_to_event(input3,n_samples=1)\n",
    "        \n",
    "        transitions = {\n",
    "            'pd1': ypd1,\n",
    "            'nd1': ynd1,\n",
    "            'nd2': ynd2,\n",
    "            'pd2': ypd2,\n",
    "            'mod': ymod,\n",
    "            'cc': ycc,\n",
    "            'dlt1': ydlt1,\n",
    "            'dlt2': ydlt2,\n",
    "        }\n",
    "        return outcome, temporal_outcomes, transitions\n",
    "\n",
    "    losses = []\n",
    "    loss_order = []\n",
    "    transitions = {}\n",
    "    for d1 in [0,1]:\n",
    "        for d2 in [0,1]:\n",
    "            for d3 in [0,1]:\n",
    "                outcomes, tte, transition_entry = get_outcome(d1,d2,d3)\n",
    "                loss = outcome_loss_func(outcomes,weights)\n",
    "                tloss = temporal_loss(tte,tweights,maxtime=maxtime,threshold=threshold_temporal_loss)\n",
    "                loss += tloss\n",
    "                losses.append(loss)\n",
    "                loss_order.append([d1,d2,d3])\n",
    "                transitions[str(d1)+str(d2)+str(d3)] = transition_entry\n",
    "    losses = torch.stack(losses,axis=1)\n",
    "    optimal_decisions = [loss_order[i] for i in torch.argmin(losses,axis=1)]\n",
    "    result = torch.tensor(optimal_decisions).type(torch.FloatTensor)\n",
    "    print(result.sum(axis=0),result.shape[0])\n",
    "    if get_transitions:\n",
    "        opt_transitions = {k: torch.zeros(v.shape).type(torch.FloatTensor) for k,v in transitions['000'].items()}\n",
    "        for i,od in enumerate(optimal_decisions):\n",
    "            key = ''.join([str(o) for o in od])\n",
    "            entry = transitions[key]\n",
    "            for kk,vv in entry.items():\n",
    "                opt_transitions[kk][i,:] = vv[i,:]\n",
    "        return result, opt_transitions\n",
    "    return result\n",
    "\n",
    "test, testtest = get_tt_split()\n",
    "calc_optimal_decisions(DTDataset(),\n",
    "                       testtest,model1,model2,model3,smodel3,\n",
    "                       threshold_temporal_loss=False,\n",
    "                       maxtime=48,\n",
    "                       weights=[0,0,0,0],\n",
    "                       tweights=[2,0.1,0,0],\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "122ee514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 36., 225.,   1.]) 389\n",
      "tensor([19., 79.,  1.]) 147\n",
      "torch.Size([3, 536, 87])\n",
      "tensor([0.1293, 0.5374, 0.0068], grad_fn=<MeanBackward1>)ard0>) row 46 step 2 imitation False\n",
      "______epoch 0 _____e 1 yt tensor(1., grad_fn=<SelectBackward0>) row 1 step 2 imitation False\n",
      "val reward 1.4816501140594482\n",
      "imitation reward 1.669093132019043\n",
      "distance losses 2.7673580646514893 1.936036229133606\n",
      "distributions [0.009965619072318077, 0.7813202738761902, 0.008228734135627747]\n",
      "[{'decision': 0, 'optimal_auc': 0.24136513157894737, 'imitation_auc': 0.5825381679389314, 'optimal_acc': 0.8707482993197279, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.7468354430379747, 'imitation_auc': 0.41657608695652176, 'optimal_acc': 0.5374149659863946, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.8493150684931507, 'imitation_auc': 0.8137317228226318, 'optimal_acc': 0.9931972789115646, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.1293, 0.5374, 0.0068], grad_fn=<MeanBackward1>)ard0>) row 46 step 2 imitation False\n",
      "______epoch 1 _____e 1 yt tensor(1., grad_fn=<SelectBackward0>) row 1 step 2 imitation False\n",
      "val reward 1.7158873081207275\n",
      "imitation reward 1.3119810819625854\n",
      "distance losses 2.5576417446136475 1.7445014715194702\n",
      "distributions [0.010624644346535206, 0.15965227782726288, 0.0028129785787314177]\n",
      "[{'decision': 0, 'optimal_auc': 0.11389802631578948, 'imitation_auc': 0.49761450381679384, 'optimal_acc': 0.8707482993197279, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.7207743857036485, 'imitation_auc': 0.439945652173913, 'optimal_acc': 0.46258503401360546, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.1917808219178082, 'imitation_auc': 0.6967577876668786, 'optimal_acc': 0.9931972789115646, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.1293, 0.5374, 0.0068], grad_fn=<MeanBackward1>)ard0>) row 46 step 2 imitation False\n",
      "______epoch 2 _____e 1 yt tensor(1., grad_fn=<SelectBackward0>) row 1 step 2 imitation False\n",
      "val reward 1.2041293382644653\n",
      "imitation reward 1.201632022857666\n",
      "distance losses 2.5348711013793945 1.9369856119155884\n",
      "distributions [0.034910231828689575, 0.5970985293388367, 0.005489114671945572]\n",
      "[{'decision': 0, 'optimal_auc': 0.1644736842105263, 'imitation_auc': 0.4799618320610687, 'optimal_acc': 0.8707482993197279, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.7699180938198064, 'imitation_auc': 0.47146739130434784, 'optimal_acc': 0.5374149659863946, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.013698630136986356, 'imitation_auc': 0.6700572155117609, 'optimal_acc': 0.9931972789115646, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.1293, 0.5374, 0.0068], grad_fn=<MeanBackward1>)ard0>) row 46 step 2 imitation False\n",
      "______epoch 3 _____e 1 yt tensor(1., grad_fn=<SelectBackward0>) row 1 step 2 imitation False\n",
      "val reward 1.2983943223953247\n",
      "imitation reward 1.1752930879592896\n",
      "distance losses 2.335986614227295 1.87104332447052\n",
      "distributions [0.05463947728276253, 0.7783247232437134, 0.004663855768740177]\n",
      "[{'decision': 0, 'optimal_auc': 0.1648848684210526, 'imitation_auc': 0.47089694656488545, 'optimal_acc': 0.8707482993197279, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.7771779597915115, 'imitation_auc': 0.44157608695652173, 'optimal_acc': 0.5374149659863946, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.03424657534246578, 'imitation_auc': 0.670057215511761, 'optimal_acc': 0.9931972789115646, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.1293, 0.5374, 0.0068], grad_fn=<MeanBackward1>)ard0>) row 46 step 2 imitation False\n",
      "______epoch 4 _____e 1 yt tensor(1., grad_fn=<SelectBackward0>) row 1 step 2 imitation False\n",
      "val reward 1.1925007104873657\n",
      "imitation reward 1.1629769802093506\n",
      "distance losses 2.5411617755889893 2.1100668907165527\n",
      "distributions [0.06589812785387039, 0.706706702709198, 0.0026849466376006603]\n",
      "[{'decision': 0, 'optimal_auc': 0.2833059210526316, 'imitation_auc': 0.5295801526717557, 'optimal_acc': 0.8707482993197279, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.7916976917349218, 'imitation_auc': 0.4706521739130435, 'optimal_acc': 0.5374149659863946, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.11643835616438358, 'imitation_auc': 0.6652892561983471, 'optimal_acc': 0.9931972789115646, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.1293, 0.5374, 0.0068], grad_fn=<MeanBackward1>)ard0>) row 46 step 2 imitation False\n",
      "______epoch 5 _____e 1 yt tensor(1., grad_fn=<SelectBackward0>) row 1 step 2 imitation False\n",
      "val reward 1.107973575592041\n",
      "imitation reward 1.1584621667861938\n",
      "distance losses 2.375786304473877 2.234543561935425\n",
      "distributions [0.08124831318855286, 0.4985361397266388, 0.0017507814336568117]\n",
      "[{'decision': 0, 'optimal_auc': 0.6274671052631579, 'imitation_auc': 0.5343511450381679, 'optimal_acc': 0.8707482993197279, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.7933730454207, 'imitation_auc': 0.5961956521739131, 'optimal_acc': 0.7278911564625851, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.0821917808219178, 'imitation_auc': 0.6875397329942785, 'optimal_acc': 0.9931972789115646, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.1293, 0.5374, 0.0068], grad_fn=<MeanBackward1>)ard0>) row 46 step 2 imitation False\n",
      "______epoch 6 _____e 1 yt tensor(1., grad_fn=<SelectBackward0>) row 1 step 2 imitation False\n",
      "val reward 1.102789282798767\n",
      "imitation reward 1.1423990726470947\n",
      "distance losses 2.4018025398254395 2.233139753341675\n",
      "distributions [0.10779555886983871, 0.4339101016521454, 0.0013807408977299929]\n",
      "[{'decision': 0, 'optimal_auc': 0.8421052631578947, 'imitation_auc': 0.5562977099236641, 'optimal_acc': 0.8707482993197279, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.7948622486969471, 'imitation_auc': 0.6785326086956522, 'optimal_acc': 0.46258503401360546, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.1643835616438356, 'imitation_auc': 0.6884933248569612, 'optimal_acc': 0.9931972789115646, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.1293, 0.5374, 0.0068], grad_fn=<MeanBackward1>)ard0>) row 46 step 2 imitation False\n",
      "______epoch 7 _____e 1 yt tensor(1., grad_fn=<SelectBackward0>) row 1 step 2 imitation False\n",
      "val reward 1.0691978931427002\n",
      "imitation reward 1.136480450630188\n",
      "distance losses 2.492255687713623 2.053197145462036\n",
      "distributions [0.11982711404561996, 0.5570741295814514, 0.001058247173205018]\n",
      "[{'decision': 0, 'optimal_auc': 0.8914473684210527, 'imitation_auc': 0.5546278625954199, 'optimal_acc': 0.8707482993197279, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.778108711839166, 'imitation_auc': 0.6891304347826087, 'optimal_acc': 0.5714285714285714, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.6849315068493151, 'imitation_auc': 0.7015257469802925, 'optimal_acc': 0.9931972789115646, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.1293, 0.5374, 0.0068], grad_fn=<MeanBackward1>)ard0>) row 46 step 2 imitation False\n",
      "______epoch 8 _____e 1 yt tensor(1., grad_fn=<SelectBackward0>) row 1 step 2 imitation False\n",
      "val reward 1.108651041984558\n",
      "imitation reward 1.1355490684509277\n",
      "distance losses 2.7289836406707764 2.1198465824127197\n",
      "distributions [0.10280260443687439, 0.6818086504936218, 0.0006962607149034739]\n",
      "[{'decision': 0, 'optimal_auc': 0.897203947368421, 'imitation_auc': 0.6316793893129771, 'optimal_acc': 0.8707482993197279, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.7712211466865228, 'imitation_auc': 0.6918478260869565, 'optimal_acc': 0.5374149659863946, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.7671232876712328, 'imitation_auc': 0.6999364272091545, 'optimal_acc': 0.9931972789115646, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.1293, 0.5374, 0.0068], grad_fn=<MeanBackward1>)ard0>) row 46 step 2 imitation False\n",
      "______epoch 9 _____e 1 yt tensor(1., grad_fn=<SelectBackward0>) row 1 step 2 imitation False\n",
      "val reward 1.107210636138916\n",
      "imitation reward 1.130886197090149\n",
      "distance losses 2.499417781829834 1.9141075611114502\n",
      "distributions [0.07928098738193512, 0.6783874034881592, 0.0004735344846267253]\n",
      "[{'decision': 0, 'optimal_auc': 0.8988486842105263, 'imitation_auc': 0.6526717557251909, 'optimal_acc': 0.8707482993197279, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.7784810126582279, 'imitation_auc': 0.6842391304347826, 'optimal_acc': 0.5374149659863946, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.8424657534246576, 'imitation_auc': 0.7031150667514303, 'optimal_acc': 0.9931972789115646, 'imitation_acc': 0.8231292517006803}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1293, 0.5374, 0.0068], grad_fn=<MeanBackward1>)ard0>) row 46 step 2 imitation False\n",
      "______epoch 10 _____ 1 yt tensor(1., grad_fn=<SelectBackward0>) row 1 step 2 imitation False\n",
      "val reward 1.0539815425872803\n",
      "imitation reward 1.1145068407058716\n",
      "distance losses 2.4934797286987305 1.9977811574935913\n",
      "distributions [0.07767312973737717, 0.5819215178489685, 0.00045560396392829716]\n",
      "[{'decision': 0, 'optimal_auc': 0.9124177631578947, 'imitation_auc': 0.5901717557251909, 'optimal_acc': 0.8707482993197279, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.7822040208488458, 'imitation_auc': 0.6874999999999999, 'optimal_acc': 0.564625850340136, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.8767123287671232, 'imitation_auc': 0.7301335028607756, 'optimal_acc': 0.9931972789115646, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.1293, 0.5374, 0.0068], grad_fn=<MeanBackward1>)ard0>) row 46 step 2 imitation False\n",
      "______epoch 11 _____ 1 yt tensor(1., grad_fn=<SelectBackward0>) row 1 step 2 imitation False\n",
      "val reward 1.0271748304367065\n",
      "imitation reward 1.113604187965393\n",
      "distance losses 2.4632561206817627 1.877981424331665\n",
      "distributions [0.08362746238708496, 0.503136396408081, 0.0004776720888912678]\n",
      "[{'decision': 0, 'optimal_auc': 0.9169407894736843, 'imitation_auc': 0.5558206106870229, 'optimal_acc': 0.8707482993197279, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.7911392405063292, 'imitation_auc': 0.698641304347826, 'optimal_acc': 0.7551020408163265, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.8013698630136986, 'imitation_auc': 0.7469802924348379, 'optimal_acc': 0.9931972789115646, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.1293, 0.5374, 0.0068], grad_fn=<MeanBackward1>)ard0>) row 46 step 2 imitation False\n",
      "______epoch 12 _____ 1 yt tensor(1., grad_fn=<SelectBackward0>) row 1 step 2 imitation False\n",
      "val reward 1.0067925453186035\n",
      "imitation reward 1.119436502456665\n",
      "distance losses 2.492171049118042 1.6616528034210205\n",
      "distributions [0.08336826413869858, 0.5558234453201294, 0.00044348425581119955]\n",
      "[{'decision': 0, 'optimal_auc': 0.8930921052631579, 'imitation_auc': 0.5820610687022901, 'optimal_acc': 0.8707482993197279, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.7985852568875652, 'imitation_auc': 0.6858695652173913, 'optimal_acc': 0.7006802721088435, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.8835616438356164, 'imitation_auc': 0.6929434202161475, 'optimal_acc': 0.9931972789115646, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.1293, 0.5374, 0.0068], grad_fn=<MeanBackward1>)ard0>) row 46 step 2 imitation False\n",
      "______epoch 13 _____ 1 yt tensor(1., grad_fn=<SelectBackward0>) row 1 step 2 imitation False\n",
      "val reward 1.0170997381210327\n",
      "imitation reward 1.1138548851013184\n",
      "distance losses 2.70291805267334 1.882391333580017\n",
      "distributions [0.11197875440120697, 0.6586441993713379, 0.0005842128884978592]\n",
      "[{'decision': 0, 'optimal_auc': 0.884046052631579, 'imitation_auc': 0.5973282442748091, 'optimal_acc': 0.8707482993197279, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.7954206999255399, 'imitation_auc': 0.6845108695652175, 'optimal_acc': 0.5510204081632653, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.8835616438356164, 'imitation_auc': 0.7088366179275271, 'optimal_acc': 0.9931972789115646, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.1293, 0.5374, 0.0068], grad_fn=<MeanBackward1>)ard0>) row 46 step 2 imitation False\n",
      "______epoch 14 _____ 1 yt tensor(1., grad_fn=<SelectBackward0>) row 1 step 2 imitation False\n",
      "val reward 1.004092812538147\n",
      "imitation reward 1.1006810665130615\n",
      "distance losses 2.691601276397705 2.2393550872802734\n",
      "distributions [0.13681574165821075, 0.6595410704612732, 0.0007833425770513713]\n",
      "[{'decision': 0, 'optimal_auc': 0.8939144736842105, 'imitation_auc': 0.6054389312977099, 'optimal_acc': 0.8707482993197279, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.7946760982874161, 'imitation_auc': 0.6972826086956523, 'optimal_acc': 0.5578231292517006, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': 0.8698630136986302, 'imitation_auc': 0.7212333121424032, 'optimal_acc': 0.9931972789115646, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.1293, 0.5374, 0.0068], grad_fn=<MeanBackward1>)ard0>) row 46 step 2 imitation False\n",
      "______epoch 15 _____ 1 yt tensor(1., grad_fn=<SelectBackward0>) row 1 step 2 imitation False\n",
      "val reward 0.9448351263999939\n",
      "imitation reward 1.0917621850967407\n",
      "distance losses 2.745574951171875 1.9484925270080566\n",
      "distributions [0.09870821237564087, 0.5663960576057434, 0.0007888894760981202]\n",
      "[{'decision': 0, 'optimal_auc': 0.9000822368421053, 'imitation_auc': 0.6135496183206107, 'optimal_acc': 0.8707482993197279, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.8209233060312733, 'imitation_auc': 0.686141304347826, 'optimal_acc': 0.7551020408163265, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': 0.863013698630137, 'imitation_auc': 0.7581055308328035, 'optimal_acc': 0.9931972789115646, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.1293, 0.5374, 0.0068], grad_fn=<MeanBackward1>)ard0>) row 46 step 2 imitation False\n",
      "______epoch 16 _____ 1 yt tensor(1., grad_fn=<SelectBackward0>) row 1 step 2 imitation False\n",
      "val reward 0.9393212795257568\n",
      "imitation reward 1.0875431299209595\n",
      "distance losses 2.5400447845458984 1.855258822441101\n",
      "distributions [0.08244344592094421, 0.5918229222297668, 0.0008614649996161461]\n",
      "[{'decision': 0, 'optimal_auc': 0.8856907894736842, 'imitation_auc': 0.6669847328244275, 'optimal_acc': 0.8707482993197279, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.8451228592702903, 'imitation_auc': 0.6682065217391304, 'optimal_acc': 0.7074829931972789, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.904109589041096, 'imitation_auc': 0.7711379529561347, 'optimal_acc': 0.9931972789115646, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.1293, 0.5374, 0.0068], grad_fn=<MeanBackward1>)ard0>) row 46 step 2 imitation False\n",
      "______epoch 17 _____ 1 yt tensor(1., grad_fn=<SelectBackward0>) row 1 step 2 imitation False\n",
      "val reward 0.9496380686759949\n",
      "imitation reward 1.0705745220184326\n",
      "distance losses 2.6175127029418945 2.000683546066284\n",
      "distributions [0.07707104831933975, 0.6485092043876648, 0.0008528660400770605]\n",
      "[{'decision': 0, 'optimal_auc': 0.8951480263157895, 'imitation_auc': 0.6851145038167938, 'optimal_acc': 0.8707482993197279, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.8549888309754281, 'imitation_auc': 0.7073369565217391, 'optimal_acc': 0.6122448979591837, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': 0.9315068493150684, 'imitation_auc': 0.7835346471710107, 'optimal_acc': 0.9931972789115646, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.1293, 0.5374, 0.0068], grad_fn=<MeanBackward1>)ard0>) row 46 step 2 imitation False\n",
      "______epoch 18 _____ 1 yt tensor(1., grad_fn=<SelectBackward0>) row 1 step 2 imitation False\n",
      "val reward 0.9243049621582031\n",
      "imitation reward 1.1358267068862915\n",
      "distance losses 2.5831148624420166 1.9025884866714478\n",
      "distributions [0.12100622057914734, 0.6349664330482483, 0.0013609986053779721]\n",
      "[{'decision': 0, 'optimal_auc': 0.8745888157894737, 'imitation_auc': 0.6545801526717557, 'optimal_acc': 0.8775510204081632, 'imitation_acc': 0.8843537414965986}, {'decision': 1, 'optimal_auc': 0.8549888309754281, 'imitation_auc': 0.685054347826087, 'optimal_acc': 0.6394557823129252, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': 0.863013698630137, 'imitation_auc': 0.7851239669421488, 'optimal_acc': 0.9931972789115646, 'imitation_acc': 0.8095238095238095}]\n",
      "tensor([0.1293, 0.5374, 0.0068], grad_fn=<MeanBackward1>)ard0>) row 46 step 2 imitation False\n",
      "______epoch 19 _____ 1 yt tensor(1., grad_fn=<SelectBackward0>) row 1 step 2 imitation False\n",
      "val reward 0.925571858882904\n",
      "imitation reward 1.1474721431732178\n",
      "distance losses 2.6366188526153564 2.2590155601501465\n",
      "distributions [0.0640639141201973, 0.5720150470733643, 0.0009623845107853413]\n",
      "[{'decision': 0, 'optimal_auc': 0.7927631578947368, 'imitation_auc': 0.5291030534351145, 'optimal_acc': 0.8707482993197279, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.862807148175726, 'imitation_auc': 0.5801630434782609, 'optimal_acc': 0.7687074829931972, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.8493150684931507, 'imitation_auc': 0.7600127145581692, 'optimal_acc': 0.9931972789115646, 'imitation_acc': 0.8231292517006803}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1293, 0.5374, 0.0068], grad_fn=<MeanBackward1>)ard0>) row 46 step 2 imitation False\n",
      "______epoch 20 _____ 1 yt tensor(1., grad_fn=<SelectBackward0>) row 1 step 2 imitation False\n",
      "val reward 0.9443033933639526\n",
      "imitation reward 1.121117353439331\n",
      "distance losses 2.6660776138305664 2.3742964267730713\n",
      "distributions [0.1068795919418335, 0.682583749294281, 0.0017162533476948738]\n",
      "[{'decision': 0, 'optimal_auc': 0.8523848684210525, 'imitation_auc': 0.5286259541984734, 'optimal_acc': 0.8775510204081632, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.8648548026805659, 'imitation_auc': 0.622554347826087, 'optimal_acc': 0.5850340136054422, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.8835616438356164, 'imitation_auc': 0.7444373808010171, 'optimal_acc': 0.9931972789115646, 'imitation_acc': 0.8367346938775511}]\n",
      "tensor([0.1293, 0.5374, 0.0068], grad_fn=<MeanBackward1>)ard0>) row 46 step 2 imitation False\n",
      "______epoch 21 _____ 1 yt tensor(1., grad_fn=<SelectBackward0>) row 1 step 2 imitation False\n",
      "val reward 0.9375718235969543\n",
      "imitation reward 1.1326954364776611\n",
      "distance losses 2.670144557952881 2.392756462097168\n",
      "distributions [0.15432150661945343, 0.6953427195549011, 0.002893578028306365]\n",
      "[{'decision': 0, 'optimal_auc': 0.876233552631579, 'imitation_auc': 0.5467557251908397, 'optimal_acc': 0.9047619047619048, 'imitation_acc': 0.8843537414965986}, {'decision': 1, 'optimal_auc': 0.8594564408041697, 'imitation_auc': 0.6584239130434782, 'optimal_acc': 0.6122448979591837, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': 0.8287671232876712, 'imitation_auc': 0.7294977749523204, 'optimal_acc': 0.9931972789115646, 'imitation_acc': 0.8163265306122449}]\n",
      "tensor([0.1293, 0.5374, 0.0068], grad_fn=<MeanBackward1>)ard0>) row 46 step 2 imitation False\n",
      "______epoch 22 _____ 1 yt tensor(1., grad_fn=<SelectBackward0>) row 1 step 2 imitation False\n",
      "val reward 0.8580878376960754\n",
      "imitation reward 1.0927674770355225\n",
      "distance losses 2.6079885959625244 2.3355846405029297\n",
      "distributions [0.09407813102006912, 0.5892941355705261, 0.0027604629285633564]\n",
      "[{'decision': 0, 'optimal_auc': 0.9383223684210527, 'imitation_auc': 0.6297709923664122, 'optimal_acc': 0.8843537414965986, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.8655994043186895, 'imitation_auc': 0.7375, 'optimal_acc': 0.7755102040816326, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': 0.726027397260274, 'imitation_auc': 0.7349014621741895, 'optimal_acc': 0.9931972789115646, 'imitation_acc': 0.8163265306122449}]\n",
      "tensor([0.1293, 0.5374, 0.0068], grad_fn=<MeanBackward1>)ard0>) row 46 step 2 imitation False\n",
      "______epoch 23 _____ 1 yt tensor(1., grad_fn=<SelectBackward0>) row 1 step 2 imitation False\n",
      "val reward 0.8812910318374634\n",
      "imitation reward 1.0762397050857544\n",
      "distance losses 2.710052490234375 2.2524166107177734\n",
      "distributions [0.060842085629701614, 0.5748727321624756, 0.002535283798351884]\n",
      "[{'decision': 0, 'optimal_auc': 0.9416118421052632, 'imitation_auc': 0.642175572519084, 'optimal_acc': 0.8707482993197279, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.8702531645569621, 'imitation_auc': 0.7258152173913044, 'optimal_acc': 0.782312925170068, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': 0.726027397260274, 'imitation_auc': 0.7514303877940242, 'optimal_acc': 0.9931972789115646, 'imitation_acc': 0.8367346938775511}]\n",
      "tensor([0.1293, 0.5374, 0.0068], grad_fn=<MeanBackward1>)ard0>) row 46 step 2 imitation False\n",
      "______epoch 24 _____ 1 yt tensor(1., grad_fn=<SelectBackward0>) row 1 step 2 imitation False\n",
      "val reward 0.9002757668495178\n",
      "imitation reward 1.077246069908142\n",
      "distance losses 2.692417621612549 2.002286672592163\n",
      "distributions [0.08240579813718796, 0.6845766305923462, 0.0033216283190995455]\n",
      "[{'decision': 0, 'optimal_auc': 0.936266447368421, 'imitation_auc': 0.6512404580152672, 'optimal_acc': 0.8707482993197279, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.8698808637379001, 'imitation_auc': 0.7211956521739131, 'optimal_acc': 0.6802721088435374, 'imitation_acc': 0.7687074829931972}, {'decision': 2, 'optimal_auc': 0.7671232876712328, 'imitation_auc': 0.763191354100445, 'optimal_acc': 0.9931972789115646, 'imitation_acc': 0.8299319727891157}]\n",
      "tensor([0.1293, 0.5374, 0.0068], grad_fn=<MeanBackward1>)ard0>) row 46 step 2 imitation False\n",
      "______epoch 25 _____ 1 yt tensor(1., grad_fn=<SelectBackward0>) row 1 step 2 imitation False\n",
      "val reward 1.0229063034057617\n",
      "imitation reward 1.1022471189498901\n",
      "distance losses 2.4572064876556396 1.9561823606491089\n",
      "distributions [0.12598572671413422, 0.7876601219177246, 0.0044263554736971855]\n",
      "[{'decision': 0, 'optimal_auc': 0.9317434210526316, 'imitation_auc': 0.6502862595419847, 'optimal_acc': 0.891156462585034, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.8607594936708861, 'imitation_auc': 0.7016304347826087, 'optimal_acc': 0.5374149659863946, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': 0.8698630136986302, 'imitation_auc': 0.760012714558169, 'optimal_acc': 0.9931972789115646, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.1293, 0.5374, 0.0068], grad_fn=<MeanBackward1>)ard0>) row 46 step 2 imitation False\n",
      "______epoch 26 _____ 1 yt tensor(1., grad_fn=<SelectBackward0>) row 1 step 2 imitation False\n",
      "val reward 0.9249851107597351\n",
      "imitation reward 1.1450927257537842\n",
      "distance losses 2.608476161956787 2.0486791133880615\n",
      "distributions [0.1316460222005844, 0.7103140354156494, 0.0044511957094073296]\n",
      "[{'decision': 0, 'optimal_auc': 0.9202302631578947, 'imitation_auc': 0.6168893129770993, 'optimal_acc': 0.9115646258503401, 'imitation_acc': 0.8775510204081632}, {'decision': 1, 'optimal_auc': 0.8540580789277736, 'imitation_auc': 0.6956521739130435, 'optimal_acc': 0.5714285714285714, 'imitation_acc': 0.7891156462585034}, {'decision': 2, 'optimal_auc': 0.9383561643835616, 'imitation_auc': 0.7476160203432931, 'optimal_acc': 0.9931972789115646, 'imitation_acc': 0.7959183673469388}]\n",
      "tensor([0.1293, 0.5374, 0.0068], grad_fn=<MeanBackward1>)ard0>) row 46 step 2 imitation False\n",
      "______epoch 27 _____ 1 yt tensor(1., grad_fn=<SelectBackward0>) row 1 step 2 imitation False\n",
      "val reward 0.878653883934021\n",
      "imitation reward 1.1376725435256958\n",
      "distance losses 2.6508371829986572 2.210529088973999\n",
      "distributions [0.062265485525131226, 0.6042559742927551, 0.002671551890671253]\n",
      "[{'decision': 0, 'optimal_auc': 0.8824013157894737, 'imitation_auc': 0.6092557251908397, 'optimal_acc': 0.8843537414965986, 'imitation_acc': 0.8843537414965986}, {'decision': 1, 'optimal_auc': 0.8557334326135516, 'imitation_auc': 0.6383152173913044, 'optimal_acc': 0.7687074829931972, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': 0.9657534246575342, 'imitation_auc': 0.7463445645263828, 'optimal_acc': 0.9931972789115646, 'imitation_acc': 0.8367346938775511}]\n",
      "tensor([0.1293, 0.5374, 0.0068], grad_fn=<MeanBackward1>)ard0>) row 46 step 2 imitation False\n",
      "______epoch 28 _____ 1 yt tensor(1., grad_fn=<SelectBackward0>) row 1 step 2 imitation False\n",
      "val reward 0.8069247007369995\n",
      "imitation reward 1.1568760871887207\n",
      "distance losses 2.538788318634033 1.9112683534622192\n",
      "distributions [0.13162171840667725, 0.6133537292480469, 0.004601818975061178]\n",
      "[{'decision': 0, 'optimal_auc': 0.8519736842105263, 'imitation_auc': 0.5772900763358779, 'optimal_acc': 0.9183673469387755, 'imitation_acc': 0.8571428571428571}, {'decision': 1, 'optimal_auc': 0.8559195830230827, 'imitation_auc': 0.7043478260869565, 'optimal_acc': 0.7687074829931972, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': 0.9726027397260274, 'imitation_auc': 0.7517482517482518, 'optimal_acc': 0.9931972789115646, 'imitation_acc': 0.8027210884353742}]\n",
      "tensor([0.1293, 0.5374, 0.0068], grad_fn=<MeanBackward1>)ard0>) row 46 step 2 imitation False\n",
      "______epoch 29 _____ 1 yt tensor(1., grad_fn=<SelectBackward0>) row 1 step 2 imitation False\n",
      "val reward 0.9238669276237488\n",
      "imitation reward 1.2940678596496582\n",
      "distance losses 2.536639928817749 2.0022287368774414\n",
      "distributions [0.22871625423431396, 0.7282641530036926, 0.007288978435099125]\n",
      "[{'decision': 0, 'optimal_auc': 0.9078947368421053, 'imitation_auc': 0.6517175572519084, 'optimal_acc': 0.9047619047619048, 'imitation_acc': 0.8027210884353742}, {'decision': 1, 'optimal_auc': 0.8607594936708861, 'imitation_auc': 0.7508152173913042, 'optimal_acc': 0.6870748299319728, 'imitation_acc': 0.7346938775510204}, {'decision': 2, 'optimal_auc': 0.9863013698630136, 'imitation_auc': 0.7673235855054037, 'optimal_acc': 0.9931972789115646, 'imitation_acc': 0.7687074829931972}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1293, 0.5374, 0.0068], grad_fn=<MeanBackward1>)ard0>) row 46 step 2 imitation False\n",
      "______epoch 30 _____ 1 yt tensor(1., grad_fn=<SelectBackward0>) row 1 step 2 imitation False\n",
      "val reward 0.9508066773414612\n",
      "imitation reward 1.122540831565857\n",
      "distance losses 2.5828661918640137 1.865167260169983\n",
      "distributions [0.09965973347425461, 0.7562637329101562, 0.004887317307293415]\n",
      "[{'decision': 0, 'optimal_auc': 0.9161184210526316, 'imitation_auc': 0.6894083969465649, 'optimal_acc': 0.9115646258503401, 'imitation_acc': 0.8639455782312925}, {'decision': 1, 'optimal_auc': 0.8518242740134028, 'imitation_auc': 0.7046195652173912, 'optimal_acc': 0.564625850340136, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.9726027397260274, 'imitation_auc': 0.753973299427845, 'optimal_acc': 0.9931972789115646, 'imitation_acc': 0.8299319727891157}]\n",
      "tensor([0.1293, 0.5374, 0.0068], grad_fn=<MeanBackward1>)ard0>) row 46 step 2 imitation False\n",
      "______epoch 31 _____ 1 yt tensor(1., grad_fn=<SelectBackward0>) row 1 step 2 imitation False\n",
      "val reward 0.9006599187850952\n",
      "imitation reward 1.1361457109451294\n",
      "distance losses 2.637255907058716 2.000864267349243\n",
      "distributions [0.1014011800289154, 0.6539382338523865, 0.006713753566145897]\n",
      "[{'decision': 0, 'optimal_auc': 0.900904605263158, 'imitation_auc': 0.6569656488549619, 'optimal_acc': 0.891156462585034, 'imitation_acc': 0.8843537414965986}, {'decision': 1, 'optimal_auc': 0.8464259121370068, 'imitation_auc': 0.6684782608695652, 'optimal_acc': 0.6802721088435374, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.9726027397260274, 'imitation_auc': 0.7450731087094723, 'optimal_acc': 0.9931972789115646, 'imitation_acc': 0.8095238095238095}]\n",
      "tensor([0.1293, 0.5374, 0.0068], grad_fn=<MeanBackward1>)ard0>) row 46 step 2 imitation False\n",
      "______epoch 32 _____ 1 yt tensor(1., grad_fn=<SelectBackward0>) row 1 step 2 imitation False\n",
      "val reward 0.8735376000404358\n",
      "imitation reward 1.2641746997833252\n",
      "distance losses 2.4113194942474365 2.184399127960205\n",
      "distributions [0.17790266871452332, 0.6192991137504578, 0.010111382231116295]\n",
      "[{'decision': 0, 'optimal_auc': 0.8984375, 'imitation_auc': 0.6855916030534351, 'optimal_acc': 0.9115646258503401, 'imitation_acc': 0.8707482993197279}, {'decision': 1, 'optimal_auc': 0.8548026805658971, 'imitation_auc': 0.6945652173913044, 'optimal_acc': 0.7551020408163265, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.9794520547945206, 'imitation_auc': 0.7905276541640178, 'optimal_acc': 0.9931972789115646, 'imitation_acc': 0.7755102040816326}]\n",
      "tensor([0.1293, 0.5374, 0.0068], grad_fn=<MeanBackward1>)ard0>) row 46 step 2 imitation False\n",
      "______epoch 33 _____ 1 yt tensor(1., grad_fn=<SelectBackward0>) row 1 step 2 imitation False\n",
      "val reward 0.9113587141036987\n",
      "imitation reward 1.181372880935669\n",
      "distance losses 2.721565008163452 2.208259105682373\n",
      "distributions [0.17801648378372192, 0.7148355841636658, 0.008853962644934654]\n",
      "[{'decision': 0, 'optimal_auc': 0.9087171052631579, 'imitation_auc': 0.6812977099236641, 'optimal_acc': 0.8979591836734694, 'imitation_acc': 0.8707482993197279}, {'decision': 1, 'optimal_auc': 0.8393521965748325, 'imitation_auc': 0.7081521739130434, 'optimal_acc': 0.6802721088435374, 'imitation_acc': 0.7891156462585034}, {'decision': 2, 'optimal_auc': 0.9726027397260274, 'imitation_auc': 0.8003814367450731, 'optimal_acc': 0.9931972789115646, 'imitation_acc': 0.782312925170068}]\n",
      "++++++++++Final+++++++++++\n",
      "best tensor(1.4876, grad_fn=<AddBackward0>)\n",
      "[{'decision': 0, 'optimal_auc': 0.9383223684210527, 'imitation_auc': 0.6297709923664122, 'optimal_acc': 0.8843537414965986, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.8655994043186895, 'imitation_auc': 0.7375, 'optimal_acc': 0.7755102040816326, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': 0.726027397260274, 'imitation_auc': 0.7349014621741895, 'optimal_acc': 0.9931972789115646, 'imitation_acc': 0.8163265306122449}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionAttentionModel(\n",
       "  (input_dropout): Dropout(p=0.25, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=100, out_features=1000, bias=True)\n",
       "  )\n",
       "  (batchnorm): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       "  (relu): Softplus(beta=1, threshold=20)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (softmax): Softmax(dim=1)\n",
       "  (final_opt_layer): Linear(in_features=1000, out_features=100, bias=True)\n",
       "  (final_imitation_layer): Linear(in_features=1000, out_features=100, bias=True)\n",
       "  (final_layer): Linear(in_features=1000, out_features=6, bias=True)\n",
       "  (resize_layer): Linear(in_features=91, out_features=100, bias=True)\n",
       "  (attentions): ModuleList(\n",
       "    (0): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (norms): ModuleList(\n",
       "    (0): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (activation): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_unique_sequence(array):\n",
    "    #converts a row of boolean values to a unique number e.g. [1,1,0] => 11, [0,0,1] => 100\n",
    "    uniqueify = lambda r: torch.sum(torch.stack([i*(10**ii) for ii,i in enumerate(r)]))\n",
    "    return torch_apply_along_axis(uniqueify,array)\n",
    "\n",
    "def train_decision_model_triplet(\n",
    "    tmodel1,\n",
    "    tmodel2,\n",
    "    tmodel3,\n",
    "    smodel3,\n",
    "    use_default_split=True,\n",
    "    use_bagging_split=False,\n",
    "    use_attention=True,\n",
    "    lr=.001,\n",
    "    epochs=10000,\n",
    "    patience=5,\n",
    "    weights=[0,.5,.5,0], #realtive weight of survival, feeding tube, aspiration, andl lrc\n",
    "    tweights=[1,1,1,0], #weight for OS, LRC, FDM, and event (any + FT or AS at 6m) as time to event in weeks\n",
    "    opt_weights=[1,1,1], #weights for policy model for optimal decisions\n",
    "    imitation_weights=[.5,1,1],#weights of imitation decisions, because ic overtrains too quickly\n",
    "    imitation_weight=1,\n",
    "    reward_weight=1,\n",
    "    imitation_triplet_weight=2,\n",
    "    reward_triplet_weight = 2,\n",
    "    shufflecol_chance = 0.1,\n",
    "    split=.7,\n",
    "    resample_training=False,\n",
    "    save_path='../data/models/',\n",
    "    file_suffix='',\n",
    "    verbose=True,\n",
    "    use_gpu=False,\n",
    "    **model_kwargs,\n",
    "):\n",
    "    tmodel1.eval()\n",
    "    tmodel2.eval()\n",
    "    tmodel3.eval()\n",
    "\n",
    "    train_ids, test_ids = get_tt_split(use_default_split=use_default_split,use_bagging_split=use_bagging_split,resample_training=resample_training)\n",
    "    true_ids = train_ids + test_ids #for saving memory without upsampling\n",
    "\n",
    "    dataset = DTDataset()\n",
    "    data = dataset.processed_df.copy()\n",
    "    \n",
    "    def get_dlt(state):\n",
    "        if state == 2:\n",
    "            return data[Const.dlt2].copy()\n",
    "        d = data[Const.dlt1].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_pd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.primary_disease_states2].copy()\n",
    "        d = data[Const.primary_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_nd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.nodal_disease_states2].copy()\n",
    "        d = data[Const.nodal_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_cc(state):\n",
    "        res = data[Const.ccs].copy()\n",
    "        if state == 1:\n",
    "            res.values[:,:] = np.zeros(res.values.shape)\n",
    "        return res\n",
    "    \n",
    "    def get_mod(state):\n",
    "        res = data[Const.modifications].copy()\n",
    "        #this should have an ic condition but we don't use it anumore anywa\n",
    "        return res\n",
    "        \n",
    "    outcomedf = data[Const.outcomes]\n",
    "    baseline = dataset.get_state('baseline')\n",
    "    \n",
    "    def formatdf(d,dids=train_ids):\n",
    "        d = df_to_torch(d.loc[dids]).to(model.get_device())\n",
    "        return d\n",
    "    \n",
    "    def makegrad(v):\n",
    "        if not v.requires_grad:\n",
    "            v.requires_grad=True\n",
    "        return v\n",
    "    \n",
    "    if use_attention:\n",
    "        model = DecisionAttentionModel(baseline.shape[1],**model_kwargs)\n",
    "    else:\n",
    "        model_kwargs = {k:v for k,v in model_kwargs.items() if 'attention' not in k and 'embed' not in k}\n",
    "        model = DecisionModel(baseline.shape[1],**model_kwargs)\n",
    "        \n",
    "    device = 'cpu'\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "        \n",
    "    model.set_device(device)\n",
    "\n",
    "    tmodel1.set_device(device)\n",
    "    tmodel2.set_device(device)\n",
    "    tmodel3.set_device(device)\n",
    "    smodel3.set_device(device)\n",
    "    hashcode = str(hash(','.join([str(i) for i in train_ids])))\n",
    "    \n",
    "    save_file = save_path + 'model_' + model.identifier +'_hash' + hashcode + file_suffix + '.tar'\n",
    "    model.fit_normalizer(df_to_torch(baseline.loc[train_ids]))\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "    \n",
    "    \n",
    "    optimal_train,transitions_train = calc_optimal_decisions(dataset,train_ids,tmodel1,tmodel2,tmodel3,smodel3,\n",
    "                                           weights=weights,tweights=tweights,\n",
    "                                          )\n",
    "    optimal_test,transitions_test = calc_optimal_decisions(dataset,test_ids,tmodel1,tmodel2,tmodel3,smodel3,\n",
    "                                          weights=weights,tweights=tweights,\n",
    "                                         )\n",
    "    optimal_train = optimal_train.to(model.get_device())\n",
    "    optimal_test = optimal_test.to(model.get_device())\n",
    "    mse = torch.nn.MSELoss()\n",
    "    bce = torch.nn.BCELoss()\n",
    "    \n",
    "    def compare_decisions(d1,d2,d3,ids):\n",
    "#         ypred = np.concatenate([dd.cpu().detach().numpy().reshape(-1,1) for dd in [d1,d2,d3]],axis=1)\n",
    "        ytrue = df_to_torch(outcomedf.loc[ids])\n",
    "        dloss = bce(d1.view(-1),ytrue[:,0])\n",
    "        dloss += bce(d2.view(-1),ytrue[:,1])\n",
    "        dloss += bce(d3,view(-1),ytrue[:,2])\n",
    "        return dloss\n",
    "        \n",
    "    def remove_decisions(df):\n",
    "        cols = [c for c in df.columns if c not in Const.decisions ]\n",
    "        ddf = df[cols]\n",
    "        return ddf\n",
    "    \n",
    "    makeinput = lambda step,dids: df_to_torch(remove_decisions(dataset.get_input_state(step=step,ids=dids)))\n",
    "    threshold = lambda x: torch.gt(x,torch.rand(x.shape[0])).type(torch.FloatTensor)\n",
    "\n",
    "    randchoice = lambda x: x[torch.randint(len(x),(1,))[0]]\n",
    "    tloss_func = torch.nn.TripletMarginLoss()\n",
    "    def get_tloss(row,step,yt,x,imitation=True):\n",
    "        if yt[:,step].std() < .001:\n",
    "            return torch.tensor([0]).to(device)\n",
    "        positive_idx= torch.nonzero(yt[:,step] == yt[row,step])\n",
    "        if len(positive_idx) <= 1:\n",
    "            print('no losses','n positive',len(positive_idx),'yt',yt[row,step],'row',row,'step',step,'imitation',imitation,end='\\r')\n",
    "            return torch.tensor([0]).to(device)\n",
    "        positive_idx = torch.stack([ii for ii in positive_idx if ii != row]).view(-1)\n",
    "        negative_idx = torch.tensor([ii for ii in range(x.shape[0]) if ii not in positive_idx and ii != row])\n",
    "        if len(positive_idx) < 1 or len(negative_idx) < 1:\n",
    "            print('no losses','n positive',len(positive_idx),'n negative',len(negative_idx),'yt',yt[row,step],'row',row,'step',step,'imitation',imitation,end='\\r')\n",
    "            return torch.tensor([0]).to(device)\n",
    "        positive = x[randchoice(positive_idx)]\n",
    "        negative = x[randchoice(negative_idx)]\n",
    "        anchor = x[row]\n",
    "        if use_attention:\n",
    "            [anchor_embedding,pos_embedding,neg_embedding] = [model.get_embedding(xx.view(1,-1),position=step,use_saved_memory=True) for xx in [anchor,positive,negative]]\n",
    "        else:    \n",
    "            [anchor_embedding,pos_embedding,neg_embedding] = [model.get_embedding(xx.view(1,-1),position=step,concatenate=False)[int(imitation)] for xx in [anchor,positive,negative]]\n",
    "        tloss = tloss_func(anchor_embedding,pos_embedding,neg_embedding)\n",
    "        return tloss\n",
    "    #save the inputs from the whole dataset for future reference\n",
    "    if use_attention:\n",
    "        full_data = []\n",
    "        for mstep in [0,1,2]:\n",
    "            full_data_step = [baseline, get_dlt(min(mstep,1)),\n",
    "                         get_dlt(mstep),get_pd(mstep),get_nd(mstep),get_cc(mstep),get_mod(mstep)]\n",
    "            full_data_step = torch.cat([formatdf(fd,true_ids) for fd in full_data_step],axis=1)\n",
    "            full_data.append(full_data_step)\n",
    "        full_data = torch.stack(full_data)\n",
    "        model.save_memory(full_data)\n",
    "        print(full_data.shape)\n",
    "        \n",
    "    def step(train=True):\n",
    "        if train:\n",
    "            model.train(True)\n",
    "            tmodel1.train(True)\n",
    "            tmodel2.train(True)\n",
    "            tmodel3.train(True)\n",
    "            optimizer.zero_grad()\n",
    "            ids = train_ids\n",
    "            y_opt = makegrad(optimal_train)\n",
    "            transition_dict = {k: torch.clone(v).detach() for k,v in transitions_train.items()}\n",
    "        else:\n",
    "            ids = test_ids\n",
    "            model.eval()\n",
    "            tmodel1.eval()\n",
    "            tmodel2.eval()\n",
    "            tmodel3.eval()\n",
    "            y_opt = makegrad(optimal_test)\n",
    "            print(y_opt.mean(axis=0))\n",
    "            transition_dict = {k: torch.clone(v).detach() for k,v in transitions_test.items()}\n",
    "        model.set_device(device)\n",
    "        ytrain = df_to_torch(outcomedf.loc[ids]).to(device)\n",
    "        #imitation losses and decision 1\n",
    "        xxtrained = [baseline, get_dlt(0),get_dlt(0),get_pd(0),get_nd(0),get_cc(0),get_mod(0)]\n",
    "        xxtrain = [formatdf(xx,ids) for xx in xxtrained]\n",
    "        xxtrain = torch.cat(xxtrain,axis=1).to(device)\n",
    "        o1 = model(xxtrain,position=0,use_saved_memory= (not train))\n",
    "        decision1_imitation = o1[:,3]\n",
    "        decision1_opt = o1[:,0]\n",
    "    \n",
    "        imitation_loss1 = bce(decision1_imitation,ytrain[:,0])\n",
    "        imitation_loss1 = torch.mul(imitation_loss1,imitation_weights[0])\n",
    "        opt_loss1 = bce(decision1_opt,y_opt[:,0])\n",
    "        opt_loss1 = torch.mul(opt_loss1,opt_weights[0])\n",
    "        \n",
    "        x1_imitation = [baseline, get_dlt(1),get_dlt(0),get_pd(1),get_nd(1),get_cc(1),get_mod(1)]\n",
    "        x1_imitation = [formatdf(xx1,ids) for xx1 in x1_imitation]\n",
    "        x1_imitation = torch.cat(x1_imitation,axis=1).to(device)\n",
    "        \n",
    "        o2 = model(x1_imitation,position=1,use_saved_memory= (not train))\n",
    "            \n",
    "        decision2_imitation = o2[:,4]\n",
    "            \n",
    "        imitation_loss2 =  bce(decision2_imitation,ytrain[:,1])\n",
    "        imitation_loss2 = torch.mul(imitation_loss2,imitation_weights[1])\n",
    "        \n",
    "        \n",
    "        x2_imitation = [baseline, get_dlt(1),get_dlt(2),get_pd(2),get_nd(2),get_cc(2),get_mod(2)]\n",
    "        \n",
    "        x2_imitation = [formatdf(xx2,ids) for xx2 in x2_imitation]\n",
    "        x2_imitation = torch.cat(x2_imitation,axis=1).to(device)\n",
    "        \n",
    "        \n",
    "        o3 = model(x2_imitation,position=2,use_saved_memory= (not train))\n",
    "        \n",
    "        decision3_imitation = o3[:,5]\n",
    "        \n",
    "        imitation_loss3 = bce(decision3_imitation,ytrain[:,2])\n",
    "        imitation_loss3 = torch.mul(imitation_loss3,imitation_weights[2])\n",
    "        \n",
    "        opt_input2 = [\n",
    "            formatdf(baseline,ids), \n",
    "            transition_dict['dlt1'],\n",
    "            formatdf(get_dlt(0),ids),\n",
    "            transition_dict['pd1'],\n",
    "            transition_dict['nd1'], \n",
    "            formatdf(get_cc(0),ids),\n",
    "            transition_dict['mod']\n",
    "                 ]\n",
    "        opt_input2 = [o.to(device) for o in opt_input2]\n",
    "\n",
    "        opt_input2 = torch.cat(opt_input2,axis=1).to(device)\n",
    "        decision2_opt = model(opt_input2,position=1,use_saved_memory= (not train))[:,1]\n",
    "        \n",
    "        opt_loss2 = bce(decision2_opt,y_opt[:,1])\n",
    "        opt_loss2 = torch.mul(opt_loss2,opt_weights[1])\n",
    "        \n",
    "        opt_input3 = [\n",
    "            formatdf(baseline,ids),\n",
    "            transition_dict['dlt1'],\n",
    "            transition_dict['dlt2'],\n",
    "            transition_dict['pd2'],\n",
    "            transition_dict['nd2'],\n",
    "            transition_dict['cc'],\n",
    "            transition_dict['mod'],\n",
    "        ]\n",
    "        opt_input3 = [o.to(device) for o in opt_input3]\n",
    "        opt_input3 = torch.cat(opt_input3,axis=1).to(device)\n",
    "        decision3_opt = model(opt_input3,position=2,use_saved_memory= (not train))[:,2]\n",
    "        \n",
    "        opt_loss3 = bce(decision3_opt,y_opt[:,2])\n",
    "        opt_loss3 = torch.mul(opt_loss3,opt_weights[2])\n",
    "        \n",
    "        iloss = torch.add(torch.add(imitation_loss1,imitation_loss2),imitation_loss3)\n",
    "        iloss = torch.mul(iloss,imitation_weight)\n",
    "        \n",
    "        reward_loss = torch.add(torch.add(opt_loss1,opt_loss2),opt_loss3)\n",
    "        reward_loss =torch.mul(reward_loss,reward_weight)\n",
    "        \n",
    "        loss = torch.add(iloss,reward_loss)\n",
    "        \n",
    "        imitation_tloss = torch.FloatTensor([0]).to(device)\n",
    "        opt_tloss = torch.FloatTensor([0]).to(device)\n",
    "        n_rows = xxtrain.shape[0]\n",
    "        if reward_triplet_weight + imitation_triplet_weight > 0.0001:\n",
    "            for i in range(n_rows):\n",
    "                \n",
    "                if imitation_triplet_weight > .0001:\n",
    "                    imitation_tloss += get_tloss(i,0,ytrain,xxtrain,True)\n",
    "                    imitation_tloss += get_tloss(i,1,ytrain,x1_imitation,True)\n",
    "                    imitation_tloss += get_tloss(i,2,ytrain,x2_imitation,True)\n",
    "                if reward_triplet_weight > .0001:\n",
    "                    opt_tloss += get_tloss(i,0,y_opt,xxtrain,False)\n",
    "                    opt_tloss += get_tloss(i,1,y_opt,opt_input2,False)\n",
    "                    opt_tloss += get_tloss(i,2,y_opt,opt_input3,False)\n",
    "            loss += torch.mul(imitation_tloss[0],imitation_triplet_weight/n_rows)\n",
    "            loss += torch.mul(opt_tloss[0],reward_triplet_weight/n_rows)\n",
    "        losses = [iloss,reward_loss,imitation_tloss*imitation_triplet_weight/n_rows,opt_tloss*reward_triplet_weight/n_rows]\n",
    "        \n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            return losses\n",
    "        else:\n",
    "            scores = []\n",
    "            distributions = [decision1_opt.mean().item(),decision2_opt.mean().item(),decision3_opt.mean().item()]\n",
    "            imitation = [decision1_imitation,decision2_imitation,decision3_imitation]\n",
    "            optimal = [decision1_opt,decision2_opt,decision3_opt]\n",
    "            for i,decision_im in enumerate(imitation):\n",
    "                deci = decision_im.cpu().detach().numpy()\n",
    "                deci0 = (deci > .5).astype(int)\n",
    "                iout = ytrain[:,i].cpu().detach().numpy()\n",
    "                acci = accuracy_score(iout,deci0)\n",
    "                try:\n",
    "                    auci = roc_auc_score(iout,deci)\n",
    "                except:\n",
    "                    auci = -1\n",
    "                \n",
    "                deco = optimal[i].cpu().detach().numpy()\n",
    "                deci0 = (deco > .5).astype(int)\n",
    "                oout = y_opt[:,i].cpu().detach().numpy()\n",
    "                acco = accuracy_score(oout,deci0)\n",
    "                try:\n",
    "                    auco = roc_auc_score(oout,deco)\n",
    "                except:\n",
    "                    auco=-1\n",
    "                scores.append({'decision': i,'optimal_auc': auco,'imitation_auc': auci,'optimal_acc': acco,'imitation_acc': acci})\n",
    "            return losses, scores, distributions\n",
    "        \n",
    "    best_val_loss = torch.tensor(1000000000.0)\n",
    "    steps_since_improvement = 0\n",
    "    best_val_score = {}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        losses = step(True)\n",
    "        val_losses,val_metrics,val_distributions = step(False)\n",
    "        vl = val_losses[0] + val_losses[1]\n",
    "        for vm in val_metrics:\n",
    "            vl += (-((vm['optimal_auc']*reward_weight) + (vm['imitation_auc']*imitation_weight)))/10\n",
    "        if verbose:\n",
    "            print('______epoch',str(epoch),'_____')\n",
    "            print('val reward',val_losses[1].item())\n",
    "            print('imitation reward', val_losses[0].item())\n",
    "            print('distance losses',val_losses[2].item(),val_losses[-1].item())\n",
    "            print('distributions',val_distributions)\n",
    "            print(val_metrics)\n",
    "        if vl < best_val_loss:\n",
    "            best_val_loss = vl\n",
    "            best_val_score = val_metrics\n",
    "            best_val_distributions = val_distributions\n",
    "            steps_since_improvement = 0\n",
    "            torch.save(model.state_dict(),save_file)\n",
    "        else:\n",
    "            steps_since_improvement += 1\n",
    "        if steps_since_improvement > patience:\n",
    "            break\n",
    "    print('++++++++++Final+++++++++++')\n",
    "    print('best',best_val_loss)\n",
    "    print(best_val_score)\n",
    "    model.load_state_dict(torch.load(save_file))\n",
    "    model.eval()\n",
    "    return model, best_val_score, best_val_loss, best_val_distributions\n",
    "\n",
    "from Models import *\n",
    "args = {\n",
    "    'hidden_layers': [1000], \n",
    "    'opt_layer_size': 20, \n",
    "    'imitation_layer_size': 20, \n",
    "    'dropout': 0.25, \n",
    "    'input_dropout': 0.25, \n",
    "    'shufflecol_chance': 0.5\n",
    "}\n",
    "\n",
    "#1.8424\n",
    "decision_model, decision_score, decision_loss, _ = train_decision_model_triplet(\n",
    "    model1,model2,model3,smodel3,\n",
    "    lr=.01,\n",
    "    imitation_weight=1,\n",
    "    reward_weight=1,\n",
    "    patience=10,\n",
    "    imitation_triplet_weight=1,\n",
    "    reward_triplet_weight =1,\n",
    "    verbose=True,\n",
    "    weights=[0,0,0,0], #realtive weight of survival, feeding tube, aspiration, andl lrc\n",
    "    tweights=[2,.1,0,0], #weight for OS, LRC, FDM, and event (any + FT or AS at 6m) as time to event in weeks\n",
    "    use_attention=True,\n",
    "    **args)\n",
    "decision_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "73f37f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_model.set_device('cpu')\n",
    "torch.save(decision_model,'../resources/decision_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b25922a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 536, 87])\n",
      "tensor([0.2313, 0.0612, 0.0000], grad_fn=<MeanBackward1>)ard0>) row 305 step 2 imitation False\n",
      "______epoch 0 _____\n",
      "val reward 0.7669771909713745\n",
      "imitation reward 1.4131238460540771\n",
      "distance losses 2.4731297492980957 1.2148008346557617\n",
      "distributions [0.16884014010429382, 0.06170494854450226, 0.04187864437699318]\n",
      "[{'decision': 0, 'optimal_auc': 0.8222280062467464, 'imitation_auc': 0.5, 'optimal_acc': 0.7687074829931972, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.4758454106280193, 'imitation_auc': 0.42038043478260867, 'optimal_acc': 0.9387755102040817, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.5664335664335665, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.2313, 0.0612, 0.0000], grad_fn=<MeanBackward1>)ard0>) row 305 step 2 imitation False\n",
      "______epoch 1 _____\n",
      "val reward 0.9681809544563293\n",
      "imitation reward 1.3798258304595947\n",
      "distance losses 2.712568998336792 1.349366307258606\n",
      "distributions [0.5780573487281799, 0.05749140679836273, 0.012773395515978336]\n",
      "[{'decision': 0, 'optimal_auc': 0.8308172826652785, 'imitation_auc': 0.5057251908396947, 'optimal_acc': 0.5034013605442177, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.4573268921095008, 'imitation_auc': 0.47744565217391305, 'optimal_acc': 0.9387755102040817, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.6109345200254291, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.2313, 0.0612, 0.0000], grad_fn=<MeanBackward1>)ard0>) row 305 step 2 imitation False\n",
      "______epoch 2 _____\n",
      "val reward 0.7321910262107849\n",
      "imitation reward 1.1887084245681763\n",
      "distance losses 2.5939412117004395 1.2083306312561035\n",
      "distributions [0.2902979254722595, 0.08919787406921387, 0.020897218957543373]\n",
      "[{'decision': 0, 'optimal_auc': 0.8695991671004685, 'imitation_auc': 0.47185114503816794, 'optimal_acc': 0.7687074829931972, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.6843800322061191, 'imitation_auc': 0.628804347826087, 'optimal_acc': 0.9387755102040817, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8080101716465353, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.2313, 0.0612, 0.0000], grad_fn=<MeanBackward1>)ard0>) row 305 step 2 imitation False\n",
      "______epoch 3 _____\n",
      "val reward 0.7616854310035706\n",
      "imitation reward 1.1516257524490356\n",
      "distance losses 2.751648426055908 1.1441125869750977\n",
      "distributions [0.37499937415122986, 0.10667790472507477, 0.019129574298858643]\n",
      "[{'decision': 0, 'optimal_auc': 0.8867777199375324, 'imitation_auc': 0.46183206106870234, 'optimal_acc': 0.8231292517006803, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.45088566827697263, 'imitation_auc': 0.6817934782608696, 'optimal_acc': 0.9387755102040817, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8296249205340115, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.2313, 0.0612, 0.0000], grad_fn=<MeanBackward1>)ard0>) row 305 step 2 imitation False\n",
      "______epoch 4 _____\n",
      "val reward 0.7957612872123718\n",
      "imitation reward 1.1245548725128174\n",
      "distance losses 2.564070224761963 1.113240361213684\n",
      "distributions [0.45417654514312744, 0.10078415274620056, 0.011281385086476803]\n",
      "[{'decision': 0, 'optimal_auc': 0.8977095262883915, 'imitation_auc': 0.5815839694656488, 'optimal_acc': 0.8231292517006803, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.5434782608695652, 'imitation_auc': 0.6896739130434782, 'optimal_acc': 0.9387755102040817, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8410680228862046, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.2313, 0.0612, 0.0000], grad_fn=<MeanBackward1>)ard0>) row 305 step 2 imitation False\n",
      "______epoch 5 _____\n",
      "val reward 0.6625261306762695\n",
      "imitation reward 1.1215100288391113\n",
      "distance losses 2.5446810722351074 1.0269883871078491\n",
      "distributions [0.31244176626205444, 0.08686328679323196, 0.007148659788072109]\n",
      "[{'decision': 0, 'optimal_auc': 0.9057782404997398, 'imitation_auc': 0.7108778625954199, 'optimal_acc': 0.8367346938775511, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.9154589371980677, 'imitation_auc': 0.7491847826086957, 'optimal_acc': 0.9387755102040817, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8445645263827082, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.2313, 0.0612, 0.0000], grad_fn=<MeanBackward1>)ard0>) row 305 step 2 imitation False\n",
      "______epoch 6 _____\n",
      "val reward 0.6523581743240356\n",
      "imitation reward 1.1009135246276855\n",
      "distance losses 2.548330545425415 1.0360537767410278\n",
      "distributions [0.3049166202545166, 0.09791195392608643, 0.006392927374690771]\n",
      "[{'decision': 0, 'optimal_auc': 0.9096824570536179, 'imitation_auc': 0.728530534351145, 'optimal_acc': 0.8503401360544217, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.926731078904992, 'imitation_auc': 0.7426630434782608, 'optimal_acc': 0.9387755102040817, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8452002542911634, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.2313, 0.0612, 0.0000], grad_fn=<MeanBackward1>)ard0>) row 305 step 2 imitation False\n",
      "______epoch 7 _____\n",
      "val reward 0.7305532097816467\n",
      "imitation reward 1.0823034048080444\n",
      "distance losses 2.8069264888763428 1.0390838384628296\n",
      "distributions [0.43299150466918945, 0.11977048963308334, 0.006262045353651047]\n",
      "[{'decision': 0, 'optimal_auc': 0.9021343050494535, 'imitation_auc': 0.6898854961832062, 'optimal_acc': 0.8775510204081632, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.9388083735909823, 'imitation_auc': 0.7296195652173914, 'optimal_acc': 0.9387755102040817, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8436109345200253, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.2313, 0.0612, 0.0000], grad_fn=<MeanBackward1>)ard0>) row 305 step 2 imitation False\n",
      "______epoch 8 _____\n",
      "val reward 0.6306530237197876\n",
      "imitation reward 1.0590133666992188\n",
      "distance losses 2.5635640621185303 1.0723302364349365\n",
      "distributions [0.3523111045360565, 0.10861667990684509, 0.004786964971572161]\n",
      "[{'decision': 0, 'optimal_auc': 0.9034357105674128, 'imitation_auc': 0.6168893129770991, 'optimal_acc': 0.8503401360544217, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.9388083735909823, 'imitation_auc': 0.7271739130434782, 'optimal_acc': 0.9387755102040817, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8340750158931978, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.2313, 0.0612, 0.0000], grad_fn=<MeanBackward1>)ard0>) row 305 step 2 imitation False\n",
      "______epoch 9 _____\n",
      "val reward 0.6912715435028076\n",
      "imitation reward 1.078160285949707\n",
      "distance losses 2.54038667678833 0.9654542803764343\n",
      "distributions [0.4166788160800934, 0.10476822406053543, 0.003406618721783161]\n",
      "[{'decision': 0, 'optimal_auc': 0.8919833420093701, 'imitation_auc': 0.6598282442748091, 'optimal_acc': 0.8843537414965986, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.9524959742351047, 'imitation_auc': 0.7051630434782609, 'optimal_acc': 0.9387755102040817, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8363000635727909, 'optimal_acc': 1.0, 'imitation_acc': 0.8163265306122449}]\n",
      "tensor([0.2313, 0.0612, 0.0000], grad_fn=<MeanBackward1>)ard0>) row 305 step 2 imitation False\n",
      "______epoch 10 _____\n",
      "val reward 0.6318811178207397\n",
      "imitation reward 1.0801769495010376\n",
      "distance losses 2.6445977687835693 0.9363393187522888\n",
      "distributions [0.37024039030075073, 0.098935067653656, 0.0021870427299290895]\n",
      "[{'decision': 0, 'optimal_auc': 0.8878188443519, 'imitation_auc': 0.685114503816794, 'optimal_acc': 0.8639455782312925, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.9541062801932367, 'imitation_auc': 0.6703804347826087, 'optimal_acc': 0.9387755102040817, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8235855054036872, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2313, 0.0612, 0.0000], grad_fn=<MeanBackward1>)ard0>) row 305 step 2 imitation False\n",
      "______epoch 11 _____\n",
      "val reward 0.6613147854804993\n",
      "imitation reward 1.066909670829773\n",
      "distance losses 2.575427293777466 0.9470703601837158\n",
      "distributions [0.42316266894340515, 0.11512630432844162, 0.0018668401753529906]\n",
      "[{'decision': 0, 'optimal_auc': 0.9023945861530452, 'imitation_auc': 0.6507633587786259, 'optimal_acc': 0.8843537414965986, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.9436392914653784, 'imitation_auc': 0.654891304347826, 'optimal_acc': 0.9387755102040817, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8080101716465352, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.2313, 0.0612, 0.0000], grad_fn=<MeanBackward1>)ard0>) row 305 step 2 imitation False\n",
      "______epoch 12 _____\n",
      "val reward 0.7296757102012634\n",
      "imitation reward 1.0491209030151367\n",
      "distance losses 2.667898416519165 0.8663128018379211\n",
      "distributions [0.4892040491104126, 0.13006380200386047, 0.0017159349517896771]\n",
      "[{'decision': 0, 'optimal_auc': 0.9091618948464343, 'imitation_auc': 0.6202290076335878, 'optimal_acc': 0.8435374149659864, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.9533011272141707, 'imitation_auc': 0.678804347826087, 'optimal_acc': 0.9387755102040817, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8223140495867769, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.2313, 0.0612, 0.0000], grad_fn=<MeanBackward1>)ard0>) row 305 step 2 imitation False\n",
      "______epoch 13 _____\n",
      "val reward 0.5296207666397095\n",
      "imitation reward 1.0399599075317383\n",
      "distance losses 2.3583779335021973 0.9432783126831055\n",
      "distributions [0.3179855942726135, 0.10730782151222229, 0.0014937586383894086]\n",
      "[{'decision': 0, 'optimal_auc': 0.9312857886517438, 'imitation_auc': 0.586354961832061, 'optimal_acc': 0.8707482993197279, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.9444444444444444, 'imitation_auc': 0.7263586956521738, 'optimal_acc': 0.9387755102040817, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8382072472981564, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.2313, 0.0612, 0.0000], grad_fn=<MeanBackward1>)ard0>) row 305 step 2 imitation False\n",
      "______epoch 14 _____\n",
      "val reward 0.5619155764579773\n",
      "imitation reward 1.0313937664031982\n",
      "distance losses 2.6464626789093018 0.9260284304618835\n",
      "distributions [0.34588462114334106, 0.11861898750066757, 0.0017061916878446937]\n",
      "[{'decision': 0, 'optimal_auc': 0.9232170744403956, 'imitation_auc': 0.5949427480916031, 'optimal_acc': 0.891156462585034, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.9484702093397746, 'imitation_auc': 0.7307065217391304, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.7687074829931972}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8417037507946599, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.2313, 0.0612, 0.0000], grad_fn=<MeanBackward1>)ard0>) row 305 step 2 imitation False\n",
      "______epoch 15 _____\n",
      "val reward 0.8377806544303894\n",
      "imitation reward 1.139865517616272\n",
      "distance losses 2.7087652683258057 0.9410547018051147\n",
      "distributions [0.5500231981277466, 0.1579107642173767, 0.002130428096279502]\n",
      "[{'decision': 0, 'optimal_auc': 0.9060385216033315, 'imitation_auc': 0.6111641221374047, 'optimal_acc': 0.7414965986394558, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.9452495974235104, 'imitation_auc': 0.7165760869565218, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.7891156462585034}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8254926891290528, 'optimal_acc': 1.0, 'imitation_acc': 0.8095238095238095}]\n",
      "tensor([0.2313, 0.0612, 0.0000], grad_fn=<MeanBackward1>)ard0>) row 305 step 2 imitation False\n",
      "______epoch 16 _____\n",
      "val reward 0.5936764478683472\n",
      "imitation reward 1.0840985774993896\n",
      "distance losses 2.612673044204712 0.8978281617164612\n",
      "distributions [0.3465377390384674, 0.11166651546955109, 0.0017077600350603461]\n",
      "[{'decision': 0, 'optimal_auc': 0.9060385216033316, 'imitation_auc': 0.6603053435114503, 'optimal_acc': 0.8707482993197279, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.9363929146537842, 'imitation_auc': 0.6782608695652174, 'optimal_acc': 0.9387755102040817, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7625556261919898, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.2313, 0.0612, 0.0000], grad_fn=<MeanBackward1>)ard0>) row 305 step 2 imitation False\n",
      "______epoch 17 _____\n",
      "val reward 0.6318924427032471\n",
      "imitation reward 1.0835604667663574\n",
      "distance losses 2.5173962116241455 0.9980324506759644\n",
      "distributions [0.4032016396522522, 0.1230863705277443, 0.0014697568258270621]\n",
      "[{'decision': 0, 'optimal_auc': 0.9151483602290473, 'imitation_auc': 0.6827290076335877, 'optimal_acc': 0.8843537414965986, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.9267310789049918, 'imitation_auc': 0.7054347826086957, 'optimal_acc': 0.9387755102040817, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7670057215511762, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.2313, 0.0612, 0.0000], grad_fn=<MeanBackward1>)ard0>) row 305 step 2 imitation False\n",
      "______epoch 18 _____\n",
      "val reward 0.9896529316902161\n",
      "imitation reward 1.0907211303710938\n",
      "distance losses 2.694364070892334 0.861370325088501\n",
      "distributions [0.6359015107154846, 0.18712252378463745, 0.001496729557402432]\n",
      "[{'decision': 0, 'optimal_auc': 0.9302446642373763, 'imitation_auc': 0.6345419847328244, 'optimal_acc': 0.46258503401360546, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.9500805152979066, 'imitation_auc': 0.7247282608695652, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7848061029879211, 'optimal_acc': 1.0, 'imitation_acc': 0.8299319727891157}]\n",
      "no losses n positive 1 yt tensor(1., grad_fn=<SelectBackward0>) row 305 step 2 imitation False\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#1.8424\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m decision_model2, decision_score2, decision_loss2, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_decision_model_triplet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel3\u001b[49m\u001b[43m,\u001b[49m\u001b[43msmodel3\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimitation_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreward_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimitation_triplet_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreward_triplet_weight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#realtive weight of survival, feeding tube, aspiration, andl lrc\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#weight for OS, LRC, FDM, and event (any + FT or AS at 6m) as time to event in weeks\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_attention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 329\u001b[0m, in \u001b[0;36mtrain_decision_model_triplet\u001b[0;34m(tmodel1, tmodel2, tmodel3, smodel3, use_default_split, use_bagging_split, use_attention, lr, epochs, patience, weights, tweights, opt_weights, imitation_weights, imitation_weight, reward_weight, imitation_triplet_weight, reward_triplet_weight, shufflecol_chance, split, resample_training, save_path, file_suffix, verbose, use_gpu, **model_kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m best_val_score \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m--> 329\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m     val_losses,val_metrics,val_distributions \u001b[38;5;241m=\u001b[39m step(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    331\u001b[0m     vl \u001b[38;5;241m=\u001b[39m val_losses[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m val_losses[\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[6], line 283\u001b[0m, in \u001b[0;36mtrain_decision_model_triplet.<locals>.step\u001b[0;34m(train)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_rows):\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m imitation_triplet_weight \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m.0001\u001b[39m:\n\u001b[0;32m--> 283\u001b[0m         imitation_tloss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mget_tloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mytrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43mxxtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m         imitation_tloss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m get_tloss(i,\u001b[38;5;241m1\u001b[39m,ytrain,x1_imitation,\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    285\u001b[0m         imitation_tloss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m get_tloss(i,\u001b[38;5;241m2\u001b[39m,ytrain,x2_imitation,\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[6], line 159\u001b[0m, in \u001b[0;36mtrain_decision_model_triplet.<locals>.get_tloss\u001b[0;34m(row, step, yt, x, imitation)\u001b[0m\n\u001b[1;32m    157\u001b[0m anchor \u001b[38;5;241m=\u001b[39m x[row]\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_attention:\n\u001b[0;32m--> 159\u001b[0m     [anchor_embedding,pos_embedding,neg_embedding] \u001b[38;5;241m=\u001b[39m [model\u001b[38;5;241m.\u001b[39mget_embedding(xx\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),position\u001b[38;5;241m=\u001b[39mstep,use_saved_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m xx \u001b[38;5;129;01min\u001b[39;00m [anchor,positive,negative]]\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:    \n\u001b[1;32m    161\u001b[0m     [anchor_embedding,pos_embedding,neg_embedding] \u001b[38;5;241m=\u001b[39m [model\u001b[38;5;241m.\u001b[39mget_embedding(xx\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),position\u001b[38;5;241m=\u001b[39mstep,concatenate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;28mint\u001b[39m(imitation)] \u001b[38;5;28;01mfor\u001b[39;00m xx \u001b[38;5;129;01min\u001b[39;00m [anchor,positive,negative]]\n",
      "Cell \u001b[0;32mIn[6], line 159\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    157\u001b[0m anchor \u001b[38;5;241m=\u001b[39m x[row]\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_attention:\n\u001b[0;32m--> 159\u001b[0m     [anchor_embedding,pos_embedding,neg_embedding] \u001b[38;5;241m=\u001b[39m [\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mposition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43muse_saved_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m xx \u001b[38;5;129;01min\u001b[39;00m [anchor,positive,negative]]\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:    \n\u001b[1;32m    161\u001b[0m     [anchor_embedding,pos_embedding,neg_embedding] \u001b[38;5;241m=\u001b[39m [model\u001b[38;5;241m.\u001b[39mget_embedding(xx\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),position\u001b[38;5;241m=\u001b[39mstep,concatenate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;28mint\u001b[39m(imitation)] \u001b[38;5;28;01mfor\u001b[39;00m xx \u001b[38;5;129;01min\u001b[39;00m [anchor,positive,negative]]\n",
      "File \u001b[0;32m/data/DigitalTwinVis/python/Models.py:640\u001b[0m, in \u001b[0;36mDecisionAttentionModel.get_embedding\u001b[0;34m(self, x, position, memory, use_saved_memory, **kwargs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attention,layer,norm \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattentions,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorms):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 640\u001b[0m         x2, attention_weights \u001b[38;5;241m=\u001b[39m \u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    641\u001b[0m         x2 \u001b[38;5;241m=\u001b[39m norm(x2 \u001b[38;5;241m+\u001b[39m x)\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/activation.py:1167\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[1;32m   1156\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1157\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1158\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         q_proj_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj_weight, k_proj_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj_weight,\n\u001b[1;32m   1165\u001b[0m         v_proj_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj_weight, average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights)\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:5156\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights)\u001b[0m\n\u001b[1;32m   5151\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   5152\u001b[0m \u001b[38;5;66;03m# (deep breath) calculate attention and out projection\u001b[39;00m\n\u001b[1;32m   5153\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   5155\u001b[0m B, Nt, E \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m-> 5156\u001b[0m q_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5158\u001b[0m     attn_output_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbaddbmm(attn_mask, q_scaled, k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#1.8424\n",
    "decision_model2, decision_score2, decision_loss2, _ = train_decision_model_triplet(\n",
    "    model1,model2,model3,smodel3,\n",
    "    lr=.01,\n",
    "    imitation_weight=1,\n",
    "    reward_weight=1,\n",
    "    patience=10,\n",
    "    imitation_triplet_weight=1,\n",
    "    reward_triplet_weight =1,\n",
    "    verbose=True,\n",
    "    weights=[0,0,0,0], #realtive weight of survival, feeding tube, aspiration, andl lrc\n",
    "    tweights=[0,0,0,1], #weight for OS, LRC, FDM, and event (any + FT or AS at 6m) as time to event in weeks\n",
    "    use_attention=True,\n",
    "    **args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a7faad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decision_model(\n",
    "    tmodel1,\n",
    "    tmodel2,\n",
    "    tmodel3,\n",
    "    smodel3,\n",
    "    use_default_split=True,\n",
    "    use_bagging_split=False,\n",
    "    lr=.0001,\n",
    "    epochs=10000,\n",
    "    patience=50,\n",
    "    weights=[0,.5,.5,0], #realtive weight of survival, feeding tube, aspiration, andl lrc\n",
    "    tweights=[1,1,1,0]\n",
    "    imitation_weights=[.5,1,1],#weights of imitation decisions, because ic overtrains too quickly\n",
    "    imitation_weight=0.1,\n",
    "    shufflecol_chance = 0.1,\n",
    "    reward_weight=1,\n",
    "    imitation_triplet_weight=0,\n",
    "    reward_triplet_weight = 0,\n",
    "    split=.7,\n",
    "    resample_training=False,\n",
    "    save_path='../data/models/',\n",
    "    file_suffix='',\n",
    "    use_gpu=True,\n",
    "    use_attention=True,\n",
    "    verbose=True,\n",
    "    threshold_decisions=True,#convert decisiosn to binary in simulation, usually breaks it\n",
    "    use_smote=False,\n",
    "    validate_with_memory=True,\n",
    "    **model_kwargs):\n",
    "    #outdated method of doing stuff, haven't updated with new loss functions idk\n",
    "    tmodel1.eval()\n",
    "    tmodel2.eval()\n",
    "    tmodel3.eval()\n",
    "    smodel3.eval()\n",
    "    train_ids, test_ids = get_tt_split(use_default_split=use_default_split,use_bagging_split=use_bagging_split,resample_training=resample_training)\n",
    "    true_ids = train_ids + test_ids #for saving memory without upsampling\n",
    "    if use_smote:\n",
    "        dataset = DTDataset(use_smote=True,smote_ids = train_ids)\n",
    "        train_ids = [i for i in dataset.processed_df.index.values if i not in test_ids]\n",
    "    else:\n",
    "        dataset = DTDataset()\n",
    "    data = dataset.processed_df.copy()\n",
    "    \n",
    "    def get_dlt(state):\n",
    "        if state == 2:\n",
    "            return data[Const.dlt2].copy()\n",
    "        d = data[Const.dlt1].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_pd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.primary_disease_states2].copy()\n",
    "        d = data[Const.primary_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_nd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.nodal_disease_states2].copy()\n",
    "        d = data[Const.nodal_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_cc(state):\n",
    "        res = data[Const.ccs].copy()\n",
    "        if state == 1:\n",
    "            res.values[:,:] = np.zeros(res.values.shape)\n",
    "        return res\n",
    "    \n",
    "    def get_mod(state):\n",
    "        res = data[Const.modifications].copy()\n",
    "        #this should have an ic condition but we don't use it anumore anywa\n",
    "        return res\n",
    "        \n",
    "    outcomedf = data[Const.outcomes]\n",
    "    baseline = dataset.get_state('baseline')\n",
    "    \n",
    "    def formatdf(d,dids=train_ids):\n",
    "        d = df_to_torch(d.loc[dids]).to(model.get_device())\n",
    "        return d\n",
    "    \n",
    "    def makegrad(v):\n",
    "        if not v.requires_grad:\n",
    "            v.requires_grad=True\n",
    "        return v\n",
    "    \n",
    "    if use_attention:\n",
    "        model = DecisionAttentionModel(baseline.shape[1],**model_kwargs)\n",
    "    else:\n",
    "        model_kwargs = {k:v for k,v in model_kwargs.items() if 'attention' not in k and 'embed' not in k}\n",
    "        model = DecisionModel(baseline.shape[1],**model_kwargs)\n",
    "\n",
    "    device = 'cpu'\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "        \n",
    "    model.set_device(device)\n",
    "\n",
    "    tmodel1.set_device(device)\n",
    "    tmodel2.set_device(device)\n",
    "    tmodel3.set_device(device)\n",
    "    smodel3.set_device(device)\n",
    "    \n",
    "    hashcode = str(hash(','.join([str(i) for i in train_ids])))\n",
    "    \n",
    "    save_file = save_path + 'model_' + model.identifier +'_hash' + hashcode + file_suffix + '.tar'\n",
    "    model.fit_normalizer(df_to_torch(baseline.loc[train_ids]).to(model.get_device()))\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "    \n",
    "    mse = torch.nn.MSELoss()\n",
    "    bce = torch.nn.BCELoss()\n",
    "    device = model.get_device()\n",
    "    def compare_decisions(d1,d2,d3,ids):\n",
    "#         ypred = np.concatenate([dd.cpu().detach().numpy().reshape(-1,1) for dd in [d1,d2,d3]],axis=1)\n",
    "        ytrue = df_to_torch(outcomedf.loc[ids])\n",
    "        dloss = bce(d1.view(-1),ytrue[:,0])\n",
    "        dloss += bce(d2.view(-1),ytrue[:,1])\n",
    "        dloss += bce(d3,view(-1),ytrue[:,2])\n",
    "        return dloss\n",
    "        \n",
    "    def remove_decisions(df):\n",
    "        cols = [c for c in df.columns if c not in Const.decisions ]\n",
    "        ddf = df[cols]\n",
    "        return ddf\n",
    "    \n",
    "    makeinput = lambda step,dids: df_to_torch(remove_decisions(dataset.get_input_state(step=step,ids=dids))).to(device)\n",
    "    thresh = lambda x: torch.sigmoid(100000000*(x - .5))\n",
    "\n",
    "    optimal_train,transitions_train = calc_optimal_decisions(dataset,train_ids,tmodel1,tmodel2,tmodel3,smodel3,\n",
    "                                           weights=weights,tweights=tweights,\n",
    "                                          )\n",
    "    optimal_test,transitions_test = calc_optimal_decisions(dataset,test_ids,tmodel1,tmodel2,tmodel3,smodel3,\n",
    "                                          weights=weights,tweights=tweights,\n",
    "                                         )\n",
    "    optimal_train = optimal_train.to(model.get_device())\n",
    "    optimal_test = optimal_test.to(model.get_device())\n",
    "    \n",
    "    #save the inputs from the whole dataset for future reference\n",
    "    if use_attention:\n",
    "        full_data = []\n",
    "        for mstep in [0,1,2]:\n",
    "            full_data_step = [baseline, get_dlt(min(mstep,1)),\n",
    "                         get_dlt(mstep),get_pd(mstep),get_nd(mstep),get_cc(mstep),get_mod(mstep)]\n",
    "            full_data_step = torch.cat([formatdf(fd,true_ids) for fd in full_data_step],axis=1)\n",
    "            full_data.append(full_data_step)\n",
    "        full_data = torch.stack(full_data)\n",
    "        model.save_memory(full_data.to(device))\n",
    "        print(full_data.shape)\n",
    "        \n",
    "    randchoice = lambda x: x[torch.randint(len(x),(1,))[0]]\n",
    "    tloss_func = torch.nn.TripletMarginLoss()\n",
    "    def get_tloss(row,step,yt,x,imitation=True):\n",
    "        if yt[:,step].std() < .001:\n",
    "            return torch.tensor([0]).to(device)\n",
    "        positive_idx= torch.nonzero(yt[:,step] == yt[row,step])\n",
    "        positive_idx = torch.stack([ii for ii in positive_idx if ii != row]).view(-1)\n",
    "        negative_idx = torch.tensor([ii for ii in range(x.shape[0]) if ii not in positive_idx and ii != row])\n",
    "        if len(positive_idx) < 1 or len(negative_idx) < 1:\n",
    "            print('no losses','n positive',len(positive_idx),'n negative',len(negative_idx),'yt',yt[row,step],'row',row,'step',step,'imitation',imitation,end='\\r')\n",
    "            return torch.tensor([0]).to(device)\n",
    "        positive = x[randchoice(positive_idx)]\n",
    "        negative = x[randchoice(negative_idx)]\n",
    "        anchor = x[row]\n",
    "        if use_attention:\n",
    "            [anchor_embedding,pos_embedding,neg_embedding] = [model.get_embedding(xx.view(1,-1),position=step,use_saved_memory=True) for xx in [anchor,positive,negative]]\n",
    "        else:    \n",
    "            [anchor_embedding,pos_embedding,neg_embedding] = [model.get_embedding(xx.view(1,-1),position=step,concatenate=False)[int(imitation)] for xx in [anchor,positive,negative]]\n",
    "        tloss = tloss_func(anchor_embedding,pos_embedding,neg_embedding)\n",
    "        return tloss\n",
    "    \n",
    "    def step(train=True):\n",
    "        if train:\n",
    "            model.train(True)\n",
    "            tmodel1.train(True)\n",
    "            tmodel2.train(True)\n",
    "            tmodel3.train(True)\n",
    "            optimizer.zero_grad()\n",
    "            ids = train_ids\n",
    "            y_opt = makegrad(optimal_train)\n",
    "        else:\n",
    "            ids = test_ids\n",
    "            model.eval()\n",
    "            tmodel1.eval()\n",
    "            tmodel2.eval()\n",
    "            tmodel3.eval()\n",
    "            y_opt = makegrad(optimal_test)\n",
    "            \n",
    "            \n",
    "        ytrain = df_to_torch(outcomedf.loc[ids]).to(device)\n",
    "        #imitation losses and decision 1\n",
    "        xxtrained = [baseline, get_dlt(0),get_dlt(0),get_pd(0),get_nd(0),get_cc(0),get_mod(0)]\n",
    "        xxtrain = torch.cat([formatdf(xx,ids) for xx in xxtrained],axis=1).to(device)\n",
    "        \n",
    "        use_memory = (not train) and validate_with_memory\n",
    "\n",
    "        o1 = model(xxtrain,position=0,use_saved_memory = use_memory)\n",
    "\n",
    "        decision1_imitation = o1[:,3]\n",
    "        \n",
    "        decision1_opt = o1[:,0]\n",
    "        if threshold_decisions:\n",
    "            decision1_opt = thresh(decision1_opt)\n",
    "\n",
    "        imitation_loss1 = bce(decision1_imitation,ytrain[:,0])\n",
    "        imitation_loss1 = torch.mul(imitation_loss1,imitation_weights[0])\n",
    "        \n",
    "        x1_imitation = [baseline, get_dlt(1),get_dlt(0),get_pd(1),get_nd(1),get_cc(1),get_mod(1)]\n",
    "        x1_imitation = [formatdf(xx1,ids) for xx1 in x1_imitation]\n",
    "        x1_imitation = torch.cat(x1_imitation,axis=1).to(device)\n",
    "        decision2_imitation = model(x1_imitation,position=1,use_saved_memory = use_memory)[:,4]\n",
    "\n",
    "        imitation_loss2 =  bce(decision2_imitation,ytrain[:,1])\n",
    "        imitation_loss2 = torch.mul(imitation_loss2,imitation_weights[1])\n",
    "        \n",
    "        x2_imitation = [baseline, get_dlt(1),get_dlt(2),get_pd(2),get_nd(2),get_cc(2),get_mod(2)]\n",
    "        \n",
    "        x2_imitation = [formatdf(xx2,ids) for xx2 in x2_imitation]\n",
    "        x2_imitation = torch.cat(x2_imitation,axis=1)\n",
    "        decision3_imitation = model(x2_imitation,position=2,use_saved_memory = use_memory)[:,5]\n",
    "\n",
    "        imitation_loss3 = bce(decision3_imitation,ytrain[:,2])\n",
    "        imitation_loss3 = torch.mul(imitation_loss3,imitation_weights[2])\n",
    "        \n",
    "        #reward decisions\n",
    "        xx1 = makeinput(1,ids)\n",
    "        xx2 = makeinput(2,ids)\n",
    "        xx3 = makeinput(3,ids)\n",
    "\n",
    "        xx1 = makegrad(xx1)\n",
    "        xx2 = makegrad(xx2)\n",
    "        xx3 = makegrad(xx3)\n",
    "        baseline_train_base = formatdf(baseline,ids)\n",
    "            \n",
    "        baseline_train = torch.clone(baseline_train_base)\n",
    "\n",
    "        \n",
    "        xi1 = torch.cat([xx1,decision1_opt.view(-1,1)],axis=1)\n",
    "        print(train,tmodel1.training,tmodel1.dropout.training)\n",
    "        [ypd1, ynd1, ymod, ydlt1] = tmodel1(xi1)['predictions']\n",
    "        print(train,tmodel1.training,tmodel1.dropout.training)\n",
    "        d1_thresh = torch.gt(decision1_opt.view(-1,1),.5).to(ypd1.device)\n",
    "        d1_scale = torch.cat([d1_thresh,d1_thresh,torch.ones(d1_thresh.view(-1,1).shape).to(ypd1.device)],dim=1)\n",
    "        ypd1= torch.mul(ypd1,d1_scale)\n",
    "        ynd1= torch.mul(ynd1,d1_scale)\n",
    "        \n",
    "        x1 = [baseline_train,ydlt1,formatdf(get_dlt(0),ids),ypd1,ynd1,formatdf(get_cc(1),ids),ymod]\n",
    "        x1= torch.cat([xx1.to(model.get_device()) for xx1 in x1],axis=1)\n",
    "        \n",
    "        decision2_opt = model(x1,position=1,use_saved_memory = use_memory)[:,1] \n",
    "        if threshold_decisions:\n",
    "            decision2_opt = thresh(decision2_opt)\n",
    "            \n",
    "        xi2 = torch.cat([xx2,decision1_opt.view(-1,1),decision2_opt.view(-1,1)],axis=1)\n",
    "        [ypd2,ynd2,ycc,ydlt2] = tmodel2(xi2)['predictions']\n",
    "\n",
    "        x2 = [baseline_train,ydlt1,ydlt2,ypd2,ynd2,ycc,ymod]\n",
    "        x2 = torch.cat([xx2.to(model.get_device()) for xx2 in x2],axis=1)\n",
    "        decision3_opt = model(x2,position=2,use_saved_memory = use_memory)[:,2]\n",
    "        \n",
    "        if threshold_decisions:\n",
    "            decision3_opt = thresh(decision3_opt)\n",
    "            \n",
    "        xi3 = torch.cat([xx3,decision1_opt.view(-1,1),decision2_opt.view(-1,1),decision3_opt.view(-1,1)],axis=1)\n",
    "        \n",
    "        outcomes = tmodel3(xi3)['predictions']\n",
    "        survival = smodel3.time_to_event(xi3,n_samples=1)\n",
    "        if not train and verbose:\n",
    "            print(torch.mean(outcomes,dim=0))\n",
    "            \n",
    "        reward_loss = torch.mean(outcome_loss(outcomes,weights) + temporal_loss(survival,tweights))\n",
    "        loss = torch.add(imitation_loss1,imitation_loss2)\n",
    "        loss = torch.add(loss,imitation_loss3)\n",
    "        loss = torch.mul(loss,imitation_weight/3)\n",
    "        loss = torch.add(loss,torch.mul(reward_loss,reward_weight))\n",
    "        \n",
    "        imitation_tloss = torch.FloatTensor([0]).to(device)\n",
    "        opt_tloss = torch.FloatTensor([0]).to(device)\n",
    "        n_rows = x1.shape[0]\n",
    "        if reward_triplet_weight + imitation_triplet_weight > 0.0001:\n",
    "            for i in range(n_rows):\n",
    "                #skip if we're using an attention model idk\n",
    "                if not use_attention and imitation_triplet_weight > .0001:\n",
    "                    imitation_tloss += get_tloss(i,0,ytrain,xxtrain,True)\n",
    "                    imitation_tloss += get_tloss(i,1,ytrain,x1_imitation,True)\n",
    "                    imitation_tloss += get_tloss(i,2,ytrain,x2_imitation,True)\n",
    "                if reward_triplet_weight > .0001:\n",
    "                    opt_tloss += get_tloss(i,0,y_opt,xxtrain,False)\n",
    "                    opt_tloss += get_tloss(i,1,y_opt,x1,False)\n",
    "                    opt_tloss += get_tloss(i,2,y_opt,x2,False)\n",
    "            loss += torch.mul(imitation_tloss[0],imitation_triplet_weight/n_rows)\n",
    "            loss += torch.mul(opt_tloss[0],reward_triplet_weight/n_rows)\n",
    "        \n",
    "        losses = [imitation_loss1+imitation_loss2+imitation_loss3,reward_loss,imitation_tloss,opt_tloss]\n",
    "\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            return losses\n",
    "        else:\n",
    "            scores = []\n",
    "            distributions = [decision1_opt.mean().item(),decision2_opt.mean().item(),decision3_opt.mean().item()]\n",
    "            imitation = [decision1_imitation,decision2_imitation,decision3_imitation]\n",
    "            optimal = [decision1_opt,decision2_opt,decision3_opt]\n",
    "            for i,decision_im in enumerate(imitation):\n",
    "                deci = decision_im.cpu().detach().numpy()\n",
    "                deci0 = (deci > .5).astype(int)\n",
    "                iout = ytrain[:,i].cpu().detach().numpy()\n",
    "                acci = accuracy_score(iout,deci0)\n",
    "                try:\n",
    "                    auci = roc_auc_score(iout,deci)\n",
    "                except:\n",
    "                    auci = -1\n",
    "                \n",
    "                deco = optimal[i].cpu().detach().numpy()\n",
    "                deci0 = (deco > .5).astype(int)\n",
    "                oout = y_opt[:,i].cpu().detach().numpy()\n",
    "                acco = accuracy_score(oout,deci0)\n",
    "                try:\n",
    "                    auco = roc_auc_score(oout,deco)\n",
    "                except:\n",
    "                    auco=-1\n",
    "                scores.append({'decision': i,'optimal_auc': auco,'imitation_auc': auci,'optimal_acc': acco,'imitation_acc': acci})\n",
    "            return losses, scores, distributions\n",
    "        \n",
    "    best_val_loss = torch.tensor(1000000000.0)\n",
    "    steps_since_improvement = 0\n",
    "    best_val_score = {}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        losses = step(True)\n",
    "        val_losses,val_metrics,val_distributions = step(False)\n",
    "        vl = val_losses[0] + val_losses[1]\n",
    "        for vm in val_metrics:\n",
    "            vl += (-((vm['optimal_auc']*reward_weight) + (vm['imitation_auc']*imitation_weight)))/10\n",
    "        if verbose:\n",
    "            print('______epoch',str(epoch),'_____')\n",
    "            print('val reward',val_losses[1].item())\n",
    "            print('imitation reward', val_losses[0].item())\n",
    "            if len(val_losses) > 2:\n",
    "                print('distance losses',val_losses[2].item(),val_losses[-1].item())\n",
    "            print('distributions',val_distributions)\n",
    "            print(val_metrics)\n",
    "        if vl < best_val_loss:\n",
    "            best_val_loss = vl\n",
    "            best_val_score = val_metrics\n",
    "            best_val_distributions = val_distributions\n",
    "            steps_since_improvement = 0\n",
    "            torch.save(model.state_dict(),save_file)\n",
    "        else:\n",
    "            steps_since_improvement += 1\n",
    "        if steps_since_improvement > patience:\n",
    "            break\n",
    "    print('++++++++++Final+++++++++++')\n",
    "    print('best',best_val_loss)\n",
    "    print(best_val_score)\n",
    "    model.load_state_dict(torch.load(save_file))\n",
    "    model.eval()\n",
    "    return model, best_val_score, best_val_loss, best_val_distributions\n",
    "\n",
    "from Models import *\n",
    "# args = {\n",
    "#     'hidden_layers': [50,50], \n",
    "#     'attention_heads': [2,2],\n",
    "#     'embed_size': 120, \n",
    "#     'dropout': 0.5, \n",
    "#     'input_dropout': 0.2, \n",
    "#     'shufflecol_chance':  0.2,\n",
    "# }\n",
    "args = {\n",
    "    'hidden_layers': [500], \n",
    "    'opt_layer_size': 20, \n",
    "    'imitation_layer_size': 20, \n",
    "    'dropout': 0.25, \n",
    "    'input_dropout': 0.25, \n",
    "    'shufflecol_chance': 0.5\n",
    "}\n",
    "from Models import *\n",
    "decision_model, _, _, _ = train_decision_model(\n",
    "    model1,model2,model3,smodel3,\n",
    "    lr=.001,\n",
    "    use_attention=True,\n",
    "    imitation_weight=1,\n",
    "    imitation_triplet_weight=0,\n",
    "    reward_triplet_weight=0,\n",
    "    reward_weight=2,\n",
    "    validate_with_memory=True,\n",
    "    use_smote=False,\n",
    "    **args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
