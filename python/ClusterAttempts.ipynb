{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "050c878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7418d82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score,accuracy_score,precision_recall_fscore_support\n",
    "from Constants import *\n",
    "from Preprocessing import *\n",
    "from Models import *\n",
    "import copy\n",
    "from Utils import *\n",
    "from DeepSurvivalModels import *\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c427599a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(536, 60)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = DTDataset(use_smote=False)\n",
    "data.processed_df.T\n",
    "data.get_input_state(1).shape\n",
    "# data.processed_df#.shape, len(data.processed_df.index.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "055e18c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    3,     5,     6,     7,     8,     9,    10,    11,    13,\n",
       "          14,    15,    16,    17,    18,    21,    23,    24,    25,\n",
       "          26,    27,    28,    31,    32,    33,    35,    36,    37,\n",
       "          38,    39,    40,    41,    42,    44,    45,    47,    48,\n",
       "          49,    50,    51,    53,    55,    56,    57,    60,    64,\n",
       "          65,    67,    68,    69,    71,    74,    75,    77,    78,\n",
       "          79,    80,    81,    82,    87,    88,    91,    94,    96,\n",
       "          99,   103,   109,   116,   117,   119,   120,   121,   125,\n",
       "         133,   148,   150,   153,   168,   178,   181,   183,   184,\n",
       "         185,   186,   187,   188,   189,   190,   191,   192,   193,\n",
       "         194,   195,   196,   197,   198,   199,   200,   201,   202,\n",
       "         203,   204,   205,   206,   207,   208,   209,   210,   211,\n",
       "         212,   213,   214,   215,   216,   217,   218,   219,   220,\n",
       "         221,   222,   223,   224,   225,   226,   227,   228,   229,\n",
       "         230,   231,   232,   233,   234,   235,   236,   237,   238,\n",
       "         239,   240,   241,   242,   243,   244,   245,   246,   247,\n",
       "         248,   249,   251,   252,   253,   254,   255,   256,   257,\n",
       "         258,   259,   260,   261,   262,   263,   264,   265,   266,\n",
       "         267,   268,   269,   270,   271,   272,   273,   274,   275,\n",
       "         276,   277,   278,   279,   280,   281,   282,   283,   284,\n",
       "         285,   286,   287,   288,   289,  2000,  2001,  2002,  2003,\n",
       "        2004,  2005,  2006,  2007,  2008,  2009,  2010,  2011,  2012,\n",
       "        2013,  2014,  2015,  2016,  2017,  2018,  2019,  2020,  2021,\n",
       "        2022,  2023,  2024,  2025,  2026,  2027,  2028,  2029,  2030,\n",
       "        2031,  2032,  2033,  5000,  5001,  5002,  5003,  5004,  5005,\n",
       "        5006,  5007,  5008,  5009,  5010,  5011,  5012,  5013,  5014,\n",
       "        5015,  5016,  5017,  5018,  5019,  5020,  5021,  5022,  5023,\n",
       "        5024,  5025,  5026,  5027,  5028,  5029,  5030,  5031,  5032,\n",
       "        5033,  5034,  5035,  5036,  5037,  5038,  5039,  5040,  5041,\n",
       "        5042,  5043,  5044,  5045,  5047,  5048,  5049,  5050,  5051,\n",
       "        5052,  5053,  5054,  5055,  5056,  5057,  5058,  5059,  5060,\n",
       "        5061,  5062,  5063,  5064,  5065,  5066,  5067,  5068,  5069,\n",
       "        5070,  5071,  5072,  5073,  5074,  5075,  5076,  5077,  5078,\n",
       "        5079,  5080,  5081,  5082,  5083,  5084,  5085,  5086,  5087,\n",
       "        5088,  5089,  5090,  5091,  5092,  5093,  5094,  5095,  5096,\n",
       "        5097,  5098,  5099,  5100,  5101,  5102,  5103,  5104,  5105,\n",
       "        5106,  5107,  5108,  5109,  5110,  5111,  5112,  5113,  5114,\n",
       "        5115,  5117,  5118,  5119,  5120, 10001, 10002, 10003, 10004,\n",
       "       10005, 10006, 10007, 10008, 10009, 10010, 10011, 10012, 10013,\n",
       "       10014, 10015, 10016, 10017, 10018, 10019, 10020, 10021, 10022,\n",
       "       10023, 10024, 10025, 10026, 10027, 10028, 10029, 10031, 10033,\n",
       "       10034, 10035, 10036, 10037, 10038, 10039, 10040, 10041, 10042,\n",
       "       10043, 10044, 10045, 10046, 10047, 10048, 10049, 10050, 10051,\n",
       "       10052, 10053, 10054, 10055, 10056, 10057, 10058, 10059, 10060,\n",
       "       10061, 10062, 10063, 10064, 10065, 10066, 10067, 10068, 10069,\n",
       "       10070, 10071, 10072, 10073, 10074, 10075, 10077, 10078, 10079,\n",
       "       10080, 10081, 10082, 10083, 10084, 10085, 10086, 10087, 10088,\n",
       "       10089, 10090, 10091, 10092, 10093, 10094, 10095, 10096, 10097,\n",
       "       10098, 10099, 10100, 10101, 10102, 10103, 10104, 10105, 10106,\n",
       "       10107, 10108, 10109, 10110, 10111, 10113, 10114, 10115, 10116,\n",
       "       10117, 10118, 10119, 10120, 10121, 10123, 10124, 10125, 10126,\n",
       "       10127, 10128, 10129, 10130, 10131, 10132, 10133, 10134, 10135,\n",
       "       10136, 10137, 10138, 10139, 10140, 10141, 10142, 10143, 10144,\n",
       "       10145, 10146, 10147, 10148, 10149, 10150, 10151, 10152, 10153,\n",
       "       10154, 10155, 10156, 10157, 10158, 10159, 10160, 10161, 10162,\n",
       "       10163, 10164, 10165, 10167, 10168, 10169, 10170, 10171, 10172,\n",
       "       10173, 10174, 10175, 10176, 10177, 10178, 10180, 10181, 10182,\n",
       "       10183, 10184, 10185, 10186, 10187, 10188, 10189, 10190, 10191,\n",
       "       10192, 10193, 10194, 10195, 10196, 10197, 10198, 10199, 10200,\n",
       "       10201, 10202, 10203, 10204, 10205])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.processed_df\n",
    "dt_ids = data.processed_df.index.values\n",
    "dt_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc1f869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, test_ids = get_tt_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b855c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(165, 77, 215, 185, 127)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def overlap(v1,v2):\n",
    "    return sorted([vv for vv in v1 if vv in v2])\n",
    "\n",
    "def get_id_mapped_r01(file = '../data/key_map.xlsx',reverse=False):\n",
    "    df = pd.read_excel('../data/key_map.xlsx').drop('Unnamed: 0',axis=1)\n",
    "    df['mdasi_id'] = df['STIEFEL'].apply(lambda x: int(x.replace(\"STIEFEL_\",'')))\n",
    "    df = df[['mdasi_id','ID']]\n",
    "    if reverse:\n",
    "        return df.set_index('mdasi_id').to_dict()['ID']\n",
    "    idmap = df.set_index('ID').to_dict()['mdasi_id']\n",
    "    r01 = pd.read_csv('../data/distance_csv.csv').drop('Unnamed: 0',axis=1)\n",
    "    r01['old_id'] = r01.id\n",
    "    r01['id'] = r01['id'].apply(lambda x: idmap.get(x,x))\n",
    "    return r01\n",
    "\n",
    "camprt = pd.read_csv('../data/camprtdists.csv').rename({'ID':'id'},axis=1)\n",
    "mdasi = pd.read_csv('../data/MDASI_0909201_surgery_updated.csv')\n",
    "r01 = get_id_mapped_r01()\n",
    "camprt_overlap = set(camprt.id.values.astype(int)).intersection(data.processed_df.index.values.astype(int))\n",
    "r01_overlap = set(r01.id.values.astype(int)).intersection(data.processed_df.index.values.astype(int))\n",
    "mdasi_overlap = set(mdasi.id.values.astype(int)).intersection(data.processed_df.index.values.astype(int))\n",
    "len(camprt_overlap), len(r01_overlap), len(camprt_overlap.union(r01_overlap)),len(mdasi_overlap), len(mdasi_overlap.intersection(r01_overlap.union(camprt_overlap)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2937b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 102, 42)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "clusters = pd.read_excel('../data/mdasi_dose_clusters_v2.xlsx')\n",
    "cluster_overlap = overlap(clusters.id.values.astype('int'),dt_ids)\n",
    "len(cluster_overlap), len(overlap(clusters.id.values.astype('int'),train_ids)), len(overlap(clusters.id.values.astype('int'),test_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8aacc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cluster_ids = overlap(clusters.id.values.astype('int'),train_ids)\n",
    "test_cluster_ids = overlap(clusters.id.values.astype('int'),test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6efdcdd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42, 7),\n",
       " (102, 7),\n",
       " ['dose_clusters_drymouth_filtered',\n",
       "  'dose_clusters_voice_filtered',\n",
       "  'dose_clusters_choke_filtered',\n",
       "  'dose_clusters_swallow_filtered',\n",
       "  'dose_clusters_mucus_filtered',\n",
       "  'dose_clusters_mucositis_filtered',\n",
       "  'dose_clusters_taste_filtered'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xctrain = data.get_input_state(step=1,ids=train_cluster_ids).drop(Const.decisions[0],axis=1)\n",
    "xctest = data.get_input_state(step=1,ids=test_cluster_ids).drop(Const.decisions[0],axis=1)\n",
    "yctrain = clusters.set_index('id').loc[train_cluster_ids]\n",
    "yctest = clusters.set_index('id').loc[test_cluster_ids]\n",
    "cluster_cols = [c for c in yctrain.columns if '_filtered' in c]\n",
    "yctrain = yctrain[cluster_cols]\n",
    "yctest = yctest[cluster_cols]\n",
    "yctest.shape, yctrain.shape, cluster_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61e8e794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dose_clusters_drymouth_filtered</th>\n",
       "      <th>dose_clusters_voice_filtered</th>\n",
       "      <th>dose_clusters_choke_filtered</th>\n",
       "      <th>dose_clusters_swallow_filtered</th>\n",
       "      <th>dose_clusters_mucus_filtered</th>\n",
       "      <th>dose_clusters_mucositis_filtered</th>\n",
       "      <th>dose_clusters_taste_filtered</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dose_clusters_drymouth_filtered  dose_clusters_voice_filtered  \\\n",
       "id                                                                   \n",
       "7                                  2                             0   \n",
       "9                                  1                             1   \n",
       "10                                 2                             2   \n",
       "25                                 2                             0   \n",
       "31                                 1                             1   \n",
       "35                                 0                             1   \n",
       "44                                 2                             0   \n",
       "45                                 1                             1   \n",
       "47                                 1                             0   \n",
       "68                                 2                             2   \n",
       "77                                 2                             0   \n",
       "117                                2                             0   \n",
       "133                                1                             1   \n",
       "168                                1                             1   \n",
       "184                                2                             0   \n",
       "187                                0                             1   \n",
       "189                                0                             1   \n",
       "190                                2                             0   \n",
       "194                                2                             0   \n",
       "202                                2                             1   \n",
       "208                                0                             1   \n",
       "209                                0                             1   \n",
       "211                                1                             1   \n",
       "215                                2                             0   \n",
       "217                                1                             1   \n",
       "224                                2                             0   \n",
       "227                                0                             1   \n",
       "228                                1                             1   \n",
       "236                                2                             0   \n",
       "242                                2                             0   \n",
       "245                                1                             1   \n",
       "254                                1                             1   \n",
       "264                                1                             1   \n",
       "267                                2                             0   \n",
       "268                                1                             0   \n",
       "271                                0                             1   \n",
       "272                                0                             1   \n",
       "274                                2                             0   \n",
       "279                                2                             2   \n",
       "284                                2                             1   \n",
       "287                                2                             0   \n",
       "288                                0                             1   \n",
       "\n",
       "     dose_clusters_choke_filtered  dose_clusters_swallow_filtered  \\\n",
       "id                                                                  \n",
       "7                               1                               2   \n",
       "9                               2                               1   \n",
       "10                              0                               0   \n",
       "25                              0                               2   \n",
       "31                              2                               1   \n",
       "35                              2                               1   \n",
       "44                              0                               2   \n",
       "45                              2                               2   \n",
       "47                              0                               2   \n",
       "68                              0                               0   \n",
       "77                              0                               0   \n",
       "117                             1                               2   \n",
       "133                             2                               1   \n",
       "168                             1                               2   \n",
       "184                             1                               2   \n",
       "187                             2                               1   \n",
       "189                             2                               1   \n",
       "190                             1                               2   \n",
       "194                             1                               2   \n",
       "202                             1                               2   \n",
       "208                             2                               1   \n",
       "209                             2                               1   \n",
       "211                             2                               1   \n",
       "215                             0                               2   \n",
       "217                             2                               1   \n",
       "224                             1                               2   \n",
       "227                             2                               1   \n",
       "228                             1                               2   \n",
       "236                             1                               2   \n",
       "242                             1                               2   \n",
       "245                             1                               2   \n",
       "254                             2                               1   \n",
       "264                             2                               1   \n",
       "267                             1                               2   \n",
       "268                             1                               2   \n",
       "271                             2                               1   \n",
       "272                             2                               1   \n",
       "274                             0                               0   \n",
       "279                             0                               0   \n",
       "284                             1                               2   \n",
       "287                             1                               2   \n",
       "288                             2                               1   \n",
       "\n",
       "     dose_clusters_mucus_filtered  dose_clusters_mucositis_filtered  \\\n",
       "id                                                                    \n",
       "7                               0                                 0   \n",
       "9                               1                                 2   \n",
       "10                              0                                 0   \n",
       "25                              0                                 1   \n",
       "31                              1                                 2   \n",
       "35                              2                                 2   \n",
       "44                              1                                 1   \n",
       "45                              1                                 0   \n",
       "47                              1                                 1   \n",
       "68                              0                                 1   \n",
       "77                              0                                 1   \n",
       "117                             0                                 1   \n",
       "133                             1                                 2   \n",
       "168                             1                                 2   \n",
       "184                             0                                 1   \n",
       "187                             2                                 2   \n",
       "189                             2                                 2   \n",
       "190                             0                                 1   \n",
       "194                             0                                 1   \n",
       "202                             0                                 0   \n",
       "208                             2                                 2   \n",
       "209                             2                                 2   \n",
       "211                             1                                 2   \n",
       "215                             0                                 1   \n",
       "217                             1                                 0   \n",
       "224                             0                                 1   \n",
       "227                             2                                 2   \n",
       "228                             1                                 1   \n",
       "236                             0                                 0   \n",
       "242                             0                                 1   \n",
       "245                             1                                 1   \n",
       "254                             1                                 2   \n",
       "264                             1                                 2   \n",
       "267                             0                                 0   \n",
       "268                             1                                 1   \n",
       "271                             2                                 2   \n",
       "272                             2                                 2   \n",
       "274                             0                                 1   \n",
       "279                             0                                 1   \n",
       "284                             0                                 1   \n",
       "287                             0                                 1   \n",
       "288                             2                                 2   \n",
       "\n",
       "     dose_clusters_taste_filtered  \n",
       "id                                 \n",
       "7                               0  \n",
       "9                               1  \n",
       "10                              0  \n",
       "25                              0  \n",
       "31                              2  \n",
       "35                              1  \n",
       "44                              1  \n",
       "45                              2  \n",
       "47                              1  \n",
       "68                              1  \n",
       "77                              1  \n",
       "117                             1  \n",
       "133                             1  \n",
       "168                             1  \n",
       "184                             0  \n",
       "187                             1  \n",
       "189                             1  \n",
       "190                             1  \n",
       "194                             0  \n",
       "202                             2  \n",
       "208                             1  \n",
       "209                             1  \n",
       "211                             2  \n",
       "215                             0  \n",
       "217                             2  \n",
       "224                             0  \n",
       "227                             1  \n",
       "228                             1  \n",
       "236                             1  \n",
       "242                             0  \n",
       "245                             2  \n",
       "254                             2  \n",
       "264                             1  \n",
       "267                             1  \n",
       "268                             0  \n",
       "271                             1  \n",
       "272                             1  \n",
       "274                             1  \n",
       "279                             0  \n",
       "284                             2  \n",
       "287                             0  \n",
       "288                             1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yctest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c2e8dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClusterImputer(\n",
       "  (input_dropout): Dropout(p=0, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=59, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (batchnorm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (relu): Softplus(beta=1, threshold=20)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (softmax): Softmax(dim=1)\n",
       "  (final_layers): ModuleList(\n",
       "    (0-6): 7 x Linear(in_features=100, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Models import ClusterImputer\n",
    "imputer = ClusterImputer(xctrain.shape[1],yctest.shape[1])\n",
    "imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c60fa41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.3313, 0.3413, 0.3274],\n",
       "         [0.3306, 0.3568, 0.3126],\n",
       "         [0.3713, 0.3253, 0.3035],\n",
       "         [0.3472, 0.3290, 0.3238],\n",
       "         [0.3325, 0.3542, 0.3133],\n",
       "         [0.3613, 0.3145, 0.3242],\n",
       "         [0.3515, 0.3270, 0.3214],\n",
       "         [0.3572, 0.3293, 0.3135],\n",
       "         [0.3459, 0.3302, 0.3240],\n",
       "         [0.3339, 0.3579, 0.3082],\n",
       "         [0.3183, 0.3710, 0.3107],\n",
       "         [0.2387, 0.3633, 0.3980],\n",
       "         [0.3390, 0.3535, 0.3074],\n",
       "         [0.3666, 0.3357, 0.2977],\n",
       "         [0.3482, 0.3334, 0.3184],\n",
       "         [0.3049, 0.3570, 0.3381],\n",
       "         [0.3094, 0.4000, 0.2905],\n",
       "         [0.2951, 0.3728, 0.3321],\n",
       "         [0.3405, 0.3537, 0.3058],\n",
       "         [0.3159, 0.3808, 0.3033],\n",
       "         [0.3318, 0.3410, 0.3272],\n",
       "         [0.3512, 0.3057, 0.3431],\n",
       "         [0.3268, 0.3340, 0.3392],\n",
       "         [0.3460, 0.3471, 0.3070],\n",
       "         [0.3227, 0.3480, 0.3293],\n",
       "         [0.2938, 0.3616, 0.3446],\n",
       "         [0.3363, 0.3403, 0.3234],\n",
       "         [0.3722, 0.3526, 0.2752],\n",
       "         [0.3180, 0.3414, 0.3406],\n",
       "         [0.3243, 0.3831, 0.2925],\n",
       "         [0.3074, 0.3475, 0.3451],\n",
       "         [0.2960, 0.4006, 0.3034],\n",
       "         [0.3347, 0.3396, 0.3257],\n",
       "         [0.3725, 0.3053, 0.3222],\n",
       "         [0.3353, 0.3628, 0.3019],\n",
       "         [0.3146, 0.3244, 0.3610],\n",
       "         [0.3804, 0.3524, 0.2671],\n",
       "         [0.3005, 0.3563, 0.3432],\n",
       "         [0.3444, 0.3714, 0.2842],\n",
       "         [0.2590, 0.4121, 0.3289],\n",
       "         [0.3302, 0.3491, 0.3207],\n",
       "         [0.3137, 0.3377, 0.3485],\n",
       "         [0.2767, 0.3994, 0.3239],\n",
       "         [0.3442, 0.3561, 0.2997],\n",
       "         [0.2745, 0.3821, 0.3434],\n",
       "         [0.3223, 0.3825, 0.2951],\n",
       "         [0.3417, 0.3930, 0.2653],\n",
       "         [0.3506, 0.3663, 0.2832],\n",
       "         [0.3344, 0.3501, 0.3155],\n",
       "         [0.3827, 0.3652, 0.2521],\n",
       "         [0.3553, 0.3419, 0.3028],\n",
       "         [0.3323, 0.3523, 0.3155],\n",
       "         [0.3496, 0.3942, 0.2562],\n",
       "         [0.3434, 0.3622, 0.2944],\n",
       "         [0.3713, 0.3584, 0.2702],\n",
       "         [0.3420, 0.3266, 0.3315],\n",
       "         [0.2847, 0.3791, 0.3362],\n",
       "         [0.3186, 0.3891, 0.2923],\n",
       "         [0.3441, 0.3567, 0.2992],\n",
       "         [0.3197, 0.3924, 0.2879],\n",
       "         [0.3788, 0.3461, 0.2750],\n",
       "         [0.3411, 0.3292, 0.3297],\n",
       "         [0.3417, 0.3500, 0.3083],\n",
       "         [0.3239, 0.3793, 0.2969],\n",
       "         [0.3501, 0.3617, 0.2882],\n",
       "         [0.2974, 0.3963, 0.3064],\n",
       "         [0.3474, 0.3277, 0.3249],\n",
       "         [0.3643, 0.3193, 0.3164],\n",
       "         [0.3031, 0.3989, 0.2979],\n",
       "         [0.3093, 0.3581, 0.3326],\n",
       "         [0.3884, 0.3231, 0.2885],\n",
       "         [0.3322, 0.3405, 0.3273],\n",
       "         [0.2735, 0.3849, 0.3417],\n",
       "         [0.3180, 0.3955, 0.2865],\n",
       "         [0.3499, 0.3814, 0.2687],\n",
       "         [0.3242, 0.3870, 0.2888],\n",
       "         [0.3315, 0.3659, 0.3026],\n",
       "         [0.3226, 0.3724, 0.3050],\n",
       "         [0.3154, 0.3799, 0.3047],\n",
       "         [0.3600, 0.3216, 0.3184],\n",
       "         [0.3317, 0.3149, 0.3534],\n",
       "         [0.3045, 0.3817, 0.3138],\n",
       "         [0.3282, 0.3898, 0.2819],\n",
       "         [0.3333, 0.3704, 0.2963],\n",
       "         [0.3436, 0.3338, 0.3226],\n",
       "         [0.3366, 0.3579, 0.3054],\n",
       "         [0.2868, 0.3700, 0.3432],\n",
       "         [0.2560, 0.4135, 0.3305],\n",
       "         [0.3416, 0.3418, 0.3166],\n",
       "         [0.3686, 0.3262, 0.3052],\n",
       "         [0.3151, 0.3653, 0.3196],\n",
       "         [0.3565, 0.3608, 0.2828],\n",
       "         [0.3414, 0.3549, 0.3037],\n",
       "         [0.3284, 0.3313, 0.3403],\n",
       "         [0.3444, 0.3385, 0.3170],\n",
       "         [0.3296, 0.3431, 0.3274],\n",
       "         [0.3461, 0.3465, 0.3074],\n",
       "         [0.3640, 0.3384, 0.2976],\n",
       "         [0.3111, 0.3871, 0.3018],\n",
       "         [0.3957, 0.3037, 0.3006],\n",
       "         [0.3542, 0.3382, 0.3075],\n",
       "         [0.3103, 0.3686, 0.3210]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.3567, 0.2721, 0.3712],\n",
       "         [0.3427, 0.2935, 0.3638],\n",
       "         [0.3498, 0.3281, 0.3221],\n",
       "         [0.3549, 0.3027, 0.3423],\n",
       "         [0.3631, 0.2934, 0.3436],\n",
       "         [0.3045, 0.3185, 0.3770],\n",
       "         [0.3572, 0.3171, 0.3257],\n",
       "         [0.3217, 0.3555, 0.3229],\n",
       "         [0.3472, 0.3007, 0.3521],\n",
       "         [0.3410, 0.3618, 0.2972],\n",
       "         [0.3419, 0.3225, 0.3356],\n",
       "         [0.3824, 0.3123, 0.3053],\n",
       "         [0.3481, 0.3108, 0.3411],\n",
       "         [0.3251, 0.3186, 0.3563],\n",
       "         [0.3256, 0.3265, 0.3479],\n",
       "         [0.3611, 0.3020, 0.3369],\n",
       "         [0.3562, 0.2777, 0.3661],\n",
       "         [0.3661, 0.2889, 0.3450],\n",
       "         [0.3380, 0.3270, 0.3350],\n",
       "         [0.3481, 0.3145, 0.3373],\n",
       "         [0.3235, 0.3310, 0.3455],\n",
       "         [0.3453, 0.3498, 0.3050],\n",
       "         [0.3123, 0.3063, 0.3814],\n",
       "         [0.3407, 0.3137, 0.3457],\n",
       "         [0.3622, 0.2981, 0.3397],\n",
       "         [0.3264, 0.3038, 0.3698],\n",
       "         [0.3217, 0.3195, 0.3588],\n",
       "         [0.3188, 0.3039, 0.3773],\n",
       "         [0.3228, 0.3428, 0.3345],\n",
       "         [0.3240, 0.3182, 0.3578],\n",
       "         [0.3287, 0.2845, 0.3867],\n",
       "         [0.3154, 0.3294, 0.3552],\n",
       "         [0.3421, 0.3221, 0.3358],\n",
       "         [0.3392, 0.3212, 0.3396],\n",
       "         [0.3629, 0.2735, 0.3635],\n",
       "         [0.3691, 0.2970, 0.3339],\n",
       "         [0.3066, 0.3646, 0.3288],\n",
       "         [0.3627, 0.3125, 0.3248],\n",
       "         [0.3325, 0.3206, 0.3469],\n",
       "         [0.4168, 0.2741, 0.3091],\n",
       "         [0.3492, 0.3120, 0.3388],\n",
       "         [0.3264, 0.3270, 0.3466],\n",
       "         [0.3579, 0.2928, 0.3493],\n",
       "         [0.3221, 0.3157, 0.3623],\n",
       "         [0.3644, 0.2786, 0.3570],\n",
       "         [0.3211, 0.3348, 0.3441],\n",
       "         [0.3386, 0.3004, 0.3610],\n",
       "         [0.3195, 0.3253, 0.3552],\n",
       "         [0.3560, 0.3124, 0.3316],\n",
       "         [0.3246, 0.3087, 0.3667],\n",
       "         [0.3761, 0.3022, 0.3217],\n",
       "         [0.3341, 0.3337, 0.3322],\n",
       "         [0.3706, 0.3109, 0.3185],\n",
       "         [0.3716, 0.3041, 0.3242],\n",
       "         [0.2956, 0.3399, 0.3645],\n",
       "         [0.3246, 0.3196, 0.3558],\n",
       "         [0.3665, 0.3234, 0.3101],\n",
       "         [0.3415, 0.3079, 0.3506],\n",
       "         [0.3243, 0.3273, 0.3484],\n",
       "         [0.3448, 0.3082, 0.3469],\n",
       "         [0.3228, 0.3383, 0.3390],\n",
       "         [0.3206, 0.3439, 0.3355],\n",
       "         [0.3501, 0.3013, 0.3486],\n",
       "         [0.3184, 0.3084, 0.3732],\n",
       "         [0.3289, 0.3133, 0.3577],\n",
       "         [0.3327, 0.2890, 0.3783],\n",
       "         [0.3299, 0.3344, 0.3356],\n",
       "         [0.3223, 0.3252, 0.3525],\n",
       "         [0.3094, 0.3145, 0.3762],\n",
       "         [0.3502, 0.3253, 0.3245],\n",
       "         [0.3463, 0.3082, 0.3455],\n",
       "         [0.3386, 0.3005, 0.3609],\n",
       "         [0.3942, 0.2650, 0.3408],\n",
       "         [0.3449, 0.2864, 0.3687],\n",
       "         [0.3259, 0.3156, 0.3585],\n",
       "         [0.3329, 0.3363, 0.3307],\n",
       "         [0.3478, 0.3230, 0.3292],\n",
       "         [0.3736, 0.2944, 0.3320],\n",
       "         [0.3432, 0.3101, 0.3467],\n",
       "         [0.3593, 0.3046, 0.3360],\n",
       "         [0.3485, 0.3220, 0.3295],\n",
       "         [0.3570, 0.2977, 0.3453],\n",
       "         [0.3561, 0.2817, 0.3622],\n",
       "         [0.3331, 0.3078, 0.3591],\n",
       "         [0.3418, 0.3462, 0.3120],\n",
       "         [0.3295, 0.3097, 0.3608],\n",
       "         [0.3383, 0.3200, 0.3417],\n",
       "         [0.3667, 0.2612, 0.3721],\n",
       "         [0.3256, 0.2961, 0.3783],\n",
       "         [0.3403, 0.3316, 0.3281],\n",
       "         [0.3581, 0.3046, 0.3373],\n",
       "         [0.3309, 0.3371, 0.3319],\n",
       "         [0.3456, 0.2907, 0.3637],\n",
       "         [0.3318, 0.3221, 0.3461],\n",
       "         [0.3265, 0.3511, 0.3224],\n",
       "         [0.3303, 0.3383, 0.3314],\n",
       "         [0.3251, 0.3311, 0.3438],\n",
       "         [0.3601, 0.3052, 0.3347],\n",
       "         [0.3721, 0.3184, 0.3095],\n",
       "         [0.3272, 0.3431, 0.3297],\n",
       "         [0.3615, 0.2986, 0.3400],\n",
       "         [0.3510, 0.3065, 0.3426]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.2681, 0.3638, 0.3681],\n",
       "         [0.2749, 0.3583, 0.3668],\n",
       "         [0.2978, 0.3689, 0.3333],\n",
       "         [0.2904, 0.3535, 0.3561],\n",
       "         [0.3054, 0.3428, 0.3518],\n",
       "         [0.2465, 0.3627, 0.3908],\n",
       "         [0.2983, 0.3666, 0.3351],\n",
       "         [0.2861, 0.3763, 0.3377],\n",
       "         [0.2891, 0.3498, 0.3611],\n",
       "         [0.2297, 0.3949, 0.3754],\n",
       "         [0.2710, 0.3614, 0.3676],\n",
       "         [0.2397, 0.3629, 0.3974],\n",
       "         [0.3078, 0.3587, 0.3335],\n",
       "         [0.3404, 0.3416, 0.3180],\n",
       "         [0.3410, 0.3311, 0.3279],\n",
       "         [0.2897, 0.2984, 0.4120],\n",
       "         [0.3151, 0.3605, 0.3245],\n",
       "         [0.3089, 0.3018, 0.3893],\n",
       "         [0.2681, 0.3400, 0.3919],\n",
       "         [0.2467, 0.4006, 0.3527],\n",
       "         [0.3072, 0.3686, 0.3242],\n",
       "         [0.2569, 0.3683, 0.3749],\n",
       "         [0.3048, 0.3279, 0.3672],\n",
       "         [0.2973, 0.3645, 0.3382],\n",
       "         [0.2405, 0.3860, 0.3736],\n",
       "         [0.2933, 0.3522, 0.3545],\n",
       "         [0.3163, 0.3606, 0.3230],\n",
       "         [0.2800, 0.4092, 0.3108],\n",
       "         [0.2577, 0.3766, 0.3657],\n",
       "         [0.2917, 0.3413, 0.3670],\n",
       "         [0.2810, 0.3651, 0.3539],\n",
       "         [0.3215, 0.3255, 0.3530],\n",
       "         [0.2982, 0.3621, 0.3397],\n",
       "         [0.3377, 0.3400, 0.3223],\n",
       "         [0.2660, 0.3560, 0.3780],\n",
       "         [0.3056, 0.3485, 0.3459],\n",
       "         [0.2829, 0.3847, 0.3325],\n",
       "         [0.2290, 0.4083, 0.3627],\n",
       "         [0.3063, 0.3758, 0.3179],\n",
       "         [0.2682, 0.3748, 0.3571],\n",
       "         [0.2788, 0.3868, 0.3344],\n",
       "         [0.3132, 0.3646, 0.3221],\n",
       "         [0.2547, 0.3700, 0.3753],\n",
       "         [0.2798, 0.3809, 0.3394],\n",
       "         [0.2552, 0.3767, 0.3681],\n",
       "         [0.2693, 0.3641, 0.3665],\n",
       "         [0.3162, 0.3418, 0.3420],\n",
       "         [0.2796, 0.3585, 0.3618],\n",
       "         [0.2763, 0.4085, 0.3153],\n",
       "         [0.3640, 0.3426, 0.2934],\n",
       "         [0.2659, 0.3861, 0.3480],\n",
       "         [0.2927, 0.3833, 0.3240],\n",
       "         [0.3006, 0.3457, 0.3537],\n",
       "         [0.2771, 0.3606, 0.3623],\n",
       "         [0.2974, 0.3936, 0.3091],\n",
       "         [0.3103, 0.3510, 0.3387],\n",
       "         [0.2547, 0.3926, 0.3527],\n",
       "         [0.3004, 0.3521, 0.3475],\n",
       "         [0.2825, 0.3684, 0.3492],\n",
       "         [0.2850, 0.3626, 0.3524],\n",
       "         [0.3289, 0.3380, 0.3330],\n",
       "         [0.2439, 0.3776, 0.3785],\n",
       "         [0.2717, 0.3625, 0.3658],\n",
       "         [0.2857, 0.3693, 0.3450],\n",
       "         [0.3038, 0.3471, 0.3491],\n",
       "         [0.2633, 0.3677, 0.3690],\n",
       "         [0.2697, 0.3397, 0.3906],\n",
       "         [0.3012, 0.3425, 0.3563],\n",
       "         [0.2999, 0.3764, 0.3237],\n",
       "         [0.2694, 0.3644, 0.3663],\n",
       "         [0.2958, 0.3765, 0.3277],\n",
       "         [0.2924, 0.3585, 0.3491],\n",
       "         [0.2490, 0.3721, 0.3789],\n",
       "         [0.3227, 0.3460, 0.3313],\n",
       "         [0.2917, 0.3962, 0.3121],\n",
       "         [0.2820, 0.3573, 0.3608],\n",
       "         [0.2975, 0.3487, 0.3539],\n",
       "         [0.3128, 0.3455, 0.3418],\n",
       "         [0.2649, 0.3711, 0.3640],\n",
       "         [0.2762, 0.3920, 0.3318],\n",
       "         [0.2610, 0.3955, 0.3435],\n",
       "         [0.2561, 0.3676, 0.3762],\n",
       "         [0.3004, 0.3585, 0.3411],\n",
       "         [0.2812, 0.3617, 0.3571],\n",
       "         [0.2777, 0.3625, 0.3598],\n",
       "         [0.3021, 0.3482, 0.3497],\n",
       "         [0.2809, 0.3678, 0.3513],\n",
       "         [0.2087, 0.3555, 0.4358],\n",
       "         [0.3059, 0.3774, 0.3166],\n",
       "         [0.3073, 0.3497, 0.3430],\n",
       "         [0.2856, 0.3377, 0.3767],\n",
       "         [0.2936, 0.3631, 0.3433],\n",
       "         [0.2893, 0.3741, 0.3365],\n",
       "         [0.2761, 0.3686, 0.3553],\n",
       "         [0.2932, 0.3483, 0.3586],\n",
       "         [0.2984, 0.3545, 0.3471],\n",
       "         [0.2877, 0.3775, 0.3348],\n",
       "         [0.2933, 0.3456, 0.3610],\n",
       "         [0.2522, 0.3993, 0.3484],\n",
       "         [0.2919, 0.3758, 0.3323],\n",
       "         [0.2507, 0.3840, 0.3652],\n",
       "         [0.3067, 0.3259, 0.3674]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.3343, 0.3408, 0.3249],\n",
       "         [0.3096, 0.3466, 0.3438],\n",
       "         [0.3211, 0.3441, 0.3348],\n",
       "         [0.3358, 0.3390, 0.3252],\n",
       "         [0.3276, 0.3513, 0.3211],\n",
       "         [0.2737, 0.3841, 0.3422],\n",
       "         [0.3269, 0.3156, 0.3575],\n",
       "         [0.3366, 0.3303, 0.3331],\n",
       "         [0.3141, 0.3587, 0.3272],\n",
       "         [0.2874, 0.3429, 0.3697],\n",
       "         [0.3083, 0.3582, 0.3335],\n",
       "         [0.3064, 0.3592, 0.3344],\n",
       "         [0.3085, 0.3550, 0.3366],\n",
       "         [0.3287, 0.3311, 0.3402],\n",
       "         [0.3396, 0.3782, 0.2822],\n",
       "         [0.2996, 0.3869, 0.3135],\n",
       "         [0.2604, 0.3791, 0.3606],\n",
       "         [0.3229, 0.3483, 0.3288],\n",
       "         [0.2949, 0.3511, 0.3540],\n",
       "         [0.2959, 0.3755, 0.3286],\n",
       "         [0.3198, 0.3440, 0.3363],\n",
       "         [0.3288, 0.3762, 0.2950],\n",
       "         [0.3243, 0.3630, 0.3127],\n",
       "         [0.3394, 0.3349, 0.3257],\n",
       "         [0.3203, 0.3370, 0.3427],\n",
       "         [0.3028, 0.3661, 0.3310],\n",
       "         [0.3065, 0.3679, 0.3255],\n",
       "         [0.3408, 0.3364, 0.3228],\n",
       "         [0.3553, 0.3304, 0.3143],\n",
       "         [0.2950, 0.3612, 0.3438],\n",
       "         [0.2867, 0.3780, 0.3354],\n",
       "         [0.3432, 0.3417, 0.3151],\n",
       "         [0.3300, 0.3549, 0.3151],\n",
       "         [0.3545, 0.3302, 0.3153],\n",
       "         [0.3154, 0.3465, 0.3381],\n",
       "         [0.3239, 0.3447, 0.3314],\n",
       "         [0.3408, 0.3078, 0.3515],\n",
       "         [0.3133, 0.3672, 0.3195],\n",
       "         [0.2990, 0.3414, 0.3596],\n",
       "         [0.2683, 0.3780, 0.3537],\n",
       "         [0.2996, 0.4041, 0.2963],\n",
       "         [0.3167, 0.3684, 0.3149],\n",
       "         [0.2775, 0.3768, 0.3457],\n",
       "         [0.3084, 0.3403, 0.3513],\n",
       "         [0.2925, 0.3651, 0.3424],\n",
       "         [0.3141, 0.3618, 0.3241],\n",
       "         [0.2871, 0.3612, 0.3517],\n",
       "         [0.3114, 0.3545, 0.3341],\n",
       "         [0.3044, 0.3938, 0.3017],\n",
       "         [0.3401, 0.3620, 0.2979],\n",
       "         [0.2850, 0.3645, 0.3505],\n",
       "         [0.3240, 0.3795, 0.2965],\n",
       "         [0.2910, 0.3622, 0.3468],\n",
       "         [0.3181, 0.3520, 0.3299],\n",
       "         [0.2967, 0.3643, 0.3390],\n",
       "         [0.3325, 0.3628, 0.3046],\n",
       "         [0.3252, 0.3627, 0.3121],\n",
       "         [0.3056, 0.3318, 0.3626],\n",
       "         [0.3022, 0.3321, 0.3657],\n",
       "         [0.2999, 0.3406, 0.3595],\n",
       "         [0.3087, 0.3419, 0.3493],\n",
       "         [0.3156, 0.3583, 0.3261],\n",
       "         [0.2957, 0.3516, 0.3526],\n",
       "         [0.2816, 0.3726, 0.3457],\n",
       "         [0.3183, 0.3473, 0.3345],\n",
       "         [0.2352, 0.4510, 0.3137],\n",
       "         [0.2700, 0.4016, 0.3284],\n",
       "         [0.3121, 0.3437, 0.3441],\n",
       "         [0.3040, 0.3927, 0.3034],\n",
       "         [0.3025, 0.3496, 0.3479],\n",
       "         [0.3273, 0.3408, 0.3319],\n",
       "         [0.3035, 0.3552, 0.3413],\n",
       "         [0.3106, 0.3381, 0.3513],\n",
       "         [0.3154, 0.3433, 0.3412],\n",
       "         [0.3071, 0.3605, 0.3324],\n",
       "         [0.3260, 0.3514, 0.3226],\n",
       "         [0.3173, 0.3436, 0.3391],\n",
       "         [0.3213, 0.3355, 0.3432],\n",
       "         [0.3123, 0.3502, 0.3375],\n",
       "         [0.3126, 0.3707, 0.3167],\n",
       "         [0.3295, 0.3637, 0.3068],\n",
       "         [0.3234, 0.3473, 0.3292],\n",
       "         [0.3000, 0.3622, 0.3378],\n",
       "         [0.2956, 0.3973, 0.3071],\n",
       "         [0.3045, 0.3650, 0.3305],\n",
       "         [0.3165, 0.3322, 0.3513],\n",
       "         [0.3078, 0.3627, 0.3295],\n",
       "         [0.2654, 0.3839, 0.3506],\n",
       "         [0.3198, 0.3545, 0.3256],\n",
       "         [0.3172, 0.3513, 0.3316],\n",
       "         [0.3070, 0.3308, 0.3622],\n",
       "         [0.3162, 0.3460, 0.3379],\n",
       "         [0.3142, 0.3713, 0.3144],\n",
       "         [0.3161, 0.3331, 0.3509],\n",
       "         [0.3114, 0.3582, 0.3304],\n",
       "         [0.3124, 0.3441, 0.3436],\n",
       "         [0.3665, 0.3170, 0.3165],\n",
       "         [0.3029, 0.3619, 0.3352],\n",
       "         [0.3397, 0.3449, 0.3154],\n",
       "         [0.3417, 0.3644, 0.2939],\n",
       "         [0.3065, 0.3479, 0.3456],\n",
       "         [0.3063, 0.3831, 0.3106]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.3861, 0.3146, 0.2993],\n",
       "         [0.3256, 0.3306, 0.3438],\n",
       "         [0.3368, 0.3335, 0.3297],\n",
       "         [0.2925, 0.3925, 0.3150],\n",
       "         [0.3197, 0.3832, 0.2971],\n",
       "         [0.3502, 0.3289, 0.3208],\n",
       "         [0.3453, 0.3655, 0.2891],\n",
       "         [0.3034, 0.4008, 0.2958],\n",
       "         [0.3135, 0.3548, 0.3316],\n",
       "         [0.3608, 0.3454, 0.2938],\n",
       "         [0.3028, 0.3722, 0.3249],\n",
       "         [0.2326, 0.4769, 0.2905],\n",
       "         [0.3256, 0.3548, 0.3196],\n",
       "         [0.3202, 0.3536, 0.3262],\n",
       "         [0.2913, 0.4067, 0.3020],\n",
       "         [0.2655, 0.4007, 0.3337],\n",
       "         [0.3669, 0.3606, 0.2725],\n",
       "         [0.3060, 0.4282, 0.2659],\n",
       "         [0.3291, 0.3990, 0.2720],\n",
       "         [0.3811, 0.3335, 0.2855],\n",
       "         [0.3013, 0.3675, 0.3312],\n",
       "         [0.3038, 0.3283, 0.3679],\n",
       "         [0.3022, 0.3554, 0.3424],\n",
       "         [0.3144, 0.3725, 0.3130],\n",
       "         [0.3631, 0.3696, 0.2673],\n",
       "         [0.2843, 0.3814, 0.3343],\n",
       "         [0.3150, 0.3587, 0.3262],\n",
       "         [0.3696, 0.3602, 0.2702],\n",
       "         [0.3218, 0.3413, 0.3369],\n",
       "         [0.3246, 0.3601, 0.3153],\n",
       "         [0.3379, 0.3402, 0.3219],\n",
       "         [0.2850, 0.4031, 0.3119],\n",
       "         [0.3086, 0.4039, 0.2875],\n",
       "         [0.3086, 0.3691, 0.3224],\n",
       "         [0.3415, 0.3608, 0.2977],\n",
       "         [0.3022, 0.3665, 0.3312],\n",
       "         [0.3613, 0.3110, 0.3278],\n",
       "         [0.3314, 0.3654, 0.3032],\n",
       "         [0.3372, 0.3460, 0.3167],\n",
       "         [0.3584, 0.3867, 0.2549],\n",
       "         [0.3153, 0.3857, 0.2991],\n",
       "         [0.3202, 0.3882, 0.2915],\n",
       "         [0.3162, 0.3975, 0.2863],\n",
       "         [0.3351, 0.3571, 0.3077],\n",
       "         [0.3378, 0.3366, 0.3256],\n",
       "         [0.3379, 0.3302, 0.3319],\n",
       "         [0.3477, 0.3642, 0.2881],\n",
       "         [0.3132, 0.3790, 0.3077],\n",
       "         [0.3289, 0.3730, 0.2980],\n",
       "         [0.3333, 0.3798, 0.2869],\n",
       "         [0.4240, 0.3103, 0.2657],\n",
       "         [0.3405, 0.3563, 0.3031],\n",
       "         [0.3373, 0.3708, 0.2919],\n",
       "         [0.3358, 0.3748, 0.2894],\n",
       "         [0.3549, 0.3339, 0.3111],\n",
       "         [0.3010, 0.3695, 0.3296],\n",
       "         [0.3136, 0.3790, 0.3074],\n",
       "         [0.3338, 0.3420, 0.3242],\n",
       "         [0.3266, 0.3712, 0.3023],\n",
       "         [0.3480, 0.3326, 0.3194],\n",
       "         [0.3677, 0.3353, 0.2970],\n",
       "         [0.3132, 0.3515, 0.3353],\n",
       "         [0.2908, 0.3722, 0.3370],\n",
       "         [0.3231, 0.3755, 0.3014],\n",
       "         [0.3387, 0.3289, 0.3324],\n",
       "         [0.2981, 0.4385, 0.2633],\n",
       "         [0.2953, 0.3467, 0.3580],\n",
       "         [0.3009, 0.3734, 0.3257],\n",
       "         [0.3488, 0.3580, 0.2932],\n",
       "         [0.3461, 0.3750, 0.2790],\n",
       "         [0.3140, 0.3533, 0.3327],\n",
       "         [0.3113, 0.3508, 0.3378],\n",
       "         [0.3551, 0.3899, 0.2550],\n",
       "         [0.3351, 0.3632, 0.3018],\n",
       "         [0.3830, 0.3340, 0.2830],\n",
       "         [0.3577, 0.3152, 0.3270],\n",
       "         [0.3113, 0.3636, 0.3251],\n",
       "         [0.3267, 0.3909, 0.2824],\n",
       "         [0.3461, 0.3495, 0.3044],\n",
       "         [0.3573, 0.3242, 0.3185],\n",
       "         [0.3320, 0.3550, 0.3130],\n",
       "         [0.3372, 0.3340, 0.3288],\n",
       "         [0.3572, 0.3546, 0.2881],\n",
       "         [0.3369, 0.3725, 0.2906],\n",
       "         [0.3000, 0.3867, 0.3133],\n",
       "         [0.3217, 0.3500, 0.3283],\n",
       "         [0.3451, 0.3386, 0.3163],\n",
       "         [0.2395, 0.4610, 0.2995],\n",
       "         [0.3356, 0.3316, 0.3329],\n",
       "         [0.3102, 0.3716, 0.3182],\n",
       "         [0.3291, 0.3541, 0.3168],\n",
       "         [0.3534, 0.3386, 0.3080],\n",
       "         [0.3768, 0.3338, 0.2894],\n",
       "         [0.3090, 0.3563, 0.3348],\n",
       "         [0.3285, 0.3502, 0.3213],\n",
       "         [0.3069, 0.3531, 0.3400],\n",
       "         [0.3372, 0.3416, 0.3213],\n",
       "         [0.3105, 0.3740, 0.3155],\n",
       "         [0.3795, 0.3745, 0.2459],\n",
       "         [0.3597, 0.3412, 0.2991],\n",
       "         [0.3596, 0.3469, 0.2934],\n",
       "         [0.3158, 0.3872, 0.2969]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.4004, 0.2839, 0.3157],\n",
       "         [0.3489, 0.3099, 0.3413],\n",
       "         [0.3093, 0.3248, 0.3658],\n",
       "         [0.3517, 0.3177, 0.3306],\n",
       "         [0.3566, 0.3213, 0.3221],\n",
       "         [0.3420, 0.3289, 0.3291],\n",
       "         [0.3420, 0.3306, 0.3274],\n",
       "         [0.4026, 0.3150, 0.2824],\n",
       "         [0.3454, 0.3353, 0.3193],\n",
       "         [0.3679, 0.3347, 0.2975],\n",
       "         [0.3343, 0.3293, 0.3364],\n",
       "         [0.3533, 0.3626, 0.2840],\n",
       "         [0.3228, 0.3364, 0.3408],\n",
       "         [0.3330, 0.3261, 0.3409],\n",
       "         [0.3079, 0.4365, 0.2556],\n",
       "         [0.3660, 0.3432, 0.2908],\n",
       "         [0.3702, 0.2480, 0.3819],\n",
       "         [0.2744, 0.4129, 0.3127],\n",
       "         [0.3335, 0.3491, 0.3174],\n",
       "         [0.3583, 0.3141, 0.3275],\n",
       "         [0.3245, 0.3522, 0.3233],\n",
       "         [0.3386, 0.3795, 0.2819],\n",
       "         [0.3336, 0.3510, 0.3154],\n",
       "         [0.3482, 0.2882, 0.3636],\n",
       "         [0.3186, 0.2990, 0.3824],\n",
       "         [0.3515, 0.3355, 0.3130],\n",
       "         [0.3523, 0.3213, 0.3264],\n",
       "         [0.3354, 0.3095, 0.3550],\n",
       "         [0.3649, 0.3061, 0.3289],\n",
       "         [0.3781, 0.3080, 0.3139],\n",
       "         [0.3499, 0.3126, 0.3375],\n",
       "         [0.3289, 0.3351, 0.3360],\n",
       "         [0.3430, 0.3626, 0.2944],\n",
       "         [0.3227, 0.3420, 0.3352],\n",
       "         [0.3486, 0.3733, 0.2781],\n",
       "         [0.3469, 0.3367, 0.3165],\n",
       "         [0.3489, 0.3071, 0.3440],\n",
       "         [0.3144, 0.3528, 0.3328],\n",
       "         [0.3145, 0.3560, 0.3295],\n",
       "         [0.3394, 0.3774, 0.2832],\n",
       "         [0.3719, 0.3253, 0.3028],\n",
       "         [0.3444, 0.3019, 0.3536],\n",
       "         [0.3676, 0.3474, 0.2850],\n",
       "         [0.3384, 0.3258, 0.3358],\n",
       "         [0.3667, 0.3037, 0.3296],\n",
       "         [0.3390, 0.3377, 0.3232],\n",
       "         [0.3374, 0.3331, 0.3294],\n",
       "         [0.3580, 0.3211, 0.3209],\n",
       "         [0.3727, 0.3042, 0.3231],\n",
       "         [0.3242, 0.3352, 0.3406],\n",
       "         [0.3466, 0.2972, 0.3562],\n",
       "         [0.3699, 0.3211, 0.3090],\n",
       "         [0.3482, 0.3120, 0.3398],\n",
       "         [0.3639, 0.3323, 0.3038],\n",
       "         [0.3118, 0.3441, 0.3442],\n",
       "         [0.3364, 0.3439, 0.3197],\n",
       "         [0.3599, 0.3801, 0.2601],\n",
       "         [0.3376, 0.3183, 0.3440],\n",
       "         [0.3559, 0.3023, 0.3418],\n",
       "         [0.3434, 0.3191, 0.3375],\n",
       "         [0.3502, 0.3194, 0.3304],\n",
       "         [0.3434, 0.3276, 0.3290],\n",
       "         [0.3465, 0.3202, 0.3333],\n",
       "         [0.3249, 0.3305, 0.3446],\n",
       "         [0.3282, 0.3394, 0.3324],\n",
       "         [0.4322, 0.2462, 0.3216],\n",
       "         [0.3875, 0.3228, 0.2897],\n",
       "         [0.3383, 0.3271, 0.3346],\n",
       "         [0.3235, 0.3669, 0.3096],\n",
       "         [0.3643, 0.3481, 0.2876],\n",
       "         [0.3567, 0.3193, 0.3240],\n",
       "         [0.3405, 0.3440, 0.3155],\n",
       "         [0.3900, 0.3352, 0.2748],\n",
       "         [0.3415, 0.3176, 0.3410],\n",
       "         [0.3005, 0.3090, 0.3905],\n",
       "         [0.3629, 0.3412, 0.2959],\n",
       "         [0.3501, 0.3144, 0.3354],\n",
       "         [0.3261, 0.3244, 0.3495],\n",
       "         [0.3224, 0.3352, 0.3424],\n",
       "         [0.3359, 0.3520, 0.3121],\n",
       "         [0.3701, 0.2870, 0.3429],\n",
       "         [0.3342, 0.3490, 0.3168],\n",
       "         [0.3472, 0.3305, 0.3223],\n",
       "         [0.3100, 0.3260, 0.3640],\n",
       "         [0.3525, 0.3660, 0.2815],\n",
       "         [0.3567, 0.3276, 0.3157],\n",
       "         [0.4089, 0.2850, 0.3061],\n",
       "         [0.3508, 0.3636, 0.2857],\n",
       "         [0.3510, 0.3049, 0.3440],\n",
       "         [0.3531, 0.3296, 0.3173],\n",
       "         [0.3771, 0.2886, 0.3343],\n",
       "         [0.3458, 0.3323, 0.3219],\n",
       "         [0.3276, 0.3457, 0.3266],\n",
       "         [0.3676, 0.2994, 0.3330],\n",
       "         [0.3492, 0.3301, 0.3208],\n",
       "         [0.3430, 0.3139, 0.3431],\n",
       "         [0.3382, 0.3353, 0.3264],\n",
       "         [0.3560, 0.3262, 0.3178],\n",
       "         [0.3423, 0.3219, 0.3358],\n",
       "         [0.3286, 0.3954, 0.2761],\n",
       "         [0.3494, 0.3275, 0.3231],\n",
       "         [0.3433, 0.3536, 0.3031]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.3230, 0.3517, 0.3254],\n",
       "         [0.3143, 0.3668, 0.3189],\n",
       "         [0.3324, 0.3365, 0.3311],\n",
       "         [0.3387, 0.3205, 0.3409],\n",
       "         [0.3373, 0.3162, 0.3465],\n",
       "         [0.3055, 0.3470, 0.3475],\n",
       "         [0.3298, 0.3729, 0.2972],\n",
       "         [0.3232, 0.3815, 0.2954],\n",
       "         [0.3056, 0.3674, 0.3269],\n",
       "         [0.2732, 0.4110, 0.3159],\n",
       "         [0.3299, 0.3597, 0.3104],\n",
       "         [0.3100, 0.3658, 0.3242],\n",
       "         [0.3263, 0.3409, 0.3327],\n",
       "         [0.3184, 0.3581, 0.3235],\n",
       "         [0.3046, 0.3621, 0.3333],\n",
       "         [0.3534, 0.3427, 0.3039],\n",
       "         [0.3001, 0.3818, 0.3181],\n",
       "         [0.3617, 0.2559, 0.3825],\n",
       "         [0.3448, 0.3158, 0.3394],\n",
       "         [0.3035, 0.3729, 0.3236],\n",
       "         [0.3218, 0.3259, 0.3523],\n",
       "         [0.3043, 0.3594, 0.3363],\n",
       "         [0.3101, 0.3755, 0.3144],\n",
       "         [0.3063, 0.3418, 0.3519],\n",
       "         [0.3574, 0.3260, 0.3166],\n",
       "         [0.3233, 0.3445, 0.3322],\n",
       "         [0.3396, 0.3212, 0.3392],\n",
       "         [0.3342, 0.3338, 0.3319],\n",
       "         [0.3452, 0.3480, 0.3068],\n",
       "         [0.2829, 0.3930, 0.3241],\n",
       "         [0.3400, 0.2701, 0.3899],\n",
       "         [0.3621, 0.3043, 0.3336],\n",
       "         [0.3334, 0.3297, 0.3369],\n",
       "         [0.3261, 0.3379, 0.3360],\n",
       "         [0.2957, 0.3370, 0.3673],\n",
       "         [0.3092, 0.3406, 0.3502],\n",
       "         [0.3132, 0.3300, 0.3568],\n",
       "         [0.3629, 0.3279, 0.3092],\n",
       "         [0.3267, 0.3422, 0.3311],\n",
       "         [0.2853, 0.4311, 0.2835],\n",
       "         [0.2904, 0.3496, 0.3600],\n",
       "         [0.3324, 0.3436, 0.3240],\n",
       "         [0.2995, 0.3354, 0.3651],\n",
       "         [0.3282, 0.3519, 0.3199],\n",
       "         [0.2941, 0.3604, 0.3455],\n",
       "         [0.3101, 0.3670, 0.3229],\n",
       "         [0.3350, 0.3551, 0.3099],\n",
       "         [0.3284, 0.3638, 0.3077],\n",
       "         [0.3414, 0.3120, 0.3465],\n",
       "         [0.3258, 0.3703, 0.3039],\n",
       "         [0.3305, 0.3785, 0.2910],\n",
       "         [0.3111, 0.3100, 0.3789],\n",
       "         [0.3224, 0.3842, 0.2934],\n",
       "         [0.3423, 0.3433, 0.3144],\n",
       "         [0.2927, 0.3427, 0.3646],\n",
       "         [0.3332, 0.3253, 0.3415],\n",
       "         [0.2982, 0.3161, 0.3857],\n",
       "         [0.3245, 0.3462, 0.3293],\n",
       "         [0.3294, 0.3593, 0.3113],\n",
       "         [0.3173, 0.3586, 0.3241],\n",
       "         [0.3196, 0.3463, 0.3341],\n",
       "         [0.3105, 0.3176, 0.3719],\n",
       "         [0.3190, 0.3531, 0.3279],\n",
       "         [0.3149, 0.3622, 0.3230],\n",
       "         [0.3430, 0.3423, 0.3147],\n",
       "         [0.2719, 0.4298, 0.2983],\n",
       "         [0.3051, 0.3853, 0.3096],\n",
       "         [0.3259, 0.3408, 0.3333],\n",
       "         [0.3130, 0.3448, 0.3423],\n",
       "         [0.3067, 0.3508, 0.3424],\n",
       "         [0.3194, 0.3747, 0.3060],\n",
       "         [0.3300, 0.3582, 0.3118],\n",
       "         [0.2982, 0.3869, 0.3150],\n",
       "         [0.3686, 0.3526, 0.2788],\n",
       "         [0.3585, 0.3221, 0.3194],\n",
       "         [0.3252, 0.3334, 0.3414],\n",
       "         [0.3259, 0.3583, 0.3158],\n",
       "         [0.3764, 0.3061, 0.3174],\n",
       "         [0.3342, 0.3232, 0.3425],\n",
       "         [0.2768, 0.3744, 0.3488],\n",
       "         [0.3350, 0.3040, 0.3610],\n",
       "         [0.3249, 0.3733, 0.3017],\n",
       "         [0.3134, 0.3618, 0.3248],\n",
       "         [0.3069, 0.3467, 0.3464],\n",
       "         [0.3341, 0.3575, 0.3084],\n",
       "         [0.3092, 0.3622, 0.3287],\n",
       "         [0.3093, 0.3685, 0.3222],\n",
       "         [0.3639, 0.3879, 0.2482],\n",
       "         [0.3487, 0.3478, 0.3035],\n",
       "         [0.3612, 0.3510, 0.2878],\n",
       "         [0.3096, 0.3875, 0.3029],\n",
       "         [0.3596, 0.3580, 0.2824],\n",
       "         [0.2979, 0.3638, 0.3383],\n",
       "         [0.3186, 0.3625, 0.3189],\n",
       "         [0.3393, 0.3724, 0.2883],\n",
       "         [0.3303, 0.3362, 0.3335],\n",
       "         [0.3356, 0.3219, 0.3425],\n",
       "         [0.3218, 0.3817, 0.2965],\n",
       "         [0.3025, 0.3453, 0.3522],\n",
       "         [0.2950, 0.3486, 0.3564],\n",
       "         [0.3075, 0.3555, 0.3370],\n",
       "         [0.2965, 0.3617, 0.3418]], grad_fn=<SoftmaxBackward0>)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain = df_to_torch(xctrain)\n",
    "ytrain = df_to_torch(yctrain)\n",
    "xtest = df_to_torch(xctest)\n",
    "ytest = df_to_torch(yctest)\n",
    "imputer.fit_normalizer(xtrain)\n",
    "imputer(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f9bf008d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.2033, grad_fn=<AddBackward0>)\n",
      "tensor(8.1640, grad_fn=<AddBackward0>)\n",
      "tensor(8.0954, grad_fn=<AddBackward0>)\n",
      "tensor(8.1542, grad_fn=<AddBackward0>)\n",
      "tensor(8.1836, grad_fn=<AddBackward0>)\n",
      "tensor(8.1346, grad_fn=<AddBackward0>)\n",
      "tensor(8.1836, grad_fn=<AddBackward0>)\n",
      "tensor(8.1444, grad_fn=<AddBackward0>)\n",
      "tensor(8.0660, grad_fn=<AddBackward0>)\n",
      "tensor(8.1346, grad_fn=<AddBackward0>)\n",
      "tensor(8.1444, grad_fn=<AddBackward0>)\n",
      "tensor(8.1640, grad_fn=<AddBackward0>)\n",
      "tensor(8.1934, grad_fn=<AddBackward0>)\n",
      "tensor(8.1248, grad_fn=<AddBackward0>)\n",
      "tensor(8.1640, grad_fn=<AddBackward0>)\n",
      "tensor(8.0954, grad_fn=<AddBackward0>)\n",
      "tensor(8.1444, grad_fn=<AddBackward0>)\n",
      "tensor(8.1640, grad_fn=<AddBackward0>)\n",
      "tensor(8.2621, grad_fn=<AddBackward0>)\n",
      "tensor(8.2033, grad_fn=<AddBackward0>)\n",
      "tensor(8.0660, grad_fn=<AddBackward0>)\n",
      "tensor(8.1640, grad_fn=<AddBackward0>)\n",
      "tensor(8.2229, grad_fn=<AddBackward0>)\n",
      "tensor(8.1444, grad_fn=<AddBackward0>)\n",
      "tensor(8.0954, grad_fn=<AddBackward0>)\n",
      "tensor(8.2033, grad_fn=<AddBackward0>)\n",
      "tensor(8.0072, grad_fn=<AddBackward0>)\n",
      "tensor(8.1346, grad_fn=<AddBackward0>)\n",
      "tensor(8.2229, grad_fn=<AddBackward0>)\n",
      "tensor(8.1836, grad_fn=<AddBackward0>)\n",
      "tensor(8.1052, grad_fn=<AddBackward0>)\n",
      "tensor(8.0758, grad_fn=<AddBackward0>)\n",
      "tensor(8.1444, grad_fn=<AddBackward0>)\n",
      "tensor(8.2033, grad_fn=<AddBackward0>)\n",
      "tensor(8.0660, grad_fn=<AddBackward0>)\n",
      "tensor(8.1346, grad_fn=<AddBackward0>)\n",
      "tensor(8.0660, grad_fn=<AddBackward0>)\n",
      "tensor(8.1738, grad_fn=<AddBackward0>)\n",
      "tensor(8.1150, grad_fn=<AddBackward0>)\n",
      "tensor(8.1346, grad_fn=<AddBackward0>)\n",
      "tensor(8.0758, grad_fn=<AddBackward0>)\n",
      "tensor(8.1836, grad_fn=<AddBackward0>)\n",
      "tensor(8.2131, grad_fn=<AddBackward0>)\n",
      "tensor(8.0954, grad_fn=<AddBackward0>)\n",
      "tensor(8.1738, grad_fn=<AddBackward0>)\n",
      "tensor(8.1640, grad_fn=<AddBackward0>)\n",
      "tensor(8.1640, grad_fn=<AddBackward0>)\n",
      "tensor(8.1346, grad_fn=<AddBackward0>)\n",
      "tensor(8.1444, grad_fn=<AddBackward0>)\n",
      "tensor(8.1640, grad_fn=<AddBackward0>)\n",
      "tensor(8.1738, grad_fn=<AddBackward0>)\n",
      "tensor(8.1934, grad_fn=<AddBackward0>)\n",
      "tensor(8.0758, grad_fn=<AddBackward0>)\n",
      "tensor(8.2719, grad_fn=<AddBackward0>)\n",
      "tensor(8.0660, grad_fn=<AddBackward0>)\n",
      "tensor(8.1934, grad_fn=<AddBackward0>)\n",
      "tensor(8.1052, grad_fn=<AddBackward0>)\n",
      "tensor(8.1738, grad_fn=<AddBackward0>)\n",
      "tensor(8.1444, grad_fn=<AddBackward0>)\n",
      "tensor(8.1444, grad_fn=<AddBackward0>)\n",
      "tensor(8.0758, grad_fn=<AddBackward0>)\n",
      "tensor(8.1738, grad_fn=<AddBackward0>)\n",
      "tensor(8.1542, grad_fn=<AddBackward0>)\n",
      "tensor(8.1738, grad_fn=<AddBackward0>)\n",
      "tensor(8.1444, grad_fn=<AddBackward0>)\n",
      "tensor(8.1346, grad_fn=<AddBackward0>)\n",
      "tensor(8.1934, grad_fn=<AddBackward0>)\n",
      "tensor(8.0170, grad_fn=<AddBackward0>)\n",
      "tensor(8.1934, grad_fn=<AddBackward0>)\n",
      "tensor(8.1150, grad_fn=<AddBackward0>)\n",
      "tensor(8.1738, grad_fn=<AddBackward0>)\n",
      "tensor(8.1444, grad_fn=<AddBackward0>)\n",
      "tensor(8.2229, grad_fn=<AddBackward0>)\n",
      "tensor(8.1444, grad_fn=<AddBackward0>)\n",
      "tensor(8.1150, grad_fn=<AddBackward0>)\n",
      "tensor(8.2131, grad_fn=<AddBackward0>)\n",
      "tensor(8.1150, grad_fn=<AddBackward0>)\n",
      "tensor(8.2229, grad_fn=<AddBackward0>)\n",
      "tensor(8.2033, grad_fn=<AddBackward0>)\n",
      "tensor(8.0856, grad_fn=<AddBackward0>)\n",
      "tensor(8.1150, grad_fn=<AddBackward0>)\n",
      "tensor(8.0660, grad_fn=<AddBackward0>)\n",
      "tensor(8.1150, grad_fn=<AddBackward0>)\n",
      "tensor(8.2719, grad_fn=<AddBackward0>)\n",
      "tensor(8.2817, grad_fn=<AddBackward0>)\n",
      "tensor(8.1346, grad_fn=<AddBackward0>)\n",
      "tensor(8.1542, grad_fn=<AddBackward0>)\n",
      "tensor(8.1150, grad_fn=<AddBackward0>)\n",
      "tensor(8.1346, grad_fn=<AddBackward0>)\n",
      "tensor(8.1640, grad_fn=<AddBackward0>)\n",
      "tensor(8.1738, grad_fn=<AddBackward0>)\n",
      "tensor(8.1836, grad_fn=<AddBackward0>)\n",
      "tensor(8.1346, grad_fn=<AddBackward0>)\n",
      "tensor(8.1934, grad_fn=<AddBackward0>)\n",
      "tensor(8.1346, grad_fn=<AddBackward0>)\n",
      "tensor(8.0954, grad_fn=<AddBackward0>)\n",
      "tensor(8.1248, grad_fn=<AddBackward0>)\n",
      "tensor(8.1836, grad_fn=<AddBackward0>)\n",
      "tensor(8.1934, grad_fn=<AddBackward0>)\n",
      "tensor(8.0660, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def cluster_loss(ytrue,ypred,weights=None):\n",
    "    if weights is None:\n",
    "        weights = [1 for i in range(ytrue.shape[1])]\n",
    "    loss = 0\n",
    "    nloss = torch.nn.BCELoss()\n",
    "    for i in range(len(weights)):\n",
    "        iloss = nloss(ypred[:,i],ytrue[:,i])*weights[i]\n",
    "        loss += iloss\n",
    "    return loss\n",
    "\n",
    "epochs = 100\n",
    "best_val_loss = 10000\n",
    "best_loss_metrics = {}\n",
    "patience = 10\n",
    "save_file = '../data/models/clusterModel.tar'\n",
    "optimizer = torch.optim.Adam(imputer.parameters(),lr=.01)\n",
    "lossF = torch.nn.CrossEntropyLoss()\n",
    "for epoch in range(epochs):\n",
    "    ypreds = imputer(xtrain)\n",
    "    losses = 0\n",
    "    for i,ypred in enumerate(ypreds):\n",
    "        losses+= lossF(ypred,ytrain[:,i].long())\n",
    "    losses.backward()\n",
    "    optimizer.step()\n",
    "    if loss < "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "122ee514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([205.,  44.,   7.]) 389\n",
      "tensor([69., 18.,  6.]) 147\n",
      "torch.Size([3, 536, 85])\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 0 _____\n",
      "val reward 1.5337661504745483\n",
      "imitation reward 2.2648766040802\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.7510436177253723, 0.004403852391988039, 0.0016084234230220318]\n",
      "[{'decision': 0, 'optimal_auc': 0.9234485321441843, 'imitation_auc': 0.5333969465648856, 'optimal_acc': 0.5714285714285714, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.954349698535745, 'imitation_auc': 0.647554347826087, 'optimal_acc': 0.8775510204081632, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.8026004728132388, 'imitation_auc': 0.6993006993006993, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 1 _____\n",
      "val reward 1.6847261190414429\n",
      "imitation reward 1.781301498413086\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.10992904007434845, 0.011692939326167107, 0.0004267352051101625]\n",
      "[{'decision': 0, 'optimal_auc': 0.9026384243775548, 'imitation_auc': 0.48854961832061067, 'optimal_acc': 0.5306122448979592, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.9328165374677002, 'imitation_auc': 0.6445652173913043, 'optimal_acc': 0.8775510204081632, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.4940898345153665, 'imitation_auc': 0.7113795295613478, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 2 _____\n",
      "val reward 1.1242891550064087\n",
      "imitation reward 1.2755963802337646\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.6826441884040833, 0.034179966896772385, 0.0007347496575675905]\n",
      "[{'decision': 0, 'optimal_auc': 0.9321813452248235, 'imitation_auc': 0.44179389312977096, 'optimal_acc': 0.7619047619047619, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.9595176571920758, 'imitation_auc': 0.647554347826087, 'optimal_acc': 0.8775510204081632, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.4787234042553191, 'imitation_auc': 0.6795931341385888, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.6394557823129252}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 3 _____\n",
      "val reward 0.9645942449569702\n",
      "imitation reward 1.0888067483901978\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.6561893820762634, 0.1572721004486084, 0.0012432544026523829]\n",
      "[{'decision': 0, 'optimal_auc': 0.9305091044221478, 'imitation_auc': 0.5262404580152672, 'optimal_acc': 0.7687074829931972, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.9638242894056848, 'imitation_auc': 0.660054347826087, 'optimal_acc': 0.9319727891156463, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.5780141843971631, 'imitation_auc': 0.7596948506039415, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 4 _____\n",
      "val reward 0.91025310754776\n",
      "imitation reward 1.1622142791748047\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.4913337230682373, 0.2522747218608856, 0.0006600032793357968]\n",
      "[{'decision': 0, 'optimal_auc': 0.927536231884058, 'imitation_auc': 0.571087786259542, 'optimal_acc': 0.8367346938775511, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.9646856158484065, 'imitation_auc': 0.6546195652173913, 'optimal_acc': 0.8231292517006803, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': 0.9716312056737589, 'imitation_auc': 0.7832167832167832, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 5 _____\n",
      "val reward 0.7629498243331909\n",
      "imitation reward 1.1960818767547607\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.45403942465782166, 0.13963112235069275, 0.00041806144872680306]\n",
      "[{'decision': 0, 'optimal_auc': 0.9316239316239316, 'imitation_auc': 0.5935114503816794, 'optimal_acc': 0.8571428571428571, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.9728682170542635, 'imitation_auc': 0.6548913043478262, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.7551020408163265}, {'decision': 2, 'optimal_auc': 0.9917257683215129, 'imitation_auc': 0.7889383343928797, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 6 _____\n",
      "val reward 0.7280138731002808\n",
      "imitation reward 1.1159520149230957\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.5580188632011414, 0.12425674498081207, 0.0007261065184138715]\n",
      "[{'decision': 0, 'optimal_auc': 0.9407283537718321, 'imitation_auc': 0.6211832061068702, 'optimal_acc': 0.8163265306122449, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.9771748492678726, 'imitation_auc': 0.660054347826087, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.7482993197278912}, {'decision': 2, 'optimal_auc': 0.9940898345153664, 'imitation_auc': 0.7879847425301971, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.8095238095238095}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 7 _____\n",
      "val reward 0.7209341526031494\n",
      "imitation reward 1.1795762777328491\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.5774122476577759, 0.16645672917366028, 0.001591524458490312]\n",
      "[{'decision': 0, 'optimal_auc': 0.9420289855072463, 'imitation_auc': 0.6369274809160305, 'optimal_acc': 0.8231292517006803, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.9806201550387597, 'imitation_auc': 0.6603260869565217, 'optimal_acc': 0.9183673469387755, 'imitation_acc': 0.7482993197278912}, {'decision': 2, 'optimal_auc': 0.9952718676122931, 'imitation_auc': 0.7813095994914177, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.7755102040816326}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 8 _____\n",
      "val reward 0.651024580001831\n",
      "imitation reward 1.1851986646652222\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.5483449101448059, 0.16513141989707947, 0.0027091731317341328]\n",
      "[{'decision': 0, 'optimal_auc': 0.9427722036417688, 'imitation_auc': 0.6407442748091603, 'optimal_acc': 0.8299319727891157, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.9810508182601205, 'imitation_auc': 0.6546195652173914, 'optimal_acc': 0.9115646258503401, 'imitation_acc': 0.7414965986394558}, {'decision': 2, 'optimal_auc': 0.9964539007092198, 'imitation_auc': 0.7930705657978385, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.8027210884353742}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 9 _____\n",
      "val reward 0.5636880397796631\n",
      "imitation reward 1.2030971050262451\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.49506062269210815, 0.14040586352348328, 0.004634405020624399]\n",
      "[{'decision': 0, 'optimal_auc': 0.9435154217762913, 'imitation_auc': 0.6374045801526718, 'optimal_acc': 0.8571428571428571, 'imitation_acc': 0.8843537414965986}, {'decision': 1, 'optimal_auc': 0.9827734711455641, 'imitation_auc': 0.647554347826087, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.7551020408163265}, {'decision': 2, 'optimal_auc': 0.9976359338061466, 'imitation_auc': 0.8003814367450731, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.7959183673469388}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 10 _____\n",
      "val reward 0.5538517832756042\n",
      "imitation reward 1.1581895351409912\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.4824775755405426, 0.17274615168571472, 0.010003807954490185]\n",
      "[{'decision': 0, 'optimal_auc': 0.94611668524712, 'imitation_auc': 0.6278625954198475, 'optimal_acc': 0.8503401360544217, 'imitation_acc': 0.8707482993197279}, {'decision': 1, 'optimal_auc': 0.9832041343669251, 'imitation_auc': 0.6567934782608695, 'optimal_acc': 0.9183673469387755, 'imitation_acc': 0.7551020408163265}, {'decision': 2, 'optimal_auc': 0.9988179669030732, 'imitation_auc': 0.8080101716465352, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 11 _____\n",
      "val reward 0.5145827531814575\n",
      "imitation reward 1.137531042098999\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.5155688524246216, 0.1512889415025711, 0.01925537921488285]\n",
      "[{'decision': 0, 'optimal_auc': 0.9483463396506875, 'imitation_auc': 0.6288167938931298, 'optimal_acc': 0.8503401360544217, 'imitation_acc': 0.8571428571428571}, {'decision': 1, 'optimal_auc': 0.9840654608096469, 'imitation_auc': 0.6605978260869565, 'optimal_acc': 0.9387755102040817, 'imitation_acc': 0.7414965986394558}, {'decision': 2, 'optimal_auc': 0.9976359338061466, 'imitation_auc': 0.8127781309599491, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.8231292517006803}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 12 _____\n",
      "val reward 0.49431464076042175\n",
      "imitation reward 1.1419498920440674\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.5348871946334839, 0.12724439799785614, 0.0265685822814703]\n",
      "[{'decision': 0, 'optimal_auc': 0.9516908212560387, 'imitation_auc': 0.623091603053435, 'optimal_acc': 0.8435374149659864, 'imitation_acc': 0.8435374149659864}, {'decision': 1, 'optimal_auc': 0.9827734711455641, 'imitation_auc': 0.6551630434782608, 'optimal_acc': 0.9523809523809523, 'imitation_acc': 0.7551020408163265}, {'decision': 2, 'optimal_auc': 0.9976359338061466, 'imitation_auc': 0.8070565797838525, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.8163265306122449}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 13 _____\n",
      "val reward 0.4872055649757385\n",
      "imitation reward 1.1538927555084229\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.5186882019042969, 0.16160738468170166, 0.029097622260451317]\n",
      "[{'decision': 0, 'optimal_auc': 0.9574507617985879, 'imitation_auc': 0.6288167938931297, 'optimal_acc': 0.8843537414965986, 'imitation_acc': 0.8571428571428571}, {'decision': 1, 'optimal_auc': 0.9801894918173988, 'imitation_auc': 0.6464673913043477, 'optimal_acc': 0.9387755102040817, 'imitation_acc': 0.7551020408163265}, {'decision': 2, 'optimal_auc': 0.9976359338061466, 'imitation_auc': 0.8057851239669422, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.8095238095238095}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 14 _____\n",
      "val reward 0.5011643171310425\n",
      "imitation reward 1.2092411518096924\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.4914214015007019, 0.1847756952047348, 0.025336677208542824]\n",
      "[{'decision': 0, 'optimal_auc': 0.9585655890003716, 'imitation_auc': 0.6326335877862596, 'optimal_acc': 0.8775510204081632, 'imitation_acc': 0.8639455782312925}, {'decision': 1, 'optimal_auc': 0.9801894918173988, 'imitation_auc': 0.6385869565217391, 'optimal_acc': 0.9183673469387755, 'imitation_acc': 0.7619047619047619}, {'decision': 2, 'optimal_auc': 0.9976359338061466, 'imitation_auc': 0.8022886204704387, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.8163265306122449}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 15 _____\n",
      "val reward 0.4305866062641144\n",
      "imitation reward 1.313634991645813\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.5000457763671875, 0.13021108508110046, 0.020905688405036926]\n",
      "[{'decision': 0, 'optimal_auc': 0.9589371980676329, 'imitation_auc': 0.6307251908396947, 'optimal_acc': 0.8843537414965986, 'imitation_acc': 0.8775510204081632}, {'decision': 1, 'optimal_auc': 0.9832041343669251, 'imitation_auc': 0.6195652173913043, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': 0.9964539007092198, 'imitation_auc': 0.79815638906548, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.8163265306122449}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 16 _____\n",
      "val reward 0.484078586101532\n",
      "imitation reward 1.2010010480880737\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.510075032711029, 0.17605118453502655, 0.02459491416811943]\n",
      "[{'decision': 0, 'optimal_auc': 0.9607952434039391, 'imitation_auc': 0.6292938931297709, 'optimal_acc': 0.8775510204081632, 'imitation_acc': 0.8571428571428571}, {'decision': 1, 'optimal_auc': 0.9806201550387597, 'imitation_auc': 0.626358695652174, 'optimal_acc': 0.9319727891156463, 'imitation_acc': 0.7687074829931972}, {'decision': 2, 'optimal_auc': 0.9964539007092198, 'imitation_auc': 0.7952956134774316, 'optimal_acc': 0.9795918367346939, 'imitation_acc': 0.8163265306122449}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 17 _____\n",
      "val reward 0.4376947581768036\n",
      "imitation reward 1.190829873085022\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.5141815543174744, 0.14158056676387787, 0.02395659126341343]\n",
      "[{'decision': 0, 'optimal_auc': 0.9609810479375697, 'imitation_auc': 0.6140267175572519, 'optimal_acc': 0.8707482993197279, 'imitation_acc': 0.8367346938775511}, {'decision': 1, 'optimal_auc': 0.9823428079242033, 'imitation_auc': 0.6247282608695652, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.7619047619047619}, {'decision': 2, 'optimal_auc': 0.9964539007092198, 'imitation_auc': 0.7975206611570247, 'optimal_acc': 0.9795918367346939, 'imitation_acc': 0.8163265306122449}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 18 _____\n",
      "val reward 0.4222581684589386\n",
      "imitation reward 1.2779427766799927\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.49046221375465393, 0.10846579074859619, 0.02304469794034958]\n",
      "[{'decision': 0, 'optimal_auc': 0.9606094388703084, 'imitation_auc': 0.6068702290076337, 'optimal_acc': 0.8843537414965986, 'imitation_acc': 0.8435374149659864}, {'decision': 1, 'optimal_auc': 0.9857881136950903, 'imitation_auc': 0.6252717391304348, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': 0.9964539007092198, 'imitation_auc': 0.7997457088366179, 'optimal_acc': 0.9795918367346939, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 19 _____\n",
      "val reward 0.4717499613761902\n",
      "imitation reward 1.3164355754852295\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.5057424306869507, 0.15870174765586853, 0.026207534596323967]\n",
      "[{'decision': 0, 'optimal_auc': 0.9611668524712003, 'imitation_auc': 0.6235687022900763, 'optimal_acc': 0.8775510204081632, 'imitation_acc': 0.8367346938775511}, {'decision': 1, 'optimal_auc': 0.9823428079242034, 'imitation_auc': 0.6293478260869566, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.7687074829931972}, {'decision': 2, 'optimal_auc': 0.9964539007092198, 'imitation_auc': 0.8026064844246663, 'optimal_acc': 0.9795918367346939, 'imitation_acc': 0.8367346938775511}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 20 _____\n",
      "val reward 0.5445703268051147\n",
      "imitation reward 1.3181467056274414\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.5098860263824463, 0.18568944931030273, 0.02942318469285965]\n",
      "[{'decision': 0, 'optimal_auc': 0.9619100706057229, 'imitation_auc': 0.6331106870229007, 'optimal_acc': 0.8979591836734694, 'imitation_acc': 0.8367346938775511}, {'decision': 1, 'optimal_auc': 0.9801894918173988, 'imitation_auc': 0.6331521739130435, 'optimal_acc': 0.9251700680272109, 'imitation_acc': 0.7687074829931972}, {'decision': 2, 'optimal_auc': 0.9964539007092198, 'imitation_auc': 0.7994278448823903, 'optimal_acc': 0.9795918367346939, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 21 _____\n",
      "val reward 0.48049110174179077\n",
      "imitation reward 1.3492180109024048\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.5143001675605774, 0.14887647330760956, 0.031042780727148056]\n",
      "[{'decision': 0, 'optimal_auc': 0.9615384615384616, 'imitation_auc': 0.6359732824427481, 'optimal_acc': 0.891156462585034, 'imitation_acc': 0.8435374149659864}, {'decision': 1, 'optimal_auc': 0.9810508182601205, 'imitation_auc': 0.6418478260869566, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.7687074829931972}, {'decision': 2, 'optimal_auc': 0.9952718676122931, 'imitation_auc': 0.7860775588048315, 'optimal_acc': 0.9795918367346939, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 22 _____\n",
      "val reward 0.47507691383361816\n",
      "imitation reward 1.3724004030227661\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.5212827920913696, 0.11096969991922379, 0.032218966633081436]\n",
      "[{'decision': 0, 'optimal_auc': 0.9600520252694166, 'imitation_auc': 0.633587786259542, 'optimal_acc': 0.8843537414965986, 'imitation_acc': 0.8571428571428571}, {'decision': 1, 'optimal_auc': 0.9806201550387597, 'imitation_auc': 0.651358695652174, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.7619047619047619}, {'decision': 2, 'optimal_auc': 0.9940898345153664, 'imitation_auc': 0.7705022250476796, 'optimal_acc': 0.9795918367346939, 'imitation_acc': 0.8299319727891157}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 23 _____\n",
      "val reward 0.4692211449146271\n",
      "imitation reward 1.377138614654541\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.5033467411994934, 0.11550411581993103, 0.03296384587883949]\n",
      "[{'decision': 0, 'optimal_auc': 0.9589371980676329, 'imitation_auc': 0.6307251908396947, 'optimal_acc': 0.9115646258503401, 'imitation_acc': 0.8435374149659864}, {'decision': 1, 'optimal_auc': 0.9797588285960379, 'imitation_auc': 0.6510869565217391, 'optimal_acc': 0.9659863945578231, 'imitation_acc': 0.7619047619047619}, {'decision': 2, 'optimal_auc': 0.9940898345153664, 'imitation_auc': 0.7546090273363001, 'optimal_acc': 0.9795918367346939, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______epoch 24 _____\n",
      "val reward 0.4863758981227875\n",
      "imitation reward 1.4479124546051025\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.4986669719219208, 0.13891984522342682, 0.03281077370047569]\n",
      "[{'decision': 0, 'optimal_auc': 0.9581939799331104, 'imitation_auc': 0.625, 'optimal_acc': 0.9183673469387755, 'imitation_acc': 0.8503401360544217}, {'decision': 1, 'optimal_auc': 0.9788975021533161, 'imitation_auc': 0.6475543478260869, 'optimal_acc': 0.9659863945578231, 'imitation_acc': 0.7551020408163265}, {'decision': 2, 'optimal_auc': 0.9940898345153664, 'imitation_auc': 0.7453909726636999, 'optimal_acc': 0.9795918367346939, 'imitation_acc': 0.8163265306122449}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 25 _____\n",
      "val reward 0.5377295613288879\n",
      "imitation reward 1.5792698860168457\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.5153266787528992, 0.1599303036928177, 0.03194335848093033]\n",
      "[{'decision': 0, 'optimal_auc': 0.9570791527313267, 'imitation_auc': 0.6164122137404581, 'optimal_acc': 0.8843537414965986, 'imitation_acc': 0.8367346938775511}, {'decision': 1, 'optimal_auc': 0.9793281653746769, 'imitation_auc': 0.6448369565217391, 'optimal_acc': 0.9523809523809523, 'imitation_acc': 0.7482993197278912}, {'decision': 2, 'optimal_auc': 0.9940898345153664, 'imitation_auc': 0.7374443738080101, 'optimal_acc': 0.9795918367346939, 'imitation_acc': 0.8095238095238095}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 26 _____\n",
      "val reward 0.5351510643959045\n",
      "imitation reward 1.6859853267669678\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.514539361000061, 0.1513669639825821, 0.03068169392645359]\n",
      "[{'decision': 0, 'optimal_auc': 0.9541062801932366, 'imitation_auc': 0.6145038167938932, 'optimal_acc': 0.8639455782312925, 'imitation_acc': 0.8231292517006803}, {'decision': 1, 'optimal_auc': 0.9797588285960379, 'imitation_auc': 0.6315217391304347, 'optimal_acc': 0.9523809523809523, 'imitation_acc': 0.7482993197278912}, {'decision': 2, 'optimal_auc': 0.9952718676122931, 'imitation_auc': 0.7301335028607756, 'optimal_acc': 0.9795918367346939, 'imitation_acc': 0.8027210884353742}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 27 _____\n",
      "val reward 0.5161556601524353\n",
      "imitation reward 1.7876050472259521\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.4991632103919983, 0.10919716209173203, 0.029361816123127937]\n",
      "[{'decision': 0, 'optimal_auc': 0.9518766257896693, 'imitation_auc': 0.6063931297709924, 'optimal_acc': 0.891156462585034, 'imitation_acc': 0.8367346938775511}, {'decision': 1, 'optimal_auc': 0.979328165374677, 'imitation_auc': 0.621195652173913, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.7482993197278912}, {'decision': 2, 'optimal_auc': 0.9952718676122931, 'imitation_auc': 0.7190082644628099, 'optimal_acc': 0.9863945578231292, 'imitation_acc': 0.8027210884353742}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 28 _____\n",
      "val reward 0.5359873175621033\n",
      "imitation reward 1.6603002548217773\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.51347815990448, 0.12277456372976303, 0.03219631686806679]\n",
      "[{'decision': 0, 'optimal_auc': 0.9507617985878856, 'imitation_auc': 0.6073473282442747, 'optimal_acc': 0.8571428571428571, 'imitation_acc': 0.8435374149659864}, {'decision': 1, 'optimal_auc': 0.9776055124892333, 'imitation_auc': 0.6187499999999999, 'optimal_acc': 0.9727891156462585, 'imitation_acc': 0.7482993197278912}, {'decision': 2, 'optimal_auc': 0.9952718676122931, 'imitation_auc': 0.7158296249205339, 'optimal_acc': 0.9931972789115646, 'imitation_acc': 0.8027210884353742}]\n",
      "++++++++++Final+++++++++++\n",
      "best tensor(1.1309, grad_fn=<AddBackward0>)\n",
      "[{'decision': 0, 'optimal_auc': 0.9609810479375697, 'imitation_auc': 0.6140267175572519, 'optimal_acc': 0.8707482993197279, 'imitation_acc': 0.8367346938775511}, {'decision': 1, 'optimal_auc': 0.9823428079242033, 'imitation_auc': 0.6247282608695652, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.7619047619047619}, {'decision': 2, 'optimal_auc': 0.9964539007092198, 'imitation_auc': 0.7975206611570247, 'optimal_acc': 0.9795918367346939, 'imitation_acc': 0.8163265306122449}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionAttentionModel(\n",
       "  (input_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=100, out_features=1000, bias=True)\n",
       "  )\n",
       "  (batchnorm): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       "  (relu): Softplus(beta=1, threshold=20)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (softmax): Softmax(dim=1)\n",
       "  (final_opt_layer): Linear(in_features=1000, out_features=100, bias=True)\n",
       "  (final_imitation_layer): Linear(in_features=1000, out_features=100, bias=True)\n",
       "  (final_layer): Linear(in_features=1000, out_features=6, bias=True)\n",
       "  (resize_layer): Linear(in_features=89, out_features=100, bias=True)\n",
       "  (attentions): ModuleList(\n",
       "    (0): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (norms): ModuleList(\n",
       "    (0): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (activation): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_unique_sequence(array):\n",
    "    #converts a row of boolean values to a unique number e.g. [1,1,0] => 11, [0,0,1] => 100\n",
    "    uniqueify = lambda r: torch.sum(torch.stack([i*(10**ii) for ii,i in enumerate(r)]))\n",
    "    return torch_apply_along_axis(uniqueify,array)\n",
    "\n",
    "def train_decision_model_triplet(\n",
    "    tmodel1,\n",
    "    tmodel2,\n",
    "    tmodel3,\n",
    "    smodel3,\n",
    "    use_default_split=True,\n",
    "    use_bagging_split=False,\n",
    "    use_attention=True,\n",
    "    lr=.001,\n",
    "    epochs=10000,\n",
    "    patience=5,\n",
    "    weights=[0,.5,.5,0], #realtive weight of survival, feeding tube, aspiration, andl lrc\n",
    "    tweights=[1,1,1,0], #weight for OS, LRC, FDM, and event (any + FT or AS at 6m) as time to event in weeks\n",
    "    opt_weights=[1,1,1], #weights for policy model for optimal decisions\n",
    "    imitation_weights=[.5,1,1],#weights of imitation decisions, because ic overtrains too quickly\n",
    "    imitation_weight=1,\n",
    "    reward_weight=1,\n",
    "    imitation_triplet_weight=2,\n",
    "    reward_triplet_weight = 2,\n",
    "    shufflecol_chance = 0.2,\n",
    "    split=.7,\n",
    "    resample_training=False,\n",
    "    save_path='../data/models/',\n",
    "    file_suffix='',\n",
    "    verbose=True,\n",
    "    use_gpu=False,\n",
    "    **model_kwargs,\n",
    "):\n",
    "    tmodel1.eval()\n",
    "    tmodel2.eval()\n",
    "    tmodel3.eval()\n",
    "\n",
    "    train_ids, test_ids = get_tt_split(use_default_split=use_default_split,use_bagging_split=use_bagging_split,resample_training=resample_training)\n",
    "    true_ids = train_ids + test_ids #for saving memory without upsampling\n",
    "\n",
    "    dataset = DTDataset()\n",
    "    data = dataset.processed_df.copy()\n",
    "    \n",
    "    def get_dlt(state):\n",
    "        if state == 2:\n",
    "            return data[Const.dlt2].copy()\n",
    "        d = data[Const.dlt1].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_pd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.primary_disease_states2].copy()\n",
    "        d = data[Const.primary_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_nd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.nodal_disease_states2].copy()\n",
    "        d = data[Const.nodal_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_cc(state):\n",
    "        res = data[Const.ccs].copy()\n",
    "        if state == 1:\n",
    "            res.values[:,:] = np.zeros(res.values.shape)\n",
    "        return res\n",
    "    \n",
    "    def get_mod(state):\n",
    "        res = data[Const.modifications].copy()\n",
    "        #this should have an ic condition but we don't use it anumore anywa\n",
    "        return res\n",
    "        \n",
    "    outcomedf = data[Const.outcomes]\n",
    "    baseline = dataset.get_state('baseline')\n",
    "    \n",
    "    def formatdf(d,dids=train_ids):\n",
    "        d = df_to_torch(d.loc[dids]).to(model.get_device())\n",
    "        return d\n",
    "    \n",
    "    def makegrad(v):\n",
    "        if not v.requires_grad:\n",
    "            v.requires_grad=True\n",
    "        return v\n",
    "    \n",
    "    if use_attention:\n",
    "        model = DecisionAttentionModel(baseline.shape[1],**model_kwargs)\n",
    "    else:\n",
    "        model_kwargs = {k:v for k,v in model_kwargs.items() if 'attention' not in k and 'embed' not in k}\n",
    "        model = DecisionModel(baseline.shape[1],**model_kwargs)\n",
    "        \n",
    "    device = 'cpu'\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "        \n",
    "    model.set_device(device)\n",
    "\n",
    "    tmodel1.set_device(device)\n",
    "    tmodel2.set_device(device)\n",
    "    tmodel3.set_device(device)\n",
    "    smodel3.set_device(device)\n",
    "    hashcode = str(hash(','.join([str(i) for i in train_ids])))\n",
    "    \n",
    "    save_file = save_path + 'model_' + model.identifier +'_hash' + hashcode + file_suffix + '.tar'\n",
    "    model.fit_normalizer(df_to_torch(baseline.loc[train_ids]))\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "    \n",
    "    \n",
    "    optimal_train,transitions_train = calc_optimal_decisions(dataset,train_ids,tmodel1,tmodel2,tmodel3,smodel3,\n",
    "                                           weights=weights,tweights=tweights,\n",
    "                                          )\n",
    "    optimal_test,transitions_test = calc_optimal_decisions(dataset,test_ids,tmodel1,tmodel2,tmodel3,smodel3,\n",
    "                                          weights=weights,tweights=tweights,\n",
    "                                         )\n",
    "    optimal_train = optimal_train.to(model.get_device())\n",
    "    optimal_test = optimal_test.to(model.get_device())\n",
    "    mse = torch.nn.MSELoss()\n",
    "    bce = torch.nn.BCELoss()\n",
    "    \n",
    "    def compare_decisions(d1,d2,d3,ids):\n",
    "#         ypred = np.concatenate([dd.cpu().detach().numpy().reshape(-1,1) for dd in [d1,d2,d3]],axis=1)\n",
    "        ytrue = df_to_torch(outcomedf.loc[ids])\n",
    "        dloss = bce(d1.view(-1),ytrue[:,0])\n",
    "        dloss += bce(d2.view(-1),ytrue[:,1])\n",
    "        dloss += bce(d3,view(-1),ytrue[:,2])\n",
    "        return dloss\n",
    "        \n",
    "    def remove_decisions(df):\n",
    "        cols = [c for c in df.columns if c not in Const.decisions ]\n",
    "        ddf = df[cols]\n",
    "        return ddf\n",
    "    \n",
    "    makeinput = lambda step,dids: df_to_torch(remove_decisions(dataset.get_input_state(step=step,ids=dids)))\n",
    "    threshold = lambda x: torch.gt(x,torch.rand(x.shape[0])).type(torch.FloatTensor)\n",
    "\n",
    "    randchoice = lambda x: x[torch.randint(len(x),(1,))[0]]\n",
    "    tloss_func = torch.nn.TripletMarginLoss()\n",
    "    def get_tloss(row,step,yt,x,imitation=True):\n",
    "        if yt[:,step].std() < .001:\n",
    "            return torch.tensor([0]).to(device)\n",
    "        positive_idx= torch.nonzero(yt[:,step] == yt[row,step])\n",
    "        if len(positive_idx) <= 1:\n",
    "            print('no losses','n positive',len(positive_idx),'yt',yt[row,step],'row',row,'step',step,'imitation',imitation,end='\\r')\n",
    "            return torch.tensor([0]).to(device)\n",
    "        positive_idx = torch.stack([ii for ii in positive_idx if ii != row]).view(-1)\n",
    "        negative_idx = torch.tensor([ii for ii in range(x.shape[0]) if ii not in positive_idx and ii != row])\n",
    "        if len(positive_idx) < 1 or len(negative_idx) < 1:\n",
    "            print('no losses','n positive',len(positive_idx),'n negative',len(negative_idx),'yt',yt[row,step],'row',row,'step',step,'imitation',imitation,end='\\r')\n",
    "            return torch.tensor([0]).to(device)\n",
    "        positive = x[randchoice(positive_idx)]\n",
    "        negative = x[randchoice(negative_idx)]\n",
    "        anchor = x[row]\n",
    "        if use_attention:\n",
    "            [anchor_embedding,pos_embedding,neg_embedding] = [model.get_embedding(xx.view(1,-1),position=step,use_saved_memory=True) for xx in [anchor,positive,negative]]\n",
    "        else:    \n",
    "            [anchor_embedding,pos_embedding,neg_embedding] = [model.get_embedding(xx.view(1,-1),position=step,concatenate=False)[int(imitation)] for xx in [anchor,positive,negative]]\n",
    "        tloss = tloss_func(anchor_embedding,pos_embedding,neg_embedding)\n",
    "        return tloss\n",
    "    #save the inputs from the whole dataset for future reference\n",
    "    if use_attention:\n",
    "        full_data = []\n",
    "        for mstep in [0,1,2]:\n",
    "            full_data_step = [baseline, get_dlt(min(mstep,1)),\n",
    "                         get_dlt(mstep),get_pd(mstep),get_nd(mstep),get_cc(mstep),get_mod(mstep)]\n",
    "            full_data_step = torch.cat([formatdf(fd,true_ids) for fd in full_data_step],axis=1)\n",
    "            full_data.append(full_data_step)\n",
    "        full_data = torch.stack(full_data)\n",
    "        model.save_memory(full_data)\n",
    "        print(full_data.shape)\n",
    "        \n",
    "    def step(train=True):\n",
    "        if train:\n",
    "            model.train(True)\n",
    "            tmodel1.train(True)\n",
    "            tmodel2.train(True)\n",
    "            tmodel3.train(True)\n",
    "            optimizer.zero_grad()\n",
    "            ids = train_ids\n",
    "            y_opt = makegrad(optimal_train)\n",
    "            transition_dict = {k: torch.clone(v).detach() for k,v in transitions_train.items()}\n",
    "        else:\n",
    "            ids = test_ids\n",
    "            model.eval()\n",
    "            tmodel1.eval()\n",
    "            tmodel2.eval()\n",
    "            tmodel3.eval()\n",
    "            y_opt = makegrad(optimal_test)\n",
    "            print(y_opt.mean(axis=0))\n",
    "            transition_dict = {k: torch.clone(v).detach() for k,v in transitions_test.items()}\n",
    "        model.set_device(device)\n",
    "        ytrain = df_to_torch(outcomedf.loc[ids]).to(device)\n",
    "        #imitation losses and decision 1\n",
    "        xxtrained = [baseline, get_dlt(0),get_dlt(0),get_pd(0),get_nd(0),get_cc(0),get_mod(0)]\n",
    "        xxtrain = [formatdf(xx,ids) for xx in xxtrained]\n",
    "        xxtrain = torch.cat(xxtrain,axis=1).to(device)\n",
    "        o1 = model(xxtrain,position=0,use_saved_memory= (not train))\n",
    "        decision1_imitation = o1[:,3]\n",
    "        decision1_opt = o1[:,0]\n",
    "    \n",
    "        imitation_loss1 = bce(decision1_imitation,ytrain[:,0])\n",
    "        imitation_loss1 = torch.mul(imitation_loss1,imitation_weights[0])\n",
    "        opt_loss1 = bce(decision1_opt,y_opt[:,0])\n",
    "        opt_loss1 = torch.mul(opt_loss1,opt_weights[0])\n",
    "        \n",
    "        x1_imitation = [baseline, get_dlt(1),get_dlt(0),get_pd(1),get_nd(1),get_cc(1),get_mod(1)]\n",
    "        x1_imitation = [formatdf(xx1,ids) for xx1 in x1_imitation]\n",
    "        x1_imitation = torch.cat(x1_imitation,axis=1).to(device)\n",
    "        \n",
    "        o2 = model(x1_imitation,position=1,use_saved_memory= (not train))\n",
    "            \n",
    "        decision2_imitation = o2[:,4]\n",
    "            \n",
    "        imitation_loss2 =  bce(decision2_imitation,ytrain[:,1])\n",
    "        imitation_loss2 = torch.mul(imitation_loss2,imitation_weights[1])\n",
    "        \n",
    "        \n",
    "        x2_imitation = [baseline, get_dlt(1),get_dlt(2),get_pd(2),get_nd(2),get_cc(2),get_mod(2)]\n",
    "        \n",
    "        x2_imitation = [formatdf(xx2,ids) for xx2 in x2_imitation]\n",
    "        x2_imitation = torch.cat(x2_imitation,axis=1).to(device)\n",
    "        \n",
    "        \n",
    "        o3 = model(x2_imitation,position=2,use_saved_memory= (not train))\n",
    "        \n",
    "        decision3_imitation = o3[:,5]\n",
    "        \n",
    "        imitation_loss3 = bce(decision3_imitation,ytrain[:,2])\n",
    "        imitation_loss3 = torch.mul(imitation_loss3,imitation_weights[2])\n",
    "        \n",
    "        opt_input2 = [\n",
    "            formatdf(baseline,ids), \n",
    "            transition_dict['dlt1'],\n",
    "            formatdf(get_dlt(0),ids),\n",
    "            transition_dict['pd1'],\n",
    "            transition_dict['nd1'], \n",
    "            formatdf(get_cc(0),ids),\n",
    "            transition_dict['mod']\n",
    "                 ]\n",
    "        opt_input2 = [o.to(device) for o in opt_input2]\n",
    "\n",
    "        opt_input2 = torch.cat(opt_input2,axis=1).to(device)\n",
    "        decision2_opt = model(opt_input2,position=1,use_saved_memory= (not train))[:,1]\n",
    "        \n",
    "        opt_loss2 = bce(decision2_opt,y_opt[:,1])\n",
    "        opt_loss2 = torch.mul(opt_loss2,opt_weights[1])\n",
    "        \n",
    "        opt_input3 = [\n",
    "            formatdf(baseline,ids),\n",
    "            transition_dict['dlt1'],\n",
    "            transition_dict['dlt2'],\n",
    "            transition_dict['pd2'],\n",
    "            transition_dict['nd2'],\n",
    "            transition_dict['cc'],\n",
    "            transition_dict['mod'],\n",
    "        ]\n",
    "        opt_input3 = [o.to(device) for o in opt_input3]\n",
    "        opt_input3 = torch.cat(opt_input3,axis=1).to(device)\n",
    "        decision3_opt = model(opt_input3,position=2,use_saved_memory= (not train))[:,2]\n",
    "        \n",
    "        opt_loss3 = bce(decision3_opt,y_opt[:,2])\n",
    "        opt_loss3 = torch.mul(opt_loss3,opt_weights[2])\n",
    "        \n",
    "        iloss = torch.add(torch.add(imitation_loss1,imitation_loss2),imitation_loss3)\n",
    "        iloss = torch.mul(iloss,imitation_weight)\n",
    "        \n",
    "        reward_loss = torch.add(torch.add(opt_loss1,opt_loss2),opt_loss3)\n",
    "        reward_loss =torch.mul(reward_loss,reward_weight)\n",
    "        \n",
    "        loss = torch.add(iloss,reward_loss)\n",
    "        \n",
    "        imitation_tloss = torch.FloatTensor([0]).to(device)\n",
    "        opt_tloss = torch.FloatTensor([0]).to(device)\n",
    "        n_rows = xxtrain.shape[0]\n",
    "        if reward_triplet_weight + imitation_triplet_weight > 0.0001:\n",
    "            for i in range(n_rows):\n",
    "                \n",
    "                if imitation_triplet_weight > .0001:\n",
    "                    imitation_tloss += get_tloss(i,0,ytrain,xxtrain,True)\n",
    "                    imitation_tloss += get_tloss(i,1,ytrain,x1_imitation,True)\n",
    "                    imitation_tloss += get_tloss(i,2,ytrain,x2_imitation,True)\n",
    "                if reward_triplet_weight > .0001:\n",
    "                    opt_tloss += get_tloss(i,0,y_opt,xxtrain,False)\n",
    "                    opt_tloss += get_tloss(i,1,y_opt,opt_input2,False)\n",
    "                    opt_tloss += get_tloss(i,2,y_opt,opt_input3,False)\n",
    "            loss += torch.mul(imitation_tloss[0],imitation_triplet_weight/n_rows)\n",
    "            loss += torch.mul(opt_tloss[0],reward_triplet_weight/n_rows)\n",
    "        losses = [iloss,reward_loss,imitation_tloss*imitation_triplet_weight/n_rows,opt_tloss*reward_triplet_weight/n_rows]\n",
    "        \n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            return losses\n",
    "        else:\n",
    "            scores = []\n",
    "            distributions = [decision1_opt.mean().item(),decision2_opt.mean().item(),decision3_opt.mean().item()]\n",
    "            imitation = [decision1_imitation,decision2_imitation,decision3_imitation]\n",
    "            optimal = [decision1_opt,decision2_opt,decision3_opt]\n",
    "            for i,decision_im in enumerate(imitation):\n",
    "                deci = decision_im.cpu().detach().numpy()\n",
    "                deci0 = (deci > .5).astype(int)\n",
    "                iout = ytrain[:,i].cpu().detach().numpy()\n",
    "                acci = accuracy_score(iout,deci0)\n",
    "                try:\n",
    "                    auci = roc_auc_score(iout,deci)\n",
    "                except:\n",
    "                    auci = -1\n",
    "                \n",
    "                deco = optimal[i].cpu().detach().numpy()\n",
    "                deci0 = (deco > .5).astype(int)\n",
    "                oout = y_opt[:,i].cpu().detach().numpy()\n",
    "                acco = accuracy_score(oout,deci0)\n",
    "                try:\n",
    "                    auco = roc_auc_score(oout,deco)\n",
    "                except:\n",
    "                    auco=-1\n",
    "                scores.append({'decision': i,'optimal_auc': auco,'imitation_auc': auci,'optimal_acc': acco,'imitation_acc': acci})\n",
    "            return losses, scores, distributions\n",
    "        \n",
    "    best_val_loss = torch.tensor(1000000000.0)\n",
    "    steps_since_improvement = 0\n",
    "    best_val_score = {}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        losses = step(True)\n",
    "        val_losses,val_metrics,val_distributions = step(False)\n",
    "        vl = val_losses[0] + val_losses[1]\n",
    "        for vm in val_metrics:\n",
    "            vl += (-((vm['optimal_auc']*reward_weight) + (vm['imitation_auc']*imitation_weight)))/10\n",
    "        if verbose:\n",
    "            print('______epoch',str(epoch),'_____')\n",
    "            print('val reward',val_losses[1].item())\n",
    "            print('imitation reward', val_losses[0].item())\n",
    "            print('distance losses',val_losses[2].item(),val_losses[-1].item())\n",
    "            print('distributions',val_distributions)\n",
    "            print(val_metrics)\n",
    "        if vl < best_val_loss:\n",
    "            best_val_loss = vl\n",
    "            best_val_score = val_metrics\n",
    "            best_val_distributions = val_distributions\n",
    "            steps_since_improvement = 0\n",
    "            torch.save(model.state_dict(),save_file)\n",
    "        else:\n",
    "            steps_since_improvement += 1\n",
    "        if steps_since_improvement > patience:\n",
    "            break\n",
    "    print('++++++++++Final+++++++++++')\n",
    "    print('best',best_val_loss)\n",
    "    print(best_val_score)\n",
    "    model.load_state_dict(torch.load(save_file))\n",
    "    model.eval()\n",
    "    return model, best_val_score, best_val_loss, best_val_distributions\n",
    "\n",
    "from Models import *\n",
    "args = {\n",
    "    'hidden_layers': [1000], \n",
    "    'opt_layer_size': 20, \n",
    "    'imitation_layer_size': 20, \n",
    "    'dropout': 0.25, \n",
    "    'input_dropout': 0.1, \n",
    "    'shufflecol_chance': 0.5\n",
    "}\n",
    "\n",
    "#1.8424\n",
    "decision_model, decision_score, decision_loss, _ = train_decision_model_triplet(\n",
    "    model1,model2,model3,smodel3,\n",
    "    lr=.01,\n",
    "    imitation_weight=1,\n",
    "    reward_weight=1,\n",
    "    patience=10,\n",
    "    imitation_triplet_weight=0,\n",
    "    reward_triplet_weight =0,\n",
    "    verbose=True,\n",
    "    weights=[1,1,1,1], #realtive weight of survival, feeding tube, aspiration, andl lrc\n",
    "    tweights=[0,0,3,0], #weight for OS, LRC, FDM, and event (any + FT or AS at 6m) as time to event in weeks\n",
    "    use_attention=True,\n",
    "    **args)\n",
    "decision_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "73f37f8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decision</th>\n",
       "      <th>optimal_auc</th>\n",
       "      <th>imitation_auc</th>\n",
       "      <th>optimal_acc</th>\n",
       "      <th>imitation_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.960981</td>\n",
       "      <td>0.614027</td>\n",
       "      <td>0.870748</td>\n",
       "      <td>0.836735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.982343</td>\n",
       "      <td>0.624728</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>0.761905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.996454</td>\n",
       "      <td>0.797521</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.816327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   decision  optimal_auc  imitation_auc  optimal_acc  imitation_acc\n",
       "0         0     0.960981       0.614027     0.870748       0.836735\n",
       "1         1     0.982343       0.624728     0.959184       0.761905\n",
       "2         2     0.996454       0.797521     0.979592       0.816327"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_model.set_device('cpu')\n",
    "torch.save(decision_model,'../resources/decision_model.pt')\n",
    "pd.DataFrame(decision_score).to_csv('../results/policy_model_score.csv')\n",
    "pd.DataFrame(decision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc3c8bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decision</th>\n",
       "      <th>optimal_auc</th>\n",
       "      <th>imitation_auc</th>\n",
       "      <th>optimal_acc</th>\n",
       "      <th>imitation_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.958904</td>\n",
       "      <td>0.617366</td>\n",
       "      <td>0.993197</td>\n",
       "      <td>0.884354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.680163</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>0.768707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.800699</td>\n",
       "      <td>0.816910</td>\n",
       "      <td>0.965986</td>\n",
       "      <td>0.809524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   decision  optimal_auc  imitation_auc  optimal_acc  imitation_acc\n",
       "0         0     0.958904       0.617366     0.993197       0.884354\n",
       "1         1     0.865385       0.680163     0.959184       0.768707\n",
       "2         2     0.800699       0.816910     0.965986       0.809524"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(decision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b25922a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.8424\n",
    "decision_model2, decision_score2, decision_loss2, _ = train_decision_model_triplet(\n",
    "    model1,model2,model3,smodel3,\n",
    "    lr=.01,\n",
    "    imitation_weight=1,\n",
    "    reward_weight=1,\n",
    "    patience=10,\n",
    "    imitation_triplet_weight=0,\n",
    "    reward_triplet_weight =0,\n",
    "    verbose=True,\n",
    "    weights=[0,0,0,0], #realtive weight of survival, feeding tube, aspiration, andl lrc\n",
    "    tweights=[1,1,0,0], #weight for OS, LRC, FDM, and event (any + FT or AS at 6m) as time to event in weeks\n",
    "    use_attention=True,\n",
    "    **args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a7faad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decision_model(\n",
    "    tmodel1,\n",
    "    tmodel2,\n",
    "    tmodel3,\n",
    "    smodel3,\n",
    "    use_default_split=True,\n",
    "    use_bagging_split=False,\n",
    "    lr=.0001,\n",
    "    epochs=10000,\n",
    "    patience=50,\n",
    "    weights=[0,.5,.5,0], #realtive weight of survival, feeding tube, aspiration, andl lrc\n",
    "    tweights=[1,1,1,0]\n",
    "    imitation_weights=[.5,1,1],#weights of imitation decisions, because ic overtrains too quickly\n",
    "    imitation_weight=0.1,\n",
    "    shufflecol_chance = 0.1,\n",
    "    reward_weight=1,\n",
    "    imitation_triplet_weight=0,\n",
    "    reward_triplet_weight = 0,\n",
    "    split=.7,\n",
    "    resample_training=False,\n",
    "    save_path='../data/models/',\n",
    "    file_suffix='',\n",
    "    use_gpu=True,\n",
    "    use_attention=True,\n",
    "    verbose=True,\n",
    "    threshold_decisions=True,#convert decisiosn to binary in simulation, usually breaks it\n",
    "    use_smote=False,\n",
    "    validate_with_memory=True,\n",
    "    **model_kwargs):\n",
    "    #outdated method of doing stuff, haven't updated with new loss functions idk\n",
    "    tmodel1.eval()\n",
    "    tmodel2.eval()\n",
    "    tmodel3.eval()\n",
    "    smodel3.eval()\n",
    "    train_ids, test_ids = get_tt_split(use_default_split=use_default_split,use_bagging_split=use_bagging_split,resample_training=resample_training)\n",
    "    true_ids = train_ids + test_ids #for saving memory without upsampling\n",
    "    if use_smote:\n",
    "        dataset = DTDataset(use_smote=True,smote_ids = train_ids)\n",
    "        train_ids = [i for i in dataset.processed_df.index.values if i not in test_ids]\n",
    "    else:\n",
    "        dataset = DTDataset()\n",
    "    data = dataset.processed_df.copy()\n",
    "    \n",
    "    def get_dlt(state):\n",
    "        if state == 2:\n",
    "            return data[Const.dlt2].copy()\n",
    "        d = data[Const.dlt1].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_pd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.primary_disease_states2].copy()\n",
    "        d = data[Const.primary_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_nd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.nodal_disease_states2].copy()\n",
    "        d = data[Const.nodal_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_cc(state):\n",
    "        res = data[Const.ccs].copy()\n",
    "        if state == 1:\n",
    "            res.values[:,:] = np.zeros(res.values.shape)\n",
    "        return res\n",
    "    \n",
    "    def get_mod(state):\n",
    "        res = data[Const.modifications].copy()\n",
    "        #this should have an ic condition but we don't use it anumore anywa\n",
    "        return res\n",
    "        \n",
    "    outcomedf = data[Const.outcomes]\n",
    "    baseline = dataset.get_state('baseline')\n",
    "    \n",
    "    def formatdf(d,dids=train_ids):\n",
    "        d = df_to_torch(d.loc[dids]).to(model.get_device())\n",
    "        return d\n",
    "    \n",
    "    def makegrad(v):\n",
    "        if not v.requires_grad:\n",
    "            v.requires_grad=True\n",
    "        return v\n",
    "    \n",
    "    if use_attention:\n",
    "        model = DecisionAttentionModel(baseline.shape[1],**model_kwargs)\n",
    "    else:\n",
    "        model_kwargs = {k:v for k,v in model_kwargs.items() if 'attention' not in k and 'embed' not in k}\n",
    "        model = DecisionModel(baseline.shape[1],**model_kwargs)\n",
    "\n",
    "    device = 'cpu'\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "        \n",
    "    model.set_device(device)\n",
    "\n",
    "    tmodel1.set_device(device)\n",
    "    tmodel2.set_device(device)\n",
    "    tmodel3.set_device(device)\n",
    "    smodel3.set_device(device)\n",
    "    \n",
    "    hashcode = str(hash(','.join([str(i) for i in train_ids])))\n",
    "    \n",
    "    save_file = save_path + 'model_' + model.identifier +'_hash' + hashcode + file_suffix + '.tar'\n",
    "    model.fit_normalizer(df_to_torch(baseline.loc[train_ids]).to(model.get_device()))\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "    \n",
    "    mse = torch.nn.MSELoss()\n",
    "    bce = torch.nn.BCELoss()\n",
    "    device = model.get_device()\n",
    "    def compare_decisions(d1,d2,d3,ids):\n",
    "#         ypred = np.concatenate([dd.cpu().detach().numpy().reshape(-1,1) for dd in [d1,d2,d3]],axis=1)\n",
    "        ytrue = df_to_torch(outcomedf.loc[ids])\n",
    "        dloss = bce(d1.view(-1),ytrue[:,0])\n",
    "        dloss += bce(d2.view(-1),ytrue[:,1])\n",
    "        dloss += bce(d3,view(-1),ytrue[:,2])\n",
    "        return dloss\n",
    "        \n",
    "    def remove_decisions(df):\n",
    "        cols = [c for c in df.columns if c not in Const.decisions ]\n",
    "        ddf = df[cols]\n",
    "        return ddf\n",
    "    \n",
    "    makeinput = lambda step,dids: df_to_torch(remove_decisions(dataset.get_input_state(step=step,ids=dids))).to(device)\n",
    "    thresh = lambda x: torch.sigmoid(100000000*(x - .5))\n",
    "\n",
    "    optimal_train,transitions_train = calc_optimal_decisions(dataset,train_ids,tmodel1,tmodel2,tmodel3,smodel3,\n",
    "                                           weights=weights,tweights=tweights,\n",
    "                                          )\n",
    "    optimal_test,transitions_test = calc_optimal_decisions(dataset,test_ids,tmodel1,tmodel2,tmodel3,smodel3,\n",
    "                                          weights=weights,tweights=tweights,\n",
    "                                         )\n",
    "    optimal_train = optimal_train.to(model.get_device())\n",
    "    optimal_test = optimal_test.to(model.get_device())\n",
    "    \n",
    "    #save the inputs from the whole dataset for future reference\n",
    "    if use_attention:\n",
    "        full_data = []\n",
    "        for mstep in [0,1,2]:\n",
    "            full_data_step = [baseline, get_dlt(min(mstep,1)),\n",
    "                         get_dlt(mstep),get_pd(mstep),get_nd(mstep),get_cc(mstep),get_mod(mstep)]\n",
    "            full_data_step = torch.cat([formatdf(fd,true_ids) for fd in full_data_step],axis=1)\n",
    "            full_data.append(full_data_step)\n",
    "        full_data = torch.stack(full_data)\n",
    "        model.save_memory(full_data.to(device))\n",
    "        print(full_data.shape)\n",
    "        \n",
    "    randchoice = lambda x: x[torch.randint(len(x),(1,))[0]]\n",
    "    tloss_func = torch.nn.TripletMarginLoss()\n",
    "    def get_tloss(row,step,yt,x,imitation=True):\n",
    "        if yt[:,step].std() < .001:\n",
    "            return torch.tensor([0]).to(device)\n",
    "        positive_idx= torch.nonzero(yt[:,step] == yt[row,step])\n",
    "        positive_idx = torch.stack([ii for ii in positive_idx if ii != row]).view(-1)\n",
    "        negative_idx = torch.tensor([ii for ii in range(x.shape[0]) if ii not in positive_idx and ii != row])\n",
    "        if len(positive_idx) < 1 or len(negative_idx) < 1:\n",
    "            print('no losses','n positive',len(positive_idx),'n negative',len(negative_idx),'yt',yt[row,step],'row',row,'step',step,'imitation',imitation,end='\\r')\n",
    "            return torch.tensor([0]).to(device)\n",
    "        positive = x[randchoice(positive_idx)]\n",
    "        negative = x[randchoice(negative_idx)]\n",
    "        anchor = x[row]\n",
    "        if use_attention:\n",
    "            [anchor_embedding,pos_embedding,neg_embedding] = [model.get_embedding(xx.view(1,-1),position=step,use_saved_memory=True) for xx in [anchor,positive,negative]]\n",
    "        else:    \n",
    "            [anchor_embedding,pos_embedding,neg_embedding] = [model.get_embedding(xx.view(1,-1),position=step,concatenate=False)[int(imitation)] for xx in [anchor,positive,negative]]\n",
    "        tloss = tloss_func(anchor_embedding,pos_embedding,neg_embedding)\n",
    "        return tloss\n",
    "    \n",
    "    def step(train=True):\n",
    "        if train:\n",
    "            model.train(True)\n",
    "            tmodel1.train(True)\n",
    "            tmodel2.train(True)\n",
    "            tmodel3.train(True)\n",
    "            optimizer.zero_grad()\n",
    "            ids = train_ids\n",
    "            y_opt = makegrad(optimal_train)\n",
    "        else:\n",
    "            ids = test_ids\n",
    "            model.eval()\n",
    "            tmodel1.eval()\n",
    "            tmodel2.eval()\n",
    "            tmodel3.eval()\n",
    "            y_opt = makegrad(optimal_test)\n",
    "            \n",
    "            \n",
    "        ytrain = df_to_torch(outcomedf.loc[ids]).to(device)\n",
    "        #imitation losses and decision 1\n",
    "        xxtrained = [baseline, get_dlt(0),get_dlt(0),get_pd(0),get_nd(0),get_cc(0),get_mod(0)]\n",
    "        xxtrain = torch.cat([formatdf(xx,ids) for xx in xxtrained],axis=1).to(device)\n",
    "        \n",
    "        use_memory = (not train) and validate_with_memory\n",
    "\n",
    "        o1 = model(xxtrain,position=0,use_saved_memory = use_memory)\n",
    "\n",
    "        decision1_imitation = o1[:,3]\n",
    "        \n",
    "        decision1_opt = o1[:,0]\n",
    "        if threshold_decisions:\n",
    "            decision1_opt = thresh(decision1_opt)\n",
    "\n",
    "        imitation_loss1 = bce(decision1_imitation,ytrain[:,0])\n",
    "        imitation_loss1 = torch.mul(imitation_loss1,imitation_weights[0])\n",
    "        \n",
    "        x1_imitation = [baseline, get_dlt(1),get_dlt(0),get_pd(1),get_nd(1),get_cc(1),get_mod(1)]\n",
    "        x1_imitation = [formatdf(xx1,ids) for xx1 in x1_imitation]\n",
    "        x1_imitation = torch.cat(x1_imitation,axis=1).to(device)\n",
    "        decision2_imitation = model(x1_imitation,position=1,use_saved_memory = use_memory)[:,4]\n",
    "\n",
    "        imitation_loss2 =  bce(decision2_imitation,ytrain[:,1])\n",
    "        imitation_loss2 = torch.mul(imitation_loss2,imitation_weights[1])\n",
    "        \n",
    "        x2_imitation = [baseline, get_dlt(1),get_dlt(2),get_pd(2),get_nd(2),get_cc(2),get_mod(2)]\n",
    "        \n",
    "        x2_imitation = [formatdf(xx2,ids) for xx2 in x2_imitation]\n",
    "        x2_imitation = torch.cat(x2_imitation,axis=1)\n",
    "        decision3_imitation = model(x2_imitation,position=2,use_saved_memory = use_memory)[:,5]\n",
    "\n",
    "        imitation_loss3 = bce(decision3_imitation,ytrain[:,2])\n",
    "        imitation_loss3 = torch.mul(imitation_loss3,imitation_weights[2])\n",
    "        \n",
    "        #reward decisions\n",
    "        xx1 = makeinput(1,ids)\n",
    "        xx2 = makeinput(2,ids)\n",
    "        xx3 = makeinput(3,ids)\n",
    "\n",
    "        xx1 = makegrad(xx1)\n",
    "        xx2 = makegrad(xx2)\n",
    "        xx3 = makegrad(xx3)\n",
    "        baseline_train_base = formatdf(baseline,ids)\n",
    "            \n",
    "        baseline_train = torch.clone(baseline_train_base)\n",
    "\n",
    "        \n",
    "        xi1 = torch.cat([xx1,decision1_opt.view(-1,1)],axis=1)\n",
    "        print(train,tmodel1.training,tmodel1.dropout.training)\n",
    "        [ypd1, ynd1, ymod, ydlt1] = tmodel1(xi1)['predictions']\n",
    "        print(train,tmodel1.training,tmodel1.dropout.training)\n",
    "        d1_thresh = torch.gt(decision1_opt.view(-1,1),.5).to(ypd1.device)\n",
    "        d1_scale = torch.cat([d1_thresh,d1_thresh,torch.ones(d1_thresh.view(-1,1).shape).to(ypd1.device)],dim=1)\n",
    "        ypd1= torch.mul(ypd1,d1_scale)\n",
    "        ynd1= torch.mul(ynd1,d1_scale)\n",
    "        \n",
    "        x1 = [baseline_train,ydlt1,formatdf(get_dlt(0),ids),ypd1,ynd1,formatdf(get_cc(1),ids),ymod]\n",
    "        x1= torch.cat([xx1.to(model.get_device()) for xx1 in x1],axis=1)\n",
    "        \n",
    "        decision2_opt = model(x1,position=1,use_saved_memory = use_memory)[:,1] \n",
    "        if threshold_decisions:\n",
    "            decision2_opt = thresh(decision2_opt)\n",
    "            \n",
    "        xi2 = torch.cat([xx2,decision1_opt.view(-1,1),decision2_opt.view(-1,1)],axis=1)\n",
    "        [ypd2,ynd2,ycc,ydlt2] = tmodel2(xi2)['predictions']\n",
    "\n",
    "        x2 = [baseline_train,ydlt1,ydlt2,ypd2,ynd2,ycc,ymod]\n",
    "        x2 = torch.cat([xx2.to(model.get_device()) for xx2 in x2],axis=1)\n",
    "        decision3_opt = model(x2,position=2,use_saved_memory = use_memory)[:,2]\n",
    "        \n",
    "        if threshold_decisions:\n",
    "            decision3_opt = thresh(decision3_opt)\n",
    "            \n",
    "        xi3 = torch.cat([xx3,decision1_opt.view(-1,1),decision2_opt.view(-1,1),decision3_opt.view(-1,1)],axis=1)\n",
    "        \n",
    "        outcomes = tmodel3(xi3)['predictions']\n",
    "        survival = smodel3.time_to_event(xi3,n_samples=1)\n",
    "        if not train and verbose:\n",
    "            print(torch.mean(outcomes,dim=0))\n",
    "            \n",
    "        reward_loss = torch.mean(outcome_loss(outcomes,weights) + temporal_loss(survival,tweights))\n",
    "        loss = torch.add(imitation_loss1,imitation_loss2)\n",
    "        loss = torch.add(loss,imitation_loss3)\n",
    "        loss = torch.mul(loss,imitation_weight/3)\n",
    "        loss = torch.add(loss,torch.mul(reward_loss,reward_weight))\n",
    "        \n",
    "        imitation_tloss = torch.FloatTensor([0]).to(device)\n",
    "        opt_tloss = torch.FloatTensor([0]).to(device)\n",
    "        n_rows = x1.shape[0]\n",
    "        if reward_triplet_weight + imitation_triplet_weight > 0.0001:\n",
    "            for i in range(n_rows):\n",
    "                #skip if we're using an attention model idk\n",
    "                if not use_attention and imitation_triplet_weight > .0001:\n",
    "                    imitation_tloss += get_tloss(i,0,ytrain,xxtrain,True)\n",
    "                    imitation_tloss += get_tloss(i,1,ytrain,x1_imitation,True)\n",
    "                    imitation_tloss += get_tloss(i,2,ytrain,x2_imitation,True)\n",
    "                if reward_triplet_weight > .0001:\n",
    "                    opt_tloss += get_tloss(i,0,y_opt,xxtrain,False)\n",
    "                    opt_tloss += get_tloss(i,1,y_opt,x1,False)\n",
    "                    opt_tloss += get_tloss(i,2,y_opt,x2,False)\n",
    "            loss += torch.mul(imitation_tloss[0],imitation_triplet_weight/n_rows)\n",
    "            loss += torch.mul(opt_tloss[0],reward_triplet_weight/n_rows)\n",
    "        \n",
    "        losses = [imitation_loss1+imitation_loss2+imitation_loss3,reward_loss,imitation_tloss,opt_tloss]\n",
    "\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            return losses\n",
    "        else:\n",
    "            scores = []\n",
    "            distributions = [decision1_opt.mean().item(),decision2_opt.mean().item(),decision3_opt.mean().item()]\n",
    "            imitation = [decision1_imitation,decision2_imitation,decision3_imitation]\n",
    "            optimal = [decision1_opt,decision2_opt,decision3_opt]\n",
    "            for i,decision_im in enumerate(imitation):\n",
    "                deci = decision_im.cpu().detach().numpy()\n",
    "                deci0 = (deci > .5).astype(int)\n",
    "                iout = ytrain[:,i].cpu().detach().numpy()\n",
    "                acci = accuracy_score(iout,deci0)\n",
    "                try:\n",
    "                    auci = roc_auc_score(iout,deci)\n",
    "                except:\n",
    "                    auci = -1\n",
    "                \n",
    "                deco = optimal[i].cpu().detach().numpy()\n",
    "                deci0 = (deco > .5).astype(int)\n",
    "                oout = y_opt[:,i].cpu().detach().numpy()\n",
    "                acco = accuracy_score(oout,deci0)\n",
    "                try:\n",
    "                    auco = roc_auc_score(oout,deco)\n",
    "                except:\n",
    "                    auco=-1\n",
    "                scores.append({'decision': i,'optimal_auc': auco,'imitation_auc': auci,'optimal_acc': acco,'imitation_acc': acci})\n",
    "            return losses, scores, distributions\n",
    "        \n",
    "    best_val_loss = torch.tensor(1000000000.0)\n",
    "    steps_since_improvement = 0\n",
    "    best_val_score = {}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        losses = step(True)\n",
    "        val_losses,val_metrics,val_distributions = step(False)\n",
    "        vl = val_losses[0] + val_losses[1]\n",
    "        for vm in val_metrics:\n",
    "            vl += (-((vm['optimal_auc']*reward_weight) + (vm['imitation_auc']*imitation_weight)))/10\n",
    "        if verbose:\n",
    "            print('______epoch',str(epoch),'_____')\n",
    "            print('val reward',val_losses[1].item())\n",
    "            print('imitation reward', val_losses[0].item())\n",
    "            if len(val_losses) > 2:\n",
    "                print('distance losses',val_losses[2].item(),val_losses[-1].item())\n",
    "            print('distributions',val_distributions)\n",
    "            print(val_metrics)\n",
    "        if vl < best_val_loss:\n",
    "            best_val_loss = vl\n",
    "            best_val_score = val_metrics\n",
    "            best_val_distributions = val_distributions\n",
    "            steps_since_improvement = 0\n",
    "            torch.save(model.state_dict(),save_file)\n",
    "        else:\n",
    "            steps_since_improvement += 1\n",
    "        if steps_since_improvement > patience:\n",
    "            break\n",
    "    print('++++++++++Final+++++++++++')\n",
    "    print('best',best_val_loss)\n",
    "    print(best_val_score)\n",
    "    model.load_state_dict(torch.load(save_file))\n",
    "    model.eval()\n",
    "    return model, best_val_score, best_val_loss, best_val_distributions\n",
    "\n",
    "from Models import *\n",
    "# args = {\n",
    "#     'hidden_layers': [50,50], \n",
    "#     'attention_heads': [2,2],\n",
    "#     'embed_size': 120, \n",
    "#     'dropout': 0.5, \n",
    "#     'input_dropout': 0.2, \n",
    "#     'shufflecol_chance':  0.2,\n",
    "# }\n",
    "args = {\n",
    "    'hidden_layers': [500], \n",
    "    'opt_layer_size': 20, \n",
    "    'imitation_layer_size': 20, \n",
    "    'dropout': 0.25, \n",
    "    'input_dropout': 0.25, \n",
    "    'shufflecol_chance': 0.5\n",
    "}\n",
    "from Models import *\n",
    "decision_model, _, _, _ = train_decision_model(\n",
    "    model1,model2,model3,smodel3,\n",
    "    lr=.001,\n",
    "    use_attention=True,\n",
    "    imitation_weight=1,\n",
    "    imitation_triplet_weight=0,\n",
    "    reward_triplet_weight=0,\n",
    "    reward_weight=2,\n",
    "    validate_with_memory=True,\n",
    "    use_smote=False,\n",
    "    **args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
