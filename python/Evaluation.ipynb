{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fa9329b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from Constants import *\n",
    "import simplejson\n",
    "from Preprocessing import *\n",
    "from Utils import *\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score,accuracy_score, precision_recall_fscore_support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0847c358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransitionEnsemble(\n",
       "  (base_models): ModuleList(\n",
       "    (0-3): 4 x OutcomeSimulator(\n",
       "      (input_dropout): Dropout(p=0.25, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=56, out_features=400, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=400, out_features=400, bias=True)\n",
       "        (3): ReLU()\n",
       "      )\n",
       "      (batchnorm): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "      (relu): Softplus(beta=1, threshold=20)\n",
       "      (sigmoid): Sigmoid()\n",
       "      (softmax): LogSoftmax(dim=1)\n",
       "      (disease_layer): Linear(in_features=400, out_features=3, bias=True)\n",
       "      (nodal_disease_layer): Linear(in_features=400, out_features=3, bias=True)\n",
       "      (dlt_layers): ModuleList(\n",
       "        (0-7): 8 x Linear(in_features=400, out_features=1, bias=True)\n",
       "      )\n",
       "      (treatment_layer): Linear(in_features=400, out_features=7, bias=True)\n",
       "    )\n",
       "    (4-7): 4 x OutcomeSimulator(\n",
       "      (input_dropout): Dropout(p=0.25, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=56, out_features=10, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "        (3): ReLU()\n",
       "      )\n",
       "      (batchnorm): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "      (relu): Softplus(beta=1, threshold=20)\n",
       "      (sigmoid): Sigmoid()\n",
       "      (softmax): LogSoftmax(dim=1)\n",
       "      (disease_layer): Linear(in_features=10, out_features=3, bias=True)\n",
       "      (nodal_disease_layer): Linear(in_features=10, out_features=3, bias=True)\n",
       "      (dlt_layers): ModuleList(\n",
       "        (0-7): 8 x Linear(in_features=10, out_features=1, bias=True)\n",
       "      )\n",
       "      (treatment_layer): Linear(in_features=10, out_features=7, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (error_models): ModuleList(\n",
       "    (0-19): 20 x OutcomeSimulator(\n",
       "      (input_dropout): Dropout(p=0.25, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=56, out_features=400, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=400, out_features=400, bias=True)\n",
       "        (3): ReLU()\n",
       "      )\n",
       "      (batchnorm): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "      (relu): Softplus(beta=1, threshold=20)\n",
       "      (sigmoid): Sigmoid()\n",
       "      (softmax): LogSoftmax(dim=1)\n",
       "      (disease_layer): Linear(in_features=400, out_features=3, bias=True)\n",
       "      (nodal_disease_layer): Linear(in_features=400, out_features=3, bias=True)\n",
       "      (dlt_layers): ModuleList(\n",
       "        (0-7): 8 x Linear(in_features=400, out_features=1, bias=True)\n",
       "      )\n",
       "      (treatment_layer): Linear(in_features=400, out_features=7, bias=True)\n",
       "    )\n",
       "    (20-39): 20 x OutcomeSimulator(\n",
       "      (input_dropout): Dropout(p=0.25, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=56, out_features=10, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "        (3): ReLU()\n",
       "      )\n",
       "      (batchnorm): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "      (relu): Softplus(beta=1, threshold=20)\n",
       "      (sigmoid): Sigmoid()\n",
       "      (softmax): LogSoftmax(dim=1)\n",
       "      (disease_layer): Linear(in_features=10, out_features=3, bias=True)\n",
       "      (nodal_disease_layer): Linear(in_features=10, out_features=3, bias=True)\n",
       "      (dlt_layers): ModuleList(\n",
       "        (0-7): 8 x Linear(in_features=10, out_features=1, bias=True)\n",
       "      )\n",
       "      (treatment_layer): Linear(in_features=10, out_features=7, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_model,transition_model1,transition_model2, outcome_model = load_models()\n",
    "dataset = DTDataset()\n",
    "transition_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1207ba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transition_sample(state,dataset=None):\n",
    "    if dataset is None:\n",
    "        dataset = DTDataset()\n",
    "        \n",
    "    ids = get_dt_ids(dataset.processed_df.reset_index())\n",
    "    \n",
    "    train_ids, test_ids = get_tt_split(dataset.processed_df.reset_index())\n",
    "    \n",
    "    #only train on people with  IC for state 1 since other people can't have any outcomes otherwise\n",
    "    require = None\n",
    "    if state == 1:\n",
    "        require = Const.decisions[0] #we don't expect a state update if there is no treatment\n",
    "        valid_ids = dataset.get_input_state(require=require).index.values\n",
    "        train_ids = [t for t in train_ids if t in valid_ids]\n",
    "        test_ids = [t for t in test_ids if t in valid_ids]\n",
    "    xtrain = dataset.get_input_state(step=state,ids=train_ids,require=require)\n",
    "    xtest = dataset.get_input_state(step=state,ids=test_ids,require=require)\n",
    "    ytrain = dataset.get_intermediate_outcomes(step=state,ids=train_ids,require=require)\n",
    "    ytest = dataset.get_intermediate_outcomes(step=state,ids=test_ids,require=require)\n",
    "\n",
    "    xtrain = df_to_torch(xtrain)\n",
    "    xtest = df_to_torch(xtest)\n",
    "    ytrain = [df_to_torch(t) for t in ytrain]\n",
    "    ytest= [df_to_torch(t) for t in ytest]\n",
    "    return xtrain,xtest,ytrain,ytest\n",
    "\n",
    "transition_sample(0,dataset)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411f01ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_metrics(yt,yp,numpy=False,is_dlt=False,is_squeezed=False):\n",
    "    if not numpy:\n",
    "        yt = yt.cpu().detach().numpy()\n",
    "        yp = yp.cpu().detach().numpy()\n",
    "    #dlt prediction (binary)\n",
    "    if is_dlt:\n",
    "        acc = accuracy_score(yt,yp>.5)\n",
    "        if yt.sum() > 1:\n",
    "            auc = roc_auc_score(yt,yp)\n",
    "        else:\n",
    "            auc=-1\n",
    "        error = np.mean((yt-yp)**2)\n",
    "        return {'accuracy': acc, 'mse': error, 'auc': auc}\n",
    "    #this is a catch for when I se the dlt prediction format (encoded integer ordinal, predict as a categorical and take the argmax)\n",
    "    elif yt.ndim > 1 or is_squeezed:\n",
    "        try:\n",
    "            bacc = balanced_accuracy_score(yt.argmax(axis=1),yp.argmax(axis=1))\n",
    "        except:\n",
    "            bacc = -1\n",
    "        try:\n",
    "            roc_micro = roc_auc_score(yt,yp,average='micro')\n",
    "        except:\n",
    "            roc_micro=-1\n",
    "        try:\n",
    "            roc_macro = roc_auc_score(yt,yp,average='macro')\n",
    "        except Exception as e:\n",
    "            try: \n",
    "                roc_macro = roc_auc_score(yt[:,0:2],yp[:,0:2],average='macro')\n",
    "            except:\n",
    "                roc_macro = -1\n",
    "        try:\n",
    "            roc_weighted = roc_auc_score(yt,yp,average='weighted')\n",
    "        except:\n",
    "            try:\n",
    "                roc_weighted = roc_auc_score(yt[:,0:2],yp[:,0:2],average='weighted')\n",
    "            except:\n",
    "                roc_weighted= -1\n",
    "        return {'accuracy': bacc, 'auc_micro': roc_micro,'auc_mean': roc_macro,'auc_weighted': roc_weighted}\n",
    "    #outcomes (binary)\n",
    "    else:\n",
    "        multiclass = yp.ndim > 1\n",
    "        if multiclass:\n",
    "            yp = yp.argmax(axis=1)\n",
    "        try:\n",
    "            if not multiclass:\n",
    "                bacc = accuracy_score(yt,(yp>.5).astype(int))\n",
    "            else:\n",
    "                bacc = accuracy_score(yt,yp)\n",
    "        except Exception as e:\n",
    "            print(e,yp,yt)\n",
    "            bacc = -1\n",
    "        try:\n",
    "            roc = roc_auc_score(yt,yp)\n",
    "        except:\n",
    "            roc = -1\n",
    "        try:\n",
    "            if not multiclass:\n",
    "                pr,re,fscore,supp = precision_recall_fscore_support(yt,(yp>.5).astype(int),average='binary')\n",
    "            else:\n",
    "                pr,re,fscore,supp = precision_recall_fscore_support(yt,yp,average='macro')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            [pr,re,fscore,supp] = [-1,-1,-1,-1]\n",
    "        error = np.mean((yt-yp)**2)\n",
    "        return {'accuracy': bacc, 'mse': error, 'auc': roc,'precision': pr,'recall':re,'f1':fscore}\n",
    "\n",
    "def state_metrics(ytrue,ypred,numpy=False):\n",
    "    pd_metrics = mc_metrics(ytrue[0],ypred[0],numpy=numpy)\n",
    "    nd_metrics = mc_metrics(ytrue[1],ypred[1],numpy=numpy)\n",
    "    mod_metrics = mc_metrics(ytrue[1],ypred[1],numpy=numpy)\n",
    "    \n",
    "    dlt_metrics = []\n",
    "    dlt_true = ytrue[3]\n",
    "    dlt_pred = ypred[3]\n",
    "    ndlt = dlt_true.shape[1]\n",
    "    nloss = torch.nn.NLLLoss()\n",
    "    for i in range(ndlt):\n",
    "        dm = mc_metrics(dlt_true[:,i],dlt_pred[:,i].view(-1),is_dlt=True)\n",
    "        dlt_metrics.append(dm)\n",
    "    dlt_acc =[d['accuracy'] for d in dlt_metrics]\n",
    "    dlt_error = [d['mse'] for d in dlt_metrics]\n",
    "    dlt_auc = [d['auc'] for d in dlt_metrics]\n",
    "    \n",
    "    acc_mean = np.mean([a for a in dlt_acc if a >= 0 and a < 1])\n",
    "    auc_mean = np.mean([a for a in dlt_auc if a >= 0])\n",
    "    results = {'pd': pd_metrics,'nd': nd_metrics,'mod': mod_metrics,\n",
    "               'dlts': {'accuracy': dlt_acc,'accuracy_mean': acc_mean,'auc': dlt_auc,'auc_mean': auc_mean}\n",
    "              }\n",
    "    return results\n",
    "def outcome_metrics(ytrue,ypred,numpy=False):\n",
    "    res = {}\n",
    "    for i, outcome in enumerate(Const.outcomes):\n",
    "        metrics = mc_metrics(ytrue[i],ypred[:,i])\n",
    "        res[outcome] = metrics\n",
    "    return res\n",
    "\n",
    "def evaluate_transition_model(model,state,ensemble=True,dataset=None):\n",
    "    [xtrain,xtest, ytrain,ytest] = transition_sample(state,dataset=dataset)\n",
    "    xtest = xtest.to(model.get_device())\n",
    "    ytest = [yy.to(model.get_device()) for yy in ytest]\n",
    "    if ensemble:\n",
    "        ypred = model(xtest)['predictions']\n",
    "    else:\n",
    "        ypred = model(xtest)\n",
    "    if state < 3:\n",
    "        metrics = state_metrics(ytest,ypred)\n",
    "    else:\n",
    "        metrics = outcome_metrics(ytest,ypred)\n",
    "    return metrics\n",
    "\n",
    "def format_main_result(res,as_df=True):\n",
    "    newres = {}\n",
    "    for key,value in res.items():\n",
    "        if key != 'dlts':\n",
    "            entry = {}\n",
    "            for key2, value2 in value.items():\n",
    "                if value2 > -1 and value2 < 1:\n",
    "                    entry[key2] = value2\n",
    "            newres[key] = entry\n",
    "        else:\n",
    "            for i,dltname in enumerate(Const.dlt1):\n",
    "                entry = {}\n",
    "                name = 'DLT-'+dltname\n",
    "                for key2, value2 in value.items():\n",
    "                    if type(value2) != type([]): \n",
    "                        continue\n",
    "                    name2 = key2 if key2 != 'auc' else 'auc_mean'\n",
    "                    newval = value2[i]\n",
    "                    if newval > -1 and newval < 1:\n",
    "                        entry[name2] = newval\n",
    "                newres[name] = entry\n",
    "    if as_df:\n",
    "        df = pd.DataFrame(newres).T\n",
    "        df = df.dropna(axis=0,how='all',subset = [c for c in df.columns if 'auc' in c])\n",
    "        return df.reindex(sorted(df.columns), axis=1)\n",
    "    return newres\n",
    "\n",
    "def aggregate_evaluations(res_list):\n",
    "    medians = {}\n",
    "    means = {}\n",
    "    lower = {}\n",
    "    upper = {}\n",
    "    nitems = len(res_list)\n",
    "    for key, entry in res_list[0].items():\n",
    "        if key != 'dlts':\n",
    "            mean_entry = {}\n",
    "            median_entry = {}\n",
    "            lentry = {}\n",
    "            uentry = {}\n",
    "            for key2,vv in entry.items():\n",
    "                values = [r[key][key2] for r in res_list]\n",
    "                values = [v for v in values if v >= 0 and v < 1]\n",
    "                val_mean = np.mean(values)\n",
    "                val_median = np.median(values)\n",
    "                val_cf = np.quantile(values,[.05,.95]) if len(values) > 1 else [np.NaN,np.NaN]\n",
    "                lentry[key2] = val_cf[0]\n",
    "                uentry[key2] = val_cf[1]\n",
    "                mean_entry[key2] = val_mean\n",
    "                median_entry[key2] = val_median\n",
    "                medians[key] = median_entry\n",
    "                means[key] = mean_entry\n",
    "                lower[key] = lentry\n",
    "                upper[key] = uentry\n",
    "        else:\n",
    "            for i,dltname in enumerate(Const.dlt1):\n",
    "                mean_entry = {}\n",
    "                median_entry = {}\n",
    "                lentry = {}\n",
    "                uentry = {}\n",
    "                name = 'DLT-'+dltname\n",
    "                for key2,vv in entry.items():\n",
    "                    if type(vv) != type([]):\n",
    "                        continue\n",
    "                    values = [r[key][key2][i] for r in res_list]\n",
    "                    values = [v for v in values if v >= 0 and v < 1]\n",
    "                    val_mean = np.mean(values)\n",
    "                    val_median = np.median(values)\n",
    "                    val_cf = np.quantile(values,[.05,.95]) if len(values) > 1 else [np.NaN,np.NaN]\n",
    "                    name2 = key2 if key2 != 'auc' else 'auc_mean'\n",
    "                    mean_entry[name2] = val_mean\n",
    "                    median_entry[name2] = val_median\n",
    "                    lentry[name2] = val_cf[0]\n",
    "                    uentry[name2] = val_cf[1]\n",
    "                    medians[name] = median_entry\n",
    "                    means[name] = mean_entry\n",
    "                    lower[name] = lentry\n",
    "                    upper[name] = uentry\n",
    "    suffixes = ['mean','median','5%','95%']\n",
    "    dfs = []\n",
    "    for suffix,data in zip(suffixes,[means,medians,lower,upper]):\n",
    "        newdf = pd.DataFrame(data).T\n",
    "        newdf.columns = [c+'-'+suffix for c in newdf.columns]\n",
    "        dfs.append(newdf)\n",
    "    #drop anything without and valid aucs isnce htats a bad class\n",
    "    df= pd.concat(dfs,axis=1).dropna(axis=0,how='all',subset = [c for c in dfs[0].columns if 'auc' in c])\n",
    "    return df.reindex(sorted(df.columns), axis=1)\n",
    "\n",
    "def evaluate_ensemble(emodel,state,dataset=None):\n",
    "    if dataset is None:\n",
    "        dataset = DTDataset()\n",
    "    overall_res = evaluate_transition_model(emodel,state,True,dataset=dataset)\n",
    "    model_res = [evaluate_transition_model(m,state,False,dataset=dataset) for m in emodel.base_models]\n",
    "    error_model_res = [evaluate_transition_model(m,state,False,dataset=dataset) for m in emodel.error_models]\n",
    "    return overall_res, model_res,error_model_res\n",
    "\n",
    "evaluate_ensemble(transition_model1,1,dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904d004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_ensemble_results(ores,mres,eres,concat=False):\n",
    "    res = [format_main_result(ores), aggregate_evaluations(mres),aggregate_evaluations(eres)]\n",
    "    if concat:\n",
    "        return pd.concat(res,axis=1)\n",
    "    return res\n",
    "# aggregate_ensemble_results(*results1,concat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ed387b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'LRC_auc'\n",
    "test[len(test)-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a840949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unravel_result(res):\n",
    "    res = format_main_result(res,as_df=False)\n",
    "    r2 = {}\n",
    "    for key,value in res.items():\n",
    "        for key2, value2 in value.items():\n",
    "            r2[key+'_'+key2] = value2\n",
    "    return r2\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "def vis_ensemble_results(resultlist,ax=None):\n",
    "    plotstuff = []\n",
    "    for r in resultlist[1]:\n",
    "        rr = unravel_result(r)\n",
    "        for k,v in rr.items():\n",
    "            entry = {'name': k,'value': v,'resampled': False}\n",
    "            plotstuff.append(entry)\n",
    "    for r in resultlist[-1]:\n",
    "        rr = unravel_result(r)\n",
    "        for k,v in rr.items():\n",
    "            entry = {'name': k,'value': v,'resampled': True}\n",
    "            plotstuff.append(entry)\n",
    "#     plotstuff = [v for v in plotstuff if 'accuracy' not in v['name'] and 'mse' not in v['name']]\n",
    "    plotstuff = [v for v in plotstuff if 'auc_mean' in v['name'] or v['name'][len(v['name'])-3:] == 'auc']\n",
    "    tempdf = pd.DataFrame(plotstuff)\n",
    "    if ax is None:\n",
    "        fig,ax = plt.subplots(1,1,figsize=(10,10))\n",
    "    sns.boxplot(\n",
    "        data=tempdf,\n",
    "        orient='h',\n",
    "        hue='resampled',\n",
    "        x='value',\n",
    "        y='name',\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_xlim(.3,1)\n",
    "fig,axes = plt.subplots(1,3,figsize=(60,20))\n",
    "for i,model in enumerate([transition_model1,transition_model2,outcome_model]):\n",
    "    results = evaluate_ensemble(model,i+1,dataset=dataset)\n",
    "    vis_ensemble_results(results,ax=axes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf69093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_decision_model(model,tmodel1,tmodel2,tmodel3,dataset=None):\n",
    "    if dataset is None:\n",
    "        dataset = DTDataset()\n",
    "        \n",
    "    data = dataset.processed_df.copy()\n",
    "    \n",
    "    train_ids, test_ids = get_tt_split(dataset)\n",
    "\n",
    "    def get_dlt(state):\n",
    "        if state == 2:\n",
    "            return data[Const.dlt2].copy()\n",
    "        d = data[Const.dlt1].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_pd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.primary_disease_states2].copy()\n",
    "        d = data[Const.primary_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_nd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.nodal_disease_states2].copy()\n",
    "        d = data[Const.nodal_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_cc(state):\n",
    "        res = data[Const.ccs].copy()\n",
    "        if state == 1:\n",
    "            res.values[:,:] = np.zeros(res.values.shape)\n",
    "        return res\n",
    "    \n",
    "    def get_mod(state):\n",
    "        res = data[Const.modifications].copy()\n",
    "        #this should have an ic condition but we don't use it anumore anywa\n",
    "        return res\n",
    "        \n",
    "    outcomedf = data[Const.outcomes]\n",
    "    baseline = dataset.get_state('baseline')\n",
    "    \n",
    "    def formatdf(d,dids=train_ids):\n",
    "        d = df_to_torch(d.loc[dids]).to(model.get_device())\n",
    "        return d\n",
    "    \n",
    "    def remove_decisions(df):\n",
    "        cols = [c for c in df.columns if c not in Const.decisions ]\n",
    "        ddf = df[cols]\n",
    "        return ddf\n",
    "    \n",
    "    makeinput = lambda step,dids: df_to_torch(remove_decisions(dataset.get_input_state(step=step,ids=dids)))\n",
    "    \n",
    "    y = df_to_torch(outcomedf.loc[test_ids])\n",
    "    xxtrained = [baseline, get_dlt(0),get_dlt(0),get_pd(0),get_nd(0),get_cc(0),get_mod(0)]\n",
    "    xxtrain = [formatdf(xx,test_ids) for xx in xxtrained]\n",
    "    o1 = model(torch.cat(xxtrain,axis=1),position=0,use_saved_memory = True)\n",
    "    decision1_imitation = o1[:,3]\n",
    "    \n",
    "    x1_imitation = [baseline, get_dlt(1),get_dlt(0),get_pd(1),get_nd(1),get_cc(1),get_mod(1)]\n",
    "    x1_imitation = [formatdf(xx1,test_ids) for xx1 in x1_imitation]\n",
    "    \n",
    "    decision2_imitation = model(torch.cat(x1_imitation,axis=1),position=1,use_saved_memory = True)[:,4]\n",
    "    \n",
    "    x2_imitation = [baseline, get_dlt(1),get_dlt(2),get_pd(2),get_nd(2),get_cc(2),get_mod(2)]\n",
    "        \n",
    "    x2_imitation = [formatdf(xx2,test_ids) for xx2 in x2_imitation]\n",
    "    decision3_imitation = model(torch.cat(x2_imitation,axis=1),position=2,use_saved_memory = True)[:,5]\n",
    "    \n",
    "    scores = []\n",
    "    for i,decision in enumerate([decision1_imitation,decision2_imitation,decision3_imitation]):\n",
    "        dec = decision.cpu().detach().numpy()\n",
    "        dec0 = (dec > .5).astype(int)\n",
    "        out = y[:,i].cpu().detach().numpy()\n",
    "        acc = accuracy_score(out,dec > .5)\n",
    "        auc = roc_auc_score(out,dec)\n",
    "        scores.append({'decision': i,'accuracy': acc,'auc': auc})\n",
    "    return scores\n",
    "\n",
    "eval_decision_model(decision_model,transition_model1,transition_model2,outcome_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "caad4cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pd1': {'accuracy': 0.45759103641456583,\n",
       "  'auc_micro': 0.8397712636967355,\n",
       "  'auc_mean': 0.8397712636967355,\n",
       "  'auc_weighted': 0.9018325128646307},\n",
       " 'nd1': {'accuracy': 0.6336633663366337,\n",
       "  'auc_micro': 0.8938885178930379,\n",
       "  'auc_mean': 0.8938885178930379,\n",
       "  'auc_weighted': 0.9273717569525023},\n",
       " 'pd2': {'accuracy': 0.3333333333333333,\n",
       "  'auc_micro': 0.5531864252611799,\n",
       "  'auc_mean': 0.5531864252611799,\n",
       "  'auc_weighted': 0.6030935647200405},\n",
       " 'nd2': {'accuracy': 0.35793573515092497,\n",
       "  'auc_micro': 0.5730755857932152,\n",
       "  'auc_mean': 0.5730755857932152,\n",
       "  'auc_weighted': 0.5310578838581503}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "def baseline_mc_metrics(yt,yp):\n",
    "    #this is a catch for when I se the dlt prediction format (encoded integer ordinal, predict as a categorical and take the argmax)\n",
    "    try:\n",
    "        bacc = balanced_accuracy_score(yt,np.argmax(yp,axis=1))\n",
    "    except Exception as e:\n",
    "        print('bacc',e)\n",
    "        bacc = -1\n",
    "    try:\n",
    "        roc_micro = roc_auc_score(yt,yp,average='macro',multi_class='ovr')\n",
    "    except Exception as e:\n",
    "        print('micro',e)\n",
    "        roc_micro = -1\n",
    "    try:\n",
    "        roc_macro = roc_auc_score(yt,yp,average='macro',multi_class='ovr')\n",
    "    except Exception as e:\n",
    "        print('macro',e)\n",
    "        roc_macro = -1\n",
    "    try:\n",
    "        roc_weighted = roc_auc_score(yt,yp,average='weighted',multi_class='ovr')\n",
    "    except Exception as e:\n",
    "        print('weighted',e)\n",
    "        roc_weighted= -1\n",
    "    return {'accuracy': bacc,'auc_micro':roc_micro,'auc_mean':roc_macro,'auc_weighted':roc_weighted}\n",
    "    \n",
    "def mc_svc_baseline(dataset,outcome='pd_states1',class_weight='balanced',**svc_args):\n",
    "    state = 0\n",
    "    if outcome in ['pd_states1','nd_states1']:\n",
    "        state = 1\n",
    "    elif outcome in ['pd_states2','nd_states2']:\n",
    "        state = 2\n",
    "    else:\n",
    "        print('nope')\n",
    "        return\n",
    "    train_ids, test_ids = get_tt_split(dataset)\n",
    "    xtrain = dataset.get_input_state(step=state,ids=train_ids)\n",
    "    xtest = dataset.get_input_state(step=state,ids=test_ids)\n",
    "    ytrain = dataset.get_state(outcome,ids=train_ids)\n",
    "    ytest= dataset.get_state(outcome,ids=test_ids)\n",
    "    ytrain = np.argmax(ytrain.values,axis=1)\n",
    "    ytest = np.argmax(ytest.values,axis=1)\n",
    "        \n",
    "    normalize = lambda x: (x - xtrain.values.mean(axis=0))/(xtrain.values.std(axis=0)+.0001)\n",
    "    model = SVC(**svc_args,probability=True,class_weight=class_weight,random_state=0)\n",
    "    model = model.fit(xtrain,ytrain)\n",
    "    ypred= model.predict_proba(xtest)\n",
    "    return baseline_mc_metrics(ytest,ypred)\n",
    "\n",
    "def eval_svcs_progression(dataset,**args):\n",
    "    res = {}\n",
    "    for state in [1,2]:\n",
    "        for outcome in ['pd','nd']:\n",
    "            name = outcome+'_states'+str(state)\n",
    "            results = mc_svc_baseline(dataset,outcome=outcome+'_states'+str(state),**args)\n",
    "            res[outcome+str(state)] = results\n",
    "    return res\n",
    "\n",
    "eval_svcs_progression(dataset, kernel='linear', class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba932f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310c6db5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
