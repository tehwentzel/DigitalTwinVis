{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "050c878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7418d82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score,accuracy_score,precision_recall_fscore_support\n",
    "from Constants import *\n",
    "from Preprocessing import *\n",
    "from Models import *\n",
    "import copy\n",
    "from Utils import *\n",
    "from DeepSurvivalModels import *\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c427599a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(536, 61)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = DTDataset(use_smote=False)\n",
    "data.processed_df.T\n",
    "data.get_input_state(1).shape\n",
    "# data.processed_df#.shape, len(data.processed_df.index.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "055e18c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hpv</th>\n",
       "      <th>age</th>\n",
       "      <th>packs_per_year</th>\n",
       "      <th>gender</th>\n",
       "      <th>Aspiration rate Pre-therapy</th>\n",
       "      <th>total_dose</th>\n",
       "      <th>dose_fraction</th>\n",
       "      <th>OS (Calculated)</th>\n",
       "      <th>Locoregional control (Time)</th>\n",
       "      <th>FDM (months)</th>\n",
       "      <th>...</th>\n",
       "      <th>4_ipsi</th>\n",
       "      <th>4_contra</th>\n",
       "      <th>5A_ipsi</th>\n",
       "      <th>5A_contra</th>\n",
       "      <th>5B_ipsi</th>\n",
       "      <th>5B_contra</th>\n",
       "      <th>6_ipsi</th>\n",
       "      <th>6_contra</th>\n",
       "      <th>RPLN_ipsi</th>\n",
       "      <th>RPLN_contra</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>55.969444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>66.00</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>6.033333</td>\n",
       "      <td>4.700000</td>\n",
       "      <td>6.033333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>20.950000</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>72.00</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>69.930556</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>70.00</td>\n",
       "      <td>2.121212</td>\n",
       "      <td>7.466667</td>\n",
       "      <td>7.466667</td>\n",
       "      <td>7.466667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>72.319444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>70.00</td>\n",
       "      <td>2.121212</td>\n",
       "      <td>7.800000</td>\n",
       "      <td>7.800000</td>\n",
       "      <td>7.800000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>59.730556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>66.00</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>8.066667</td>\n",
       "      <td>8.066667</td>\n",
       "      <td>8.066667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10201</th>\n",
       "      <td>1</td>\n",
       "      <td>49.566667</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>70.00</td>\n",
       "      <td>2.121212</td>\n",
       "      <td>143.200000</td>\n",
       "      <td>143.200000</td>\n",
       "      <td>143.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10202</th>\n",
       "      <td>0</td>\n",
       "      <td>48.705556</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>72.00</td>\n",
       "      <td>1.714286</td>\n",
       "      <td>144.366667</td>\n",
       "      <td>144.366667</td>\n",
       "      <td>144.366667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10203</th>\n",
       "      <td>1</td>\n",
       "      <td>77.116667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>70.00</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>148.366667</td>\n",
       "      <td>148.366667</td>\n",
       "      <td>136.033333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10204</th>\n",
       "      <td>0</td>\n",
       "      <td>45.950000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>69.96</td>\n",
       "      <td>2.120000</td>\n",
       "      <td>152.600000</td>\n",
       "      <td>152.600000</td>\n",
       "      <td>152.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10205</th>\n",
       "      <td>1</td>\n",
       "      <td>49.733333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>69.96</td>\n",
       "      <td>2.120000</td>\n",
       "      <td>155.533333</td>\n",
       "      <td>155.533333</td>\n",
       "      <td>155.533333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>536 rows × 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       hpv        age  packs_per_year  gender  Aspiration rate Pre-therapy  \\\n",
       "id                                                                           \n",
       "3        1  55.969444             0.0       1                            0   \n",
       "5        0  20.950000            38.0       1                            0   \n",
       "6        1  69.930556            35.0       0                            1   \n",
       "7        1  72.319444             0.0       1                            0   \n",
       "8        1  59.730556             0.0       1                            0   \n",
       "...    ...        ...             ...     ...                          ...   \n",
       "10201    1  49.566667            30.0       1                            0   \n",
       "10202    0  48.705556            30.0       1                            0   \n",
       "10203    1  77.116667             0.0       1                            0   \n",
       "10204    0  45.950000             5.0       1                            0   \n",
       "10205    1  49.733333             0.0       1                            0   \n",
       "\n",
       "       total_dose  dose_fraction  OS (Calculated)  \\\n",
       "id                                                  \n",
       "3           66.00       2.200000         6.033333   \n",
       "5           72.00       1.800000         7.333333   \n",
       "6           70.00       2.121212         7.466667   \n",
       "7           70.00       2.121212         7.800000   \n",
       "8           66.00       2.200000         8.066667   \n",
       "...           ...            ...              ...   \n",
       "10201       70.00       2.121212       143.200000   \n",
       "10202       72.00       1.714286       144.366667   \n",
       "10203       70.00       2.333333       148.366667   \n",
       "10204       69.96       2.120000       152.600000   \n",
       "10205       69.96       2.120000       155.533333   \n",
       "\n",
       "       Locoregional control (Time)  FDM (months)  ...  4_ipsi  4_contra  \\\n",
       "id                                                ...                     \n",
       "3                         4.700000      6.033333  ...     0.0       0.0   \n",
       "5                         7.333333      7.333333  ...     0.0       0.0   \n",
       "6                         7.466667      7.466667  ...     0.0       0.0   \n",
       "7                         7.800000      7.800000  ...     0.0       0.0   \n",
       "8                         8.066667      8.066667  ...     0.0       0.0   \n",
       "...                            ...           ...  ...     ...       ...   \n",
       "10201                   143.200000    143.200000  ...     0.0       0.0   \n",
       "10202                   144.366667    144.366667  ...     0.0       0.0   \n",
       "10203                   148.366667    136.033333  ...     0.0       0.0   \n",
       "10204                   152.600000    152.600000  ...     0.0       0.0   \n",
       "10205                   155.533333    155.533333  ...     0.0       0.0   \n",
       "\n",
       "       5A_ipsi  5A_contra  5B_ipsi  5B_contra  6_ipsi  6_contra  RPLN_ipsi  \\\n",
       "id                                                                           \n",
       "3          0.0        0.0      0.0        0.0     0.0       0.0        0.0   \n",
       "5          0.0        0.0      0.0        0.0     0.0       0.0        0.0   \n",
       "6          0.0        0.0      0.0        0.0     0.0       0.0        0.0   \n",
       "7          0.0        0.0      0.0        0.0     0.0       0.0        0.0   \n",
       "8          0.0        0.0      0.0        0.0     0.0       0.0        0.0   \n",
       "...        ...        ...      ...        ...     ...       ...        ...   \n",
       "10201      0.0        0.0      0.0        0.0     0.0       0.0        0.0   \n",
       "10202      0.0        0.0      0.0        0.0     0.0       0.0        0.0   \n",
       "10203      0.0        0.0      0.0        0.0     0.0       0.0        0.0   \n",
       "10204      0.0        0.0      0.0        0.0     0.0       0.0        0.0   \n",
       "10205      0.0        0.0      0.0        0.0     0.0       0.0        0.0   \n",
       "\n",
       "       RPLN_contra  \n",
       "id                  \n",
       "3              0.0  \n",
       "5              0.0  \n",
       "6              0.0  \n",
       "7              0.0  \n",
       "8              0.0  \n",
       "...            ...  \n",
       "10201          0.0  \n",
       "10202          0.0  \n",
       "10203          0.0  \n",
       "10204          0.0  \n",
       "10205          0.0  \n",
       "\n",
       "[536 rows x 109 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2937b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DSM(\n",
       "  (act): Tanh()\n",
       "  (shape): ParameterList(\n",
       "      (0): Parameter containing: [torch.float32 of size 6]\n",
       "      (1): Parameter containing: [torch.float32 of size 6]\n",
       "      (2): Parameter containing: [torch.float32 of size 6]\n",
       "      (3): Parameter containing: [torch.float32 of size 6]\n",
       "  )\n",
       "  (scale): ParameterList(\n",
       "      (0): Parameter containing: [torch.float32 of size 6]\n",
       "      (1): Parameter containing: [torch.float32 of size 6]\n",
       "      (2): Parameter containing: [torch.float32 of size 6]\n",
       "      (3): Parameter containing: [torch.float32 of size 6]\n",
       "  )\n",
       "  (gate): ModuleList(\n",
       "    (0-3): 4 x Sequential(\n",
       "      (0): Linear(in_features=103, out_features=6, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (scaleg): ModuleList(\n",
       "    (0-3): 4 x Sequential(\n",
       "      (0): Linear(in_features=103, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (shapeg): ModuleList(\n",
       "    (0-3): 4 x Sequential(\n",
       "      (0): Linear(in_features=103, out_features=6, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (embedding): Sequential(\n",
       "    (0): Linear(in_features=78, out_features=100, bias=False)\n",
       "    (1): Tanh()\n",
       "  )\n",
       "  (input_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (embedding_dropout): Dropout(p=0.5, inplace=False)\n",
       "  (squish): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Utils import *\n",
    "model1,model2,model3,smodel3 = load_transition_models()\n",
    "smodel3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b29d259b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([115.,   1., 137.]) 147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 1., 0.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 1.]]),\n",
       " {'pd1': tensor([[2.0924e-01, 7.9076e-01, 1.5778e-22],\n",
       "          [9.6747e-01, 3.2288e-02, 2.4502e-04],\n",
       "          [1.3833e-02, 9.8411e-01, 2.0531e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [4.7540e-01, 5.0797e-01, 1.6636e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [8.2756e-01, 1.6092e-01, 1.1516e-02],\n",
       "          [5.2761e-01, 4.5284e-01, 1.9554e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [7.6993e-01, 2.1634e-01, 1.3737e-02],\n",
       "          [6.9413e-01, 2.9794e-01, 7.9282e-03],\n",
       "          [4.4932e-01, 5.2751e-01, 2.3169e-02],\n",
       "          [7.2501e-01, 2.6165e-01, 1.3339e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [6.2162e-01, 3.5363e-01, 2.4759e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [5.0773e-01, 4.7543e-01, 1.6838e-02],\n",
       "          [8.3213e-01, 1.5390e-01, 1.3973e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [2.3580e-01, 7.5268e-01, 1.1527e-02],\n",
       "          [7.5501e-01, 2.2418e-01, 2.0812e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [8.8014e-01, 1.1986e-01, 5.6567e-22],\n",
       "          [4.0385e-01, 5.7868e-01, 1.7465e-02],\n",
       "          [9.4176e-01, 5.8245e-02, 3.6821e-22],\n",
       "          [7.5958e-01, 2.3356e-01, 6.8564e-03],\n",
       "          [6.7933e-01, 3.0655e-01, 1.4119e-02],\n",
       "          [4.8912e-01, 4.9384e-01, 1.7042e-02],\n",
       "          [8.8674e-01, 1.0067e-01, 1.2587e-02],\n",
       "          [7.6059e-01, 2.2149e-01, 1.7919e-02],\n",
       "          [8.1435e-01, 1.6960e-01, 1.6054e-02],\n",
       "          [9.2053e-01, 7.1608e-02, 7.8604e-03],\n",
       "          [7.1261e-01, 2.7092e-01, 1.6469e-02],\n",
       "          [1.8779e-02, 9.8122e-01, 4.6740e-23],\n",
       "          [5.6754e-01, 4.1812e-01, 1.4345e-02],\n",
       "          [8.3620e-01, 1.5111e-01, 1.2693e-02],\n",
       "          [7.4944e-01, 2.4440e-01, 6.1631e-03],\n",
       "          [5.4051e-01, 4.4400e-01, 1.5488e-02],\n",
       "          [7.4166e-01, 2.4133e-01, 1.7005e-02],\n",
       "          [1.3647e-01, 8.6247e-01, 1.0648e-03],\n",
       "          [1.7628e-01, 8.1391e-01, 9.8069e-03],\n",
       "          [2.3775e-01, 7.5617e-01, 6.0754e-03],\n",
       "          [6.3307e-01, 3.4741e-01, 1.9521e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [8.7960e-01, 1.1291e-01, 7.4926e-03],\n",
       "          [8.7422e-01, 1.1344e-01, 1.2334e-02],\n",
       "          [3.3553e-01, 6.5019e-01, 1.4280e-02],\n",
       "          [3.1154e-01, 6.8431e-01, 4.1533e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [2.2839e-01, 7.5219e-01, 1.9423e-02],\n",
       "          [6.3762e-01, 3.3962e-01, 2.2761e-02],\n",
       "          [4.8071e-01, 5.0211e-01, 1.7184e-02],\n",
       "          [9.7875e-02, 8.9396e-01, 8.1613e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [3.4193e-01, 6.3715e-01, 2.0920e-02],\n",
       "          [4.5843e-01, 5.4091e-01, 6.6228e-04],\n",
       "          [7.9055e-01, 1.9434e-01, 1.5102e-02],\n",
       "          [8.9960e-01, 8.8408e-02, 1.1989e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [8.5611e-01, 1.3306e-01, 1.0835e-02],\n",
       "          [8.6473e-01, 1.2377e-01, 1.1497e-02],\n",
       "          [8.2564e-01, 1.7436e-01, 3.8708e-22],\n",
       "          [7.9839e-01, 1.8647e-01, 1.5138e-02],\n",
       "          [7.7723e-01, 2.0445e-01, 1.8328e-02],\n",
       "          [2.2395e-01, 7.6226e-01, 1.3786e-02],\n",
       "          [9.1250e-01, 7.8433e-02, 9.0689e-03],\n",
       "          [8.0906e-01, 1.7823e-01, 1.2702e-02],\n",
       "          [1.3869e-01, 8.5251e-01, 8.8032e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [7.9230e-01, 1.9083e-01, 1.6868e-02],\n",
       "          [8.6252e-01, 1.2543e-01, 1.2045e-02],\n",
       "          [4.3284e-01, 5.5938e-01, 7.7763e-03],\n",
       "          [5.7393e-01, 4.1822e-01, 7.8540e-03],\n",
       "          [5.3008e-03, 9.9351e-01, 1.1924e-03],\n",
       "          [9.3820e-02, 8.9806e-01, 8.1226e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [5.4076e-01, 4.4211e-01, 1.7131e-02],\n",
       "          [6.2028e-01, 3.5956e-01, 2.0166e-02],\n",
       "          [2.5762e-01, 7.2602e-01, 1.6357e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [8.4231e-01, 1.4851e-01, 9.1807e-03],\n",
       "          [7.6312e-01, 2.2372e-01, 1.3165e-02],\n",
       "          [4.9383e-01, 4.8944e-01, 1.6728e-02],\n",
       "          [5.0024e-01, 4.9279e-01, 6.9713e-03],\n",
       "          [7.7167e-01, 2.1252e-01, 1.5807e-02],\n",
       "          [5.2694e-02, 9.4731e-01, 9.2817e-23],\n",
       "          [6.7730e-01, 3.0333e-01, 1.9367e-02],\n",
       "          [7.8206e-01, 2.0545e-01, 1.2488e-02],\n",
       "          [2.0938e-01, 7.7698e-01, 1.3640e-02],\n",
       "          [6.9277e-01, 2.9013e-01, 1.7098e-02],\n",
       "          [2.7415e-01, 7.2500e-01, 8.5402e-04],\n",
       "          [1.2617e-01, 8.6914e-01, 4.6946e-03],\n",
       "          [7.7878e-01, 2.1004e-01, 1.1189e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [8.2540e-01, 1.5945e-01, 1.5152e-02],\n",
       "          [2.0765e-02, 9.7696e-01, 2.2759e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [9.1597e-01, 7.5263e-02, 8.7636e-03],\n",
       "          [7.7147e-01, 2.1120e-01, 1.7332e-02],\n",
       "          [1.5720e-01, 8.3794e-01, 4.8637e-03],\n",
       "          [7.7217e-01, 2.1356e-01, 1.4272e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [7.9946e-01, 1.8515e-01, 1.5393e-02],\n",
       "          [4.4869e-01, 5.3269e-01, 1.8620e-02],\n",
       "          [7.8812e-01, 2.0020e-01, 1.1678e-02],\n",
       "          [6.5835e-01, 3.2470e-01, 1.6947e-02],\n",
       "          [3.0284e-01, 6.8470e-01, 1.2469e-02],\n",
       "          [8.4225e-01, 1.4382e-01, 1.3928e-02],\n",
       "          [6.5390e-01, 3.2371e-01, 2.2390e-02],\n",
       "          [2.6681e-01, 7.1431e-01, 1.8878e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [6.0520e-01, 3.7331e-01, 2.1485e-02],\n",
       "          [6.3474e-01, 3.4652e-01, 1.8740e-02],\n",
       "          [4.2296e-01, 5.5539e-01, 2.1645e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [2.6982e-01, 7.1840e-01, 1.1787e-02],\n",
       "          [6.7638e-01, 3.0921e-01, 1.4411e-02],\n",
       "          [1.2924e-01, 8.5861e-01, 1.2152e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [5.4087e-01, 4.3564e-01, 2.3485e-02],\n",
       "          [7.9781e-01, 1.8902e-01, 1.3166e-02],\n",
       "          [9.0081e-01, 8.7357e-02, 1.1837e-02],\n",
       "          [9.0182e-02, 9.0393e-01, 5.8860e-03],\n",
       "          [2.3781e-01, 7.5606e-01, 6.1234e-03],\n",
       "          [7.0272e-01, 2.9284e-01, 4.4428e-03],\n",
       "          [7.9357e-01, 1.9126e-01, 1.5165e-02],\n",
       "          [5.4260e-01, 4.4631e-01, 1.1093e-02],\n",
       "          [4.8694e-01, 4.9601e-01, 1.7045e-02],\n",
       "          [2.9372e-01, 6.8539e-01, 2.0886e-02],\n",
       "          [7.2824e-01, 2.5766e-01, 1.4101e-02],\n",
       "          [7.2282e-01, 2.5830e-01, 1.8885e-02],\n",
       "          [8.3273e-01, 1.5327e-01, 1.4002e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [9.0608e-01, 9.3350e-02, 5.6693e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [5.3325e-01, 4.5324e-01, 1.3502e-02],\n",
       "          [4.2085e-01, 5.5723e-01, 2.1912e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01]], grad_fn=<CopySlices>),\n",
       "  'nd1': tensor([[2.2124e-34, 1.0000e+00, 2.2124e-34],\n",
       "          [4.6167e-05, 9.9991e-01, 4.5898e-05],\n",
       "          [2.1214e-03, 9.9576e-01, 2.1161e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [4.0973e-03, 9.9179e-01, 4.1104e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [4.6644e-03, 9.9065e-01, 4.6825e-03],\n",
       "          [5.5287e-03, 9.8898e-01, 5.4912e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [7.3592e-03, 9.8532e-01, 7.3197e-03],\n",
       "          [1.7947e-03, 9.9642e-01, 1.7883e-03],\n",
       "          [8.8949e-03, 9.8236e-01, 8.7480e-03],\n",
       "          [3.9735e-03, 9.9204e-01, 3.9848e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [1.0458e-02, 9.7923e-01, 1.0315e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [4.3402e-03, 9.9141e-01, 4.2492e-03],\n",
       "          [8.0328e-03, 9.8404e-01, 7.9269e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [3.9928e-03, 9.9204e-01, 3.9623e-03],\n",
       "          [8.9904e-03, 9.8212e-01, 8.8941e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [1.4630e-31, 1.0000e+00, 1.4630e-31],\n",
       "          [5.8860e-03, 9.8832e-01, 5.7946e-03],\n",
       "          [1.2170e-31, 1.0000e+00, 1.2170e-31],\n",
       "          [1.5123e-03, 9.9700e-01, 1.4900e-03],\n",
       "          [4.7974e-03, 9.9043e-01, 4.7696e-03],\n",
       "          [4.4441e-03, 9.9111e-01, 4.4507e-03],\n",
       "          [6.8379e-03, 9.8634e-01, 6.8251e-03],\n",
       "          [8.1959e-03, 9.8378e-01, 8.0216e-03],\n",
       "          [8.6242e-03, 9.8283e-01, 8.5410e-03],\n",
       "          [6.5535e-03, 9.8693e-01, 6.5164e-03],\n",
       "          [4.6789e-03, 9.9066e-01, 4.6567e-03],\n",
       "          [2.2424e-34, 1.0000e+00, 2.2424e-34],\n",
       "          [4.9114e-03, 9.9026e-01, 4.8314e-03],\n",
       "          [6.3983e-03, 9.8723e-01, 6.3719e-03],\n",
       "          [1.8317e-03, 9.9633e-01, 1.8361e-03],\n",
       "          [4.0439e-03, 9.9192e-01, 4.0394e-03],\n",
       "          [5.7265e-03, 9.8858e-01, 5.6975e-03],\n",
       "          [1.2475e-04, 9.9975e-01, 1.2466e-04],\n",
       "          [2.9387e-03, 9.9414e-01, 2.9196e-03],\n",
       "          [1.2702e-03, 9.9746e-01, 1.2706e-03],\n",
       "          [5.5958e-03, 9.8881e-01, 5.5908e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [2.3402e-03, 9.9532e-01, 2.3377e-03],\n",
       "          [7.9336e-03, 9.8431e-01, 7.7577e-03],\n",
       "          [5.1509e-03, 9.8978e-01, 5.0671e-03],\n",
       "          [7.0059e-04, 9.9860e-01, 6.9850e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [8.3397e-03, 9.8352e-01, 8.1428e-03],\n",
       "          [9.3784e-03, 9.8136e-01, 9.2570e-03],\n",
       "          [4.4516e-03, 9.9109e-01, 4.4607e-03],\n",
       "          [4.2970e-03, 9.9141e-01, 4.2972e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [7.8168e-03, 9.8442e-01, 7.7637e-03],\n",
       "          [3.6341e-05, 9.9993e-01, 3.6228e-05],\n",
       "          [5.5178e-03, 9.8906e-01, 5.4210e-03],\n",
       "          [7.6071e-03, 9.8484e-01, 7.5511e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [7.3490e-03, 9.8540e-01, 7.2499e-03],\n",
       "          [7.1315e-03, 9.8586e-01, 7.0035e-03],\n",
       "          [8.9138e-32, 1.0000e+00, 8.9138e-32],\n",
       "          [5.3866e-03, 9.8921e-01, 5.4025e-03],\n",
       "          [8.4673e-03, 9.8316e-01, 8.3756e-03],\n",
       "          [6.2568e-03, 9.8757e-01, 6.1763e-03],\n",
       "          [5.7188e-03, 9.8865e-01, 5.6347e-03],\n",
       "          [6.7161e-03, 9.8672e-01, 6.5658e-03],\n",
       "          [4.3343e-03, 9.9136e-01, 4.3009e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [8.1926e-03, 9.8369e-01, 8.1222e-03],\n",
       "          [6.8866e-03, 9.8627e-01, 6.8448e-03],\n",
       "          [1.9611e-03, 9.9611e-01, 1.9300e-03],\n",
       "          [2.4347e-03, 9.9514e-01, 2.4247e-03],\n",
       "          [2.2680e-03, 9.9548e-01, 2.2562e-03],\n",
       "          [3.8439e-03, 9.9234e-01, 3.8196e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [5.1017e-03, 9.8977e-01, 5.1238e-03],\n",
       "          [7.9501e-03, 9.8409e-01, 7.9570e-03],\n",
       "          [5.6330e-03, 9.8874e-01, 5.6279e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [3.7088e-03, 9.9265e-01, 3.6394e-03],\n",
       "          [5.0341e-03, 9.8992e-01, 5.0490e-03],\n",
       "          [5.2589e-03, 9.8955e-01, 5.1914e-03],\n",
       "          [9.9882e-04, 9.9800e-01, 1.0048e-03],\n",
       "          [5.1365e-03, 9.8970e-01, 5.1601e-03],\n",
       "          [3.4766e-34, 1.0000e+00, 3.4766e-34],\n",
       "          [7.6824e-03, 9.8476e-01, 7.5535e-03],\n",
       "          [4.9960e-03, 9.9000e-01, 5.0065e-03],\n",
       "          [5.5926e-03, 9.8885e-01, 5.5544e-03],\n",
       "          [5.3990e-03, 9.8924e-01, 5.3616e-03],\n",
       "          [7.0718e-05, 9.9986e-01, 7.0557e-05],\n",
       "          [1.0871e-03, 9.9783e-01, 1.0798e-03],\n",
       "          [4.8529e-03, 9.9041e-01, 4.7383e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [8.5323e-03, 9.8303e-01, 8.4378e-03],\n",
       "          [1.8789e-03, 9.9625e-01, 1.8730e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [5.0437e-03, 9.8995e-01, 5.0033e-03],\n",
       "          [8.1959e-03, 9.8378e-01, 8.0217e-03],\n",
       "          [1.7599e-03, 9.9649e-01, 1.7541e-03],\n",
       "          [5.2006e-03, 9.8960e-01, 5.2018e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [5.7253e-03, 9.8866e-01, 5.6101e-03],\n",
       "          [5.6814e-03, 9.8868e-01, 5.6376e-03],\n",
       "          [4.8607e-03, 9.9032e-01, 4.8198e-03],\n",
       "          [4.7803e-03, 9.9047e-01, 4.7515e-03],\n",
       "          [3.8401e-03, 9.9230e-01, 3.8581e-03],\n",
       "          [6.1678e-03, 9.8768e-01, 6.1526e-03],\n",
       "          [7.9100e-03, 9.8426e-01, 7.8343e-03],\n",
       "          [6.9594e-03, 9.8607e-01, 6.9716e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [8.7566e-03, 9.8254e-01, 8.6986e-03],\n",
       "          [6.1931e-03, 9.8762e-01, 6.1914e-03],\n",
       "          [7.6278e-03, 9.8484e-01, 7.5331e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [3.8733e-03, 9.9227e-01, 3.8586e-03],\n",
       "          [6.4389e-03, 9.8732e-01, 6.2365e-03],\n",
       "          [6.1957e-03, 9.8783e-01, 5.9744e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [8.2988e-03, 9.8346e-01, 8.2399e-03],\n",
       "          [5.7031e-03, 9.8864e-01, 5.6594e-03],\n",
       "          [7.5607e-03, 9.8494e-01, 7.5037e-03],\n",
       "          [3.1342e-03, 9.9375e-01, 3.1114e-03],\n",
       "          [1.2159e-03, 9.9758e-01, 1.2046e-03],\n",
       "          [9.4322e-04, 9.9813e-01, 9.3113e-04],\n",
       "          [6.6422e-03, 9.8682e-01, 6.5397e-03],\n",
       "          [2.6470e-03, 9.9470e-01, 2.6482e-03],\n",
       "          [4.4433e-03, 9.9111e-01, 4.4501e-03],\n",
       "          [6.7831e-03, 9.8658e-01, 6.6373e-03],\n",
       "          [6.1602e-03, 9.8774e-01, 6.0970e-03],\n",
       "          [8.9374e-03, 9.8226e-01, 8.7985e-03],\n",
       "          [7.8374e-03, 9.8443e-01, 7.7330e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [6.1535e-05, 9.9988e-01, 6.1463e-05],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [4.7100e-03, 9.9058e-01, 4.7103e-03],\n",
       "          [8.3949e-03, 9.8344e-01, 8.1607e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01]], grad_fn=<CopySlices>),\n",
       "  'nd2': tensor([[8.8885e-01, 1.0284e-01, 8.3092e-03],\n",
       "          [8.3659e-01, 1.6291e-01, 5.0229e-04],\n",
       "          [6.5342e-01, 3.3993e-01, 6.6457e-03],\n",
       "          [6.2856e-01, 3.4393e-01, 2.7503e-02],\n",
       "          [6.1731e-01, 3.4138e-01, 4.1309e-02],\n",
       "          [5.4812e-01, 4.1321e-01, 3.8666e-02],\n",
       "          [6.0715e-01, 3.8119e-01, 1.1660e-02],\n",
       "          [5.1573e-01, 4.4956e-01, 3.4704e-02],\n",
       "          [6.3362e-01, 3.4249e-01, 2.3897e-02],\n",
       "          [4.8666e-01, 4.7735e-01, 3.5990e-02],\n",
       "          [5.1294e-01, 4.4661e-01, 4.0449e-02],\n",
       "          [7.6550e-01, 2.2556e-01, 8.9338e-03],\n",
       "          [5.1441e-01, 4.3650e-01, 4.9093e-02],\n",
       "          [5.0398e-01, 4.6226e-01, 3.3754e-02],\n",
       "          [5.9858e-01, 3.7369e-01, 2.7736e-02],\n",
       "          [5.9109e-01, 3.6760e-01, 4.1311e-02],\n",
       "          [7.5542e-01, 2.4206e-01, 2.5175e-03],\n",
       "          [6.1634e-01, 3.3700e-01, 4.6654e-02],\n",
       "          [4.9594e-01, 4.6354e-01, 4.0525e-02],\n",
       "          [5.4794e-01, 4.3385e-01, 1.8206e-02],\n",
       "          [6.6389e-01, 2.9873e-01, 3.7377e-02],\n",
       "          [6.2100e-01, 3.4076e-01, 3.8239e-02],\n",
       "          [5.3534e-01, 4.2475e-01, 3.9917e-02],\n",
       "          [6.1673e-01, 3.7549e-01, 7.7786e-03],\n",
       "          [6.9805e-01, 3.0164e-01, 3.0567e-04],\n",
       "          [6.6528e-01, 3.1097e-01, 2.3748e-02],\n",
       "          [7.6608e-01, 2.3368e-01, 2.3774e-04],\n",
       "          [7.1830e-01, 2.7561e-01, 6.0909e-03],\n",
       "          [5.5261e-01, 4.0249e-01, 4.4903e-02],\n",
       "          [5.3721e-01, 4.2627e-01, 3.6528e-02],\n",
       "          [5.4561e-01, 4.1491e-01, 3.9480e-02],\n",
       "          [6.3048e-01, 3.2762e-01, 4.1904e-02],\n",
       "          [5.3043e-01, 4.2551e-01, 4.4061e-02],\n",
       "          [6.2107e-01, 3.4394e-01, 3.4993e-02],\n",
       "          [6.5878e-01, 3.2073e-01, 2.0494e-02],\n",
       "          [8.3152e-01, 1.6029e-01, 8.1950e-03],\n",
       "          [5.8997e-01, 3.7127e-01, 3.8759e-02],\n",
       "          [6.6035e-01, 3.1394e-01, 2.5708e-02],\n",
       "          [5.3098e-01, 4.5048e-01, 1.8536e-02],\n",
       "          [5.1952e-01, 4.4844e-01, 3.2037e-02],\n",
       "          [6.4870e-01, 3.1248e-01, 3.8820e-02],\n",
       "          [8.0113e-01, 1.9802e-01, 8.5158e-04],\n",
       "          [6.7738e-01, 2.9789e-01, 2.4735e-02],\n",
       "          [8.5776e-01, 1.3850e-01, 3.7386e-03],\n",
       "          [5.5230e-01, 4.0814e-01, 3.9556e-02],\n",
       "          [6.9384e-01, 2.9796e-01, 8.1973e-03],\n",
       "          [7.2217e-01, 2.5699e-01, 2.0835e-02],\n",
       "          [5.9899e-01, 3.8009e-01, 2.0916e-02],\n",
       "          [6.2388e-01, 3.2894e-01, 4.7179e-02],\n",
       "          [5.3037e-01, 4.3576e-01, 3.3876e-02],\n",
       "          [6.3793e-01, 3.5463e-01, 7.4341e-03],\n",
       "          [5.7269e-01, 3.7984e-01, 4.7466e-02],\n",
       "          [7.2952e-01, 2.5005e-01, 2.0430e-02],\n",
       "          [5.3767e-01, 4.1596e-01, 4.6373e-02],\n",
       "          [5.4116e-01, 4.2137e-01, 3.7470e-02],\n",
       "          [5.2027e-01, 4.4952e-01, 3.0202e-02],\n",
       "          [6.3485e-01, 3.3615e-01, 2.9001e-02],\n",
       "          [5.2153e-01, 4.3398e-01, 4.4490e-02],\n",
       "          [8.8079e-01, 1.1884e-01, 3.6931e-04],\n",
       "          [5.9794e-01, 3.6117e-01, 4.0890e-02],\n",
       "          [5.4015e-01, 4.1616e-01, 4.3689e-02],\n",
       "          [5.3988e-01, 4.2424e-01, 3.5885e-02],\n",
       "          [5.3152e-01, 4.4021e-01, 2.8266e-02],\n",
       "          [5.3972e-01, 4.3108e-01, 2.9197e-02],\n",
       "          [6.5123e-01, 3.4847e-01, 2.9654e-04],\n",
       "          [5.3765e-01, 4.2318e-01, 3.9172e-02],\n",
       "          [5.6400e-01, 3.9954e-01, 3.6466e-02],\n",
       "          [5.0011e-01, 4.6781e-01, 3.2080e-02],\n",
       "          [6.2877e-01, 3.2473e-01, 4.6504e-02],\n",
       "          [6.0981e-01, 3.6551e-01, 2.4680e-02],\n",
       "          [4.5728e-01, 5.1403e-01, 2.8695e-02],\n",
       "          [6.0280e-01, 3.5381e-01, 4.3392e-02],\n",
       "          [5.5454e-01, 4.0146e-01, 4.3999e-02],\n",
       "          [5.9056e-01, 3.7283e-01, 3.6604e-02],\n",
       "          [7.0982e-01, 2.7457e-01, 1.5616e-02],\n",
       "          [5.9311e-01, 3.8644e-01, 2.0451e-02],\n",
       "          [5.3412e-01, 4.5721e-01, 8.6697e-03],\n",
       "          [5.5754e-01, 4.1065e-01, 3.1811e-02],\n",
       "          [5.7367e-01, 4.1277e-01, 1.3557e-02],\n",
       "          [5.6575e-01, 4.1105e-01, 2.3197e-02],\n",
       "          [5.3401e-01, 4.2825e-01, 3.7743e-02],\n",
       "          [6.4913e-01, 3.2482e-01, 2.6049e-02],\n",
       "          [6.5523e-01, 3.1787e-01, 2.6904e-02],\n",
       "          [5.2556e-01, 4.5859e-01, 1.5847e-02],\n",
       "          [7.9957e-01, 1.9543e-01, 5.0069e-03],\n",
       "          [4.9025e-01, 4.7161e-01, 3.8133e-02],\n",
       "          [6.0908e-01, 3.6239e-01, 2.8526e-02],\n",
       "          [5.3059e-01, 4.5222e-01, 1.7189e-02],\n",
       "          [5.5415e-01, 4.0566e-01, 4.0189e-02],\n",
       "          [8.2364e-01, 1.6518e-01, 1.1182e-02],\n",
       "          [5.5228e-01, 4.1015e-01, 3.7563e-02],\n",
       "          [4.8372e-01, 4.7931e-01, 3.6971e-02],\n",
       "          [5.6360e-01, 4.0836e-01, 2.8044e-02],\n",
       "          [5.2617e-01, 4.4089e-01, 3.2940e-02],\n",
       "          [7.2391e-01, 2.6862e-01, 7.4706e-03],\n",
       "          [7.6769e-01, 2.2243e-01, 9.8788e-03],\n",
       "          [5.9307e-01, 3.7464e-01, 3.2287e-02],\n",
       "          [7.3579e-01, 2.5931e-01, 4.8915e-03],\n",
       "          [5.1329e-01, 4.4377e-01, 4.2941e-02],\n",
       "          [5.8436e-01, 4.0059e-01, 1.5054e-02],\n",
       "          [6.0794e-01, 3.6818e-01, 2.3889e-02],\n",
       "          [6.1984e-01, 3.3767e-01, 4.2496e-02],\n",
       "          [6.2079e-01, 3.3724e-01, 4.1969e-02],\n",
       "          [6.6222e-01, 3.3124e-01, 6.5429e-03],\n",
       "          [5.7503e-01, 3.7906e-01, 4.5910e-02],\n",
       "          [5.8204e-01, 3.7839e-01, 3.9573e-02],\n",
       "          [6.1000e-01, 3.4546e-01, 4.4549e-02],\n",
       "          [6.2568e-01, 3.4374e-01, 3.0578e-02],\n",
       "          [5.0824e-01, 4.5851e-01, 3.3252e-02],\n",
       "          [6.1919e-01, 3.3849e-01, 4.2313e-02],\n",
       "          [6.2191e-01, 3.5490e-01, 2.3186e-02],\n",
       "          [6.1212e-01, 3.4312e-01, 4.4758e-02],\n",
       "          [5.3807e-01, 4.1442e-01, 4.7510e-02],\n",
       "          [7.5782e-01, 2.2807e-01, 1.4112e-02],\n",
       "          [5.5850e-01, 4.0012e-01, 4.1382e-02],\n",
       "          [5.8743e-01, 3.7948e-01, 3.3090e-02],\n",
       "          [5.0968e-01, 4.5004e-01, 4.0282e-02],\n",
       "          [5.3591e-01, 4.1566e-01, 4.8427e-02],\n",
       "          [5.6515e-01, 4.1937e-01, 1.5473e-02],\n",
       "          [5.4521e-01, 4.2145e-01, 3.3341e-02],\n",
       "          [6.6117e-01, 3.1946e-01, 1.9371e-02],\n",
       "          [6.6914e-01, 3.0097e-01, 2.9892e-02],\n",
       "          [5.2923e-01, 4.3019e-01, 4.0580e-02],\n",
       "          [5.8291e-01, 3.6798e-01, 4.9116e-02],\n",
       "          [6.3858e-01, 3.2961e-01, 3.1805e-02],\n",
       "          [5.3734e-01, 4.1926e-01, 4.3401e-02],\n",
       "          [5.0784e-01, 4.7700e-01, 1.5160e-02],\n",
       "          [6.7579e-01, 3.1425e-01, 9.9584e-03],\n",
       "          [6.7224e-01, 3.1289e-01, 1.4875e-02],\n",
       "          [4.9884e-01, 4.6158e-01, 3.9578e-02],\n",
       "          [6.5514e-01, 3.2121e-01, 2.3650e-02],\n",
       "          [5.3729e-01, 4.2629e-01, 3.6418e-02],\n",
       "          [6.7460e-01, 2.9912e-01, 2.6284e-02],\n",
       "          [5.7867e-01, 3.8554e-01, 3.5788e-02],\n",
       "          [5.3807e-01, 4.2404e-01, 3.7885e-02],\n",
       "          [6.6251e-01, 2.9913e-01, 3.8365e-02],\n",
       "          [6.0371e-01, 3.6178e-01, 3.4508e-02],\n",
       "          [5.1485e-01, 4.5078e-01, 3.4379e-02],\n",
       "          [4.8483e-01, 4.8326e-01, 3.1902e-02],\n",
       "          [6.3231e-01, 3.2760e-01, 4.0097e-02],\n",
       "          [4.8912e-01, 4.8127e-01, 2.9615e-02],\n",
       "          [5.6806e-01, 4.0264e-01, 2.9298e-02],\n",
       "          [5.5777e-01, 4.4185e-01, 3.7835e-04],\n",
       "          [6.6481e-01, 3.2557e-01, 9.6153e-03],\n",
       "          [5.6388e-01, 4.0474e-01, 3.1382e-02],\n",
       "          [6.8849e-01, 2.8587e-01, 2.5641e-02],\n",
       "          [5.1193e-01, 4.5494e-01, 3.3126e-02]], grad_fn=<CopySlices>),\n",
       "  'pd2': tensor([[9.9729e-01, 1.3501e-03, 1.3634e-03],\n",
       "          [9.9999e-01, 4.4617e-06, 4.4690e-06],\n",
       "          [9.9902e-01, 4.8847e-04, 4.9487e-04],\n",
       "          [9.8918e-01, 5.3673e-03, 5.4504e-03],\n",
       "          [9.7862e-01, 1.0575e-02, 1.0802e-02],\n",
       "          [9.8261e-01, 8.5956e-03, 8.7904e-03],\n",
       "          [9.9757e-01, 1.2120e-03, 1.2187e-03],\n",
       "          [9.8662e-01, 6.6187e-03, 6.7585e-03],\n",
       "          [9.9136e-01, 4.2651e-03, 4.3745e-03],\n",
       "          [9.8310e-01, 8.3984e-03, 8.4970e-03],\n",
       "          [9.8097e-01, 9.3978e-03, 9.6317e-03],\n",
       "          [9.9815e-01, 9.1859e-04, 9.2816e-04],\n",
       "          [9.6793e-01, 1.5815e-02, 1.6257e-02],\n",
       "          [9.8788e-01, 5.9905e-03, 6.1291e-03],\n",
       "          [9.8964e-01, 5.1392e-03, 5.2174e-03],\n",
       "          [9.7730e-01, 1.1205e-02, 1.1496e-02],\n",
       "          [9.9982e-01, 8.8601e-05, 8.8907e-05],\n",
       "          [9.7058e-01, 1.4593e-02, 1.4830e-02],\n",
       "          [9.7979e-01, 9.9884e-03, 1.0221e-02],\n",
       "          [9.9518e-01, 2.3949e-03, 2.4232e-03],\n",
       "          [9.8203e-01, 8.9401e-03, 9.0317e-03],\n",
       "          [9.8114e-01, 9.3360e-03, 9.5246e-03],\n",
       "          [9.8186e-01, 8.9963e-03, 9.1423e-03],\n",
       "          [9.9882e-01, 5.8918e-04, 5.9262e-04],\n",
       "          [1.0000e+00, 1.6977e-06, 1.6988e-06],\n",
       "          [9.9304e-01, 3.4411e-03, 3.5237e-03],\n",
       "          [1.0000e+00, 1.0666e-06, 1.0672e-06],\n",
       "          [9.9930e-01, 3.5000e-04, 3.5359e-04],\n",
       "          [9.7441e-01, 1.2668e-02, 1.2922e-02],\n",
       "          [9.8439e-01, 7.7157e-03, 7.8972e-03],\n",
       "          [9.8180e-01, 9.0082e-03, 9.1891e-03],\n",
       "          [9.7604e-01, 1.1857e-02, 1.2100e-02],\n",
       "          [9.7683e-01, 1.1445e-02, 1.1726e-02],\n",
       "          [9.8173e-01, 9.0097e-03, 9.2560e-03],\n",
       "          [9.9465e-01, 2.6461e-03, 2.7066e-03],\n",
       "          [9.9800e-01, 9.9822e-04, 1.0063e-03],\n",
       "          [9.8190e-01, 8.9702e-03, 9.1342e-03],\n",
       "          [9.9084e-01, 4.5367e-03, 4.6258e-03],\n",
       "          [9.9399e-01, 2.9791e-03, 3.0354e-03],\n",
       "          [9.8800e-01, 5.9265e-03, 6.0719e-03],\n",
       "          [9.7893e-01, 1.0448e-02, 1.0620e-02],\n",
       "          [9.9997e-01, 1.5094e-05, 1.5121e-05],\n",
       "          [9.9038e-01, 4.7679e-03, 4.8519e-03],\n",
       "          [9.9950e-01, 2.4827e-04, 2.4980e-04],\n",
       "          [9.8040e-01, 9.6854e-03, 9.9101e-03],\n",
       "          [9.9893e-01, 5.3143e-04, 5.3604e-04],\n",
       "          [9.9439e-01, 2.7937e-03, 2.8190e-03],\n",
       "          [9.9327e-01, 3.3316e-03, 3.3976e-03],\n",
       "          [9.7019e-01, 1.4753e-02, 1.5055e-02],\n",
       "          [9.8642e-01, 6.6838e-03, 6.8921e-03],\n",
       "          [9.9909e-01, 4.5328e-04, 4.5825e-04],\n",
       "          [9.7150e-01, 1.4122e-02, 1.4378e-02],\n",
       "          [9.9133e-01, 4.3020e-03, 4.3729e-03],\n",
       "          [9.7215e-01, 1.3719e-02, 1.4127e-02],\n",
       "          [9.8358e-01, 8.1172e-03, 8.3051e-03],\n",
       "          [9.8695e-01, 6.4621e-03, 6.5866e-03],\n",
       "          [9.8929e-01, 5.3075e-03, 5.3984e-03],\n",
       "          [9.7297e-01, 1.3327e-02, 1.3703e-02],\n",
       "          [9.9999e-01, 2.5008e-06, 2.5032e-06],\n",
       "          [9.7866e-01, 1.0569e-02, 1.0771e-02],\n",
       "          [9.7780e-01, 1.0988e-02, 1.1214e-02],\n",
       "          [9.8441e-01, 7.7342e-03, 7.8582e-03],\n",
       "          [9.8922e-01, 5.3389e-03, 5.4429e-03],\n",
       "          [9.8790e-01, 5.9834e-03, 6.1118e-03],\n",
       "          [1.0000e+00, 1.4324e-06, 1.4333e-06],\n",
       "          [9.8224e-01, 8.7883e-03, 8.9715e-03],\n",
       "          [9.8389e-01, 7.9801e-03, 8.1316e-03],\n",
       "          [9.8553e-01, 7.1962e-03, 7.2753e-03],\n",
       "          [9.7362e-01, 1.3055e-02, 1.3325e-02],\n",
       "          [9.9111e-01, 4.4023e-03, 4.4853e-03],\n",
       "          [9.8782e-01, 6.0418e-03, 6.1378e-03],\n",
       "          [9.7683e-01, 1.1490e-02, 1.1682e-02],\n",
       "          [9.7713e-01, 1.1291e-02, 1.1576e-02],\n",
       "          [9.8398e-01, 7.9433e-03, 8.0735e-03],\n",
       "          [9.9642e-01, 1.7801e-03, 1.7955e-03],\n",
       "          [9.9457e-01, 2.6960e-03, 2.7323e-03],\n",
       "          [9.9843e-01, 7.8259e-04, 7.8310e-04],\n",
       "          [9.8447e-01, 7.7056e-03, 7.8287e-03],\n",
       "          [9.9719e-01, 1.3994e-03, 1.4131e-03],\n",
       "          [9.9309e-01, 3.4437e-03, 3.4646e-03],\n",
       "          [9.8281e-01, 8.4936e-03, 8.6979e-03],\n",
       "          [9.8911e-01, 5.3733e-03, 5.5216e-03],\n",
       "          [9.9021e-01, 4.8601e-03, 4.9293e-03],\n",
       "          [9.9620e-01, 1.8916e-03, 1.9126e-03],\n",
       "          [9.9930e-01, 3.5023e-04, 3.5370e-04],\n",
       "          [9.8423e-01, 7.7974e-03, 7.9701e-03],\n",
       "          [9.8862e-01, 5.6258e-03, 5.7543e-03],\n",
       "          [9.9574e-01, 2.1110e-03, 2.1498e-03],\n",
       "          [9.8136e-01, 9.2292e-03, 9.4134e-03],\n",
       "          [9.9617e-01, 1.9053e-03, 1.9222e-03],\n",
       "          [9.8252e-01, 8.6805e-03, 8.7992e-03],\n",
       "          [9.8509e-01, 7.3733e-03, 7.5383e-03],\n",
       "          [9.8821e-01, 5.8458e-03, 5.9412e-03],\n",
       "          [9.8604e-01, 6.8947e-03, 7.0630e-03],\n",
       "          [9.9909e-01, 4.5503e-04, 4.5690e-04],\n",
       "          [9.9824e-01, 8.7504e-04, 8.8243e-04],\n",
       "          [9.8827e-01, 5.8159e-03, 5.9152e-03],\n",
       "          [9.9966e-01, 1.6881e-04, 1.6910e-04],\n",
       "          [9.7769e-01, 1.1025e-02, 1.1288e-02],\n",
       "          [9.9702e-01, 1.4848e-03, 1.4945e-03],\n",
       "          [9.9233e-01, 3.8021e-03, 3.8685e-03],\n",
       "          [9.7749e-01, 1.1166e-02, 1.1343e-02],\n",
       "          [9.7593e-01, 1.1911e-02, 1.2155e-02],\n",
       "          [9.9935e-01, 3.2332e-04, 3.2465e-04],\n",
       "          [9.7709e-01, 1.1306e-02, 1.1600e-02],\n",
       "          [9.8231e-01, 8.7672e-03, 8.9256e-03],\n",
       "          [9.7512e-01, 1.2327e-02, 1.2548e-02],\n",
       "          [9.8737e-01, 6.2460e-03, 6.3793e-03],\n",
       "          [9.8602e-01, 6.9178e-03, 7.0630e-03],\n",
       "          [9.7979e-01, 1.0040e-02, 1.0166e-02],\n",
       "          [9.9267e-01, 3.6314e-03, 3.6988e-03],\n",
       "          [9.7528e-01, 1.2233e-02, 1.2490e-02],\n",
       "          [9.6939e-01, 1.5122e-02, 1.5493e-02],\n",
       "          [9.9664e-01, 1.6679e-03, 1.6936e-03],\n",
       "          [9.7832e-01, 1.0765e-02, 1.0915e-02],\n",
       "          [9.8704e-01, 6.4184e-03, 6.5460e-03],\n",
       "          [9.8088e-01, 9.4400e-03, 9.6784e-03],\n",
       "          [9.6994e-01, 1.4824e-02, 1.5234e-02],\n",
       "          [9.9652e-01, 1.7291e-03, 1.7473e-03],\n",
       "          [9.8527e-01, 7.3064e-03, 7.4271e-03],\n",
       "          [9.9480e-01, 2.5765e-03, 2.6202e-03],\n",
       "          [9.8423e-01, 7.8125e-03, 7.9616e-03],\n",
       "          [9.7913e-01, 1.0345e-02, 1.0523e-02],\n",
       "          [9.6929e-01, 1.5139e-02, 1.5567e-02],\n",
       "          [9.8512e-01, 7.3694e-03, 7.5132e-03],\n",
       "          [9.7801e-01, 1.0882e-02, 1.1105e-02],\n",
       "          [9.9565e-01, 2.1766e-03, 2.1770e-03],\n",
       "          [9.9843e-01, 7.7914e-04, 7.8589e-04],\n",
       "          [9.9701e-01, 1.4937e-03, 1.4996e-03],\n",
       "          [9.8021e-01, 9.7936e-03, 9.9964e-03],\n",
       "          [9.9244e-01, 3.7428e-03, 3.8187e-03],\n",
       "          [9.8446e-01, 7.6802e-03, 7.8605e-03],\n",
       "          [9.9045e-01, 4.7405e-03, 4.8115e-03],\n",
       "          [9.8283e-01, 8.4853e-03, 8.6877e-03],\n",
       "          [9.8104e-01, 9.3600e-03, 9.5983e-03],\n",
       "          [9.8210e-01, 8.8605e-03, 9.0358e-03],\n",
       "          [9.8570e-01, 7.0989e-03, 7.2020e-03],\n",
       "          [9.8660e-01, 6.6325e-03, 6.7630e-03],\n",
       "          [9.8711e-01, 6.4150e-03, 6.4791e-03],\n",
       "          [9.8017e-01, 9.8303e-03, 9.9996e-03],\n",
       "          [9.8721e-01, 6.3478e-03, 6.4464e-03],\n",
       "          [9.8778e-01, 6.0684e-03, 6.1514e-03],\n",
       "          [9.9999e-01, 3.3423e-06, 3.3439e-06],\n",
       "          [9.9850e-01, 7.4727e-04, 7.5473e-04],\n",
       "          [9.8649e-01, 6.6807e-03, 6.8303e-03],\n",
       "          [9.8905e-01, 5.4225e-03, 5.5244e-03],\n",
       "          [9.8625e-01, 6.8200e-03, 6.9339e-03]], grad_fn=<CopySlices>),\n",
       "  'mod': tensor([[1.0000e+00, 3.7489e-27, 3.7489e-27, 3.7489e-27, 3.7489e-27, 3.7489e-27],\n",
       "          [9.9836e-01, 3.2424e-04, 3.2809e-04, 3.2845e-04, 3.2513e-04, 3.3682e-04],\n",
       "          [9.7040e-01, 5.6904e-03, 6.0574e-03, 5.8744e-03, 5.8157e-03, 6.1662e-03],\n",
       "          [1.0000e+00, 1.9325e-18, 1.9325e-18, 1.9325e-18, 1.9325e-18, 1.9325e-18],\n",
       "          [1.0000e+00, 2.0861e-18, 2.0861e-18, 2.0861e-18, 2.0861e-18, 2.0861e-18],\n",
       "          [9.3868e-01, 1.1681e-02, 1.2403e-02, 1.2153e-02, 1.1889e-02, 1.3199e-02],\n",
       "          [1.0000e+00, 1.3803e-18, 1.3803e-18, 1.3803e-18, 1.3803e-18, 1.3803e-18],\n",
       "          [9.4517e-01, 1.0488e-02, 1.1084e-02, 1.0943e-02, 1.0637e-02, 1.1675e-02],\n",
       "          [9.2458e-01, 1.4273e-02, 1.5321e-02, 1.4931e-02, 1.4599e-02, 1.6291e-02],\n",
       "          [1.0000e+00, 2.2080e-18, 2.2080e-18, 2.2080e-18, 2.2080e-18, 2.2080e-18],\n",
       "          [9.3579e-01, 1.2278e-02, 1.3082e-02, 1.2677e-02, 1.2565e-02, 1.3604e-02],\n",
       "          [9.6807e-01, 6.1401e-03, 6.4372e-03, 6.3982e-03, 6.2380e-03, 6.7203e-03],\n",
       "          [9.1253e-01, 1.6581e-02, 1.7749e-02, 1.7262e-02, 1.7039e-02, 1.8842e-02],\n",
       "          [9.4504e-01, 1.0464e-02, 1.1083e-02, 1.0919e-02, 1.0667e-02, 1.1828e-02],\n",
       "          [1.0000e+00, 1.8438e-18, 1.8438e-18, 1.8438e-18, 1.8438e-18, 1.8438e-18],\n",
       "          [8.9157e-01, 2.0527e-02, 2.1940e-02, 2.1335e-02, 2.1138e-02, 2.3486e-02],\n",
       "          [1.0000e+00, 2.2662e-18, 2.2662e-18, 2.2662e-18, 2.2662e-18, 2.2662e-18],\n",
       "          [9.4033e-01, 1.1416e-02, 1.2127e-02, 1.1856e-02, 1.1613e-02, 1.2658e-02],\n",
       "          [9.2414e-01, 1.4424e-02, 1.5351e-02, 1.4958e-02, 1.4820e-02, 1.6311e-02],\n",
       "          [1.0000e+00, 1.6471e-18, 1.6471e-18, 1.6471e-18, 1.6471e-18, 1.6471e-18],\n",
       "          [9.4591e-01, 1.0396e-02, 1.1059e-02, 1.0682e-02, 1.0594e-02, 1.1359e-02],\n",
       "          [9.0194e-01, 1.8494e-02, 1.9878e-02, 1.9285e-02, 1.9092e-02, 2.1306e-02],\n",
       "          [1.0000e+00, 1.8146e-18, 1.8146e-18, 1.8146e-18, 1.8146e-18, 1.8146e-18],\n",
       "          [1.0000e+00, 2.2365e-18, 2.2365e-18, 2.2365e-18, 2.2365e-18, 2.2365e-18],\n",
       "          [1.0000e+00, 3.4305e-27, 3.4305e-27, 3.4305e-27, 3.4305e-27, 3.4305e-27],\n",
       "          [9.2753e-01, 1.3786e-02, 1.4741e-02, 1.4351e-02, 1.4009e-02, 1.5579e-02],\n",
       "          [1.0000e+00, 3.2483e-27, 3.2483e-27, 3.2483e-27, 3.2483e-27, 3.2483e-27],\n",
       "          [9.7444e-01, 4.9311e-03, 5.0967e-03, 5.1193e-03, 5.0078e-03, 5.4033e-03],\n",
       "          [9.4496e-01, 1.0556e-02, 1.1217e-02, 1.0994e-02, 1.0740e-02, 1.1530e-02],\n",
       "          [9.3673e-01, 1.2019e-02, 1.2755e-02, 1.2535e-02, 1.2263e-02, 1.3700e-02],\n",
       "          [9.2529e-01, 1.4214e-02, 1.5151e-02, 1.4764e-02, 1.4369e-02, 1.6208e-02],\n",
       "          [9.1940e-01, 1.5434e-02, 1.6382e-02, 1.5750e-02, 1.5789e-02, 1.7243e-02],\n",
       "          [9.1707e-01, 1.5772e-02, 1.6809e-02, 1.6361e-02, 1.6198e-02, 1.7790e-02],\n",
       "          [9.4289e-01, 1.1039e-02, 1.1482e-02, 1.1380e-02, 1.1204e-02, 1.2002e-02],\n",
       "          [9.3462e-01, 1.2352e-02, 1.3282e-02, 1.3022e-02, 1.2559e-02, 1.4164e-02],\n",
       "          [1.0000e+00, 3.1663e-27, 3.1663e-27, 3.1663e-27, 3.1663e-27, 3.1663e-27],\n",
       "          [9.4921e-01, 9.7892e-03, 1.0340e-02, 9.9807e-03, 9.9369e-03, 1.0746e-02],\n",
       "          [9.3002e-01, 1.3451e-02, 1.4259e-02, 1.3920e-02, 1.3634e-02, 1.4719e-02],\n",
       "          [9.7975e-01, 3.9352e-03, 4.1347e-03, 3.9797e-03, 3.9587e-03, 4.2399e-03],\n",
       "          [9.4218e-01, 1.0982e-02, 1.1632e-02, 1.1420e-02, 1.1216e-02, 1.2566e-02],\n",
       "          [9.2929e-01, 1.3517e-02, 1.4295e-02, 1.4015e-02, 1.3681e-02, 1.5199e-02],\n",
       "          [9.9660e-01, 6.6855e-04, 6.8099e-04, 6.7709e-04, 6.7285e-04, 7.0026e-04],\n",
       "          [9.5304e-01, 8.9741e-03, 9.4833e-03, 9.3795e-03, 9.1955e-03, 9.9319e-03],\n",
       "          [9.7661e-01, 4.5144e-03, 4.7667e-03, 4.6746e-03, 4.5613e-03, 4.8774e-03],\n",
       "          [9.2868e-01, 1.3542e-02, 1.4407e-02, 1.4080e-02, 1.3846e-02, 1.5439e-02],\n",
       "          [1.0000e+00, 2.6410e-18, 2.6410e-18, 2.6410e-18, 2.6410e-18, 2.6410e-18],\n",
       "          [1.0000e+00, 2.0581e-18, 2.0581e-18, 2.0581e-18, 2.0581e-18, 2.0581e-18],\n",
       "          [9.6337e-01, 7.0221e-03, 7.5133e-03, 7.1604e-03, 7.1557e-03, 7.7760e-03],\n",
       "          [9.2026e-01, 1.5335e-02, 1.6158e-02, 1.5751e-02, 1.5615e-02, 1.6885e-02],\n",
       "          [9.4254e-01, 1.0997e-02, 1.1706e-02, 1.1447e-02, 1.1207e-02, 1.2104e-02],\n",
       "          [9.8516e-01, 2.8682e-03, 2.9914e-03, 2.9379e-03, 2.9294e-03, 3.1175e-03],\n",
       "          [1.0000e+00, 2.2218e-18, 2.2218e-18, 2.2218e-18, 2.2218e-18, 2.2218e-18],\n",
       "          [8.9196e-01, 2.0465e-02, 2.2188e-02, 2.1556e-02, 2.1102e-02, 2.2725e-02],\n",
       "          [9.0697e-01, 1.7631e-02, 1.8865e-02, 1.8255e-02, 1.8169e-02, 2.0109e-02],\n",
       "          [9.3625e-01, 1.2113e-02, 1.2859e-02, 1.2635e-02, 1.2353e-02, 1.3785e-02],\n",
       "          [9.5108e-01, 9.3390e-03, 9.9902e-03, 9.5939e-03, 9.6068e-03, 1.0387e-02],\n",
       "          [1.0000e+00, 2.1753e-18, 2.1753e-18, 2.1753e-18, 2.1753e-18, 2.1753e-18],\n",
       "          [9.1475e-01, 1.6154e-02, 1.7142e-02, 1.6770e-02, 1.6678e-02, 1.8505e-02],\n",
       "          [9.9868e-01, 2.6100e-04, 2.6395e-04, 2.6385e-04, 2.6183e-04, 2.6937e-04],\n",
       "          [9.3374e-01, 1.2659e-02, 1.3317e-02, 1.3157e-02, 1.2908e-02, 1.4220e-02],\n",
       "          [9.1807e-01, 1.5547e-02, 1.6602e-02, 1.6214e-02, 1.5952e-02, 1.7616e-02],\n",
       "          [1.0000e+00, 2.2512e-18, 2.2512e-18, 2.2512e-18, 2.2512e-18, 2.2512e-18],\n",
       "          [9.4100e-01, 1.1321e-02, 1.1980e-02, 1.1662e-02, 1.1525e-02, 1.2517e-02],\n",
       "          [9.3156e-01, 1.3004e-02, 1.3646e-02, 1.3636e-02, 1.3455e-02, 1.4698e-02],\n",
       "          [1.0000e+00, 1.9732e-27, 1.9732e-27, 1.9732e-27, 1.9732e-27, 1.9732e-27],\n",
       "          [9.3097e-01, 1.3121e-02, 1.3935e-02, 1.3703e-02, 1.3378e-02, 1.4896e-02],\n",
       "          [9.1281e-01, 1.6652e-02, 1.7837e-02, 1.7402e-02, 1.6921e-02, 1.8380e-02],\n",
       "          [9.4720e-01, 1.0133e-02, 1.0699e-02, 1.0332e-02, 1.0340e-02, 1.1292e-02],\n",
       "          [9.3354e-01, 1.2782e-02, 1.3472e-02, 1.3283e-02, 1.2994e-02, 1.3931e-02],\n",
       "          [9.3640e-01, 1.2134e-02, 1.2872e-02, 1.2633e-02, 1.2344e-02, 1.3617e-02],\n",
       "          [9.5948e-01, 7.7837e-03, 8.2181e-03, 7.9438e-03, 7.9647e-03, 8.6100e-03],\n",
       "          [1.0000e+00, 2.6746e-18, 2.6746e-18, 2.6746e-18, 2.6746e-18, 2.6746e-18],\n",
       "          [9.1799e-01, 1.5617e-02, 1.6652e-02, 1.6195e-02, 1.6012e-02, 1.7534e-02],\n",
       "          [9.2786e-01, 1.3756e-02, 1.4677e-02, 1.4331e-02, 1.4038e-02, 1.5339e-02],\n",
       "          [9.7709e-01, 4.4338e-03, 4.6366e-03, 4.5181e-03, 4.5177e-03, 4.8060e-03],\n",
       "          [9.7622e-01, 4.6129e-03, 4.8083e-03, 4.7387e-03, 4.6588e-03, 4.9586e-03],\n",
       "          [9.7268e-01, 5.2240e-03, 5.5571e-03, 5.3423e-03, 5.3038e-03, 5.8886e-03],\n",
       "          [9.5134e-01, 9.2951e-03, 9.8928e-03, 9.5617e-03, 9.5579e-03, 1.0352e-02],\n",
       "          [1.0000e+00, 3.8601e-18, 3.8601e-18, 3.8601e-18, 3.8601e-18, 3.8601e-18],\n",
       "          [1.0000e+00, 2.6951e-18, 2.6951e-18, 2.6951e-18, 2.6951e-18, 2.6951e-18],\n",
       "          [9.3646e-01, 1.2126e-02, 1.2912e-02, 1.2548e-02, 1.2350e-02, 1.3608e-02],\n",
       "          [9.1670e-01, 1.5769e-02, 1.6922e-02, 1.6486e-02, 1.6200e-02, 1.7918e-02],\n",
       "          [9.3793e-01, 1.1877e-02, 1.2635e-02, 1.2213e-02, 1.2105e-02, 1.3237e-02],\n",
       "          [1.0000e+00, 1.7446e-18, 1.7446e-18, 1.7446e-18, 1.7446e-18, 1.7446e-18],\n",
       "          [9.5028e-01, 9.5387e-03, 9.9283e-03, 1.0086e-02, 9.6699e-03, 1.0494e-02],\n",
       "          [9.4252e-01, 1.0983e-02, 1.1615e-02, 1.1392e-02, 1.1173e-02, 1.2314e-02],\n",
       "          [9.3944e-01, 1.1498e-02, 1.2304e-02, 1.1940e-02, 1.1878e-02, 1.2942e-02],\n",
       "          [9.8189e-01, 3.4985e-03, 3.6612e-03, 3.5705e-03, 3.5457e-03, 3.8296e-03],\n",
       "          [9.3224e-01, 1.2896e-02, 1.3690e-02, 1.3463e-02, 1.3136e-02, 1.4573e-02],\n",
       "          [1.0000e+00, 4.0285e-27, 4.0285e-27, 4.0285e-27, 4.0285e-27, 4.0285e-27],\n",
       "          [9.1462e-01, 1.6257e-02, 1.7256e-02, 1.7068e-02, 1.6641e-02, 1.8156e-02],\n",
       "          [9.4363e-01, 1.0771e-02, 1.1385e-02, 1.1165e-02, 1.0957e-02, 1.2087e-02],\n",
       "          [9.4513e-01, 1.0496e-02, 1.1197e-02, 1.0737e-02, 1.0727e-02, 1.1711e-02],\n",
       "          [9.3229e-01, 1.2845e-02, 1.3608e-02, 1.3339e-02, 1.3169e-02, 1.4746e-02],\n",
       "          [9.9777e-01, 4.3906e-04, 4.4672e-04, 4.4103e-04, 4.3977e-04, 4.5956e-04],\n",
       "          [9.8112e-01, 3.6481e-03, 3.7897e-03, 3.7410e-03, 3.7180e-03, 3.9837e-03],\n",
       "          [9.5081e-01, 9.5300e-03, 9.9007e-03, 9.7665e-03, 9.6648e-03, 1.0326e-02],\n",
       "          [1.0000e+00, 2.1407e-18, 2.1407e-18, 2.1407e-18, 2.1407e-18, 2.1407e-18],\n",
       "          [9.1925e-01, 1.5349e-02, 1.6355e-02, 1.5922e-02, 1.5773e-02, 1.7349e-02],\n",
       "          [9.7814e-01, 4.2333e-03, 4.4547e-03, 4.3301e-03, 4.2896e-03, 4.5514e-03],\n",
       "          [1.0000e+00, 2.1494e-18, 2.1494e-18, 2.1494e-18, 2.1494e-18, 2.1494e-18],\n",
       "          [9.4058e-01, 1.1417e-02, 1.1957e-02, 1.1859e-02, 1.1537e-02, 1.2649e-02],\n",
       "          [9.2149e-01, 1.5032e-02, 1.5937e-02, 1.5335e-02, 1.5385e-02, 1.6823e-02],\n",
       "          [9.7905e-01, 4.0567e-03, 4.2370e-03, 4.1578e-03, 4.1171e-03, 4.3766e-03],\n",
       "          [9.3485e-01, 1.2452e-02, 1.3230e-02, 1.2994e-02, 1.2688e-02, 1.3784e-02],\n",
       "          [1.0000e+00, 1.9359e-18, 1.9359e-18, 1.9359e-18, 1.9359e-18, 1.9359e-18],\n",
       "          [9.3076e-01, 1.3212e-02, 1.3972e-02, 1.3782e-02, 1.3480e-02, 1.4795e-02],\n",
       "          [9.1953e-01, 1.5188e-02, 1.6502e-02, 1.6069e-02, 1.5585e-02, 1.7123e-02],\n",
       "          [9.4741e-01, 1.0044e-02, 1.0500e-02, 1.0544e-02, 1.0296e-02, 1.1205e-02],\n",
       "          [9.3074e-01, 1.3207e-02, 1.4059e-02, 1.3913e-02, 1.3466e-02, 1.4616e-02],\n",
       "          [9.5614e-01, 8.4482e-03, 8.9443e-03, 8.6450e-03, 8.5689e-03, 9.2529e-03],\n",
       "          [9.2667e-01, 1.4011e-02, 1.4840e-02, 1.4581e-02, 1.4299e-02, 1.5603e-02],\n",
       "          [9.1052e-01, 1.6878e-02, 1.8083e-02, 1.7656e-02, 1.7427e-02, 1.9431e-02],\n",
       "          [9.1936e-01, 1.5266e-02, 1.6678e-02, 1.6036e-02, 1.5567e-02, 1.7089e-02],\n",
       "          [1.0000e+00, 2.1515e-18, 2.1515e-18, 2.1515e-18, 2.1515e-18, 2.1515e-18],\n",
       "          [9.0969e-01, 1.7159e-02, 1.8627e-02, 1.8142e-02, 1.7502e-02, 1.8878e-02],\n",
       "          [9.2704e-01, 1.3897e-02, 1.4738e-02, 1.4416e-02, 1.4182e-02, 1.5724e-02],\n",
       "          [9.1896e-01, 1.5397e-02, 1.6478e-02, 1.5993e-02, 1.5798e-02, 1.7372e-02],\n",
       "          [1.0000e+00, 1.2650e-18, 1.2650e-18, 1.2650e-18, 1.2650e-18, 1.2650e-18],\n",
       "          [9.5607e-01, 8.4520e-03, 8.9312e-03, 8.6696e-03, 8.6111e-03, 9.2626e-03],\n",
       "          [9.3843e-01, 1.1808e-02, 1.2568e-02, 1.2234e-02, 1.1984e-02, 1.2972e-02],\n",
       "          [9.2681e-01, 1.3905e-02, 1.4882e-02, 1.4361e-02, 1.4352e-02, 1.5687e-02],\n",
       "          [1.0000e+00, 2.2990e-18, 2.2990e-18, 2.2990e-18, 2.2990e-18, 2.2990e-18],\n",
       "          [9.1090e-01, 1.6965e-02, 1.8082e-02, 1.7567e-02, 1.7409e-02, 1.9080e-02],\n",
       "          [9.3589e-01, 1.2352e-02, 1.2995e-02, 1.2655e-02, 1.2572e-02, 1.3540e-02],\n",
       "          [9.1859e-01, 1.5449e-02, 1.6494e-02, 1.6110e-02, 1.5850e-02, 1.7506e-02],\n",
       "          [9.6837e-01, 6.0996e-03, 6.3804e-03, 6.2099e-03, 6.1720e-03, 6.7636e-03],\n",
       "          [9.7852e-01, 4.1592e-03, 4.3078e-03, 4.2876e-03, 4.2174e-03, 4.5070e-03],\n",
       "          [9.8614e-01, 2.6918e-03, 2.7940e-03, 2.7403e-03, 2.7225e-03, 2.9150e-03],\n",
       "          [9.2769e-01, 1.3760e-02, 1.4774e-02, 1.4169e-02, 1.4074e-02, 1.5538e-02],\n",
       "          [9.6764e-01, 6.2598e-03, 6.5295e-03, 6.4419e-03, 6.3112e-03, 6.8177e-03],\n",
       "          [9.3674e-01, 1.2017e-02, 1.2753e-02, 1.2535e-02, 1.2261e-02, 1.3698e-02],\n",
       "          [9.1993e-01, 1.5264e-02, 1.6221e-02, 1.5790e-02, 1.5578e-02, 1.7215e-02],\n",
       "          [9.3851e-01, 1.1827e-02, 1.2413e-02, 1.2197e-02, 1.2037e-02, 1.3018e-02],\n",
       "          [9.1845e-01, 1.5439e-02, 1.6444e-02, 1.6051e-02, 1.5961e-02, 1.7655e-02],\n",
       "          [9.2201e-01, 1.4957e-02, 1.5970e-02, 1.5470e-02, 1.5172e-02, 1.6423e-02],\n",
       "          [1.0000e+00, 1.7322e-18, 1.7322e-18, 1.7322e-18, 1.7322e-18, 1.7322e-18],\n",
       "          [1.0000e+00, 1.7116e-18, 1.7116e-18, 1.7116e-18, 1.7116e-18, 1.7116e-18],\n",
       "          [1.0000e+00, 1.9988e-18, 1.9988e-18, 1.9988e-18, 1.9988e-18, 1.9988e-18],\n",
       "          [1.0000e+00, 2.0943e-18, 2.0943e-18, 2.0943e-18, 2.0943e-18, 2.0943e-18],\n",
       "          [1.0000e+00, 1.9757e-18, 1.9757e-18, 1.9757e-18, 1.9757e-18, 1.9757e-18],\n",
       "          [1.0000e+00, 2.0479e-18, 2.0479e-18, 2.0479e-18, 2.0479e-18, 2.0479e-18],\n",
       "          [9.9828e-01, 3.3902e-04, 3.4420e-04, 3.4126e-04, 3.3985e-04, 3.5117e-04],\n",
       "          [1.0000e+00, 2.7990e-18, 2.7990e-18, 2.7990e-18, 2.7990e-18, 2.7990e-18],\n",
       "          [9.5361e-01, 8.9134e-03, 9.3488e-03, 9.1918e-03, 9.1172e-03, 9.8172e-03],\n",
       "          [9.0447e-01, 1.8070e-02, 1.9511e-02, 1.8954e-02, 1.8621e-02, 2.0378e-02],\n",
       "          [1.0000e+00, 2.0649e-18, 2.0649e-18, 2.0649e-18, 2.0649e-18, 2.0649e-18]],\n",
       "         grad_fn=<CopySlices>),\n",
       "  'cc': tensor([[0.0919, 0.7785, 0.0679, 0.0617],\n",
       "          [0.0112, 0.9712, 0.0089, 0.0087],\n",
       "          [0.0659, 0.8336, 0.0521, 0.0484],\n",
       "          [0.1303, 0.6655, 0.1087, 0.0955],\n",
       "          [0.1562, 0.6055, 0.1248, 0.1136],\n",
       "          [0.1523, 0.6202, 0.1230, 0.1045],\n",
       "          [0.0880, 0.7757, 0.0720, 0.0643],\n",
       "          [0.1580, 0.6168, 0.1218, 0.1034],\n",
       "          [0.1375, 0.6702, 0.1018, 0.0906],\n",
       "          [0.1406, 0.6440, 0.1164, 0.0990],\n",
       "          [0.1631, 0.5921, 0.1317, 0.1132],\n",
       "          [0.0762, 0.8065, 0.0625, 0.0547],\n",
       "          [0.1631, 0.5835, 0.1356, 0.1178],\n",
       "          [0.1491, 0.6329, 0.1168, 0.1011],\n",
       "          [0.1209, 0.6865, 0.1012, 0.0915],\n",
       "          [0.1549, 0.6044, 0.1297, 0.1110],\n",
       "          [0.0341, 0.9115, 0.0279, 0.0265],\n",
       "          [0.1735, 0.5698, 0.1348, 0.1218],\n",
       "          [0.1596, 0.5995, 0.1287, 0.1122],\n",
       "          [0.1087, 0.7258, 0.0869, 0.0786],\n",
       "          [0.1403, 0.6360, 0.1176, 0.1060],\n",
       "          [0.1501, 0.6201, 0.1238, 0.1060],\n",
       "          [0.1539, 0.6081, 0.1258, 0.1122],\n",
       "          [0.0737, 0.8080, 0.0627, 0.0555],\n",
       "          [0.0080, 0.9788, 0.0067, 0.0065],\n",
       "          [0.1154, 0.7051, 0.0960, 0.0836],\n",
       "          [0.0070, 0.9813, 0.0060, 0.0058],\n",
       "          [0.0593, 0.8527, 0.0456, 0.0423],\n",
       "          [0.1586, 0.5939, 0.1326, 0.1149],\n",
       "          [0.1499, 0.6289, 0.1192, 0.1020],\n",
       "          [0.1700, 0.5874, 0.1280, 0.1147],\n",
       "          [0.1663, 0.5874, 0.1284, 0.1179],\n",
       "          [0.1653, 0.5856, 0.1332, 0.1159],\n",
       "          [0.1554, 0.6201, 0.1182, 0.1062],\n",
       "          [0.1330, 0.6875, 0.0958, 0.0837],\n",
       "          [0.0785, 0.8101, 0.0580, 0.0534],\n",
       "          [0.1601, 0.6019, 0.1259, 0.1122],\n",
       "          [0.1301, 0.6745, 0.1026, 0.0928],\n",
       "          [0.1167, 0.7093, 0.0917, 0.0823],\n",
       "          [0.1426, 0.6482, 0.1125, 0.0967],\n",
       "          [0.1651, 0.5987, 0.1238, 0.1125],\n",
       "          [0.0184, 0.9527, 0.0148, 0.0141],\n",
       "          [0.1244, 0.6881, 0.0992, 0.0884],\n",
       "          [0.0577, 0.8595, 0.0427, 0.0401],\n",
       "          [0.1614, 0.6025, 0.1269, 0.1093],\n",
       "          [0.0660, 0.8346, 0.0528, 0.0466],\n",
       "          [0.1055, 0.7217, 0.0911, 0.0818],\n",
       "          [0.1152, 0.7129, 0.0917, 0.0803],\n",
       "          [0.1731, 0.5707, 0.1336, 0.1226],\n",
       "          [0.1353, 0.6525, 0.1131, 0.0991],\n",
       "          [0.0599, 0.8456, 0.0492, 0.0453],\n",
       "          [0.1642, 0.5835, 0.1330, 0.1194],\n",
       "          [0.1205, 0.6925, 0.0981, 0.0890],\n",
       "          [0.1651, 0.5844, 0.1343, 0.1162],\n",
       "          [0.1512, 0.6250, 0.1207, 0.1030],\n",
       "          [0.1375, 0.6506, 0.1123, 0.0996],\n",
       "          [0.1437, 0.6476, 0.1093, 0.0994],\n",
       "          [0.1566, 0.6041, 0.1289, 0.1104],\n",
       "          [0.0096, 0.9752, 0.0077, 0.0074],\n",
       "          [0.1682, 0.5915, 0.1257, 0.1146],\n",
       "          [0.1645, 0.5892, 0.1320, 0.1144],\n",
       "          [0.1488, 0.6266, 0.1183, 0.1063],\n",
       "          [0.1340, 0.6654, 0.1055, 0.0951],\n",
       "          [0.1361, 0.6613, 0.1082, 0.0943],\n",
       "          [0.0082, 0.9781, 0.0069, 0.0068],\n",
       "          [0.1652, 0.5946, 0.1289, 0.1113],\n",
       "          [0.1517, 0.6177, 0.1212, 0.1094],\n",
       "          [0.1436, 0.6432, 0.1129, 0.1004],\n",
       "          [0.1660, 0.5869, 0.1292, 0.1179],\n",
       "          [0.1418, 0.6538, 0.1065, 0.0978],\n",
       "          [0.1373, 0.6576, 0.1094, 0.0957],\n",
       "          [0.1537, 0.6016, 0.1293, 0.1154],\n",
       "          [0.1653, 0.5863, 0.1330, 0.1155],\n",
       "          [0.1519, 0.6164, 0.1250, 0.1068],\n",
       "          [0.0868, 0.7818, 0.0685, 0.0629],\n",
       "          [0.1165, 0.7073, 0.0922, 0.0839],\n",
       "          [0.0680, 0.8201, 0.0593, 0.0527],\n",
       "          [0.1415, 0.6396, 0.1158, 0.1031],\n",
       "          [0.0992, 0.7527, 0.0779, 0.0702],\n",
       "          [0.1101, 0.7179, 0.0909, 0.0810],\n",
       "          [0.1577, 0.6140, 0.1235, 0.1048],\n",
       "          [0.1428, 0.6527, 0.1079, 0.0966],\n",
       "          [0.1316, 0.6738, 0.1020, 0.0926],\n",
       "          [0.1014, 0.7439, 0.0812, 0.0735],\n",
       "          [0.0625, 0.8484, 0.0463, 0.0428],\n",
       "          [0.1644, 0.5997, 0.1275, 0.1084],\n",
       "          [0.1347, 0.6655, 0.1054, 0.0943],\n",
       "          [0.1131, 0.7301, 0.0841, 0.0728],\n",
       "          [0.1662, 0.5919, 0.1301, 0.1119],\n",
       "          [0.0984, 0.7544, 0.0768, 0.0704],\n",
       "          [0.1447, 0.6258, 0.1222, 0.1074],\n",
       "          [0.1626, 0.6044, 0.1258, 0.1072],\n",
       "          [0.1324, 0.6648, 0.1057, 0.0971],\n",
       "          [0.1505, 0.6310, 0.1170, 0.1015],\n",
       "          [0.0696, 0.8309, 0.0518, 0.0478],\n",
       "          [0.0736, 0.8140, 0.0588, 0.0536],\n",
       "          [0.1496, 0.6372, 0.1124, 0.1009],\n",
       "          [0.0436, 0.8870, 0.0357, 0.0338],\n",
       "          [0.1636, 0.5898, 0.1318, 0.1149],\n",
       "          [0.0949, 0.7538, 0.0787, 0.0726],\n",
       "          [0.1221, 0.6928, 0.0971, 0.0880],\n",
       "          [0.1755, 0.5777, 0.1292, 0.1176],\n",
       "          [0.1662, 0.5881, 0.1281, 0.1176],\n",
       "          [0.0607, 0.8455, 0.0488, 0.0451],\n",
       "          [0.1634, 0.5918, 0.1313, 0.1135],\n",
       "          [0.1490, 0.6214, 0.1214, 0.1082],\n",
       "          [0.1725, 0.5800, 0.1294, 0.1181],\n",
       "          [0.1387, 0.6551, 0.1096, 0.0966],\n",
       "          [0.1471, 0.6426, 0.1141, 0.0961],\n",
       "          [0.1538, 0.6120, 0.1264, 0.1077],\n",
       "          [0.1243, 0.6937, 0.0960, 0.0860],\n",
       "          [0.1690, 0.5806, 0.1342, 0.1161],\n",
       "          [0.1637, 0.5889, 0.1326, 0.1148],\n",
       "          [0.1084, 0.7297, 0.0855, 0.0764],\n",
       "          [0.1515, 0.6135, 0.1240, 0.1110],\n",
       "          [0.1353, 0.6463, 0.1157, 0.1027],\n",
       "          [0.1665, 0.5894, 0.1308, 0.1132],\n",
       "          [0.1623, 0.5863, 0.1345, 0.1169],\n",
       "          [0.0979, 0.7539, 0.0774, 0.0708],\n",
       "          [0.1508, 0.6278, 0.1178, 0.1036],\n",
       "          [0.1342, 0.6769, 0.0988, 0.0901],\n",
       "          [0.1519, 0.6255, 0.1165, 0.1061],\n",
       "          [0.1525, 0.6131, 0.1239, 0.1106],\n",
       "          [0.1683, 0.5755, 0.1375, 0.1187],\n",
       "          [0.1456, 0.6360, 0.1143, 0.1042],\n",
       "          [0.1641, 0.5901, 0.1317, 0.1141],\n",
       "          [0.0865, 0.7761, 0.0722, 0.0652],\n",
       "          [0.0798, 0.8013, 0.0629, 0.0559],\n",
       "          [0.0961, 0.7624, 0.0746, 0.0669],\n",
       "          [0.1592, 0.6025, 0.1275, 0.1109],\n",
       "          [0.1325, 0.6768, 0.0993, 0.0914],\n",
       "          [0.1497, 0.6295, 0.1190, 0.1018],\n",
       "          [0.1289, 0.6768, 0.1021, 0.0922],\n",
       "          [0.1457, 0.6353, 0.1151, 0.1040],\n",
       "          [0.1550, 0.6134, 0.1232, 0.1084],\n",
       "          [0.1595, 0.5982, 0.1280, 0.1143],\n",
       "          [0.1354, 0.6485, 0.1146, 0.1015],\n",
       "          [0.1472, 0.6324, 0.1174, 0.1030],\n",
       "          [0.1363, 0.6511, 0.1124, 0.1002],\n",
       "          [0.1558, 0.6081, 0.1236, 0.1125],\n",
       "          [0.1395, 0.6448, 0.1155, 0.1002],\n",
       "          [0.1352, 0.6530, 0.1135, 0.0983],\n",
       "          [0.0085, 0.9768, 0.0074, 0.0072],\n",
       "          [0.0806, 0.8007, 0.0625, 0.0562],\n",
       "          [0.1494, 0.6380, 0.1121, 0.1005],\n",
       "          [0.1403, 0.6562, 0.1063, 0.0972],\n",
       "          [0.1499, 0.6237, 0.1206, 0.1058]], grad_fn=<CopySlices>),\n",
       "  'dlt1': tensor([[2.0894e-10, 1.0285e-13, 8.9041e-10, 3.5082e-13, 3.0970e-11],\n",
       "          [9.3115e-02, 5.9908e-02, 3.4212e-02, 1.4154e-02, 4.8939e-02],\n",
       "          [1.5403e-01, 7.5816e-02, 2.0141e-01, 7.7126e-02, 6.6691e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.6797e-01, 7.0020e-02, 2.1782e-01, 1.1213e-01, 1.3329e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.3623e-01, 8.7620e-02, 1.8203e-01, 1.2506e-01, 1.3760e-01],\n",
       "          [1.4975e-01, 9.9382e-02, 3.3937e-01, 1.0690e-01, 1.6868e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.3038e-01, 1.0302e-01, 1.4062e-01, 1.7155e-01, 1.0302e-01],\n",
       "          [1.2460e-01, 7.9167e-02, 1.4571e-01, 1.5441e-01, 9.2080e-02],\n",
       "          [1.9356e-01, 8.2686e-02, 1.8490e-01, 1.8778e-01, 9.6750e-02],\n",
       "          [1.4157e-01, 7.4109e-02, 2.1858e-01, 1.0539e-01, 1.3603e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.8219e-01, 1.1045e-01, 2.4534e-01, 2.0619e-01, 1.5885e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.6078e-01, 7.5344e-02, 2.2865e-01, 9.5695e-02, 1.4740e-01],\n",
       "          [1.4543e-01, 1.0498e-01, 1.9252e-01, 1.6441e-01, 1.1527e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.4298e-01, 1.0092e-01, 1.5074e-01, 1.3672e-01, 1.3209e-01],\n",
       "          [1.8965e-01, 1.1264e-01, 2.6435e-01, 1.9510e-01, 1.7769e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.1376e-10, 2.5244e-13, 5.6857e-10, 5.3323e-13, 1.1980e-12],\n",
       "          [2.0258e-01, 1.1031e-01, 1.5421e-01, 1.4221e-01, 9.5498e-02],\n",
       "          [2.4123e-10, 4.0932e-13, 2.7877e-10, 6.0485e-13, 7.1678e-13],\n",
       "          [2.6585e-01, 1.1133e-01, 1.1986e-01, 7.3539e-02, 6.5624e-02],\n",
       "          [1.7863e-01, 7.5638e-02, 1.5160e-01, 1.3836e-01, 1.0305e-01],\n",
       "          [1.5943e-01, 6.9640e-02, 2.3228e-01, 1.0733e-01, 1.3785e-01],\n",
       "          [1.4804e-01, 1.0599e-01, 2.4641e-01, 1.2678e-01, 1.8021e-01],\n",
       "          [1.7613e-01, 1.2202e-01, 1.9256e-01, 1.1352e-01, 1.5116e-01],\n",
       "          [1.5732e-01, 1.0944e-01, 1.8546e-01, 1.7889e-01, 1.1727e-01],\n",
       "          [1.2042e-01, 1.1511e-01, 1.6279e-01, 1.7831e-01, 1.5089e-01],\n",
       "          [1.6035e-01, 1.0602e-01, 2.6162e-01, 1.2601e-01, 1.5284e-01],\n",
       "          [1.2369e-10, 8.4478e-14, 7.6983e-10, 4.1209e-13, 1.9118e-11],\n",
       "          [1.7878e-01, 1.1887e-01, 1.1571e-01, 9.8645e-02, 8.7768e-02],\n",
       "          [1.9020e-01, 1.6129e-01, 1.5541e-01, 1.5549e-01, 1.1036e-01],\n",
       "          [9.4736e-02, 1.4653e-01, 5.6253e-02, 9.6834e-02, 7.2646e-02],\n",
       "          [1.4765e-01, 6.4042e-02, 2.4418e-01, 9.9097e-02, 1.3155e-01],\n",
       "          [1.3454e-01, 9.7143e-02, 2.8053e-01, 8.8995e-02, 2.1910e-01],\n",
       "          [5.4093e-02, 3.9588e-02, 3.3080e-02, 5.2710e-02, 7.6885e-02],\n",
       "          [1.6588e-01, 8.3108e-02, 1.7305e-01, 8.8733e-02, 1.6021e-01],\n",
       "          [1.3413e-01, 1.1682e-01, 1.2848e-01, 7.7702e-02, 6.2054e-02],\n",
       "          [1.5277e-01, 8.4182e-02, 2.4436e-01, 1.3030e-01, 1.4452e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.6393e-01, 1.0552e-01, 1.3253e-01, 1.0974e-01, 1.2291e-01],\n",
       "          [1.5800e-01, 1.1513e-01, 1.7710e-01, 1.4352e-01, 1.4521e-01],\n",
       "          [1.6672e-01, 7.3321e-02, 1.5398e-01, 1.4632e-01, 8.2358e-02],\n",
       "          [1.1769e-01, 4.6710e-02, 1.0611e-01, 4.4446e-02, 7.5060e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.5312e-01, 1.3815e-01, 3.5144e-01, 1.3303e-01, 1.7971e-01],\n",
       "          [1.7416e-01, 9.8810e-02, 2.1034e-01, 1.8101e-01, 1.1407e-01],\n",
       "          [1.6237e-01, 7.0395e-02, 2.2888e-01, 1.0892e-01, 1.3809e-01],\n",
       "          [1.4892e-01, 9.4043e-02, 1.1943e-01, 1.4216e-01, 6.5510e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.8503e-01, 8.1092e-02, 1.8383e-01, 1.8896e-01, 1.0332e-01],\n",
       "          [5.2323e-02, 2.2896e-02, 5.4014e-02, 1.0357e-02, 8.3571e-02],\n",
       "          [1.3796e-01, 7.4942e-02, 3.0587e-01, 8.3106e-02, 1.9518e-01],\n",
       "          [1.6885e-01, 1.1900e-01, 2.1315e-01, 1.6937e-01, 1.3391e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.8214e-01, 1.1707e-01, 1.6197e-01, 1.2491e-01, 9.2449e-02],\n",
       "          [1.3911e-01, 9.5556e-02, 1.9985e-01, 2.1761e-01, 1.2485e-01],\n",
       "          [1.6041e-10, 2.7861e-13, 1.8501e-10, 6.0138e-13, 4.5468e-13],\n",
       "          [1.4388e-01, 9.4122e-02, 2.2987e-01, 1.3582e-01, 1.4685e-01],\n",
       "          [2.1412e-01, 1.2809e-01, 1.8664e-01, 1.8158e-01, 1.2421e-01],\n",
       "          [1.4197e-01, 1.1644e-01, 1.2255e-01, 1.1366e-01, 7.8152e-02],\n",
       "          [1.8466e-01, 1.0344e-01, 1.8404e-01, 1.0800e-01, 1.5387e-01],\n",
       "          [1.8572e-01, 1.3971e-01, 1.8450e-01, 1.1591e-01, 1.0429e-01],\n",
       "          [1.2516e-01, 9.9216e-02, 9.6137e-02, 1.3289e-01, 5.4717e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.6255e-01, 1.0978e-01, 1.7761e-01, 1.8252e-01, 1.1475e-01],\n",
       "          [1.4961e-01, 1.1687e-01, 1.9556e-01, 1.8630e-01, 1.5161e-01],\n",
       "          [1.1125e-01, 5.6685e-02, 1.1368e-01, 8.0488e-02, 7.2413e-02],\n",
       "          [9.4834e-02, 5.6355e-02, 1.3050e-01, 7.4447e-02, 8.4358e-02],\n",
       "          [1.0982e-01, 7.4185e-02, 1.2071e-01, 1.2527e-01, 7.7137e-02],\n",
       "          [1.4728e-01, 8.5976e-02, 1.0086e-01, 1.5444e-01, 6.4886e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.4090e-01, 7.8136e-02, 1.8196e-01, 1.2827e-01, 1.3371e-01],\n",
       "          [2.0303e-01, 1.4227e-01, 1.9892e-01, 1.3274e-01, 1.2532e-01],\n",
       "          [1.7118e-01, 1.3155e-01, 1.0905e-01, 1.2647e-01, 8.7901e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.0015e-01, 1.5699e-01, 2.1413e-01, 1.3204e-01, 1.1482e-01],\n",
       "          [1.2656e-01, 8.4615e-02, 1.7577e-01, 1.3160e-01, 1.2928e-01],\n",
       "          [1.5032e-01, 7.7813e-02, 2.0361e-01, 1.1510e-01, 1.4888e-01],\n",
       "          [1.0482e-01, 5.9759e-02, 9.2707e-02, 6.7900e-02, 1.1018e-01],\n",
       "          [1.4985e-01, 9.5197e-02, 2.1943e-01, 1.3857e-01, 1.4421e-01],\n",
       "          [1.9039e-10, 1.5434e-13, 7.3066e-10, 4.5503e-13, 2.2129e-11],\n",
       "          [2.0548e-01, 1.0713e-01, 2.1023e-01, 1.9520e-01, 1.3425e-01],\n",
       "          [1.2355e-01, 8.3479e-02, 1.7840e-01, 1.2938e-01, 1.2912e-01],\n",
       "          [1.3552e-01, 1.1324e-01, 1.5655e-01, 1.2135e-01, 1.0191e-01],\n",
       "          [1.3832e-01, 7.8291e-02, 2.6723e-01, 1.1625e-01, 1.4501e-01],\n",
       "          [4.2435e-02, 3.1799e-02, 5.3212e-02, 3.0324e-02, 3.1743e-02],\n",
       "          [1.2906e-01, 4.9539e-02, 6.0369e-02, 8.7043e-02, 5.0414e-02],\n",
       "          [1.7361e-01, 1.2975e-01, 1.0353e-01, 8.2986e-02, 9.0395e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.5263e-01, 1.0781e-01, 1.8971e-01, 1.7390e-01, 1.1687e-01],\n",
       "          [9.6307e-02, 9.0228e-02, 1.1308e-01, 8.1313e-02, 6.3379e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.3060e-01, 9.2668e-02, 2.2353e-01, 9.6454e-02, 1.7843e-01],\n",
       "          [1.7180e-01, 1.2007e-01, 1.9365e-01, 1.1076e-01, 1.5010e-01],\n",
       "          [1.2376e-01, 7.5158e-02, 8.6240e-02, 7.1374e-02, 4.3967e-02],\n",
       "          [1.7750e-01, 9.7358e-02, 1.7153e-01, 1.4843e-01, 1.2070e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.4996e-01, 7.6488e-02, 2.9423e-01, 8.9128e-02, 1.8905e-01],\n",
       "          [2.2291e-01, 1.0007e-01, 2.4949e-01, 1.3241e-01, 1.2226e-01],\n",
       "          [1.2565e-01, 6.6783e-02, 1.7998e-01, 1.6610e-01, 1.2861e-01],\n",
       "          [2.0862e-01, 1.1784e-01, 1.6516e-01, 1.5745e-01, 1.2924e-01],\n",
       "          [1.2682e-01, 1.4736e-01, 9.4279e-02, 1.2243e-01, 7.2400e-02],\n",
       "          [1.6897e-01, 1.2427e-01, 1.7761e-01, 1.7102e-01, 1.3614e-01],\n",
       "          [1.9762e-01, 8.6737e-02, 2.2383e-01, 1.7002e-01, 1.2055e-01],\n",
       "          [1.7886e-01, 1.2372e-01, 2.5698e-01, 1.7394e-01, 1.3857e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.3814e-01, 1.1766e-01, 1.9861e-01, 1.8665e-01, 1.3931e-01],\n",
       "          [1.3380e-01, 8.9440e-02, 2.1389e-01, 1.4073e-01, 1.3516e-01],\n",
       "          [1.9559e-01, 8.1363e-02, 1.6881e-01, 1.8656e-01, 9.3973e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.4434e-01, 1.2325e-01, 8.2323e-02, 1.3338e-01, 5.9279e-02],\n",
       "          [1.5602e-01, 1.8446e-01, 1.6169e-01, 9.9182e-02, 1.1554e-01],\n",
       "          [1.7572e-01, 1.0379e-01, 2.8253e-01, 1.1023e-01, 1.2872e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.8028e-01, 1.0551e-01, 1.8198e-01, 1.8571e-01, 1.1680e-01],\n",
       "          [2.0312e-01, 1.5098e-01, 1.4327e-01, 1.3409e-01, 9.2221e-02],\n",
       "          [1.6780e-01, 1.1881e-01, 2.1377e-01, 1.6791e-01, 1.3389e-01],\n",
       "          [1.1343e-01, 9.5494e-02, 8.4302e-02, 1.0182e-01, 9.3381e-02],\n",
       "          [9.4673e-02, 3.8702e-02, 1.7605e-01, 7.4818e-02, 9.4223e-02],\n",
       "          [1.4796e-01, 7.3391e-02, 8.2130e-02, 4.3479e-02, 6.5615e-02],\n",
       "          [1.6089e-01, 1.2138e-01, 1.6468e-01, 1.7241e-01, 1.2101e-01],\n",
       "          [8.6579e-02, 6.9206e-02, 1.6997e-01, 8.1088e-02, 1.1923e-01],\n",
       "          [1.5925e-01, 6.9806e-02, 2.3205e-01, 1.0706e-01, 1.3837e-01],\n",
       "          [2.3998e-01, 1.0722e-01, 1.7008e-01, 1.0720e-01, 1.1983e-01],\n",
       "          [2.0755e-01, 1.3563e-01, 1.5061e-01, 1.1183e-01, 9.6980e-02],\n",
       "          [1.7810e-01, 1.1118e-01, 1.6634e-01, 1.4704e-01, 1.0014e-01],\n",
       "          [1.5838e-01, 1.2539e-01, 2.0613e-01, 1.5216e-01, 1.9392e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [4.5725e-02, 2.7795e-02, 4.4543e-02, 2.0156e-02, 2.8874e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.2469e-01, 1.3286e-01, 1.0908e-01, 2.0415e-01, 1.0060e-01],\n",
       "          [1.9286e-01, 1.1709e-01, 3.4273e-01, 1.2667e-01, 1.7829e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
       "         grad_fn=<CopySlices>),\n",
       "  'dlt2': tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0025, 0.0011, 0.0003, 0.0013, 0.0012],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], grad_fn=<CopySlices>)})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def temporal_loss(timestoevents,weights=None,maxtime=48,threshold=True):\n",
    "    #list of expected times to events, usualy in order of Const.temporal_outcomes\n",
    "    #basically longer = better, we count > maxtime (weeks) as no event\n",
    "    if weights is None: \n",
    "        weights = [1 for i in range(len(timestoevents))]\n",
    "    scores =  [(w*maxtime/t)for w,t in zip(weights,timestoevents)]\n",
    "    if threshold:\n",
    "        scores = [s*torch.lt(t,maxtime) for s,t in zip(scores,timestoevents)]\n",
    "    scores = torch.stack(scores).sum(axis=0)\n",
    "    return scores\n",
    "\n",
    "def outcome_loss(ypred,weights=None):\n",
    "    #default weights is bad\n",
    "    if weights is None: \n",
    "        print('using default outcome loss weights, which is probably wrong since bad stuff should be negative')\n",
    "        weights = [1 for i in range(ypred.shape[1])]\n",
    "    l = torch.mul(ypred[:,0],weights[0])\n",
    "    for i,weight in enumerate(weights[1:]):\n",
    "        #weights with negative values will invert the outcome so e.g. Regional control becomes no regional control\n",
    "        #so the penaly is correct\n",
    "        newloss = torch.mul(ypred[:,i+1],weight)\n",
    "        l = torch.add(l,newloss)\n",
    "    return l\n",
    "\n",
    "def calc_optimal_decisions(dataset,ids,m1,m2,m3,sm3,\n",
    "                           weights=[0,0.5,.5,0], #weight for OS, FT, AS, and LRC as binary probabilities\n",
    "                           tweights=[1,1,1,1], #weight for OS, LRC, FDM, and event (any + FT or AS at 6m) as time to event in weeks\n",
    "                           outcome_loss_func=None,\n",
    "                           threshold_temporal_loss = False,\n",
    "                           maxtime=48,\n",
    "                           get_transitions=True):\n",
    "    m1.eval()\n",
    "    m2.eval()\n",
    "    m3.eval()\n",
    "    sm3.eval()\n",
    "    device = m1.get_device()\n",
    "    data = dataset.processed_df.copy().loc[ids]\n",
    "    \n",
    "    def get_dlt(state):\n",
    "        if state == 2:\n",
    "            return data[Const.dlt2].copy()\n",
    "        d = data[Const.dlt1].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_pd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.primary_disease_states2].copy()\n",
    "        d = data[Const.primary_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_nd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.nodal_disease_states2].copy()\n",
    "        d = data[Const.nodal_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_cc(state):\n",
    "        res = data[Const.ccs].copy()\n",
    "        if state == 1:\n",
    "            res.values[:,:] = np.zeros(res.values.shape)\n",
    "        return res\n",
    "    \n",
    "    def get_mod(state):\n",
    "        res = data[Const.modifications].copy()\n",
    "        #this should have an ic condition but we don't use it anumore anywa\n",
    "        return res\n",
    "        \n",
    "    def formatdf(d):\n",
    "        d = df_to_torch(d).to(device)\n",
    "        return d\n",
    "    \n",
    "    \n",
    "    outcomedf = data[Const.outcomes]\n",
    "    baseline = dataset.get_state('baseline').loc[ids]\n",
    "    baseline_input = formatdf(baseline)\n",
    "\n",
    "        \n",
    "    if outcome_loss_func is None:\n",
    "        outcome_loss_func = outcome_loss\n",
    "    \n",
    "    cat = lambda x: torch.cat([xx.to(device) for xx in x],axis=1).to(device)\n",
    "    format_transition = lambda x: x.to(device)\n",
    "    def get_outcome(d1,d2,d3):\n",
    "        d1 = torch.full((len(ids),1),d1).type(torch.FloatTensor)\n",
    "        d2 = torch.full((len(ids),1),d2).type(torch.FloatTensor)\n",
    "        d3 = torch.full((len(ids),1),d3).type(torch.FloatTensor)\n",
    "        \n",
    "        tinput1 = cat([baseline_input,d1])\n",
    "        ytransition = m1(tinput1)\n",
    "        [ypd1,ynd1,ymod,ydlt1] = [format_transition(xx) for xx in ytransition['predictions']]\n",
    "        d1_thresh = torch.gt(d1,.5).view(-1,1).to(device)\n",
    "        ypd1[:,0:2] = ypd1[:,0:2]*d1_thresh\n",
    "        ynd1[:,0:2] = ynd1[:,0:2]*d1_thresh\n",
    "        \n",
    "        tinput2 = cat([baseline_input,ypd1,ynd1,ymod,ydlt1,d1,d2])\n",
    "        ytransition2 = m2(tinput2)\n",
    "        [ypd2,ynd2,ycc,ydlt2] = [format_transition(xx) for xx in ytransition2['predictions']]\n",
    "        \n",
    "        input3 = cat([baseline_input, ypd2, ynd2, ycc, ydlt2, d1, d2,d3])\n",
    "        outcome = m3(input3)['predictions']\n",
    "        temporal_outcomes = sm3.time_to_event(input3,n_samples=1)\n",
    "        \n",
    "        transitions = {\n",
    "            'pd1': ypd1,\n",
    "            'nd1': ynd1,\n",
    "            'nd2': ynd2,\n",
    "            'pd2': ypd2,\n",
    "            'mod': ymod,\n",
    "            'cc': ycc,\n",
    "            'dlt1': ydlt1,\n",
    "            'dlt2': ydlt2,\n",
    "        }\n",
    "        return outcome, temporal_outcomes, transitions\n",
    "\n",
    "    losses = []\n",
    "    loss_order = []\n",
    "    transitions = {}\n",
    "    for d1 in [0,1]:\n",
    "        for d2 in [0,1]:\n",
    "            for d3 in [0,1]:\n",
    "                outcomes, tte, transition_entry = get_outcome(d1,d2,d3)\n",
    "                loss = outcome_loss_func(outcomes,weights)\n",
    "                tloss = temporal_loss(tte,tweights,maxtime=maxtime,threshold=threshold_temporal_loss)\n",
    "                loss += tloss\n",
    "                losses.append(loss)\n",
    "                loss_order.append([d1,d2,d3])\n",
    "                transitions[str(d1)+str(d2)+str(d3)] = transition_entry\n",
    "    losses = torch.stack(losses,axis=1)\n",
    "    optimal_decisions = [loss_order[i] for i in torch.argmin(losses,axis=1)]\n",
    "    result = torch.tensor(optimal_decisions).type(torch.FloatTensor)\n",
    "    print(result.sum(axis=0),result.shape[0])\n",
    "    if get_transitions:\n",
    "        opt_transitions = {k: torch.zeros(v.shape).type(torch.FloatTensor) for k,v in transitions['000'].items()}\n",
    "        for i,od in enumerate(optimal_decisions):\n",
    "            key = ''.join([str(o) for o in od])\n",
    "            entry = transitions[key]\n",
    "            for kk,vv in entry.items():\n",
    "                opt_transitions[kk][i,:] = vv[i,:]\n",
    "        return result, opt_transitions\n",
    "    return result\n",
    "\n",
    "test, testtest = get_tt_split()\n",
    "calc_optimal_decisions(DTDataset(),\n",
    "                       testtest,model1,model2,model3,smodel3,\n",
    "                       threshold_temporal_loss=False,\n",
    "                       maxtime=48,\n",
    "                       weights=[0,0,0,0],\n",
    "                       tweights=[2,0.1,0,0],\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "122ee514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([223.,  23.,  68.]) 389\n",
      "tensor([78., 16., 28.]) 147\n",
      "torch.Size([3, 536, 86])\n",
      "tensor([0.5306, 0.1088, 0.1905], grad_fn=<MeanBackward1>)\n",
      "______epoch 0 _____\n",
      "val reward 2.494950294494629\n",
      "imitation reward 2.1218421459198\n",
      "distance losses 0.284330278635025 0.33368340134620667\n",
      "distributions [0.7034912109375, 0.0008517052629031241, 0.0036800967063754797]\n",
      "[{'decision': 0, 'optimal_auc': 0.6560758082497212, 'imitation_auc': 0.5954198473282443, 'optimal_acc': 0.54421768707483, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.38597328244274803, 'imitation_auc': 0.7046195652173912, 'optimal_acc': 0.891156462585034, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.9057623049219687, 'imitation_auc': 0.7657342657342657, 'optimal_acc': 0.8095238095238095, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.5306, 0.1088, 0.1905], grad_fn=<MeanBackward1>)\n",
      "______epoch 1 _____\n",
      "val reward 2.5483856201171875\n",
      "imitation reward 1.2520921230316162\n",
      "distance losses 0.32467222213745117 0.28137025237083435\n",
      "distributions [0.1454121470451355, 0.00069902598625049, 0.011962555348873138]\n",
      "[{'decision': 0, 'optimal_auc': 0.6859903381642513, 'imitation_auc': 0.40267175572519087, 'optimal_acc': 0.46938775510204084, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.9441793893129772, 'imitation_auc': 0.7521739130434782, 'optimal_acc': 0.891156462585034, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.9048619447779112, 'imitation_auc': 0.7047043865225684, 'optimal_acc': 0.8095238095238095, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.5306, 0.1088, 0.1905], grad_fn=<MeanBackward1>)\n",
      "______epoch 2 _____\n",
      "val reward 1.8799328804016113\n",
      "imitation reward 1.098222255706787\n",
      "distance losses 0.3926706314086914 0.3054035007953644\n",
      "distributions [0.814054012298584, 0.0014244405319914222, 0.12409394234418869]\n",
      "[{'decision': 0, 'optimal_auc': 0.6692679301374954, 'imitation_auc': 0.4208015267175573, 'optimal_acc': 0.5238095238095238, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.9680343511450382, 'imitation_auc': 0.7133152173913043, 'optimal_acc': 0.891156462585034, 'imitation_acc': 0.7959183673469388}, {'decision': 2, 'optimal_auc': 0.8928571428571428, 'imitation_auc': 0.7129688493324857, 'optimal_acc': 0.8095238095238095, 'imitation_acc': 0.8095238095238095}]\n",
      "tensor([0.5306, 0.1088, 0.1905], grad_fn=<MeanBackward1>)\n",
      "______epoch 3 _____\n",
      "val reward 1.6124889850616455\n",
      "imitation reward 1.116249442100525\n",
      "distance losses 0.3522109091281891 0.29436880350112915\n",
      "distributions [0.7218215465545654, 0.006006126757711172, 0.35218119621276855]\n",
      "[{'decision': 0, 'optimal_auc': 0.6807878112225938, 'imitation_auc': 0.48091603053435117, 'optimal_acc': 0.5510204081632653, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.9775763358778626, 'imitation_auc': 0.7133152173913044, 'optimal_acc': 0.891156462585034, 'imitation_acc': 0.7482993197278912}, {'decision': 2, 'optimal_auc': 0.9036614645858343, 'imitation_auc': 0.7596948506039415, 'optimal_acc': 0.7891156462585034, 'imitation_acc': 0.7891156462585034}]\n",
      "tensor([0.5306, 0.1088, 0.1905], grad_fn=<MeanBackward1>)\n",
      "______epoch 4 _____\n",
      "val reward 1.3793644905090332\n",
      "imitation reward 1.0900628566741943\n",
      "distance losses 0.32950517535209656 0.2583220303058624\n",
      "distributions [0.43013903498649597, 0.013068275526165962, 0.288611501455307]\n",
      "[{'decision': 0, 'optimal_auc': 0.7075436640654031, 'imitation_auc': 0.5119274809160306, 'optimal_acc': 0.6258503401360545, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.9608778625954199, 'imitation_auc': 0.710054347826087, 'optimal_acc': 0.891156462585034, 'imitation_acc': 0.7687074829931972}, {'decision': 2, 'optimal_auc': 0.9108643457382954, 'imitation_auc': 0.7819453273998729, 'optimal_acc': 0.8639455782312925, 'imitation_acc': 0.7891156462585034}]\n",
      "tensor([0.5306, 0.1088, 0.1905], grad_fn=<MeanBackward1>)\n",
      "______epoch 5 _____\n",
      "val reward 1.3261395692825317\n",
      "imitation reward 1.1169188022613525\n",
      "distance losses 0.31543630361557007 0.2431914210319519\n",
      "distributions [0.4154355227947235, 0.015482386574149132, 0.1529904454946518]\n",
      "[{'decision': 0, 'optimal_auc': 0.729654403567447, 'imitation_auc': 0.5415076335877862, 'optimal_acc': 0.6394557823129252, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.9584923664122138, 'imitation_auc': 0.6896739130434782, 'optimal_acc': 0.891156462585034, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.9204681872749099, 'imitation_auc': 0.799745708836618, 'optimal_acc': 0.8367346938775511, 'imitation_acc': 0.8163265306122449}]\n",
      "tensor([0.5306, 0.1088, 0.1905], grad_fn=<MeanBackward1>)\n",
      "______epoch 6 _____\n",
      "val reward 1.2161610126495361\n",
      "imitation reward 1.0960333347320557\n",
      "distance losses 0.31828367710113525 0.26206740736961365\n",
      "distributions [0.6791934370994568, 0.04112952575087547, 0.13284751772880554]\n",
      "[{'decision': 0, 'optimal_auc': 0.7467484206614641, 'imitation_auc': 0.538645038167939, 'optimal_acc': 0.564625850340136, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.9594465648854962, 'imitation_auc': 0.6964673913043479, 'optimal_acc': 0.891156462585034, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.9336734693877551, 'imitation_auc': 0.8092816274634457, 'optimal_acc': 0.8435374149659864, 'imitation_acc': 0.8027210884353742}]\n",
      "tensor([0.5306, 0.1088, 0.1905], grad_fn=<MeanBackward1>)\n",
      "______epoch 7 _____\n",
      "val reward 1.1212756633758545\n",
      "imitation reward 1.0503400564193726\n",
      "distance losses 0.33635982871055603 0.2636962831020355\n",
      "distributions [0.6858310103416443, 0.11233236640691757, 0.21284359693527222]\n",
      "[{'decision': 0, 'optimal_auc': 0.7504645113340765, 'imitation_auc': 0.5481870229007634, 'optimal_acc': 0.6258503401360545, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.9604007633587787, 'imitation_auc': 0.7116847826086956, 'optimal_acc': 0.891156462585034, 'imitation_acc': 0.7687074829931972}, {'decision': 2, 'optimal_auc': 0.9363745498199278, 'imitation_auc': 0.8102352193261284, 'optimal_acc': 0.8843537414965986, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.5306, 0.1088, 0.1905], grad_fn=<MeanBackward1>)\n",
      "______epoch 8 _____\n",
      "val reward 1.071048617362976\n",
      "imitation reward 1.0480648279190063\n",
      "distance losses 0.3378758728504181 0.25552013516426086\n",
      "distributions [0.5854259133338928, 0.1447385847568512, 0.2611144483089447]\n",
      "[{'decision': 0, 'optimal_auc': 0.7486064659977705, 'imitation_auc': 0.5706106870229009, 'optimal_acc': 0.6530612244897959, 'imitation_acc': 0.8843537414965986}, {'decision': 1, 'optimal_auc': 0.9623091603053435, 'imitation_auc': 0.7195652173913043, 'optimal_acc': 0.9047619047619048, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': 0.9381752701080432, 'imitation_auc': 0.8178639542275906, 'optimal_acc': 0.891156462585034, 'imitation_acc': 0.8095238095238095}]\n",
      "tensor([0.5306, 0.1088, 0.1905], grad_fn=<MeanBackward1>)\n",
      "______epoch 9 _____\n",
      "val reward 1.0050671100616455\n",
      "imitation reward 1.0635385513305664\n",
      "distance losses 0.33524537086486816 0.2636328637599945\n",
      "distributions [0.46485474705696106, 0.08692491799592972, 0.19065003097057343]\n",
      "[{'decision': 0, 'optimal_auc': 0.7560386473429952, 'imitation_auc': 0.5896946564885496, 'optimal_acc': 0.6462585034013606, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.963263358778626, 'imitation_auc': 0.7255434782608696, 'optimal_acc': 0.891156462585034, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.9423769507803121, 'imitation_auc': 0.8134138588684043, 'optimal_acc': 0.8979591836734694, 'imitation_acc': 0.8095238095238095}]\n",
      "tensor([0.5306, 0.1088, 0.1905], grad_fn=<MeanBackward1>)\n",
      "______epoch 10 _____\n",
      "val reward 1.0883275270462036\n",
      "imitation reward 1.1071757078170776\n",
      "distance losses 0.34137213230133057 0.29174095392227173\n",
      "distributions [0.4733330309391022, 0.03231756389141083, 0.10496653616428375]\n",
      "[{'decision': 0, 'optimal_auc': 0.7766629505759941, 'imitation_auc': 0.5997137404580153, 'optimal_acc': 0.7210884353741497, 'imitation_acc': 0.8843537414965986}, {'decision': 1, 'optimal_auc': 0.9642175572519084, 'imitation_auc': 0.7184782608695652, 'optimal_acc': 0.891156462585034, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': 0.946578631452581, 'imitation_auc': 0.7994278448823904, 'optimal_acc': 0.8435374149659864, 'imitation_acc': 0.8231292517006803}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5306, 0.1088, 0.1905], grad_fn=<MeanBackward1>)\n",
      "______epoch 11 _____\n",
      "val reward 0.9939001798629761\n",
      "imitation reward 1.2276113033294678\n",
      "distance losses 0.40352335572242737 0.27672460675239563\n",
      "distributions [0.6276071667671204, 0.0451757051050663, 0.1993211805820465]\n",
      "[{'decision': 0, 'optimal_auc': 0.7985878855444074, 'imitation_auc': 0.6059160305343512, 'optimal_acc': 0.6802721088435374, 'imitation_acc': 0.8707482993197279}, {'decision': 1, 'optimal_auc': 0.9708969465648856, 'imitation_auc': 0.7089673913043478, 'optimal_acc': 0.891156462585034, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': 0.9447779111644657, 'imitation_auc': 0.7924348378893833, 'optimal_acc': 0.8979591836734694, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.5306, 0.1088, 0.1905], grad_fn=<MeanBackward1>)\n",
      "______epoch 12 _____\n",
      "val reward 0.9749594926834106\n",
      "imitation reward 1.1924920082092285\n",
      "distance losses 0.40081411600112915 0.27735790610313416\n",
      "distributions [0.5836377739906311, 0.07233922928571701, 0.2660134732723236]\n",
      "[{'decision': 0, 'optimal_auc': 0.8084355258268303, 'imitation_auc': 0.5806297709923665, 'optimal_acc': 0.6938775510204082, 'imitation_acc': 0.8299319727891157}, {'decision': 1, 'optimal_auc': 0.9694656488549619, 'imitation_auc': 0.7293478260869566, 'optimal_acc': 0.9387755102040817, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.9432773109243697, 'imitation_auc': 0.784170375079466, 'optimal_acc': 0.9047619047619048, 'imitation_acc': 0.8095238095238095}]\n",
      "tensor([0.5306, 0.1088, 0.1905], grad_fn=<MeanBackward1>)\n",
      "______epoch 13 _____\n",
      "val reward 1.0031263828277588\n",
      "imitation reward 1.2565691471099854\n",
      "distance losses 0.3471152186393738 0.23142699897289276\n",
      "distributions [0.4664555788040161, 0.05499475821852684, 0.25725701451301575]\n",
      "[{'decision': 0, 'optimal_auc': 0.8108509847640283, 'imitation_auc': 0.5548664122137403, 'optimal_acc': 0.7414965986394558, 'imitation_acc': 0.8571428571428571}, {'decision': 1, 'optimal_auc': 0.9694656488549618, 'imitation_auc': 0.7442934782608696, 'optimal_acc': 0.9115646258503401, 'imitation_acc': 0.7959183673469388}, {'decision': 2, 'optimal_auc': 0.9429771908763505, 'imitation_auc': 0.7784488239033693, 'optimal_acc': 0.891156462585034, 'imitation_acc': 0.8095238095238095}]\n",
      "tensor([0.5306, 0.1088, 0.1905], grad_fn=<MeanBackward1>)\n",
      "______epoch 14 _____\n",
      "val reward 1.106224775314331\n",
      "imitation reward 1.1202893257141113\n",
      "distance losses 0.31853750348091125 0.2347991019487381\n",
      "distributions [0.4857673943042755, 0.026842739433050156, 0.09720055758953094]\n",
      "[{'decision': 0, 'optimal_auc': 0.8171683389074693, 'imitation_auc': 0.5458015267175572, 'optimal_acc': 0.7551020408163265, 'imitation_acc': 0.8571428571428571}, {'decision': 1, 'optimal_auc': 0.9666030534351145, 'imitation_auc': 0.7527173913043478, 'optimal_acc': 0.891156462585034, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.9357743097238895, 'imitation_auc': 0.7650985378258106, 'optimal_acc': 0.8367346938775511, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.5306, 0.1088, 0.1905], grad_fn=<MeanBackward1>)\n",
      "______epoch 15 _____\n",
      "val reward 0.9004396796226501\n",
      "imitation reward 1.2302464246749878\n",
      "distance losses 0.3593384623527527 0.247488871216774\n",
      "distributions [0.5705994367599487, 0.08425161242485046, 0.20109231770038605]\n",
      "[{'decision': 0, 'optimal_auc': 0.8301746562616127, 'imitation_auc': 0.5772900763358778, 'optimal_acc': 0.7551020408163265, 'imitation_acc': 0.8639455782312925}, {'decision': 1, 'optimal_auc': 0.9704198473282443, 'imitation_auc': 0.7426630434782608, 'optimal_acc': 0.9523809523809523, 'imitation_acc': 0.7891156462585034}, {'decision': 2, 'optimal_auc': 0.9462785114045619, 'imitation_auc': 0.7733630006357279, 'optimal_acc': 0.8775510204081632, 'imitation_acc': 0.8095238095238095}]\n",
      "tensor([0.5306, 0.1088, 0.1905], grad_fn=<MeanBackward1>)\n",
      "______epoch 16 _____\n",
      "val reward 0.9030932784080505\n",
      "imitation reward 1.219343900680542\n",
      "distance losses 0.3792562484741211 0.23373329639434814\n",
      "distributions [0.5904628038406372, 0.09744565933942795, 0.2284490168094635]\n",
      "[{'decision': 0, 'optimal_auc': 0.835005574136009, 'imitation_auc': 0.5916030534351144, 'optimal_acc': 0.7551020408163265, 'imitation_acc': 0.8707482993197279}, {'decision': 1, 'optimal_auc': 0.9704198473282443, 'imitation_auc': 0.7298913043478261, 'optimal_acc': 0.9523809523809523, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.9441776710684273, 'imitation_auc': 0.7838525111252383, 'optimal_acc': 0.8979591836734694, 'imitation_acc': 0.8095238095238095}]\n",
      "tensor([0.5306, 0.1088, 0.1905], grad_fn=<MeanBackward1>)\n",
      "______epoch 17 _____\n",
      "val reward 0.9107178449630737\n",
      "imitation reward 1.159985065460205\n",
      "distance losses 0.32913678884506226 0.2368539571762085\n",
      "distributions [0.5022820234298706, 0.05947573482990265, 0.20380689203739166]\n",
      "[{'decision': 0, 'optimal_auc': 0.8411371237458194, 'imitation_auc': 0.58206106870229, 'optimal_acc': 0.7891156462585034, 'imitation_acc': 0.8775510204081632}, {'decision': 1, 'optimal_auc': 0.9728053435114504, 'imitation_auc': 0.7309782608695653, 'optimal_acc': 0.9251700680272109, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': 0.9450780312124849, 'imitation_auc': 0.7933884297520661, 'optimal_acc': 0.8979591836734694, 'imitation_acc': 0.8095238095238095}]\n",
      "tensor([0.5306, 0.1088, 0.1905], grad_fn=<MeanBackward1>)\n",
      "______epoch 18 _____\n",
      "val reward 1.052358865737915\n",
      "imitation reward 1.2112846374511719\n",
      "distance losses 0.34984642267227173 0.28502461314201355\n",
      "distributions [0.41230928897857666, 0.025008905678987503, 0.1743735373020172]\n",
      "[{'decision': 0, 'optimal_auc': 0.8411371237458194, 'imitation_auc': 0.5796755725190839, 'optimal_acc': 0.7619047619047619, 'imitation_acc': 0.8707482993197279}, {'decision': 1, 'optimal_auc': 0.9708969465648856, 'imitation_auc': 0.7203804347826087, 'optimal_acc': 0.891156462585034, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': 0.9417767106842737, 'imitation_auc': 0.7956134774316592, 'optimal_acc': 0.8707482993197279, 'imitation_acc': 0.8163265306122449}]\n",
      "tensor([0.5306, 0.1088, 0.1905], grad_fn=<MeanBackward1>)\n",
      "______epoch 19 _____\n",
      "val reward 0.9785216450691223\n",
      "imitation reward 1.2459678649902344\n",
      "distance losses 0.34815993905067444 0.23676152527332306\n",
      "distributions [0.5157653093338013, 0.031380925327539444, 0.20454645156860352]\n",
      "[{'decision': 0, 'optimal_auc': 0.8511705685618729, 'imitation_auc': 0.5729961832061069, 'optimal_acc': 0.782312925170068, 'imitation_acc': 0.8503401360544217}, {'decision': 1, 'optimal_auc': 0.9732824427480916, 'imitation_auc': 0.7239130434782609, 'optimal_acc': 0.9115646258503401, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.9429771908763506, 'imitation_auc': 0.7889383343928799, 'optimal_acc': 0.8843537414965986, 'imitation_acc': 0.8027210884353742}]\n",
      "tensor([0.5306, 0.1088, 0.1905], grad_fn=<MeanBackward1>)\n",
      "______epoch 20 _____\n",
      "val reward 0.9558461308479309\n",
      "imitation reward 1.299370288848877\n",
      "distance losses 0.3695916533470154 0.24886178970336914\n",
      "distributions [0.5437428951263428, 0.046697329729795456, 0.23297861218452454]\n",
      "[{'decision': 0, 'optimal_auc': 0.8541434410999629, 'imitation_auc': 0.5591603053435115, 'optimal_acc': 0.7619047619047619, 'imitation_acc': 0.8435374149659864}, {'decision': 1, 'optimal_auc': 0.9713740458015268, 'imitation_auc': 0.728804347826087, 'optimal_acc': 0.9183673469387755, 'imitation_acc': 0.7687074829931972}, {'decision': 2, 'optimal_auc': 0.9375750300120047, 'imitation_auc': 0.7835346471710108, 'optimal_acc': 0.8775510204081632, 'imitation_acc': 0.8095238095238095}]\n",
      "tensor([0.5306, 0.1088, 0.1905], grad_fn=<MeanBackward1>)\n",
      "______epoch 21 _____\n",
      "val reward 0.9815046787261963\n",
      "imitation reward 1.3224868774414062\n",
      "distance losses 0.33989179134368896 0.20589902997016907\n",
      "distributions [0.4525318741798401, 0.03940312936902046, 0.20196016132831573]\n",
      "[{'decision': 0, 'optimal_auc': 0.8530286138981792, 'imitation_auc': 0.5448473282442747, 'optimal_acc': 0.7551020408163265, 'imitation_acc': 0.8639455782312925}, {'decision': 1, 'optimal_auc': 0.9685114503816794, 'imitation_auc': 0.7171195652173913, 'optimal_acc': 0.9115646258503401, 'imitation_acc': 0.7687074829931972}, {'decision': 2, 'optimal_auc': 0.9357743097238895, 'imitation_auc': 0.7819453273998729, 'optimal_acc': 0.8843537414965986, 'imitation_acc': 0.8163265306122449}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5306, 0.1088, 0.1905], grad_fn=<MeanBackward1>)\n",
      "______epoch 22 _____\n",
      "val reward 1.1334418058395386\n",
      "imitation reward 1.2089695930480957\n",
      "distance losses 0.29326745867729187 0.24145784974098206\n",
      "distributions [0.3738364279270172, 0.02528984285891056, 0.12241002917289734]\n",
      "[{'decision': 0, 'optimal_auc': 0.847640282422891, 'imitation_auc': 0.5443702290076337, 'optimal_acc': 0.7278911564625851, 'imitation_acc': 0.8707482993197279}, {'decision': 1, 'optimal_auc': 0.9704198473282443, 'imitation_auc': 0.7127717391304348, 'optimal_acc': 0.8979591836734694, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.9312725090036015, 'imitation_auc': 0.7784488239033693, 'optimal_acc': 0.8571428571428571, 'imitation_acc': 0.8163265306122449}]\n",
      "tensor([0.5306, 0.1088, 0.1905], grad_fn=<MeanBackward1>)\n",
      "______epoch 23 _____\n",
      "val reward 0.9867388010025024\n",
      "imitation reward 1.1870613098144531\n",
      "distance losses 0.3251011371612549 0.18892252445220947\n",
      "distributions [0.42151644825935364, 0.04411620646715164, 0.15291514992713928]\n",
      "[{'decision': 0, 'optimal_auc': 0.8651059085841695, 'imitation_auc': 0.5658396946564885, 'optimal_acc': 0.7414965986394558, 'imitation_acc': 0.8707482993197279}, {'decision': 1, 'optimal_auc': 0.9732824427480916, 'imitation_auc': 0.7116847826086956, 'optimal_acc': 0.9183673469387755, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': 0.9315726290516206, 'imitation_auc': 0.7800381436745074, 'optimal_acc': 0.8571428571428571, 'imitation_acc': 0.8095238095238095}]\n",
      "tensor([0.5306, 0.1088, 0.1905], grad_fn=<MeanBackward1>)\n",
      "______epoch 24 _____\n",
      "val reward 0.9091851711273193\n",
      "imitation reward 1.324074625968933\n",
      "distance losses 0.3291180431842804 0.20200978219509125\n",
      "distributions [0.47115781903266907, 0.055040307343006134, 0.19493430852890015]\n",
      "[{'decision': 0, 'optimal_auc': 0.8742103307320699, 'imitation_auc': 0.58206106870229, 'optimal_acc': 0.7619047619047619, 'imitation_acc': 0.8775510204081632}, {'decision': 1, 'optimal_auc': 0.9742366412213741, 'imitation_auc': 0.6942934782608696, 'optimal_acc': 0.9251700680272109, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.9303721488595438, 'imitation_auc': 0.7714558169103624, 'optimal_acc': 0.8843537414965986, 'imitation_acc': 0.8163265306122449}]\n",
      "tensor([0.5306, 0.1088, 0.1905], grad_fn=<MeanBackward1>)\n",
      "______epoch 25 _____\n",
      "val reward 0.9633868336677551\n",
      "imitation reward 1.3254624605178833\n",
      "distance losses 0.35098177194595337 0.20758765935897827\n",
      "distributions [0.4251573979854584, 0.04633929580450058, 0.21593210101127625]\n",
      "[{'decision': 0, 'optimal_auc': 0.8701226309921963, 'imitation_auc': 0.5777671755725191, 'optimal_acc': 0.7414965986394558, 'imitation_acc': 0.8639455782312925}, {'decision': 1, 'optimal_auc': 0.9747137404580153, 'imitation_auc': 0.6956521739130435, 'optimal_acc': 0.9183673469387755, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.9363745498199281, 'imitation_auc': 0.7705022250476795, 'optimal_acc': 0.8775510204081632, 'imitation_acc': 0.8163265306122449}]\n",
      "tensor([0.5306, 0.1088, 0.1905], grad_fn=<MeanBackward1>)\n",
      "______epoch 26 _____\n",
      "val reward 1.0320255756378174\n",
      "imitation reward 1.2420313358306885\n",
      "distance losses 0.3034859597682953 0.19388525187969208\n",
      "distributions [0.3933660089969635, 0.03193878009915352, 0.18622595071792603]\n",
      "[{'decision': 0, 'optimal_auc': 0.8593459680416203, 'imitation_auc': 0.5620229007633588, 'optimal_acc': 0.7074829931972789, 'imitation_acc': 0.8503401360544217}, {'decision': 1, 'optimal_auc': 0.9718511450381679, 'imitation_auc': 0.6855978260869565, 'optimal_acc': 0.9115646258503401, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.9441776710684273, 'imitation_auc': 0.756198347107438, 'optimal_acc': 0.8775510204081632, 'imitation_acc': 0.8027210884353742}]\n",
      "tensor([0.5306, 0.1088, 0.1905], grad_fn=<MeanBackward1>)\n",
      "______epoch 27 _____\n",
      "val reward 0.9957754611968994\n",
      "imitation reward 1.2956981658935547\n",
      "distance losses 0.32460150122642517 0.19403459131717682\n",
      "distributions [0.38069871068000793, 0.0423460528254509, 0.21367526054382324]\n",
      "[{'decision': 0, 'optimal_auc': 0.8680787811222593, 'imitation_auc': 0.5753816793893131, 'optimal_acc': 0.7142857142857143, 'imitation_acc': 0.8775510204081632}, {'decision': 1, 'optimal_auc': 0.9747137404580153, 'imitation_auc': 0.6711956521739131, 'optimal_acc': 0.9183673469387755, 'imitation_acc': 0.7891156462585034}, {'decision': 2, 'optimal_auc': 0.9447779111644659, 'imitation_auc': 0.7628734901462175, 'optimal_acc': 0.8843537414965986, 'imitation_acc': 0.8095238095238095}]\n",
      "tensor([0.5306, 0.1088, 0.1905], grad_fn=<MeanBackward1>)\n",
      "______epoch 28 _____\n",
      "val reward 0.9072674512863159\n",
      "imitation reward 1.3268213272094727\n",
      "distance losses 0.3685901165008545 0.2207525074481964\n",
      "distributions [0.4285230338573456, 0.055886197835206985, 0.21525044739246368]\n",
      "[{'decision': 0, 'optimal_auc': 0.8690078037904125, 'imitation_auc': 0.6044847328244275, 'optimal_acc': 0.7482993197278912, 'imitation_acc': 0.8707482993197279}, {'decision': 1, 'optimal_auc': 0.9775763358778626, 'imitation_auc': 0.6692934782608695, 'optimal_acc': 0.9251700680272109, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.9468787515006002, 'imitation_auc': 0.7781309599491417, 'optimal_acc': 0.891156462585034, 'imitation_acc': 0.8095238095238095}]\n",
      "++++++++++Final+++++++++++\n",
      "best tensor(1.5842, grad_fn=<AddBackward0>)\n",
      "[{'decision': 0, 'optimal_auc': 0.8411371237458194, 'imitation_auc': 0.58206106870229, 'optimal_acc': 0.7891156462585034, 'imitation_acc': 0.8775510204081632}, {'decision': 1, 'optimal_auc': 0.9728053435114504, 'imitation_auc': 0.7309782608695653, 'optimal_acc': 0.9251700680272109, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': 0.9450780312124849, 'imitation_auc': 0.7933884297520661, 'optimal_acc': 0.8979591836734694, 'imitation_acc': 0.8095238095238095}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionAttentionModel(\n",
       "  (input_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=100, out_features=1000, bias=True)\n",
       "  )\n",
       "  (batchnorm): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       "  (relu): Softplus(beta=1, threshold=20)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (softmax): Softmax(dim=1)\n",
       "  (final_opt_layer): Linear(in_features=1000, out_features=100, bias=True)\n",
       "  (final_imitation_layer): Linear(in_features=1000, out_features=100, bias=True)\n",
       "  (final_layer): Linear(in_features=1000, out_features=6, bias=True)\n",
       "  (resize_layer): Linear(in_features=90, out_features=100, bias=True)\n",
       "  (attentions): ModuleList(\n",
       "    (0): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (norms): ModuleList(\n",
       "    (0): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (activation): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_unique_sequence(array):\n",
    "    #converts a row of boolean values to a unique number e.g. [1,1,0] => 11, [0,0,1] => 100\n",
    "    uniqueify = lambda r: torch.sum(torch.stack([i*(10**ii) for ii,i in enumerate(r)]))\n",
    "    return torch_apply_along_axis(uniqueify,array)\n",
    "\n",
    "def train_decision_model_triplet(\n",
    "    tmodel1,\n",
    "    tmodel2,\n",
    "    tmodel3,\n",
    "    smodel3,\n",
    "    use_default_split=True,\n",
    "    use_bagging_split=False,\n",
    "    use_attention=True,\n",
    "    lr=.001,\n",
    "    epochs=10000,\n",
    "    patience=5,\n",
    "    weights=[0,.5,.5,0], #realtive weight of survival, feeding tube, aspiration, andl lrc\n",
    "    tweights=[1,1,1,0], #weight for OS, LRC, FDM, and event (any + FT or AS at 6m) as time to event in weeks\n",
    "    opt_weights=[1,1,1], #weights for policy model for optimal decisions\n",
    "    imitation_weights=[.5,1,1],#weights of imitation decisions, because ic overtrains too quickly\n",
    "    imitation_weight=1,\n",
    "    reward_weight=1,\n",
    "    imitation_triplet_weight=2,\n",
    "    reward_triplet_weight = 2,\n",
    "    shufflecol_chance = 0.2,\n",
    "    split=.7,\n",
    "    resample_training=False,\n",
    "    save_path='../data/models/',\n",
    "    file_suffix='',\n",
    "    verbose=True,\n",
    "    use_gpu=False,\n",
    "    **model_kwargs,\n",
    "):\n",
    "    tmodel1.eval()\n",
    "    tmodel2.eval()\n",
    "    tmodel3.eval()\n",
    "\n",
    "    train_ids, test_ids = get_tt_split(use_default_split=use_default_split,use_bagging_split=use_bagging_split,resample_training=resample_training)\n",
    "    true_ids = train_ids + test_ids #for saving memory without upsampling\n",
    "\n",
    "    dataset = DTDataset()\n",
    "    data = dataset.processed_df.copy()\n",
    "    \n",
    "    def get_dlt(state):\n",
    "        if state == 2:\n",
    "            return data[Const.dlt2].copy()\n",
    "        d = data[Const.dlt1].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_pd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.primary_disease_states2].copy()\n",
    "        d = data[Const.primary_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_nd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.nodal_disease_states2].copy()\n",
    "        d = data[Const.nodal_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_cc(state):\n",
    "        res = data[Const.ccs].copy()\n",
    "        if state == 1:\n",
    "            res.values[:,:] = np.zeros(res.values.shape)\n",
    "        return res\n",
    "    \n",
    "    def get_mod(state):\n",
    "        res = data[Const.modifications].copy()\n",
    "        #this should have an ic condition but we don't use it anumore anywa\n",
    "        return res\n",
    "        \n",
    "    outcomedf = data[Const.outcomes]\n",
    "    baseline = dataset.get_state('baseline')\n",
    "    \n",
    "    def formatdf(d,dids=train_ids):\n",
    "        d = df_to_torch(d.loc[dids]).to(model.get_device())\n",
    "        return d\n",
    "    \n",
    "    def makegrad(v):\n",
    "        if not v.requires_grad:\n",
    "            v.requires_grad=True\n",
    "        return v\n",
    "    \n",
    "    if use_attention:\n",
    "        model = DecisionAttentionModel(baseline.shape[1],**model_kwargs)\n",
    "    else:\n",
    "        model_kwargs = {k:v for k,v in model_kwargs.items() if 'attention' not in k and 'embed' not in k}\n",
    "        model = DecisionModel(baseline.shape[1],**model_kwargs)\n",
    "        \n",
    "    device = 'cpu'\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "        \n",
    "    model.set_device(device)\n",
    "\n",
    "    tmodel1.set_device(device)\n",
    "    tmodel2.set_device(device)\n",
    "    tmodel3.set_device(device)\n",
    "    smodel3.set_device(device)\n",
    "    hashcode = str(hash(','.join([str(i) for i in train_ids])))\n",
    "    \n",
    "    save_file = save_path + 'model_' + model.identifier +'_hash' + hashcode + file_suffix + '.tar'\n",
    "    model.fit_normalizer(df_to_torch(baseline.loc[train_ids]))\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "    \n",
    "    \n",
    "    optimal_train,transitions_train = calc_optimal_decisions(dataset,train_ids,tmodel1,tmodel2,tmodel3,smodel3,\n",
    "                                           weights=weights,tweights=tweights,\n",
    "                                          )\n",
    "    optimal_test,transitions_test = calc_optimal_decisions(dataset,test_ids,tmodel1,tmodel2,tmodel3,smodel3,\n",
    "                                          weights=weights,tweights=tweights,\n",
    "                                         )\n",
    "    optimal_train = optimal_train.to(model.get_device())\n",
    "    optimal_test = optimal_test.to(model.get_device())\n",
    "    mse = torch.nn.MSELoss()\n",
    "    bce = torch.nn.BCELoss()\n",
    "    \n",
    "    def compare_decisions(d1,d2,d3,ids):\n",
    "#         ypred = np.concatenate([dd.cpu().detach().numpy().reshape(-1,1) for dd in [d1,d2,d3]],axis=1)\n",
    "        ytrue = df_to_torch(outcomedf.loc[ids])\n",
    "        dloss = bce(d1.view(-1),ytrue[:,0])\n",
    "        dloss += bce(d2.view(-1),ytrue[:,1])\n",
    "        dloss += bce(d3,view(-1),ytrue[:,2])\n",
    "        return dloss\n",
    "        \n",
    "    def remove_decisions(df):\n",
    "        cols = [c for c in df.columns if c not in Const.decisions ]\n",
    "        ddf = df[cols]\n",
    "        return ddf\n",
    "    \n",
    "    makeinput = lambda step,dids: df_to_torch(remove_decisions(dataset.get_input_state(step=step,ids=dids)))\n",
    "    threshold = lambda x: torch.gt(x,torch.rand(x.shape[0])).type(torch.FloatTensor)\n",
    "\n",
    "    randchoice = lambda x: x[torch.randint(len(x),(1,))[0]]\n",
    "    tloss_func = torch.nn.TripletMarginLoss()\n",
    "    def get_tloss(row,step,yt,x,imitation=True):\n",
    "        if yt[:,step].std() < .001:\n",
    "            return torch.tensor([0]).to(device)\n",
    "        positive_idx= torch.nonzero(yt[:,step] == yt[row,step])\n",
    "        if len(positive_idx) <= 1:\n",
    "            print('no losses','n positive',len(positive_idx),'yt',yt[row,step],'row',row,'step',step,'imitation',imitation,end='\\r')\n",
    "            return torch.tensor([0]).to(device)\n",
    "        positive_idx = torch.stack([ii for ii in positive_idx if ii != row]).view(-1)\n",
    "        negative_idx = torch.tensor([ii for ii in range(x.shape[0]) if ii not in positive_idx and ii != row])\n",
    "        if len(positive_idx) < 1 or len(negative_idx) < 1:\n",
    "            print('no losses','n positive',len(positive_idx),'n negative',len(negative_idx),'yt',yt[row,step],'row',row,'step',step,'imitation',imitation,end='\\r')\n",
    "            return torch.tensor([0]).to(device)\n",
    "        positive = x[randchoice(positive_idx)]\n",
    "        negative = x[randchoice(negative_idx)]\n",
    "        anchor = x[row]\n",
    "        if use_attention:\n",
    "            [anchor_embedding,pos_embedding,neg_embedding] = [model.get_embedding(xx.view(1,-1),position=step,use_saved_memory=True) for xx in [anchor,positive,negative]]\n",
    "        else:    \n",
    "            [anchor_embedding,pos_embedding,neg_embedding] = [model.get_embedding(xx.view(1,-1),position=step,concatenate=False)[int(imitation)] for xx in [anchor,positive,negative]]\n",
    "        tloss = tloss_func(anchor_embedding,pos_embedding,neg_embedding)\n",
    "        return tloss\n",
    "    #save the inputs from the whole dataset for future reference\n",
    "    if use_attention:\n",
    "        full_data = []\n",
    "        for mstep in [0,1,2]:\n",
    "            full_data_step = [baseline, get_dlt(min(mstep,1)),\n",
    "                         get_dlt(mstep),get_pd(mstep),get_nd(mstep),get_cc(mstep),get_mod(mstep)]\n",
    "            full_data_step = torch.cat([formatdf(fd,true_ids) for fd in full_data_step],axis=1)\n",
    "            full_data.append(full_data_step)\n",
    "        full_data = torch.stack(full_data)\n",
    "        model.save_memory(full_data)\n",
    "        print(full_data.shape)\n",
    "        \n",
    "    def step(train=True):\n",
    "        if train:\n",
    "            model.train(True)\n",
    "            tmodel1.train(True)\n",
    "            tmodel2.train(True)\n",
    "            tmodel3.train(True)\n",
    "            optimizer.zero_grad()\n",
    "            ids = train_ids\n",
    "            y_opt = makegrad(optimal_train)\n",
    "            transition_dict = {k: torch.clone(v).detach() for k,v in transitions_train.items()}\n",
    "        else:\n",
    "            ids = test_ids\n",
    "            model.eval()\n",
    "            tmodel1.eval()\n",
    "            tmodel2.eval()\n",
    "            tmodel3.eval()\n",
    "            y_opt = makegrad(optimal_test)\n",
    "            print(y_opt.mean(axis=0))\n",
    "            transition_dict = {k: torch.clone(v).detach() for k,v in transitions_test.items()}\n",
    "        model.set_device(device)\n",
    "        ytrain = df_to_torch(outcomedf.loc[ids]).to(device)\n",
    "        #imitation losses and decision 1\n",
    "        xxtrained = [baseline, get_dlt(0),get_dlt(0),get_pd(0),get_nd(0),get_cc(0),get_mod(0)]\n",
    "        xxtrain = [formatdf(xx,ids) for xx in xxtrained]\n",
    "        xxtrain = torch.cat(xxtrain,axis=1).to(device)\n",
    "        o1 = model(xxtrain,position=0,use_saved_memory= (not train))\n",
    "        decision1_imitation = o1[:,3]\n",
    "        decision1_opt = o1[:,0]\n",
    "    \n",
    "        imitation_loss1 = bce(decision1_imitation,ytrain[:,0])\n",
    "        imitation_loss1 = torch.mul(imitation_loss1,imitation_weights[0])\n",
    "        opt_loss1 = bce(decision1_opt,y_opt[:,0])\n",
    "        opt_loss1 = torch.mul(opt_loss1,opt_weights[0])\n",
    "        \n",
    "        x1_imitation = [baseline, get_dlt(1),get_dlt(0),get_pd(1),get_nd(1),get_cc(1),get_mod(1)]\n",
    "        x1_imitation = [formatdf(xx1,ids) for xx1 in x1_imitation]\n",
    "        x1_imitation = torch.cat(x1_imitation,axis=1).to(device)\n",
    "        \n",
    "        o2 = model(x1_imitation,position=1,use_saved_memory= (not train))\n",
    "            \n",
    "        decision2_imitation = o2[:,4]\n",
    "            \n",
    "        imitation_loss2 =  bce(decision2_imitation,ytrain[:,1])\n",
    "        imitation_loss2 = torch.mul(imitation_loss2,imitation_weights[1])\n",
    "        \n",
    "        \n",
    "        x2_imitation = [baseline, get_dlt(1),get_dlt(2),get_pd(2),get_nd(2),get_cc(2),get_mod(2)]\n",
    "        \n",
    "        x2_imitation = [formatdf(xx2,ids) for xx2 in x2_imitation]\n",
    "        x2_imitation = torch.cat(x2_imitation,axis=1).to(device)\n",
    "        \n",
    "        \n",
    "        o3 = model(x2_imitation,position=2,use_saved_memory= (not train))\n",
    "        \n",
    "        decision3_imitation = o3[:,5]\n",
    "        \n",
    "        imitation_loss3 = bce(decision3_imitation,ytrain[:,2])\n",
    "        imitation_loss3 = torch.mul(imitation_loss3,imitation_weights[2])\n",
    "        \n",
    "        opt_input2 = [\n",
    "            formatdf(baseline,ids), \n",
    "            transition_dict['dlt1'],\n",
    "            formatdf(get_dlt(0),ids),\n",
    "            transition_dict['pd1'],\n",
    "            transition_dict['nd1'], \n",
    "            formatdf(get_cc(0),ids),\n",
    "            transition_dict['mod']\n",
    "                 ]\n",
    "        opt_input2 = [o.to(device) for o in opt_input2]\n",
    "\n",
    "        opt_input2 = torch.cat(opt_input2,axis=1).to(device)\n",
    "        decision2_opt = model(opt_input2,position=1,use_saved_memory= (not train))[:,1]\n",
    "        \n",
    "        opt_loss2 = bce(decision2_opt,y_opt[:,1])\n",
    "        opt_loss2 = torch.mul(opt_loss2,opt_weights[1])\n",
    "        \n",
    "        opt_input3 = [\n",
    "            formatdf(baseline,ids),\n",
    "            transition_dict['dlt1'],\n",
    "            transition_dict['dlt2'],\n",
    "            transition_dict['pd2'],\n",
    "            transition_dict['nd2'],\n",
    "            transition_dict['cc'],\n",
    "            transition_dict['mod'],\n",
    "        ]\n",
    "        opt_input3 = [o.to(device) for o in opt_input3]\n",
    "        opt_input3 = torch.cat(opt_input3,axis=1).to(device)\n",
    "        decision3_opt = model(opt_input3,position=2,use_saved_memory= (not train))[:,2]\n",
    "        \n",
    "        opt_loss3 = bce(decision3_opt,y_opt[:,2])\n",
    "        opt_loss3 = torch.mul(opt_loss3,opt_weights[2])\n",
    "        \n",
    "        iloss = torch.add(torch.add(imitation_loss1,imitation_loss2),imitation_loss3)\n",
    "        iloss = torch.mul(iloss,imitation_weight)\n",
    "        \n",
    "        reward_loss = torch.add(torch.add(opt_loss1,opt_loss2),opt_loss3)\n",
    "        reward_loss =torch.mul(reward_loss,reward_weight)\n",
    "        \n",
    "        loss = torch.add(iloss,reward_loss)\n",
    "        \n",
    "        imitation_tloss = torch.FloatTensor([0]).to(device)\n",
    "        opt_tloss = torch.FloatTensor([0]).to(device)\n",
    "        n_rows = xxtrain.shape[0]\n",
    "        if reward_triplet_weight + imitation_triplet_weight > 0.0001:\n",
    "            for i in range(n_rows):\n",
    "                \n",
    "                if imitation_triplet_weight > .0001:\n",
    "                    imitation_tloss += get_tloss(i,0,ytrain,xxtrain,True)\n",
    "                    imitation_tloss += get_tloss(i,1,ytrain,x1_imitation,True)\n",
    "                    imitation_tloss += get_tloss(i,2,ytrain,x2_imitation,True)\n",
    "                if reward_triplet_weight > .0001:\n",
    "                    opt_tloss += get_tloss(i,0,y_opt,xxtrain,False)\n",
    "                    opt_tloss += get_tloss(i,1,y_opt,opt_input2,False)\n",
    "                    opt_tloss += get_tloss(i,2,y_opt,opt_input3,False)\n",
    "            loss += torch.mul(imitation_tloss[0],imitation_triplet_weight/n_rows)\n",
    "            loss += torch.mul(opt_tloss[0],reward_triplet_weight/n_rows)\n",
    "        losses = [iloss,reward_loss,imitation_tloss*imitation_triplet_weight/n_rows,opt_tloss*reward_triplet_weight/n_rows]\n",
    "        \n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            return losses\n",
    "        else:\n",
    "            scores = []\n",
    "            distributions = [decision1_opt.mean().item(),decision2_opt.mean().item(),decision3_opt.mean().item()]\n",
    "            imitation = [decision1_imitation,decision2_imitation,decision3_imitation]\n",
    "            optimal = [decision1_opt,decision2_opt,decision3_opt]\n",
    "            for i,decision_im in enumerate(imitation):\n",
    "                deci = decision_im.cpu().detach().numpy()\n",
    "                deci0 = (deci > .5).astype(int)\n",
    "                iout = ytrain[:,i].cpu().detach().numpy()\n",
    "                acci = accuracy_score(iout,deci0)\n",
    "                try:\n",
    "                    auci = roc_auc_score(iout,deci)\n",
    "                except:\n",
    "                    auci = -1\n",
    "                \n",
    "                deco = optimal[i].cpu().detach().numpy()\n",
    "                deci0 = (deco > .5).astype(int)\n",
    "                oout = y_opt[:,i].cpu().detach().numpy()\n",
    "                acco = accuracy_score(oout,deci0)\n",
    "                try:\n",
    "                    auco = roc_auc_score(oout,deco)\n",
    "                except:\n",
    "                    auco=-1\n",
    "                scores.append({'decision': i,'optimal_auc': auco,'imitation_auc': auci,'optimal_acc': acco,'imitation_acc': acci})\n",
    "            return losses, scores, distributions\n",
    "        \n",
    "    best_val_loss = torch.tensor(1000000000.0)\n",
    "    steps_since_improvement = 0\n",
    "    best_val_score = {}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        losses = step(True)\n",
    "        val_losses,val_metrics,val_distributions = step(False)\n",
    "        vl = val_losses[0] + val_losses[1]\n",
    "        for vm in val_metrics:\n",
    "            vl += (-((vm['optimal_auc']*reward_weight) + (vm['imitation_auc']*imitation_weight)))/10\n",
    "        if verbose:\n",
    "            print('______epoch',str(epoch),'_____')\n",
    "            print('val reward',val_losses[1].item())\n",
    "            print('imitation reward', val_losses[0].item())\n",
    "            print('distance losses',val_losses[2].item(),val_losses[-1].item())\n",
    "            print('distributions',val_distributions)\n",
    "            print(val_metrics)\n",
    "        if vl < best_val_loss:\n",
    "            best_val_loss = vl\n",
    "            best_val_score = val_metrics\n",
    "            best_val_distributions = val_distributions\n",
    "            steps_since_improvement = 0\n",
    "            torch.save(model.state_dict(),save_file)\n",
    "        else:\n",
    "            steps_since_improvement += 1\n",
    "        if steps_since_improvement > patience:\n",
    "            break\n",
    "    print('++++++++++Final+++++++++++')\n",
    "    print('best',best_val_loss)\n",
    "    print(best_val_score)\n",
    "    model.load_state_dict(torch.load(save_file))\n",
    "    model.eval()\n",
    "    return model, best_val_score, best_val_loss, best_val_distributions\n",
    "\n",
    "from Models import *\n",
    "args = {\n",
    "    'hidden_layers': [1000], \n",
    "    'opt_layer_size': 20, \n",
    "    'imitation_layer_size': 20, \n",
    "    'dropout': 0.25, \n",
    "    'input_dropout': 0.1, \n",
    "    'shufflecol_chance': 0.5\n",
    "}\n",
    "\n",
    "#1.8424\n",
    "decision_model, decision_score, decision_loss, _ = train_decision_model_triplet(\n",
    "    model1,model2,model3,smodel3,\n",
    "    lr=.01,\n",
    "    imitation_weight=1,\n",
    "    reward_weight=1,\n",
    "    patience=10,\n",
    "    imitation_triplet_weight=0.1,\n",
    "    reward_triplet_weight =0.1,\n",
    "    verbose=True,\n",
    "    weights=[1,1,1,1], #realtive weight of survival, feeding tube, aspiration, andl lrc\n",
    "    tweights=[1,1,1,0], #weight for OS, LRC, FDM, and event (any + FT or AS at 6m) as time to event in weeks\n",
    "    use_attention=True,\n",
    "    **args)\n",
    "decision_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73f37f8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decision</th>\n",
       "      <th>optimal_auc</th>\n",
       "      <th>imitation_auc</th>\n",
       "      <th>optimal_acc</th>\n",
       "      <th>imitation_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.841137</td>\n",
       "      <td>0.582061</td>\n",
       "      <td>0.789116</td>\n",
       "      <td>0.877551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.972805</td>\n",
       "      <td>0.730978</td>\n",
       "      <td>0.925170</td>\n",
       "      <td>0.775510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.945078</td>\n",
       "      <td>0.793388</td>\n",
       "      <td>0.897959</td>\n",
       "      <td>0.809524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   decision  optimal_auc  imitation_auc  optimal_acc  imitation_acc\n",
       "0         0     0.841137       0.582061     0.789116       0.877551\n",
       "1         1     0.972805       0.730978     0.925170       0.775510\n",
       "2         2     0.945078       0.793388     0.897959       0.809524"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_model.set_device('cpu')\n",
    "torch.save(decision_model,'../resources/decision_model.pt')\n",
    "pd.DataFrame(decision_score).to_csv('../results/policy_model_score.csv')\n",
    "pd.DataFrame(decision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc3c8bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decision</th>\n",
       "      <th>optimal_auc</th>\n",
       "      <th>imitation_auc</th>\n",
       "      <th>optimal_acc</th>\n",
       "      <th>imitation_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.829180</td>\n",
       "      <td>0.501908</td>\n",
       "      <td>0.823129</td>\n",
       "      <td>0.863946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.972194</td>\n",
       "      <td>0.699728</td>\n",
       "      <td>0.938776</td>\n",
       "      <td>0.755102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.950043</td>\n",
       "      <td>0.776542</td>\n",
       "      <td>0.918367</td>\n",
       "      <td>0.782313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   decision  optimal_auc  imitation_auc  optimal_acc  imitation_acc\n",
       "0         0     0.829180       0.501908     0.823129       0.863946\n",
       "1         1     0.972194       0.699728     0.938776       0.755102\n",
       "2         2     0.950043       0.776542     0.918367       0.782313"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(decision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b25922a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.8424\n",
    "decision_model2, decision_score2, decision_loss2, _ = train_decision_model_triplet(\n",
    "    model1,model2,model3,smodel3,\n",
    "    lr=.01,\n",
    "    imitation_weight=1,\n",
    "    reward_weight=1,\n",
    "    patience=10,\n",
    "    imitation_triplet_weight=0,\n",
    "    reward_triplet_weight =0,\n",
    "    verbose=True,\n",
    "    weights=[0,0,0,0], #realtive weight of survival, feeding tube, aspiration, andl lrc\n",
    "    tweights=[1,1,0,0], #weight for OS, LRC, FDM, and event (any + FT or AS at 6m) as time to event in weeks\n",
    "    use_attention=True,\n",
    "    **args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a7faad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decision_model(\n",
    "    tmodel1,\n",
    "    tmodel2,\n",
    "    tmodel3,\n",
    "    smodel3,\n",
    "    use_default_split=True,\n",
    "    use_bagging_split=False,\n",
    "    lr=.0001,\n",
    "    epochs=10000,\n",
    "    patience=50,\n",
    "    weights=[0,.5,.5,0], #realtive weight of survival, feeding tube, aspiration, andl lrc\n",
    "    tweights=[1,1,1,0]\n",
    "    imitation_weights=[.5,1,1],#weights of imitation decisions, because ic overtrains too quickly\n",
    "    imitation_weight=0.1,\n",
    "    shufflecol_chance = 0.1,\n",
    "    reward_weight=1,\n",
    "    imitation_triplet_weight=0,\n",
    "    reward_triplet_weight = 0,\n",
    "    split=.7,\n",
    "    resample_training=False,\n",
    "    save_path='../data/models/',\n",
    "    file_suffix='',\n",
    "    use_gpu=True,\n",
    "    use_attention=True,\n",
    "    verbose=True,\n",
    "    threshold_decisions=True,#convert decisiosn to binary in simulation, usually breaks it\n",
    "    use_smote=False,\n",
    "    validate_with_memory=True,\n",
    "    **model_kwargs):\n",
    "    #outdated method of doing stuff, haven't updated with new loss functions idk\n",
    "    tmodel1.eval()\n",
    "    tmodel2.eval()\n",
    "    tmodel3.eval()\n",
    "    smodel3.eval()\n",
    "    train_ids, test_ids = get_tt_split(use_default_split=use_default_split,use_bagging_split=use_bagging_split,resample_training=resample_training)\n",
    "    true_ids = train_ids + test_ids #for saving memory without upsampling\n",
    "    if use_smote:\n",
    "        dataset = DTDataset(use_smote=True,smote_ids = train_ids)\n",
    "        train_ids = [i for i in dataset.processed_df.index.values if i not in test_ids]\n",
    "    else:\n",
    "        dataset = DTDataset()\n",
    "    data = dataset.processed_df.copy()\n",
    "    \n",
    "    def get_dlt(state):\n",
    "        if state == 2:\n",
    "            return data[Const.dlt2].copy()\n",
    "        d = data[Const.dlt1].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_pd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.primary_disease_states2].copy()\n",
    "        d = data[Const.primary_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_nd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.nodal_disease_states2].copy()\n",
    "        d = data[Const.nodal_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_cc(state):\n",
    "        res = data[Const.ccs].copy()\n",
    "        if state == 1:\n",
    "            res.values[:,:] = np.zeros(res.values.shape)\n",
    "        return res\n",
    "    \n",
    "    def get_mod(state):\n",
    "        res = data[Const.modifications].copy()\n",
    "        #this should have an ic condition but we don't use it anumore anywa\n",
    "        return res\n",
    "        \n",
    "    outcomedf = data[Const.outcomes]\n",
    "    baseline = dataset.get_state('baseline')\n",
    "    \n",
    "    def formatdf(d,dids=train_ids):\n",
    "        d = df_to_torch(d.loc[dids]).to(model.get_device())\n",
    "        return d\n",
    "    \n",
    "    def makegrad(v):\n",
    "        if not v.requires_grad:\n",
    "            v.requires_grad=True\n",
    "        return v\n",
    "    \n",
    "    if use_attention:\n",
    "        model = DecisionAttentionModel(baseline.shape[1],**model_kwargs)\n",
    "    else:\n",
    "        model_kwargs = {k:v for k,v in model_kwargs.items() if 'attention' not in k and 'embed' not in k}\n",
    "        model = DecisionModel(baseline.shape[1],**model_kwargs)\n",
    "\n",
    "    device = 'cpu'\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "        \n",
    "    model.set_device(device)\n",
    "\n",
    "    tmodel1.set_device(device)\n",
    "    tmodel2.set_device(device)\n",
    "    tmodel3.set_device(device)\n",
    "    smodel3.set_device(device)\n",
    "    \n",
    "    hashcode = str(hash(','.join([str(i) for i in train_ids])))\n",
    "    \n",
    "    save_file = save_path + 'model_' + model.identifier +'_hash' + hashcode + file_suffix + '.tar'\n",
    "    model.fit_normalizer(df_to_torch(baseline.loc[train_ids]).to(model.get_device()))\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "    \n",
    "    mse = torch.nn.MSELoss()\n",
    "    bce = torch.nn.BCELoss()\n",
    "    device = model.get_device()\n",
    "    def compare_decisions(d1,d2,d3,ids):\n",
    "#         ypred = np.concatenate([dd.cpu().detach().numpy().reshape(-1,1) for dd in [d1,d2,d3]],axis=1)\n",
    "        ytrue = df_to_torch(outcomedf.loc[ids])\n",
    "        dloss = bce(d1.view(-1),ytrue[:,0])\n",
    "        dloss += bce(d2.view(-1),ytrue[:,1])\n",
    "        dloss += bce(d3,view(-1),ytrue[:,2])\n",
    "        return dloss\n",
    "        \n",
    "    def remove_decisions(df):\n",
    "        cols = [c for c in df.columns if c not in Const.decisions ]\n",
    "        ddf = df[cols]\n",
    "        return ddf\n",
    "    \n",
    "    makeinput = lambda step,dids: df_to_torch(remove_decisions(dataset.get_input_state(step=step,ids=dids))).to(device)\n",
    "    thresh = lambda x: torch.sigmoid(100000000*(x - .5))\n",
    "\n",
    "    optimal_train,transitions_train = calc_optimal_decisions(dataset,train_ids,tmodel1,tmodel2,tmodel3,smodel3,\n",
    "                                           weights=weights,tweights=tweights,\n",
    "                                          )\n",
    "    optimal_test,transitions_test = calc_optimal_decisions(dataset,test_ids,tmodel1,tmodel2,tmodel3,smodel3,\n",
    "                                          weights=weights,tweights=tweights,\n",
    "                                         )\n",
    "    optimal_train = optimal_train.to(model.get_device())\n",
    "    optimal_test = optimal_test.to(model.get_device())\n",
    "    \n",
    "    #save the inputs from the whole dataset for future reference\n",
    "    if use_attention:\n",
    "        full_data = []\n",
    "        for mstep in [0,1,2]:\n",
    "            full_data_step = [baseline, get_dlt(min(mstep,1)),\n",
    "                         get_dlt(mstep),get_pd(mstep),get_nd(mstep),get_cc(mstep),get_mod(mstep)]\n",
    "            full_data_step = torch.cat([formatdf(fd,true_ids) for fd in full_data_step],axis=1)\n",
    "            full_data.append(full_data_step)\n",
    "        full_data = torch.stack(full_data)\n",
    "        model.save_memory(full_data.to(device))\n",
    "        print(full_data.shape)\n",
    "        \n",
    "    randchoice = lambda x: x[torch.randint(len(x),(1,))[0]]\n",
    "    tloss_func = torch.nn.TripletMarginLoss()\n",
    "    def get_tloss(row,step,yt,x,imitation=True):\n",
    "        if yt[:,step].std() < .001:\n",
    "            return torch.tensor([0]).to(device)\n",
    "        positive_idx= torch.nonzero(yt[:,step] == yt[row,step])\n",
    "        positive_idx = torch.stack([ii for ii in positive_idx if ii != row]).view(-1)\n",
    "        negative_idx = torch.tensor([ii for ii in range(x.shape[0]) if ii not in positive_idx and ii != row])\n",
    "        if len(positive_idx) < 1 or len(negative_idx) < 1:\n",
    "            print('no losses','n positive',len(positive_idx),'n negative',len(negative_idx),'yt',yt[row,step],'row',row,'step',step,'imitation',imitation,end='\\r')\n",
    "            return torch.tensor([0]).to(device)\n",
    "        positive = x[randchoice(positive_idx)]\n",
    "        negative = x[randchoice(negative_idx)]\n",
    "        anchor = x[row]\n",
    "        if use_attention:\n",
    "            [anchor_embedding,pos_embedding,neg_embedding] = [model.get_embedding(xx.view(1,-1),position=step,use_saved_memory=True) for xx in [anchor,positive,negative]]\n",
    "        else:    \n",
    "            [anchor_embedding,pos_embedding,neg_embedding] = [model.get_embedding(xx.view(1,-1),position=step,concatenate=False)[int(imitation)] for xx in [anchor,positive,negative]]\n",
    "        tloss = tloss_func(anchor_embedding,pos_embedding,neg_embedding)\n",
    "        return tloss\n",
    "    \n",
    "    def step(train=True):\n",
    "        if train:\n",
    "            model.train(True)\n",
    "            tmodel1.train(True)\n",
    "            tmodel2.train(True)\n",
    "            tmodel3.train(True)\n",
    "            optimizer.zero_grad()\n",
    "            ids = train_ids\n",
    "            y_opt = makegrad(optimal_train)\n",
    "        else:\n",
    "            ids = test_ids\n",
    "            model.eval()\n",
    "            tmodel1.eval()\n",
    "            tmodel2.eval()\n",
    "            tmodel3.eval()\n",
    "            y_opt = makegrad(optimal_test)\n",
    "            \n",
    "            \n",
    "        ytrain = df_to_torch(outcomedf.loc[ids]).to(device)\n",
    "        #imitation losses and decision 1\n",
    "        xxtrained = [baseline, get_dlt(0),get_dlt(0),get_pd(0),get_nd(0),get_cc(0),get_mod(0)]\n",
    "        xxtrain = torch.cat([formatdf(xx,ids) for xx in xxtrained],axis=1).to(device)\n",
    "        \n",
    "        use_memory = (not train) and validate_with_memory\n",
    "\n",
    "        o1 = model(xxtrain,position=0,use_saved_memory = use_memory)\n",
    "\n",
    "        decision1_imitation = o1[:,3]\n",
    "        \n",
    "        decision1_opt = o1[:,0]\n",
    "        if threshold_decisions:\n",
    "            decision1_opt = thresh(decision1_opt)\n",
    "\n",
    "        imitation_loss1 = bce(decision1_imitation,ytrain[:,0])\n",
    "        imitation_loss1 = torch.mul(imitation_loss1,imitation_weights[0])\n",
    "        \n",
    "        x1_imitation = [baseline, get_dlt(1),get_dlt(0),get_pd(1),get_nd(1),get_cc(1),get_mod(1)]\n",
    "        x1_imitation = [formatdf(xx1,ids) for xx1 in x1_imitation]\n",
    "        x1_imitation = torch.cat(x1_imitation,axis=1).to(device)\n",
    "        decision2_imitation = model(x1_imitation,position=1,use_saved_memory = use_memory)[:,4]\n",
    "\n",
    "        imitation_loss2 =  bce(decision2_imitation,ytrain[:,1])\n",
    "        imitation_loss2 = torch.mul(imitation_loss2,imitation_weights[1])\n",
    "        \n",
    "        x2_imitation = [baseline, get_dlt(1),get_dlt(2),get_pd(2),get_nd(2),get_cc(2),get_mod(2)]\n",
    "        \n",
    "        x2_imitation = [formatdf(xx2,ids) for xx2 in x2_imitation]\n",
    "        x2_imitation = torch.cat(x2_imitation,axis=1)\n",
    "        decision3_imitation = model(x2_imitation,position=2,use_saved_memory = use_memory)[:,5]\n",
    "\n",
    "        imitation_loss3 = bce(decision3_imitation,ytrain[:,2])\n",
    "        imitation_loss3 = torch.mul(imitation_loss3,imitation_weights[2])\n",
    "        \n",
    "        #reward decisions\n",
    "        xx1 = makeinput(1,ids)\n",
    "        xx2 = makeinput(2,ids)\n",
    "        xx3 = makeinput(3,ids)\n",
    "\n",
    "        xx1 = makegrad(xx1)\n",
    "        xx2 = makegrad(xx2)\n",
    "        xx3 = makegrad(xx3)\n",
    "        baseline_train_base = formatdf(baseline,ids)\n",
    "            \n",
    "        baseline_train = torch.clone(baseline_train_base)\n",
    "\n",
    "        \n",
    "        xi1 = torch.cat([xx1,decision1_opt.view(-1,1)],axis=1)\n",
    "        print(train,tmodel1.training,tmodel1.dropout.training)\n",
    "        [ypd1, ynd1, ymod, ydlt1] = tmodel1(xi1)['predictions']\n",
    "        print(train,tmodel1.training,tmodel1.dropout.training)\n",
    "        d1_thresh = torch.gt(decision1_opt.view(-1,1),.5).to(ypd1.device)\n",
    "        d1_scale = torch.cat([d1_thresh,d1_thresh,torch.ones(d1_thresh.view(-1,1).shape).to(ypd1.device)],dim=1)\n",
    "        ypd1= torch.mul(ypd1,d1_scale)\n",
    "        ynd1= torch.mul(ynd1,d1_scale)\n",
    "        \n",
    "        x1 = [baseline_train,ydlt1,formatdf(get_dlt(0),ids),ypd1,ynd1,formatdf(get_cc(1),ids),ymod]\n",
    "        x1= torch.cat([xx1.to(model.get_device()) for xx1 in x1],axis=1)\n",
    "        \n",
    "        decision2_opt = model(x1,position=1,use_saved_memory = use_memory)[:,1] \n",
    "        if threshold_decisions:\n",
    "            decision2_opt = thresh(decision2_opt)\n",
    "            \n",
    "        xi2 = torch.cat([xx2,decision1_opt.view(-1,1),decision2_opt.view(-1,1)],axis=1)\n",
    "        [ypd2,ynd2,ycc,ydlt2] = tmodel2(xi2)['predictions']\n",
    "\n",
    "        x2 = [baseline_train,ydlt1,ydlt2,ypd2,ynd2,ycc,ymod]\n",
    "        x2 = torch.cat([xx2.to(model.get_device()) for xx2 in x2],axis=1)\n",
    "        decision3_opt = model(x2,position=2,use_saved_memory = use_memory)[:,2]\n",
    "        \n",
    "        if threshold_decisions:\n",
    "            decision3_opt = thresh(decision3_opt)\n",
    "            \n",
    "        xi3 = torch.cat([xx3,decision1_opt.view(-1,1),decision2_opt.view(-1,1),decision3_opt.view(-1,1)],axis=1)\n",
    "        \n",
    "        outcomes = tmodel3(xi3)['predictions']\n",
    "        survival = smodel3.time_to_event(xi3,n_samples=1)\n",
    "        if not train and verbose:\n",
    "            print(torch.mean(outcomes,dim=0))\n",
    "            \n",
    "        reward_loss = torch.mean(outcome_loss(outcomes,weights) + temporal_loss(survival,tweights))\n",
    "        loss = torch.add(imitation_loss1,imitation_loss2)\n",
    "        loss = torch.add(loss,imitation_loss3)\n",
    "        loss = torch.mul(loss,imitation_weight/3)\n",
    "        loss = torch.add(loss,torch.mul(reward_loss,reward_weight))\n",
    "        \n",
    "        imitation_tloss = torch.FloatTensor([0]).to(device)\n",
    "        opt_tloss = torch.FloatTensor([0]).to(device)\n",
    "        n_rows = x1.shape[0]\n",
    "        if reward_triplet_weight + imitation_triplet_weight > 0.0001:\n",
    "            for i in range(n_rows):\n",
    "                #skip if we're using an attention model idk\n",
    "                if not use_attention and imitation_triplet_weight > .0001:\n",
    "                    imitation_tloss += get_tloss(i,0,ytrain,xxtrain,True)\n",
    "                    imitation_tloss += get_tloss(i,1,ytrain,x1_imitation,True)\n",
    "                    imitation_tloss += get_tloss(i,2,ytrain,x2_imitation,True)\n",
    "                if reward_triplet_weight > .0001:\n",
    "                    opt_tloss += get_tloss(i,0,y_opt,xxtrain,False)\n",
    "                    opt_tloss += get_tloss(i,1,y_opt,x1,False)\n",
    "                    opt_tloss += get_tloss(i,2,y_opt,x2,False)\n",
    "            loss += torch.mul(imitation_tloss[0],imitation_triplet_weight/n_rows)\n",
    "            loss += torch.mul(opt_tloss[0],reward_triplet_weight/n_rows)\n",
    "        \n",
    "        losses = [imitation_loss1+imitation_loss2+imitation_loss3,reward_loss,imitation_tloss,opt_tloss]\n",
    "\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            return losses\n",
    "        else:\n",
    "            scores = []\n",
    "            distributions = [decision1_opt.mean().item(),decision2_opt.mean().item(),decision3_opt.mean().item()]\n",
    "            imitation = [decision1_imitation,decision2_imitation,decision3_imitation]\n",
    "            optimal = [decision1_opt,decision2_opt,decision3_opt]\n",
    "            for i,decision_im in enumerate(imitation):\n",
    "                deci = decision_im.cpu().detach().numpy()\n",
    "                deci0 = (deci > .5).astype(int)\n",
    "                iout = ytrain[:,i].cpu().detach().numpy()\n",
    "                acci = accuracy_score(iout,deci0)\n",
    "                try:\n",
    "                    auci = roc_auc_score(iout,deci)\n",
    "                except:\n",
    "                    auci = -1\n",
    "                \n",
    "                deco = optimal[i].cpu().detach().numpy()\n",
    "                deci0 = (deco > .5).astype(int)\n",
    "                oout = y_opt[:,i].cpu().detach().numpy()\n",
    "                acco = accuracy_score(oout,deci0)\n",
    "                try:\n",
    "                    auco = roc_auc_score(oout,deco)\n",
    "                except:\n",
    "                    auco=-1\n",
    "                scores.append({'decision': i,'optimal_auc': auco,'imitation_auc': auci,'optimal_acc': acco,'imitation_acc': acci})\n",
    "            return losses, scores, distributions\n",
    "        \n",
    "    best_val_loss = torch.tensor(1000000000.0)\n",
    "    steps_since_improvement = 0\n",
    "    best_val_score = {}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        losses = step(True)\n",
    "        val_losses,val_metrics,val_distributions = step(False)\n",
    "        vl = val_losses[0] + val_losses[1]\n",
    "        for vm in val_metrics:\n",
    "            vl += (-((vm['optimal_auc']*reward_weight) + (vm['imitation_auc']*imitation_weight)))/10\n",
    "        if verbose:\n",
    "            print('______epoch',str(epoch),'_____')\n",
    "            print('val reward',val_losses[1].item())\n",
    "            print('imitation reward', val_losses[0].item())\n",
    "            if len(val_losses) > 2:\n",
    "                print('distance losses',val_losses[2].item(),val_losses[-1].item())\n",
    "            print('distributions',val_distributions)\n",
    "            print(val_metrics)\n",
    "        if vl < best_val_loss:\n",
    "            best_val_loss = vl\n",
    "            best_val_score = val_metrics\n",
    "            best_val_distributions = val_distributions\n",
    "            steps_since_improvement = 0\n",
    "            torch.save(model.state_dict(),save_file)\n",
    "        else:\n",
    "            steps_since_improvement += 1\n",
    "        if steps_since_improvement > patience:\n",
    "            break\n",
    "    print('++++++++++Final+++++++++++')\n",
    "    print('best',best_val_loss)\n",
    "    print(best_val_score)\n",
    "    model.load_state_dict(torch.load(save_file))\n",
    "    model.eval()\n",
    "    return model, best_val_score, best_val_loss, best_val_distributions\n",
    "\n",
    "from Models import *\n",
    "# args = {\n",
    "#     'hidden_layers': [50,50], \n",
    "#     'attention_heads': [2,2],\n",
    "#     'embed_size': 120, \n",
    "#     'dropout': 0.5, \n",
    "#     'input_dropout': 0.2, \n",
    "#     'shufflecol_chance':  0.2,\n",
    "# }\n",
    "args = {\n",
    "    'hidden_layers': [500], \n",
    "    'opt_layer_size': 20, \n",
    "    'imitation_layer_size': 20, \n",
    "    'dropout': 0.25, \n",
    "    'input_dropout': 0.25, \n",
    "    'shufflecol_chance': 0.5\n",
    "}\n",
    "from Models import *\n",
    "decision_model, _, _, _ = train_decision_model(\n",
    "    model1,model2,model3,smodel3,\n",
    "    lr=.001,\n",
    "    use_attention=True,\n",
    "    imitation_weight=1,\n",
    "    imitation_triplet_weight=0,\n",
    "    reward_triplet_weight=0,\n",
    "    reward_weight=2,\n",
    "    validate_with_memory=True,\n",
    "    use_smote=False,\n",
    "    **args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
