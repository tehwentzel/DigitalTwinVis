{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "050c878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7418d82f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'no_dose_adjustment',\n",
       " 1: 'dose_modified',\n",
       " 2: 'dose_delayed',\n",
       " 3: 'dose_cancelled',\n",
       " 4: 'dose_delayed_&_modified',\n",
       " 5: 'regiment_modification',\n",
       " 9: 'unknown'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Const:\n",
    "    data_dir = '../data'\n",
    "    twin_data = data_dir + 'digital_twin_data.csv'\n",
    "    twin_ln_data = data_dir + 'digital_twin_ln_data.csv'\n",
    "    \n",
    "    rename_dict = {\n",
    "        'Dummy ID': 'id',\n",
    "        'Age at Diagnosis (Calculated)': 'age',\n",
    "        'Feeding tube 6m': 'FT',\n",
    "        'Affected Lymph node UPPER': 'affected_nodes',\n",
    "        'Aspiration rate(Y/N)': 'AS',\n",
    "        'Neck boost (Y/N)': 'neck_boost',\n",
    "        'Gender': 'gender',\n",
    "        'Tm Laterality (R/L)': 'laterality',\n",
    "        'AJCC 8th edition': 'ajcc8',\n",
    "        'AJCC 7th edition':'ajcc7',\n",
    "        'N_category_full': 'N-category',\n",
    "        'HPV/P16 status': 'hpv',\n",
    "        'Tumor subsite (BOT/Tonsil/Soft Palate/Pharyngeal wall/GPS/NOS)': 'subsite',\n",
    "        'Total dose': 'total_dose',\n",
    "        'Therapeutic combination': 'treatment',\n",
    "        'Smoking status at Diagnosis (Never/Former/Current)': 'smoking_status',\n",
    "        'Smoking status (Packs/Year)': 'packs_per_year',\n",
    "        'Overall Survival (1=alive,0=dead)': 'os',\n",
    "        'Dose/fraction (Gy)': 'dose_fraction'\n",
    "    }\n",
    "    \n",
    "    dlt_dict = {\n",
    "         'Allergic reaction to Cetuximab': 'DLT_Other',\n",
    "         'Cardiological (A-fib)': 'DLT_Other',\n",
    "         'Dermatological': 'DLT_Dermatological',\n",
    "         'Failure to Thrive': 'DLT_Other',\n",
    "         'Failure to thrive': 'DLT_Other',\n",
    "         'GIT [elevated liver enzymes]': 'DLT_Gastrointestinal',\n",
    "         'Gastrointestina': 'DLT_Gastrointestinal',\n",
    "         'Gastrointestinal': 'DLT_Gastrointestinal',\n",
    "         'General': 'DLT_Other',\n",
    "         'Hematological': 'DLT_Hematological',\n",
    "         'Hematological (Neutropenia)': 'DLT_Hematological',\n",
    "         'Hyponatremia': 'DLT_Other',\n",
    "         'Immunological': 'DLT_Other',\n",
    "         'Infection': 'DLT_Infection (Pneumonia)',\n",
    "         'NOS': 'DLT_Other',\n",
    "         'Nephrological': 'DLT_Nephrological',\n",
    "         'Nephrological (ARF)': 'DLT_Nephrological',\n",
    "         'Neurological': 'DLT_Neurological',\n",
    "         'Neutropenia': 'DLT_Hematological',\n",
    "         'Nutritional': 'DLT_Other',\n",
    "         'Pancreatitis': 'DLT_Other',\n",
    "         'Pulmonary': 'DLT_Other',\n",
    "         'Respiratory (Pneumonia)': 'DLT_Infection (Pneumonia)',\n",
    "         'Sepsis': 'DLT_Infection (Pneumonia)',\n",
    "         'Suboptimal response to treatment' : 'DLT_Other',\n",
    "         'Vascular': 'DLT_Vascular'\n",
    "    }\n",
    "    \n",
    "    decision1 = 'Decision 1 (Induction Chemo) Y/N'\n",
    "    decision2 = 'Decision 2 (CC / RT alone)'\n",
    "    decision3 = 'Decision 3 Neck Dissection (Y/N)'\n",
    "    decisions = [decision1,decision2, decision3]\n",
    "    outcomes = ['Overall Survival (4 Years)', 'FT', 'Aspiration rate Post-therapy']\n",
    "    \n",
    "    modification_types = {\n",
    "        0: 'no_dose_adjustment',\n",
    "        1: 'dose_modified',\n",
    "        2: 'dose_delayed',\n",
    "        3: 'dose_cancelled',\n",
    "        4: 'dose_delayed_&_modified',\n",
    "        5: 'regiment_modification',\n",
    "        9: 'unknown'\n",
    "    }\n",
    "    \n",
    "    cc_types = {\n",
    "        0: 'cc_none',\n",
    "        1: 'cc_platinum',\n",
    "        2: 'cc_cetuximab',\n",
    "        3: 'cc_others',\n",
    "    }\n",
    "    \n",
    "    primary_disease_states = ['CR Primary','PR Primary','SD Primary']\n",
    "    nodal_disease_states = [t.replace('Primary','Nodal') for t in primary_disease_states]\n",
    "    dlt1 = list(set(dlt_dict.values()))\n",
    "    \n",
    "    modifications =  list(modification_types.values())\n",
    "    state2 = modifications + primary_disease_states+nodal_disease_states +dlt1 #+['No imaging 0=N,1=Y']\n",
    "    \n",
    "    primary_disease_states2 = [t + ' 2' for t in primary_disease_states]\n",
    "    nodal_disease_states2 = [t + ' 2' for t in nodal_disease_states]\n",
    "    dlt2 = [d + ' 2' for d in dlt1]\n",
    "    \n",
    "    ccs = list(cc_types.values())\n",
    "    state3 = ccs + primary_disease_states2 + nodal_disease_states2 + dlt2\n",
    "    \n",
    "Const.modification_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d02b309f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data_cleaned):\n",
    "    #this was Elisa's preprocessing except I removed all the Ifs because that's dumb\n",
    "    if len(data_cleaned.shape) < 2:\n",
    "        data_cleaned = pd.DataFrame([data], columns=data.index)\n",
    "        \n",
    "    data_cleaned.loc[data_cleaned['Aspiration rate Pre-therapy'] == 'N', 'Aspiration rate Pre-therapy'] = 0\n",
    "    data_cleaned.loc[data_cleaned['Aspiration rate Pre-therapy'] == 'Y', 'Aspiration rate Pre-therapy'] = 1\n",
    "\n",
    "    data_cleaned.loc[data_cleaned['Pathological Grade'] == 'I', 'Pathological Grade'] = 1\n",
    "    data_cleaned.loc[data_cleaned['Pathological Grade'] == 'II', 'Pathological Grade'] = 2\n",
    "    data_cleaned.loc[data_cleaned['Pathological Grade'] == 'III', 'Pathological Grade'] = 3\n",
    "    data_cleaned.loc[data_cleaned['Pathological Grade'] == 'IV', 'Pathological Grade'] = 4\n",
    "\n",
    "    data_cleaned.loc[(data_cleaned['T-category'] == 'Tx') | (data_cleaned['T-category'] == 'Tis'), 'T-category'] = 1\n",
    "    data_cleaned.loc[data_cleaned['T-category'] == 'T1', 'T-category'] = 1\n",
    "    data_cleaned.loc[data_cleaned['T-category'] == 'T2', 'T-category'] = 2\n",
    "    data_cleaned.loc[data_cleaned['T-category'] == 'T3', 'T-category'] = 3\n",
    "    data_cleaned.loc[data_cleaned['T-category'] == 'T4', 'T-category'] = 4\n",
    "\n",
    "    data_cleaned.loc[data_cleaned['N-category'] == 'N0', 'N-category'] = 0\n",
    "    data_cleaned.loc[data_cleaned['N-category'] == 'N1', 'N-category'] = 1\n",
    "    data_cleaned.loc[data_cleaned['N-category'] == 'N2', 'N-category'] = 2\n",
    "    data_cleaned.loc[data_cleaned['N-category'] == 'N3', 'N-category'] = 3\n",
    "\n",
    "    data_cleaned.loc[data_cleaned['N-category_8th_edition'] == 'N0', 'N-category_8th_edition'] = 0\n",
    "    data_cleaned.loc[data_cleaned['N-category_8th_edition'] == 'N1', 'N-category_8th_edition'] = 1\n",
    "    data_cleaned.loc[data_cleaned['N-category_8th_edition'] == 'N2', 'N-category_8th_edition'] = 2\n",
    "    data_cleaned.loc[data_cleaned['N-category_8th_edition'] == 'N3', 'N-category_8th_edition'] = 3\n",
    "\n",
    "    data_cleaned.loc[data_cleaned['AJCC 7th edition'] == 'I', 'AJCC 7th edition'] = 1\n",
    "    data_cleaned.loc[data_cleaned['AJCC 7th edition'] == 'II', 'AJCC 7th edition'] = 2\n",
    "    data_cleaned.loc[data_cleaned['AJCC 7th edition'] == 'III', 'AJCC 7th edition'] = 3\n",
    "    data_cleaned.loc[data_cleaned['AJCC 7th edition'] == 'IV', 'AJCC 7th edition'] = 4\n",
    "\n",
    "    data_cleaned.loc[data_cleaned['AJCC 8th edition'] == 'I', 'AJCC 8th edition'] = 1\n",
    "    data_cleaned.loc[data_cleaned['AJCC 8th edition'] == 'II', 'AJCC 8th edition'] = 2\n",
    "    data_cleaned.loc[data_cleaned['AJCC 8th edition'] == 'III', 'AJCC 8th edition'] = 3\n",
    "    data_cleaned.loc[data_cleaned['AJCC 8th edition'] == 'IV', 'AJCC 8th edition'] = 4\n",
    "\n",
    "    data_cleaned.loc[data_cleaned['Gender'] == 'Male', 'Gender'] = 1\n",
    "    data_cleaned.loc[data_cleaned['Gender'] == 'Female', 'Gender'] = 0\n",
    "\n",
    "\n",
    "    data_cleaned.loc[data_cleaned['HPV/P16 status'] == 'Positive', 'HPV/P16 status'] = 1\n",
    "    data_cleaned.loc[data_cleaned['HPV/P16 status'] == 'Negative', 'HPV/P16 status'] = -1\n",
    "    data_cleaned.loc[data_cleaned['HPV/P16 status'] == 'Unknown', 'HPV/P16 status'] = 0\n",
    "\n",
    "    data_cleaned.loc[data_cleaned[\n",
    "                         'Smoking status at Diagnosis (Never/Former/Current)'] == 'Formar', 'Smoking status at Diagnosis (Never/Former/Current)'] = .5\n",
    "    data_cleaned.loc[data_cleaned[\n",
    "                         'Smoking status at Diagnosis (Never/Former/Current)'] == 'Current', 'Smoking status at Diagnosis (Never/Former/Current)'] = 1\n",
    "    data_cleaned.loc[data_cleaned[\n",
    "                         'Smoking status at Diagnosis (Never/Former/Current)'] == 'Never', 'Smoking status at Diagnosis (Never/Former/Current)'] = 0\n",
    "\n",
    "\n",
    "    data_cleaned.loc[data_cleaned['Chemo Modification (Y/N)'] == 'Y', 'Chemo Modification (Y/N)'] = 1\n",
    "\n",
    "    data_cleaned.loc[data_cleaned['DLT (Y/N)'] == 'N', 'DLT (Y/N)'] = 0\n",
    "    data_cleaned.loc[data_cleaned['DLT (Y/N)'] == 'Y', 'DLT (Y/N)'] = 1\n",
    "\n",
    "    data_cleaned['DLT_Other'] = 0\n",
    "    for index, row in data_cleaned.iterrows():\n",
    "        if row['DLT_Type'] == 'None':\n",
    "            continue\n",
    "        for i in re.split('&|and|,', row['DLT_Type']):\n",
    "            if i.strip() != '' and data_cleaned.loc[index, Const.dlt_dict[i.strip()]] == 0:\n",
    "                data_cleaned.loc[index, Const.dlt_dict[i.strip()]] = 1\n",
    "\n",
    "    data_cleaned.loc[data_cleaned['Decision 2 (CC / RT alone)'] == 'RT alone', 'Decision 2 (CC / RT alone)'] = 0\n",
    "    data_cleaned.loc[data_cleaned['Decision 2 (CC / RT alone)'] == 'CC', 'Decision 2 (CC / RT alone)'] = 1\n",
    "\n",
    "    data_cleaned.loc[data_cleaned['CC modification (Y/N)'] == 'N', 'CC modification (Y/N)'] = 0\n",
    "    data_cleaned.loc[data_cleaned['CC modification (Y/N)'] == 'Y', 'CC modification (Y/N)'] = 1\n",
    "\n",
    "    data_cleaned['DLT_Dermatological 2'] = 0\n",
    "    data_cleaned['DLT_Neurological 2'] = 0\n",
    "    data_cleaned['DLT_Gastrointestinal 2'] = 0\n",
    "    data_cleaned['DLT_Hematological 2'] = 0\n",
    "    data_cleaned['DLT_Nephrological 2'] = 0\n",
    "    data_cleaned['DLT_Vascular 2'] = 0\n",
    "    data_cleaned['DLT_Infection (Pneumonia) 2'] = 0\n",
    "    data_cleaned['DLT_Other 2'] = 0\n",
    "    for index, row in data_cleaned.iterrows():\n",
    "        if row['DLT 2'] == 'None':\n",
    "            continue\n",
    "        for i in re.split('&|and|,', row['DLT 2']):\n",
    "            if i.strip() != '':\n",
    "                data_cleaned.loc[index, Const.dlt_dict[i.strip()] + ' 2'] = 1\n",
    "\n",
    "    data_cleaned.loc[\n",
    "        data_cleaned['Decision 3 Neck Dissection (Y/N)'] == 'N', 'Decision 3 Neck Dissection (Y/N)'] = 0\n",
    "    data_cleaned.loc[\n",
    "        data_cleaned['Decision 3 Neck Dissection (Y/N)'] == 'Y', 'Decision 3 Neck Dissection (Y/N)'] = 1\n",
    "\n",
    "    return data_cleaned\n",
    "\n",
    "def merge_editions(row,basecol='AJCC 8th edition',fallback='AJCC 7th edition'):\n",
    "    if pd.isnull(row[basecol]):\n",
    "        return row[fallback]\n",
    "    return row[basecol]\n",
    "\n",
    "\n",
    "def preprocess_dt_data(df,extra_to_keep=None):\n",
    "    \n",
    "    to_keep = ['id','hpv','age','packs_per_year','smoking_status','gender','Aspiration rate Pre-therapy','total_dose','dose_fraction'] \n",
    "    to_onehot = ['T-category','N-category','AJCC','Pathological Grade','subsite','treatment','ln_cluster']\n",
    "    if extra_to_keep is not None:\n",
    "        to_keep = to_keep + [c for c in extra_to_keep if c not in to_keep and c not in to_onehot]\n",
    "    \n",
    "    decisions =Const.decisions\n",
    "    outcomes = Const.outcomes\n",
    "    \n",
    "    modification_types = {\n",
    "        0: 'no_dose_adjustment',\n",
    "        1: 'dose_modified',\n",
    "        2: 'dose_delayed',\n",
    "        3: 'dose_cancelled',\n",
    "        4: 'dose_delayed_&_modified',\n",
    "        5: 'regiment_modification',\n",
    "        9: 'unknown'\n",
    "    }\n",
    "    \n",
    "    cc_types = {\n",
    "        0: 'cc_none',\n",
    "        1: 'cc_platinum',\n",
    "        2: 'cc_cetuximab',\n",
    "        3: 'cc_others',\n",
    "    }\n",
    "    \n",
    "    for k,v in Const.cc_types.items():\n",
    "        df[v] = df['CC Regimen(0= none, 1= platinum based, 2= cetuximab based, 3= others, 9=unknown)'].apply(lambda x: int(Const.cc_types.get(int(x),0) == v))\n",
    "        to_keep.append(v)\n",
    "    for k,v in Const.modification_types.items():\n",
    "        name = 'Modification Type (0= no dose adjustment, 1=dose modified, 2=dose delayed, 3=dose cancelled, 4=dose delayed & modified, 5=regimen modification, 9=unknown)'\n",
    "        df[v] = df[name].apply(lambda x: int(Const.modification_types.get(int(x),0) == v))\n",
    "        to_keep.append(v)\n",
    "    #Features to keep. I think gender is is in \n",
    "    \n",
    "    keywords = []\n",
    "    for keyword in keywords:\n",
    "        toadd = [c for c in df.columns if keyword in c and c not in to_keep]\n",
    "        to_keep = to_keep + toadd\n",
    "    \n",
    "    df['packs_per_year'] = df['packs_per_year'].apply(lambda x: str(x).replace('>','').replace('<','')).astype(float).fillna(0)\n",
    "    #so I'm actually not sure if this is biological sex or gender given this is texas\n",
    "    df['AJCC'] = df.apply(lambda row: merge_editions(row,'ajcc8','ajcc7'),axis=1)\n",
    "    df['N-category'] = df.apply(lambda row: merge_editions(row,'N-category_8th_edition','N-category'),axis=1)\n",
    "    \n",
    "    dummy_df = pd.get_dummies(df[to_onehot].fillna(0).astype(str),drop_first=False)\n",
    "    for col in dummy_df.columns:\n",
    "        df[col] = dummy_df[col]\n",
    "        to_keep.append(col)\n",
    "        \n",
    "    yn_to_binary = ['FT','Aspiration rate Post-therapy','Decision 1 (Induction Chemo) Y/N']\n",
    "    for col in yn_to_binary:\n",
    "        df[col] = df[col].apply(lambda x: int(x == 'Y'))\n",
    "        \n",
    "    to_keep = to_keep + [c for c in df.columns if 'DLT' in c]\n",
    "    \n",
    "        \n",
    "    for statelist in [Const.state2,Const.state3,Const.decisions,Const.outcomes]:\n",
    "        toadd = [c for c in statelist if c not in to_keep]\n",
    "        to_keep = to_keep + toadd\n",
    "    return df[to_keep].set_index('id')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c861bfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2197865/1992661605.py:2: DtypeWarning: Columns (55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([    3,     5,     6,     7,     8,     9,    10,    11,    13,\n",
       "          14,    15,    16,    17,    18,    21,    23,    24,    25,\n",
       "          26,    27,    28,    31,    32,    33,    35,    36,    37,\n",
       "          38,    39,    40,    41,    42,    44,    45,    47,    48,\n",
       "          49,    50,    51,    53,    55,    56,    57,    60,    64,\n",
       "          65,    67,    68,    69,    71,    74,    75,    77,    78,\n",
       "          79,    80,    81,    82,    87,    88,    91,    94,    96,\n",
       "          99,   103,   109,   116,   117,   119,   120,   121,   125,\n",
       "         133,   148,   150,   153,   168,   178,   181,   183,   184,\n",
       "         185,   186,   187,   188,   189,   190,   191,   192,   193,\n",
       "         194,   195,   196,   197,   198,   199,   200,   201,   202,\n",
       "         203,   204,   205,   206,   207,   208,   209,   210,   211,\n",
       "         212,   213,   214,   215,   216,   217,   218,   219,   220,\n",
       "         221,   222,   223,   224,   225,   226,   227,   228,   229,\n",
       "         230,   231,   232,   233,   234,   235,   236,   237,   238,\n",
       "         239,   240,   241,   242,   243,   244,   245,   246,   247,\n",
       "         248,   249,   251,   252,   253,   254,   255,   256,   257,\n",
       "         258,   259,   260,   261,   262,   263,   264,   265,   266,\n",
       "         267,   268,   269,   270,   271,   272,   273,   274,   275,\n",
       "         276,   277,   278,   279,   280,   281,   282,   283,   284,\n",
       "         285,   286,   287,   288,   289,  2000,  2001,  2002,  2003,\n",
       "        2004,  2005,  2006,  2007,  2008,  2009,  2010,  2011,  2012,\n",
       "        2013,  2014,  2015,  2016,  2017,  2018,  2019,  2020,  2021,\n",
       "        2022,  2023,  2024,  2025,  2026,  2027,  2028,  2029,  2030,\n",
       "        2031,  2032,  2033,  5000,  5001,  5002,  5003,  5004,  5005,\n",
       "        5006,  5007,  5008,  5009,  5010,  5011,  5012,  5013,  5014,\n",
       "        5015,  5016,  5017,  5018,  5019,  5020,  5021,  5022,  5023,\n",
       "        5024,  5025,  5026,  5027,  5028,  5029,  5030,  5031,  5032,\n",
       "        5033,  5034,  5035,  5036,  5037,  5038,  5039,  5040,  5041,\n",
       "        5042,  5043,  5044,  5045,  5047,  5048,  5049,  5050,  5051,\n",
       "        5052,  5053,  5054,  5055,  5056,  5057,  5058,  5059,  5060,\n",
       "        5061,  5062,  5063,  5064,  5065,  5066,  5067,  5068,  5069,\n",
       "        5070,  5071,  5072,  5073,  5074,  5075,  5076,  5077,  5078,\n",
       "        5079,  5080,  5081,  5082,  5083,  5084,  5085,  5086,  5087,\n",
       "        5088,  5089,  5090,  5091,  5092,  5093,  5094,  5095,  5096,\n",
       "        5097,  5098,  5099,  5100,  5101,  5102,  5103,  5104,  5105,\n",
       "        5106,  5107,  5108,  5109,  5110,  5111,  5112,  5113,  5114,\n",
       "        5115,  5117,  5118,  5119,  5120, 10001, 10002, 10003, 10004,\n",
       "       10005, 10006, 10007, 10008, 10009, 10010, 10011, 10012, 10013,\n",
       "       10014, 10015, 10016, 10017, 10018, 10019, 10020, 10021, 10022,\n",
       "       10023, 10024, 10025, 10026, 10027, 10028, 10029, 10031, 10033,\n",
       "       10034, 10035, 10036, 10037, 10038, 10039, 10040, 10041, 10042,\n",
       "       10043, 10044, 10045, 10046, 10047, 10048, 10049, 10050, 10051,\n",
       "       10052, 10053, 10054, 10055, 10056, 10057, 10058, 10059, 10060,\n",
       "       10061, 10062, 10063, 10064, 10065, 10066, 10067, 10068, 10069,\n",
       "       10070, 10071, 10072, 10073, 10074, 10075, 10077, 10078, 10079,\n",
       "       10080, 10081, 10082, 10083, 10084, 10085, 10086, 10087, 10088,\n",
       "       10089, 10090, 10091, 10092, 10093, 10094, 10095, 10096, 10097,\n",
       "       10098, 10099, 10100, 10101, 10102, 10103, 10104, 10105, 10106,\n",
       "       10107, 10108, 10109, 10110, 10111, 10113, 10114, 10115, 10116,\n",
       "       10117, 10118, 10119, 10120, 10121, 10123, 10124, 10125, 10126,\n",
       "       10127, 10128, 10129, 10130, 10131, 10132, 10133, 10134, 10135,\n",
       "       10136, 10137, 10138, 10139, 10140, 10141, 10142, 10143, 10144,\n",
       "       10145, 10146, 10147, 10148, 10149, 10150, 10151, 10152, 10153,\n",
       "       10154, 10155, 10156, 10157, 10158, 10159, 10160, 10161, 10162,\n",
       "       10163, 10164, 10165, 10167, 10168, 10169, 10170, 10171, 10172,\n",
       "       10173, 10174, 10175, 10176, 10177, 10178, 10180, 10181, 10182,\n",
       "       10183, 10184, 10185, 10186, 10187, 10188, 10189, 10190, 10191,\n",
       "       10192, 10193, 10194, 10195, 10196, 10197, 10198, 10199, 10200,\n",
       "       10201, 10202, 10203, 10204, 10205])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_digital_twin(file='../data/digital_twin_data.csv'):\n",
    "    df = pd.read_csv(file)\n",
    "    return df.rename(columns = Const.rename_dict)\n",
    "\n",
    "def get_dt_ids():\n",
    "    df = load_digital_twin()\n",
    "    return df.id.values\n",
    "get_dt_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e719a1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2197865/1521532340.py:4: DtypeWarning: Columns (55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_file)\n",
      "/tmp/ipykernel_2197865/1521532340.py:19: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  self.means = self.processed_df.mean(axis=0)\n",
      "/tmp/ipykernel_2197865/1521532340.py:20: FutureWarning: The default value of numeric_only in DataFrame.std is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  self.stds = self.processed_df.std(axis=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[id\n",
       " 3        0\n",
       " 5        0\n",
       " 6        0\n",
       " 7        0\n",
       " 8        0\n",
       "         ..\n",
       " 10201    0\n",
       " 10202    0\n",
       " 10203    0\n",
       " 10204    0\n",
       " 10205    0\n",
       " Name: Decision 1 (Induction Chemo) Y/N, Length: 536, dtype: int64,\n",
       " id\n",
       " 3        1\n",
       " 5        1\n",
       " 6        1\n",
       " 7        0\n",
       " 8        0\n",
       "         ..\n",
       " 10201    1\n",
       " 10202    1\n",
       " 10203    0\n",
       " 10204    1\n",
       " 10205    1\n",
       " Name: Decision 2 (CC / RT alone), Length: 536, dtype: int64,\n",
       " id\n",
       " 3        0\n",
       " 5        0\n",
       " 6        0\n",
       " 7        0\n",
       " 8        0\n",
       "         ..\n",
       " 10201    0\n",
       " 10202    1\n",
       " 10203    1\n",
       " 10204    0\n",
       " 10205    1\n",
       " Name: Decision 3 Neck Dissection (Y/N), Length: 536, dtype: int64]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DTDataset():\n",
    "    \n",
    "    def __init__(self,data_file = '../data/digital_twin_data.csv',ln_data_file = '../data/digital_twin_ln_data.csv',ids=None):\n",
    "        df = pd.read_csv(data_file)\n",
    "        \n",
    "        df = preprocess(df)\n",
    "        df = df.rename(columns = Const.rename_dict).copy()\n",
    "        df = df.drop('MRN OPC',axis=1)\n",
    "\n",
    "        ln_data = pd.read_csv(ln_data_file)\n",
    "        ln_data = ln_data.rename(columns={'cluster':'ln_cluster'})\n",
    "        self.ln_cols = [c for c in ln_data.columns if c not in df.columns]\n",
    "        df = df.merge(ln_data,on='id')\n",
    "        df.index = df.index.astype(int)\n",
    "        if ids is not None:\n",
    "            df = df[df.id.apply(lambda x: x in ids)]\n",
    "        self.processed_df = preprocess_dt_data(df,self.ln_cols).fillna(0)\n",
    "        \n",
    "        self.means = self.processed_df.mean(axis=0)\n",
    "        self.stds = self.processed_df.std(axis=0)\n",
    "        self.maxes = self.processed_df.max(axis=0)\n",
    "        self.mins = self.processed_df.min(axis=0)\n",
    "        \n",
    "        arrays = self.get_states()\n",
    "        self.state_sizes = {k: (v.shape[1] if v.ndim > 1 else 1) for k,v in arrays.items()}\n",
    "        \n",
    "    def get_data(self):\n",
    "        return self.processed_df\n",
    "    \n",
    "    def sample(self,frac=.5):\n",
    "        return self.processed_df.sample(frac=frac)\n",
    "    \n",
    "    def split_sample(self,ratio = .3):\n",
    "        assert(ratio > 0 and ratio <= 1)\n",
    "        df1 = self.processed_df.sample(frac=1-ratio)\n",
    "        df2 = self.processed_df.drop(index=df1.index)\n",
    "        return df1,df2\n",
    "    \n",
    "    def get_states(self,fixed=None,ids = None):\n",
    "        processed_df = self.processed_df.copy()\n",
    "        if ids is not None:\n",
    "            processed_df = processed_df.loc[ids]\n",
    "        if fixed is not None:\n",
    "            for col,val in fixed.items():\n",
    "                if col in processed_df.columns:\n",
    "                    processed_df[col] = val\n",
    "                else:\n",
    "                    print('bad fixed entry',col)\n",
    "                    \n",
    "        to_skip = ['CC Regimen(0= none, 1= platinum based, 2= cetuximab based, 3= others, 9=unknown)','DLT_Type','DLT 2'] + [c for c in processed_df.columns if 'treatment' in c]\n",
    "        other_states = set(Const.decisions + Const.state3 + Const.state2 + Const.outcomes  + to_skip)\n",
    "\n",
    "        base_state = sorted([c for c in processed_df.columns if c not in other_states])\n",
    "\n",
    "        dlt1 = Const.dlt1\n",
    "        dlt2 = Const.dlt2\n",
    "        \n",
    "        modifications = Const.modifications\n",
    "        ccs = Const.ccs\n",
    "        pds = Const.primary_disease_states\n",
    "        nds = Const.nodal_disease_states\n",
    "        pds2 = Const.primary_disease_states2\n",
    "        nds2 = Const.nodal_disease_states2\n",
    "        outcomes = Const.outcomes\n",
    "        decisions= Const.decisions\n",
    "        \n",
    "        #intermediate states are only udated values. Models should use baseline + state2 etc\n",
    "        results = {\n",
    "            'baseline': processed_df[base_state],\n",
    "            'pd_states1': processed_df[pds],\n",
    "            'nd_states1': processed_df[nds],\n",
    "            'modifications': processed_df[modifications],\n",
    "            'ccs': processed_df[ccs],\n",
    "            'pd_states2': processed_df[pds2],\n",
    "            'nd_states2': processed_df[nds2],\n",
    "            'outcomes': processed_df[outcomes],\n",
    "            'dlt1': processed_df[dlt1],\n",
    "            'dlt2': processed_df[dlt2],\n",
    "            'decision1': processed_df[decisions[0]],\n",
    "            'decision2': processed_df[decisions[1]],\n",
    "            'decision3': processed_df[decisions[2]],\n",
    "        }\n",
    "    \n",
    "        return results\n",
    "    \n",
    "    def get_state(self,name,**kwargs):\n",
    "        return self.get_states(**kwargs)[name]\n",
    "    \n",
    "    def normalize(self,df):\n",
    "        means = self.means[df.columns]\n",
    "        std = self.stds[df.columns]\n",
    "        return ((df - means)/std).fillna(0)\n",
    "    \n",
    "    def get_intermediate_outcomes(self,step=1,**kwargs):\n",
    "        assert(step in [1,2,3])\n",
    "        states = self.get_states(**kwargs)\n",
    "        if step == 1:\n",
    "            keys = ['pd_states1','nd_states1','modifications','dlt1']\n",
    "        if step == 2:\n",
    "            keys =  ['pd_states2','nd_states2','ccs','dlt2']\n",
    "        if step == 3:\n",
    "            keys = ['decision1','decision2','decision3']\n",
    "        return [states[key] for key in keys]\n",
    "    \n",
    "    def get_input_state(self,step=1,**kwargs):\n",
    "        assert(step in [1,2,3])\n",
    "        states = self.get_states(**kwargs)\n",
    "        if step == 1:\n",
    "            keys = ['baseline','decision1']\n",
    "        if step == 2:\n",
    "            keys =  ['baseline','pd_states1','nd_states1','modifications','dlt1','decision1','decision2']\n",
    "        if step == 3:\n",
    "            keys = ['baseline','pd_states2','nd_states2','ccs','dlt2','decision1','decision2','decision3']\n",
    "        arrays = [states[key] for key in keys]\n",
    "        return pd.concat(arrays,axis=1)\n",
    "    \n",
    "data = DTDataset()\n",
    "data.get_intermediate_outcomes(step=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c748333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_torch(df,ttype  = torch.FloatTensor):\n",
    "    values = df.values.astype(float)\n",
    "    values = torch.from_numpy(values)\n",
    "    return values.type(ttype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a87a4d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SimulatorBase(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 hidden_layers = [1000],\n",
    "                 dropout = 0.5,\n",
    "                 input_dropout=0.1,\n",
    "                 state = 1,\n",
    "                 eps = 0.01,\n",
    "                ):\n",
    "        #predicts disease state (sd, pr, cr) for primar and nodal, then dose modications or cc type (depending on state), and [dlt ratings]\n",
    "        torch.nn.Module.__init__(self)\n",
    "        self.state = state\n",
    "        self.input_dropout = torch.nn.Dropout(input_dropout)\n",
    "        \n",
    "        first_layer =torch.nn.Linear(input_size,hidden_layers[0],bias=True)\n",
    "        layers = [first_layer,torch.nn.ReLU()]\n",
    "        curr_size = hidden_layers[0]\n",
    "        for ndim in hidden_layers[1:]:\n",
    "            layer = torch.nn.Linear(curr_size,ndim)\n",
    "            curr_size = ndim\n",
    "            layers.append(layer)\n",
    "            layers.append(torch.nn.ReLU())\n",
    "        self.layers = torch.nn.ModuleList(layers)\n",
    "        self.batchnorm = torch.nn.BatchNorm1d(hidden_layers[-1])\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "    \n",
    "        input_mean = torch.tensor([0])\n",
    "        input_std = torch.tensor([1])\n",
    "        self.eps = eps\n",
    "        self.register_buffer('input_mean', input_mean)\n",
    "        self.register_buffer('input_std',input_std)\n",
    "        \n",
    "        self.softmax = torch.nn.LogSoftmax(dim=1)\n",
    "        self.identifier = 'state'  +str(state) + '_input'+str(input_size) + '_dims' + ','.join([str(h) for h in hidden_layers]) + '_dropout' + str(input_dropout) + ',' + str(dropout)\n",
    "        \n",
    "    def normalize(self,x):\n",
    "        x = (x - self.input_mean + self.eps)/(self.input_std + self.eps)\n",
    "        return x\n",
    "    \n",
    "    def fit_normalizer(self,x):\n",
    "        input_mean = x.mean(axis=0)\n",
    "        input_std = x.std(axis=0)\n",
    "        self.register_buffer('input_mean', input_mean)\n",
    "        self.register_buffer('input_std',input_std)\n",
    "        return True\n",
    "        \n",
    "class OutcomeSimulator(SimulatorBase):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 hidden_layers = [1000,1000],\n",
    "                 dropout = 0.5,\n",
    "                 input_dropout=0.1,\n",
    "                 state = 1,\n",
    "                ):\n",
    "        #predicts disease state (sd, pr, cr) for primar and nodal, then dose modications or cc type (depending on state), and [dlt ratings]\n",
    "        super(OutcomeSimulator,self).__init__(input_size,hidden_layers=hidden_layers,dropout=dropout,input_dropout=input_dropout,state=state)\n",
    "    \n",
    "        self.disease_layer = torch.nn.Linear(hidden_layers[-1],len(Const.primary_disease_states))\n",
    "        self.nodal_disease_layer = torch.nn.Linear(hidden_layers[-1],len(Const.nodal_disease_states))\n",
    "        #dlt ratings are 0-4 even though they don't always appear\n",
    "        \n",
    "        assert( state in [1,2])\n",
    "        if state == 1:\n",
    "            self.dlt_layers = torch.nn.ModuleList([torch.nn.Linear(hidden_layers[-1],5) for i in Const.dlt1])\n",
    "            self.treatment_layer = torch.nn.Linear(hidden_layers[-1],len(Const.modifications))\n",
    "        else:\n",
    "            #we only have dlt yes or no for the second state?\n",
    "            self.dlt_layers = torch.nn.ModuleList([torch.nn.Linear(hidden_layers[-1],2) for i in Const.dlt2])\n",
    "            self.treatment_layer = torch.nn.Linear(hidden_layers[-1],len(Const.ccs))\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.normalize(x)\n",
    "        x = self.input_dropout(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "#         x = self.batchnorm(x)\n",
    "        x = self.dropout(x)\n",
    "        x_pd = self.disease_layer(x)\n",
    "        x_nd = self.nodal_disease_layer(x)\n",
    "        x_mod = self.treatment_layer(x)\n",
    "        x_dlts = [layer(x) for layer in self.dlt_layers]\n",
    "        \n",
    "        x_pd = self.softmax(x_pd)\n",
    "        x_nd = self.softmax(x_nd)\n",
    "        x_mod = self.softmax(x_mod)\n",
    "        #dlts are array of nbatch x n_dlts x predictions\n",
    "        x_dlts = torch.stack([self.softmax(xx) for xx in x_dlts],axis=1)\n",
    "        return [x_pd, x_nd, x_mod, x_dlts]\n",
    "    \n",
    "class EndpointSimulator(SimulatorBase):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 hidden_layers = [500],\n",
    "                 dropout = 0.5,\n",
    "                 input_dropout=0.1,\n",
    "                 state = 1,\n",
    "                ):\n",
    "        #predicts disease state (sd, pr, cr) for primar and nodal, then dose modications or cc type (depending on state), and [dlt ratings]\n",
    "        super(EndpointSimulator,self).__init__(input_size,hidden_layers=hidden_layers,dropout=dropout,input_dropout=input_dropout,state=state)\n",
    "        \n",
    "        self.outcome_layer = torch.nn.Linear(hidden_layers[-1],len(Const.outcomes))\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "      \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.normalize(x)\n",
    "        x = self.input_dropout(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "#         x = self.batchnorm(x)\n",
    "        x = self.dropout(x)\n",
    "        x= self.outcome_layer(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "OutcomeSimulator(3).input_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ef50362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/multiclass.py:13: DeprecationWarning: Please use `spmatrix` from the `scipy.sparse` namespace, the `scipy.sparse.base` namespace is deprecated.\n",
      "  from scipy.sparse.base import spmatrix\n"
     ]
    }
   ],
   "source": [
    "def nllloss(ytrue,ypred):\n",
    "    #nll loss with argmax added in\n",
    "    loss = torch.nn.NLLLoss()\n",
    "    return loss(ypred,ytrue.argmax(axis=1))\n",
    "\n",
    "def state_loss(ytrue,ypred,weights=[1,1,1,1]):\n",
    "    pd_loss = nllloss(ytrue[0],ypred[0])*weights[0]\n",
    "    nd_loss = nllloss(ytrue[1],ypred[1])*weights[1]\n",
    "    mod_loss = nllloss(ytrue[2],ypred[2])*weights[2]\n",
    "    loss = pd_loss + nd_loss + mod_loss\n",
    "    dlt_true = ytrue[3]\n",
    "    dlt_pred = ypred[3]\n",
    "    ndlt = dlt_true.shape[1]\n",
    "    nloss = torch.nn.NLLLoss()\n",
    "    for i in range(ndlt):\n",
    "        dlt_loss = nloss(dlt_pred[:,i,:],dlt_true[:,i].type(torch.LongTensor))\n",
    "        loss += dlt_loss*weights[3]/ndlt\n",
    "    return loss\n",
    "\n",
    "def outcome_loss(ytrue,ypred,weights=[1,1,1]):\n",
    "    loss = 0\n",
    "    nloss = torch.nn.BCELoss()\n",
    "    for i in range(len(weights)):\n",
    "        iloss = nloss(ypred[:,i],ytrue[i])*weights[i]\n",
    "        loss += iloss\n",
    "    return loss\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score\n",
    "\n",
    "def mc_metrics(yt,yp,numpy=False,is_dlt=False):\n",
    "    if not numpy:\n",
    "        yt = yt .cpu().detach().numpy()\n",
    "        yp = yp.cpu().detach().numpy()\n",
    "    #this is a catch for when I se the dlt prediction format (encoded integer ordinal, predict as a categorical and take the argmax)\n",
    "    if yt.ndim > 1:\n",
    "        try:\n",
    "            bacc = balanced_accuracy_score(yt.argmax(axis=1),yp.argmax(axis=1))\n",
    "        except:\n",
    "            bacc = -1\n",
    "        try:\n",
    "            roc_micro = roc_auc_score(yt,yp,average='micro')\n",
    "        except:\n",
    "            roc_micro=-1\n",
    "        try:\n",
    "            roc_macro = roc_auc_score(yt,yp,average='macro')\n",
    "        except:\n",
    "            roc_macro = -1\n",
    "        return {'accuracy': bacc, 'roc_micro': roc_micro,'roc_macro': roc_macro}\n",
    "    else:\n",
    "        if yp.ndim > 1:\n",
    "            yp = yp.argmax(axis=1)\n",
    "        try:\n",
    "            bacc = balanced_accuracy_score(yt,yp)\n",
    "        except:\n",
    "            bacc = -1\n",
    "        try:\n",
    "            if is_dlt:\n",
    "                roc = roc_auc_score(yt > 0, yp > 0)\n",
    "            else:\n",
    "                roc = roc_auc_score(yt ,yp )\n",
    "        except:\n",
    "            roc = -1\n",
    "        error = np.mean((yt-yp)**2)\n",
    "        return {'accuracy': bacc, 'mse': error, 'auc': roc}\n",
    "\n",
    "def state_metrics(ytrue,ypred,numpy=False):\n",
    "    pd_metrics = mc_metrics(ytrue[0],ypred[0],numpy=numpy)\n",
    "    nd_metrics = mc_metrics(ytrue[1],ypred[1],numpy=numpy)\n",
    "    mod_metrics = mc_metrics(ytrue[1],ypred[1],numpy=numpy)\n",
    "    \n",
    "    dlt_metrics = []\n",
    "    dlt_true = ytrue[3]\n",
    "    dlt_pred = ypred[3]\n",
    "    ndlt = dlt_true.shape[1]\n",
    "    nloss = torch.nn.NLLLoss()\n",
    "    for i in range(ndlt):\n",
    "        dm = mc_metrics(dlt_true[:,i],dlt_pred[:,i,:],is_dlt=True)\n",
    "        dlt_metrics.append(dm)\n",
    "    dlt_acc =[d['accuracy'] for d in dlt_metrics]\n",
    "    dlt_error = [d['mse'] for d in dlt_metrics]\n",
    "    dlt_auc = [d['auc'] for d in dlt_metrics]\n",
    "    return {'pd': pd_metrics,'nd': nd_metrics,'mod': mod_metrics,'dlts': {'accuracy': dlt_acc,'accuracy_mean': np.mean(dlt_acc),'auc': dlt_auc,'auc_mean': np.mean(dlt_auc)}}\n",
    "    \n",
    "def outcome_metrics(ytrue,ypred,numpy=False):\n",
    "    res = {}\n",
    "    for i, outcome in enumerate(Const.outcomes):\n",
    "        metrics = mc_metrics(ytrue[i],ypred[:,i])\n",
    "        res[outcome] = metrics\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e95667ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/optimize.py:18: DeprecationWarning: Please use `line_search_wolfe2` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/optimize.py:18: DeprecationWarning: Please use `line_search_wolfe1` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/decomposition/online_lda.py:29: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/ensemble/gradient_boosting.py:32: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from ._gradient_boosting import predict_stages\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/ensemble/gradient_boosting.py:32: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from ._gradient_boosting import predict_stages\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def train_state_rf(model_args={}):\n",
    "    ids = get_dt_ids()\n",
    "    \n",
    "    dataset = DTDataset()\n",
    "\n",
    "    train_ids = ids[0:int(len(ids)*.7)]\n",
    "    test_ids = ids[int(len(ids)*.7):]\n",
    "    \n",
    "    #most things are multiclass, dlts are several ordinal and outcomes are multiple binary\n",
    "    xtrain1 = dataset.get_state('baseline',ids=train_ids)\n",
    "    xtest1 = dataset.get_state('baseline',ids=test_ids)\n",
    "    \n",
    "    xtrain2 = dataset.get_input_state(step=2,ids=train_ids)\n",
    "    xtest2 = dataset.get_input_state(step=2,ids=test_ids)\n",
    "    \n",
    "    xtrain3 = dataset.get_input_state(step=3,ids=train_ids)\n",
    "    xtest3 = dataset.get_input_state(step=3,ids=test_ids)\n",
    "    \n",
    "    [pd1_train,nd1_train, mod_train,dlts1_train] = dataset.get_intermediate_outcomes(ids=train_ids)\n",
    "    [pd2_train,nd2_train, cc_train,dlts2_train] = dataset.get_intermediate_outcomes(step=2,ids=train_ids)\n",
    "    [pd1_test,nd1_test, mod_test,dlts1_test] = dataset.get_intermediate_outcomes(ids=test_ids)\n",
    "    [pd2_test,nd2_test, cc_test,dlts2_test] = dataset.get_intermediate_outcomes(step=2,ids=test_ids)\n",
    "    outcomes_train = dataset.get_state('outcomes',ids=train_ids)\n",
    "    outcomes_test = dataset.get_state('outcomes',ids=test_ids)\n",
    "    \n",
    "\n",
    "    def train_multiclass_rf(xtrain,xtest,ytrain,ytest):\n",
    "        model = RandomForestClassifier(class_weight='balanced',**model_args).fit(xtrain,ytrain)\n",
    "        ypred = model.predict(xtest)\n",
    "        metrics = mc_metrics(ytest.values,ypred,numpy=True)\n",
    "        return model, metrics\n",
    "    \n",
    "    all_metrics = {}\n",
    "    pd1_model, all_metrics['pd1'] = train_multiclass_rf(xtrain1,xtest1,pd1_train,pd1_test)\n",
    "    nd1_model, all_metrics['nd1']  = train_multiclass_rf(xtrain1,xtest1,nd1_train,nd1_test)\n",
    "    mod_model, all_metrics['mod']  = train_multiclass_rf(xtrain1,xtest1,mod_train,mod_test)\n",
    "    \n",
    "    return all_metrics\n",
    "\n",
    "# train_state_rf({'max_depth': 5,'n_estimators': 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a9536fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2197865/1992661605.py:2: DtypeWarning: Columns (55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/tmp/ipykernel_2197865/1521532340.py:4: DtypeWarning: Columns (55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_file)\n",
      "/tmp/ipykernel_2197865/1521532340.py:19: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  self.means = self.processed_df.mean(axis=0)\n",
      "/tmp/ipykernel_2197865/1521532340.py:20: FutureWarning: The default value of numeric_only in DataFrame.std is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  self.stds = self.processed_df.std(axis=0)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/classification.py:1745: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/classification.py:1745: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/classification.py:1745: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/classification.py:1745: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/classification.py:1745: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/classification.py:1745: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/classification.py:1745: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/classification.py:1745: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train loss 20.31390380859375\n",
      "val loss 19.554641723632812\n",
      "______________\n",
      "epoch 1 train loss 19.594619750976562\n",
      "val loss 18.907915115356445\n",
      "______________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/classification.py:1745: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/classification.py:1745: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/classification.py:1745: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/classification.py:1745: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/classification.py:1745: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/classification.py:1745: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/classification.py:1745: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/classification.py:1745: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/classification.py:1745: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/classification.py:1745: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/classification.py:1745: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/classification.py:1745: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/classification.py:1745: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/classification.py:1745: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/classification.py:1745: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/classification.py:1745: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/classification.py:1745: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 train loss 18.947132110595703\n",
      "val loss 18.26899528503418\n",
      "______________\n",
      "epoch 3 train loss 18.26720428466797\n",
      "val loss 17.633028030395508\n",
      "______________\n",
      "epoch 4 train loss 17.62619400024414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/classification.py:1745: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/classification.py:1745: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 16.996232986450195\n",
      "______________\n",
      "epoch 5 train loss 16.96844482421875\n",
      "val loss 16.355268478393555\n",
      "______________\n",
      "epoch 6 train loss 16.319337844848633\n",
      "val loss 15.707328796386719\n",
      "______________\n",
      "epoch 7 train loss 15.674251556396484\n",
      "val loss 15.051858901977539\n",
      "______________\n",
      "epoch 8 train loss 15.008088111877441\n",
      "val loss 14.388801574707031\n",
      "______________\n",
      "epoch 9 train loss 14.296045303344727\n",
      "val loss 13.719712257385254\n",
      "______________\n",
      "epoch 10 train loss 13.625718116760254\n",
      "val loss 13.046521186828613\n",
      "______________\n",
      "epoch 11 train loss 12.941590309143066\n",
      "val loss 12.372965812683105\n",
      "______________\n",
      "epoch 12 train loss 12.259780883789062\n",
      "val loss 11.703165054321289\n",
      "______________\n",
      "epoch 13 train loss 11.576982498168945\n",
      "val loss 11.042352676391602\n",
      "______________\n",
      "epoch 14 train loss 10.88326644897461\n",
      "val loss 10.39614200592041\n",
      "______________\n",
      "epoch 15 train loss 10.168752670288086\n",
      "val loss 9.769389152526855\n",
      "______________\n",
      "epoch 16 train loss 9.645013809204102\n",
      "val loss 9.167439460754395\n",
      "______________\n",
      "epoch 17 train loss 8.93923568725586\n",
      "val loss 8.595657348632812\n",
      "______________\n",
      "epoch 18 train loss 8.394450187683105\n",
      "val loss 8.058231353759766\n",
      "______________\n",
      "epoch 19 train loss 7.851687908172607\n",
      "val loss 7.5585832595825195\n",
      "______________\n",
      "epoch 20 train loss 7.344419479370117\n",
      "val loss 7.098883152008057\n",
      "______________\n",
      "epoch 21 train loss 6.868550777435303\n",
      "val loss 6.680421829223633\n",
      "______________\n",
      "epoch 22 train loss 6.444400787353516\n",
      "val loss 6.303488731384277\n",
      "______________\n",
      "epoch 23 train loss 6.046125411987305\n",
      "val loss 5.967206001281738\n",
      "______________\n",
      "epoch 24 train loss 5.677136421203613\n",
      "val loss 5.670422554016113\n",
      "______________\n",
      "epoch 25 train loss 5.424500465393066\n",
      "val loss 5.410612106323242\n",
      "______________\n",
      "epoch 26 train loss 5.137260913848877\n",
      "val loss 5.185021877288818\n",
      "______________\n",
      "epoch 27 train loss 4.868043899536133\n",
      "val loss 4.990640163421631\n",
      "______________\n",
      "epoch 28 train loss 4.6311845779418945\n",
      "val loss 4.824095726013184\n",
      "______________\n",
      "epoch 29 train loss 4.497529983520508\n",
      "val loss 4.682061195373535\n",
      "______________\n",
      "epoch 30 train loss 4.3730316162109375\n",
      "val loss 4.561037540435791\n",
      "______________\n",
      "epoch 31 train loss 4.1956706047058105\n",
      "val loss 4.4581804275512695\n",
      "______________\n",
      "epoch 32 train loss 4.093595027923584\n",
      "val loss 4.37065315246582\n",
      "______________\n",
      "epoch 33 train loss 4.015478610992432\n",
      "val loss 4.295652389526367\n",
      "______________\n",
      "epoch 34 train loss 3.8994266986846924\n",
      "val loss 4.230891704559326\n",
      "______________\n",
      "epoch 35 train loss 3.774536609649658\n",
      "val loss 4.174422740936279\n",
      "______________\n",
      "epoch 36 train loss 3.8216464519500732\n",
      "val loss 4.1244988441467285\n",
      "______________\n",
      "epoch 37 train loss 3.7103545665740967\n",
      "val loss 4.079559326171875\n",
      "______________\n",
      "epoch 38 train loss 3.5990047454833984\n",
      "val loss 4.038149356842041\n",
      "______________\n",
      "epoch 39 train loss 3.597944498062134\n",
      "val loss 3.9991273880004883\n",
      "______________\n",
      "epoch 40 train loss 3.5690765380859375\n",
      "val loss 3.961794376373291\n",
      "______________\n",
      "epoch 41 train loss 3.479762077331543\n",
      "val loss 3.9254372119903564\n",
      "______________\n",
      "epoch 42 train loss 3.4631383419036865\n",
      "val loss 3.889388084411621\n",
      "______________\n",
      "epoch 43 train loss 3.379509449005127\n",
      "val loss 3.8535540103912354\n",
      "______________\n",
      "epoch 44 train loss 3.403355121612549\n",
      "val loss 3.8173987865448\n",
      "______________\n",
      "epoch 45 train loss 3.3362975120544434\n",
      "val loss 3.780897617340088\n",
      "______________\n",
      "epoch 46 train loss 3.319845199584961\n",
      "val loss 3.7436020374298096\n",
      "______________\n",
      "epoch 47 train loss 3.2383487224578857\n",
      "val loss 3.7058658599853516\n",
      "______________\n",
      "epoch 48 train loss 3.2205092906951904\n",
      "val loss 3.667558193206787\n",
      "______________\n",
      "epoch 49 train loss 3.1771957874298096\n",
      "val loss 3.629075527191162\n",
      "______________\n",
      "epoch 50 train loss 3.1366982460021973\n",
      "val loss 3.590777635574341\n",
      "______________\n",
      "epoch 51 train loss 3.087031364440918\n",
      "val loss 3.552919626235962\n",
      "______________\n",
      "epoch 52 train loss 3.0182113647460938\n",
      "val loss 3.516005754470825\n",
      "______________\n",
      "epoch 53 train loss 3.0192761421203613\n",
      "val loss 3.480287790298462\n",
      "______________\n",
      "epoch 54 train loss 3.0495545864105225\n",
      "val loss 3.445791721343994\n",
      "______________\n",
      "epoch 55 train loss 2.981703042984009\n",
      "val loss 3.4129762649536133\n",
      "______________\n",
      "epoch 56 train loss 2.8929665088653564\n",
      "val loss 3.381700277328491\n",
      "______________\n",
      "epoch 57 train loss 2.9731674194335938\n",
      "val loss 3.352015256881714\n",
      "______________\n",
      "epoch 58 train loss 2.880666494369507\n",
      "val loss 3.3237035274505615\n",
      "______________\n",
      "epoch 59 train loss 2.843977451324463\n",
      "val loss 3.2971038818359375\n",
      "______________\n",
      "epoch 60 train loss 2.773211717605591\n",
      "val loss 3.2715907096862793\n",
      "______________\n",
      "epoch 61 train loss 2.772000551223755\n",
      "val loss 3.247199773788452\n",
      "______________\n",
      "epoch 62 train loss 2.823410749435425\n",
      "val loss 3.2236666679382324\n",
      "______________\n",
      "epoch 63 train loss 2.7561192512512207\n",
      "val loss 3.200854539871216\n",
      "______________\n",
      "epoch 64 train loss 2.71583890914917\n",
      "val loss 3.178990364074707\n",
      "______________\n",
      "epoch 65 train loss 2.7049448490142822\n",
      "val loss 3.157869577407837\n",
      "______________\n",
      "epoch 66 train loss 2.671320676803589\n",
      "val loss 3.137441396713257\n",
      "______________\n",
      "epoch 67 train loss 2.716644525527954\n",
      "val loss 3.117323637008667\n",
      "______________\n",
      "epoch 68 train loss 2.681093215942383\n",
      "val loss 3.097831964492798\n",
      "______________\n",
      "epoch 69 train loss 2.617628335952759\n",
      "val loss 3.0791447162628174\n",
      "______________\n",
      "epoch 70 train loss 2.574805498123169\n",
      "val loss 3.061002254486084\n",
      "______________\n",
      "epoch 71 train loss 2.534473419189453\n",
      "val loss 3.0429930686950684\n",
      "______________\n",
      "epoch 72 train loss 2.5230770111083984\n",
      "val loss 3.025054693222046\n",
      "______________\n",
      "epoch 73 train loss 2.491403579711914\n",
      "val loss 3.0073015689849854\n",
      "______________\n",
      "epoch 74 train loss 2.5164074897766113\n",
      "val loss 2.9895172119140625\n",
      "______________\n",
      "epoch 75 train loss 2.5181291103363037\n",
      "val loss 2.9722485542297363\n",
      "______________\n",
      "epoch 76 train loss 2.4244279861450195\n",
      "val loss 2.9554474353790283\n",
      "______________\n",
      "epoch 77 train loss 2.4153363704681396\n",
      "val loss 2.938857078552246\n",
      "______________\n",
      "epoch 78 train loss 2.3919224739074707\n",
      "val loss 2.922851800918579\n",
      "______________\n",
      "epoch 79 train loss 2.4182233810424805\n",
      "val loss 2.907404661178589\n",
      "______________\n",
      "epoch 80 train loss 2.341935396194458\n",
      "val loss 2.8919241428375244\n",
      "______________\n",
      "epoch 81 train loss 2.376096725463867\n",
      "val loss 2.8766062259674072\n",
      "______________\n",
      "epoch 82 train loss 2.313249349594116\n",
      "val loss 2.861480712890625\n",
      "______________\n",
      "epoch 83 train loss 2.332872152328491\n",
      "val loss 2.846220016479492\n",
      "______________\n",
      "epoch 84 train loss 2.2889599800109863\n",
      "val loss 2.83095121383667\n",
      "______________\n",
      "epoch 85 train loss 2.259612560272217\n",
      "val loss 2.815624952316284\n",
      "______________\n",
      "epoch 86 train loss 2.254107713699341\n",
      "val loss 2.8001840114593506\n",
      "______________\n",
      "epoch 87 train loss 2.2644670009613037\n",
      "val loss 2.784787654876709\n",
      "______________\n",
      "epoch 88 train loss 2.192049264907837\n",
      "val loss 2.769256114959717\n",
      "______________\n",
      "epoch 89 train loss 2.195834159851074\n",
      "val loss 2.753666639328003\n",
      "______________\n",
      "epoch 90 train loss 2.1683812141418457\n",
      "val loss 2.7381973266601562\n",
      "______________\n",
      "epoch 91 train loss 2.2135393619537354\n",
      "val loss 2.723036766052246\n",
      "______________\n",
      "epoch 92 train loss 2.096402883529663\n",
      "val loss 2.708502769470215\n",
      "______________\n",
      "epoch 93 train loss 2.13266921043396\n",
      "val loss 2.694653034210205\n",
      "______________\n",
      "epoch 94 train loss 2.131516695022583\n",
      "val loss 2.681501865386963\n",
      "______________\n",
      "epoch 95 train loss 2.108449697494507\n",
      "val loss 2.6686277389526367\n",
      "______________\n",
      "epoch 96 train loss 2.1384730339050293\n",
      "val loss 2.656496524810791\n",
      "______________\n",
      "epoch 97 train loss 2.0485177040100098\n",
      "val loss 2.6449625492095947\n",
      "______________\n",
      "epoch 98 train loss 2.0703258514404297\n",
      "val loss 2.6339073181152344\n",
      "______________\n",
      "epoch 99 train loss 2.066021203994751\n",
      "val loss 2.6234419345855713\n",
      "______________\n",
      "epoch 100 train loss 2.0762295722961426\n",
      "val loss 2.613281011581421\n",
      "______________\n",
      "epoch 101 train loss 2.067645788192749\n",
      "val loss 2.6037492752075195\n",
      "______________\n",
      "epoch 102 train loss 2.029522657394409\n",
      "val loss 2.594792604446411\n",
      "______________\n",
      "epoch 103 train loss 2.0396275520324707\n",
      "val loss 2.585832357406616\n",
      "______________\n",
      "epoch 104 train loss 1.974968671798706\n",
      "val loss 2.576547145843506\n",
      "______________\n",
      "epoch 105 train loss 2.0101287364959717\n",
      "val loss 2.567089557647705\n",
      "______________\n",
      "epoch 106 train loss 1.9662916660308838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 2.557722806930542\n",
      "______________\n",
      "epoch 107 train loss 1.9708058834075928\n",
      "val loss 2.548135995864868\n",
      "______________\n",
      "epoch 108 train loss 1.9384851455688477\n",
      "val loss 2.5380640029907227\n",
      "______________\n",
      "epoch 109 train loss 1.931792974472046\n",
      "val loss 2.5281875133514404\n",
      "______________\n",
      "epoch 110 train loss 1.9543131589889526\n",
      "val loss 2.5185060501098633\n",
      "______________\n",
      "epoch 111 train loss 1.9031983613967896\n",
      "val loss 2.5094523429870605\n",
      "______________\n",
      "epoch 112 train loss 1.9226182699203491\n",
      "val loss 2.500915050506592\n",
      "______________\n",
      "epoch 113 train loss 1.9000120162963867\n",
      "val loss 2.493039846420288\n",
      "______________\n",
      "epoch 114 train loss 1.9530097246170044\n",
      "val loss 2.4857616424560547\n",
      "______________\n",
      "epoch 115 train loss 1.863186240196228\n",
      "val loss 2.4787819385528564\n",
      "______________\n",
      "epoch 116 train loss 1.8908538818359375\n",
      "val loss 2.47210955619812\n",
      "______________\n",
      "epoch 117 train loss 1.8609235286712646\n",
      "val loss 2.465716600418091\n",
      "______________\n",
      "epoch 118 train loss 1.8190964460372925\n",
      "val loss 2.459789276123047\n",
      "______________\n",
      "epoch 119 train loss 1.813546895980835\n",
      "val loss 2.454364061355591\n",
      "______________\n",
      "epoch 120 train loss 1.8488892316818237\n",
      "val loss 2.4493608474731445\n",
      "______________\n",
      "epoch 121 train loss 1.8357858657836914\n",
      "val loss 2.4449615478515625\n",
      "______________\n",
      "epoch 122 train loss 1.8075335025787354\n",
      "val loss 2.440873384475708\n",
      "______________\n",
      "epoch 123 train loss 1.7975772619247437\n",
      "val loss 2.4362213611602783\n",
      "______________\n",
      "epoch 124 train loss 1.839146614074707\n",
      "val loss 2.4322798252105713\n",
      "______________\n",
      "epoch 125 train loss 1.7531510591506958\n",
      "val loss 2.4282712936401367\n",
      "______________\n",
      "epoch 126 train loss 1.7820391654968262\n",
      "val loss 2.4244821071624756\n",
      "______________\n",
      "epoch 127 train loss 1.8189506530761719\n",
      "val loss 2.4203691482543945\n",
      "______________\n",
      "epoch 128 train loss 1.743078351020813\n",
      "val loss 2.4162425994873047\n",
      "______________\n",
      "epoch 129 train loss 1.7822633981704712\n",
      "val loss 2.4129245281219482\n",
      "______________\n",
      "epoch 130 train loss 1.7263967990875244\n",
      "val loss 2.409684419631958\n",
      "______________\n",
      "epoch 131 train loss 1.7732714414596558\n",
      "val loss 2.4063968658447266\n",
      "______________\n",
      "epoch 132 train loss 1.7599241733551025\n",
      "val loss 2.4029428958892822\n",
      "______________\n",
      "epoch 133 train loss 1.7272647619247437\n",
      "val loss 2.3995840549468994\n",
      "______________\n",
      "epoch 134 train loss 1.7444566488265991\n",
      "val loss 2.3960459232330322\n",
      "______________\n",
      "epoch 135 train loss 1.7490355968475342\n",
      "val loss 2.392622470855713\n",
      "______________\n",
      "epoch 136 train loss 1.716491937637329\n",
      "val loss 2.3884520530700684\n",
      "______________\n",
      "epoch 137 train loss 1.7083311080932617\n",
      "val loss 2.3845605850219727\n",
      "______________\n",
      "epoch 138 train loss 1.7163571119308472\n",
      "val loss 2.3808400630950928\n",
      "______________\n",
      "epoch 139 train loss 1.6909639835357666\n",
      "val loss 2.3769326210021973\n",
      "______________\n",
      "epoch 140 train loss 1.6909123659133911\n",
      "val loss 2.3725805282592773\n",
      "______________\n",
      "epoch 141 train loss 1.6539438962936401\n",
      "val loss 2.368659734725952\n",
      "______________\n",
      "epoch 142 train loss 1.653620719909668\n",
      "val loss 2.3644466400146484\n",
      "______________\n",
      "epoch 143 train loss 1.660958170890808\n",
      "val loss 2.3610939979553223\n",
      "______________\n",
      "epoch 144 train loss 1.6667613983154297\n",
      "val loss 2.358072280883789\n",
      "______________\n",
      "epoch 145 train loss 1.6322834491729736\n",
      "val loss 2.3554365634918213\n",
      "______________\n",
      "epoch 146 train loss 1.6396267414093018\n",
      "val loss 2.353102445602417\n",
      "______________\n",
      "epoch 147 train loss 1.6120147705078125\n",
      "val loss 2.350503444671631\n",
      "______________\n",
      "epoch 148 train loss 1.644123911857605\n",
      "val loss 2.3482773303985596\n",
      "______________\n",
      "epoch 149 train loss 1.6669827699661255\n",
      "val loss 2.346623182296753\n",
      "______________\n",
      "epoch 150 train loss 1.5988085269927979\n",
      "val loss 2.345186471939087\n",
      "______________\n",
      "epoch 151 train loss 1.6346721649169922\n",
      "val loss 2.3438034057617188\n",
      "______________\n",
      "epoch 152 train loss 1.6532505750656128\n",
      "val loss 2.342599630355835\n",
      "______________\n",
      "epoch 153 train loss 1.637651801109314\n",
      "val loss 2.3416242599487305\n",
      "______________\n",
      "epoch 154 train loss 1.610044240951538\n",
      "val loss 2.3405139446258545\n",
      "______________\n",
      "epoch 155 train loss 1.5863794088363647\n",
      "val loss 2.339374542236328\n",
      "______________\n",
      "epoch 156 train loss 1.6058679819107056\n",
      "val loss 2.3382935523986816\n",
      "______________\n",
      "epoch 157 train loss 1.6429789066314697\n",
      "val loss 2.3375723361968994\n",
      "______________\n",
      "epoch 158 train loss 1.5647735595703125\n",
      "val loss 2.3367233276367188\n",
      "______________\n",
      "epoch 159 train loss 1.5439517498016357\n",
      "val loss 2.33569073677063\n",
      "______________\n",
      "epoch 160 train loss 1.5564849376678467\n",
      "val loss 2.334678888320923\n",
      "______________\n",
      "epoch 161 train loss 1.497815728187561\n",
      "val loss 2.332980155944824\n",
      "______________\n",
      "epoch 162 train loss 1.6360054016113281\n",
      "val loss 2.3317008018493652\n",
      "______________\n",
      "epoch 163 train loss 1.583269715309143\n",
      "val loss 2.330505609512329\n",
      "______________\n",
      "epoch 164 train loss 1.539162278175354\n",
      "val loss 2.3291409015655518\n",
      "______________\n",
      "epoch 165 train loss 1.5650439262390137\n",
      "val loss 2.3272173404693604\n",
      "______________\n",
      "epoch 166 train loss 1.5269594192504883\n",
      "val loss 2.3251724243164062\n",
      "______________\n",
      "epoch 167 train loss 1.5412170886993408\n",
      "val loss 2.3230485916137695\n",
      "______________\n",
      "epoch 168 train loss 1.4862582683563232\n",
      "val loss 2.3208231925964355\n",
      "______________\n",
      "epoch 169 train loss 1.478954792022705\n",
      "val loss 2.3191120624542236\n",
      "______________\n",
      "epoch 170 train loss 1.531813383102417\n",
      "val loss 2.317723512649536\n",
      "______________\n",
      "epoch 171 train loss 1.524855375289917\n",
      "val loss 2.316267251968384\n",
      "______________\n",
      "epoch 172 train loss 1.4825574159622192\n",
      "val loss 2.3149912357330322\n",
      "______________\n",
      "epoch 173 train loss 1.4825806617736816\n",
      "val loss 2.314069986343384\n",
      "______________\n",
      "epoch 174 train loss 1.4642529487609863\n",
      "val loss 2.3128764629364014\n",
      "______________\n",
      "epoch 175 train loss 1.4940229654312134\n",
      "val loss 2.312319278717041\n",
      "______________\n",
      "epoch 176 train loss 1.4719706773757935\n",
      "val loss 2.3122315406799316\n",
      "______________\n",
      "epoch 177 train loss 1.5164374113082886\n",
      "val loss 2.312352180480957\n",
      "______________\n",
      "epoch 178 train loss 1.502659797668457\n",
      "val loss 2.3127617835998535\n",
      "______________\n",
      "epoch 179 train loss 1.5421236753463745\n",
      "val loss 2.313657760620117\n",
      "______________\n",
      "epoch 180 train loss 1.4839426279067993\n",
      "val loss 2.3146350383758545\n",
      "______________\n",
      "epoch 181 train loss 1.5179455280303955\n",
      "val loss 2.315922737121582\n",
      "______________\n",
      "epoch 182 train loss 1.4656599760055542\n",
      "val loss 2.317383050918579\n",
      "______________\n",
      "epoch 183 train loss 1.4820842742919922\n",
      "val loss 2.318643808364868\n",
      "______________\n",
      "epoch 184 train loss 1.452399492263794\n",
      "val loss 2.3197147846221924\n",
      "______________\n",
      "epoch 185 train loss 1.4660214185714722\n",
      "val loss 2.3199257850646973\n",
      "______________\n",
      "epoch 186 train loss 1.4211481809616089\n",
      "val loss 2.3195879459381104\n",
      "______________\n",
      "epoch 187 train loss 1.4559199810028076\n",
      "val loss 2.3186874389648438\n",
      "______________\n",
      "best loss 2.3122315406799316 {'pd': {'accuracy': 0.5017605633802816, 'roc_micro': 0.6798756798756799, 'roc_macro': 0.627723385537347}, 'nd': {'accuracy': 0.6142893465547927, 'roc_micro': 0.7360355693689027, 'roc_macro': 0.6966503992901508}, 'mod': {'accuracy': 0.6142893465547927, 'roc_micro': 0.7360355693689027, 'roc_macro': 0.6966503992901508}, 'dlts': {'accuracy': [0.3333333333333333, 0.3333333333333333, 0.5, 0.5, 0.5, 0.3214285714285714, 0.25, 0.3333333333333333], 'accuracy_mean': 0.3839285714285714, 'auc': [0.5, 0.5, 0.5, 0.5, 0.5, 0.65, 0.5, 0.5], 'auc_mean': 0.51875}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OutcomeSimulator(\n",
       "  (input_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=62, out_features=1000, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (batchnorm): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (softmax): LogSoftmax(dim=1)\n",
       "  (disease_layer): Linear(in_features=1000, out_features=3, bias=True)\n",
       "  (nodal_disease_layer): Linear(in_features=1000, out_features=3, bias=True)\n",
       "  (dlt_layers): ModuleList(\n",
       "    (0): Linear(in_features=1000, out_features=5, bias=True)\n",
       "    (1): Linear(in_features=1000, out_features=5, bias=True)\n",
       "    (2): Linear(in_features=1000, out_features=5, bias=True)\n",
       "    (3): Linear(in_features=1000, out_features=5, bias=True)\n",
       "    (4): Linear(in_features=1000, out_features=5, bias=True)\n",
       "    (5): Linear(in_features=1000, out_features=5, bias=True)\n",
       "    (6): Linear(in_features=1000, out_features=5, bias=True)\n",
       "    (7): Linear(in_features=1000, out_features=5, bias=True)\n",
       "  )\n",
       "  (treatment_layer): Linear(in_features=1000, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_state(model_args={},\n",
    "                state=1,\n",
    "                split=.7,\n",
    "                lr=.0001,\n",
    "                epochs=1000,\n",
    "                patience=10,\n",
    "                weights=[1,1,1,10],\n",
    "                save_path='../data/models/',\n",
    "                resample_training=False,#use bootstraping on training data after splitting\n",
    "                resample_all = False,# use bootstrapping, then validate with out-of-bag data\n",
    "                file_suffix=''):\n",
    "    \n",
    "    ids = get_dt_ids()\n",
    "    \n",
    "    if resample_all:\n",
    "        train_ids = np.random.choice(ids,len(ids),replace=True)\n",
    "        test_ids = [i for i in ids if i not in train_ids]\n",
    "    \n",
    "    else:\n",
    "        train_ids = ids[0:int(len(ids)*split)]\n",
    "        if resample_training:\n",
    "            train_ids = np.random.choice(train_ids,len(train_ids),replace=True)\n",
    "        test_ids = ids[int(len(ids)*split):]\n",
    "    \n",
    "    dataset = DTDataset()\n",
    "    \n",
    "    xtrain = dataset.get_input_state(step=state,ids=train_ids)\n",
    "    xtest = dataset.get_input_state(step=state,ids=test_ids)\n",
    "    ytrain = dataset.get_intermediate_outcomes(step=state,ids=train_ids)\n",
    "    ytest = dataset.get_intermediate_outcomes(step=state,ids=test_ids)\n",
    "    \n",
    "\n",
    "    if state < 3:\n",
    "        model = OutcomeSimulator(xtrain.shape[1],state=state,**model_args)\n",
    "        lfunc = state_loss\n",
    "    else:\n",
    "        model = EndpointSimulator(xtrain.shape[1],**model_args)\n",
    "        weights = weights[:3]\n",
    "        lfunc = outcome_loss\n",
    "        \n",
    "    hashcode = str(hash(','.join([str(i) for i in train_ids])))\n",
    "    save_file = save_path + 'model_' + model.identifier + '_split' + str(split) + '_resample' + str(resample_training) +  '_hash' + hashcode + file_suffix + '.tar'\n",
    "    xtrain = df_to_torch(xtrain)\n",
    "    xtest = df_to_torch(xtest)\n",
    "    ytrain = [df_to_torch(t) for t in ytrain]\n",
    "    ytest= [df_to_torch(t) for t in ytest]\n",
    "    \n",
    "    model.fit_normalizer(xtrain)\n",
    "#     normalize = lambda x: (x - xtrain.mean(axis=0)+.01)/(xtrain.std(axis=0)+.01)\n",
    "#     unnormalize = lambda x: (x * (xtrain.std(axis=0) +.01)) + xtrain.mean(axis=0) - .01\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "    best_val_loss = 1000000000000000000000000000\n",
    "    best_loss_metrics = {}\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train(True)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        xtrain_sample = xtrain#[torch.randint(len(xtrain),(len(xtrain),) )]\n",
    "        ypred = model(xtrain_sample)\n",
    "        loss = lfunc(ytrain,ypred,weights=weights)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('epoch',epoch,'train loss',loss.item())\n",
    "        \n",
    "        model.eval()\n",
    "        yval = model(xtest)\n",
    "        val_loss = lfunc(ytest,yval,weights=weights)\n",
    "        if state < 3:\n",
    "            val_metrics = state_metrics(ytest,yval)\n",
    "        else:\n",
    "            val_metrics = outcome_metrics(ytest,yval)\n",
    "        if val_loss.item() < best_val_loss:\n",
    "            best_val_loss = val_loss.item()\n",
    "            best_loss_metrics = val_metrics\n",
    "            steps_since_improvement = 0\n",
    "            torch.save(model.state_dict(),save_file)\n",
    "        else:\n",
    "            steps_since_improvement += 1\n",
    "        print('val loss',val_loss.item())\n",
    "        print('______________')\n",
    "        if steps_since_improvement > patience:\n",
    "            break\n",
    "    print('best loss',best_val_loss,best_loss_metrics)\n",
    "    model.load_state_dict(torch.load(save_file))\n",
    "    model = model.eval()\n",
    "    return model\n",
    "model = train_state(state=1)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27cdf2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2197865/1992661605.py:2: DtypeWarning: Columns (55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/tmp/ipykernel_2197865/1521532340.py:4: DtypeWarning: Columns (55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_file)\n",
      "/tmp/ipykernel_2197865/1521532340.py:19: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  self.means = self.processed_df.mean(axis=0)\n",
      "/tmp/ipykernel_2197865/1521532340.py:20: FutureWarning: The default value of numeric_only in DataFrame.std is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  self.stds = self.processed_df.std(axis=0)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/classification.py:1745: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/classification.py:1745: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/classification.py:1745: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/classification.py:1745: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/classification.py:1745: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train loss 10.293455123901367\n",
      "val loss 9.65517807006836\n",
      "______________\n",
      "epoch 1 train loss 9.782721519470215\n",
      "val loss 9.158468246459961\n",
      "______________\n",
      "epoch 2 train loss 9.307807922363281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/classification.py:1745: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/classification.py:1745: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 8.710000038146973\n",
      "______________\n",
      "epoch 3 train loss 8.863341331481934\n",
      "val loss 8.289332389831543\n",
      "______________\n",
      "epoch 4 train loss 8.45036506652832\n",
      "val loss 7.890841484069824\n",
      "______________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 train loss 8.046409606933594\n",
      "val loss 7.511332035064697\n",
      "______________\n",
      "epoch 6 train loss 7.64516544342041\n",
      "val loss 7.147595405578613\n",
      "______________\n",
      "epoch 7 train loss 7.31526517868042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 6.799703121185303\n",
      "______________\n",
      "epoch 8 train loss 6.922632217407227\n",
      "val loss 6.468106269836426\n",
      "______________\n",
      "epoch 9 train loss 6.586355209350586\n",
      "val loss 6.153602123260498\n",
      "______________\n",
      "epoch 10 train loss 6.281045436859131\n",
      "val loss 5.857736587524414\n",
      "______________\n",
      "epoch 11 train loss 6.003660678863525\n",
      "val loss 5.580427169799805\n",
      "______________\n",
      "epoch 12 train loss 5.702635288238525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 5.322508335113525\n",
      "______________\n",
      "epoch 13 train loss 5.484636306762695\n",
      "val loss 5.085526943206787\n",
      "______________\n",
      "epoch 14 train loss 5.243514537811279\n",
      "val loss 4.869521141052246\n",
      "______________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15 train loss 5.022521495819092\n",
      "val loss 4.674798965454102\n",
      "______________\n",
      "epoch 16 train loss 4.837058067321777\n",
      "val loss 4.501087188720703\n",
      "______________\n",
      "epoch 17 train loss 4.636934280395508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 4.347265243530273\n",
      "______________\n",
      "epoch 18 train loss 4.503328800201416\n",
      "val loss 4.212705612182617\n",
      "______________\n",
      "epoch 19 train loss 4.333676338195801\n",
      "val loss 4.095096111297607\n",
      "______________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20 train loss 4.257548809051514\n",
      "val loss 3.9918432235717773\n",
      "______________\n",
      "epoch 21 train loss 4.153221130371094\n",
      "val loss 3.9024770259857178\n",
      "______________\n",
      "epoch 22 train loss 4.092041969299316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 3.8250410556793213\n",
      "______________\n",
      "epoch 23 train loss 4.03037166595459\n",
      "val loss 3.7579050064086914\n",
      "______________\n",
      "epoch 24 train loss 3.9390017986297607\n",
      "val loss 3.6987476348876953\n",
      "______________\n",
      "epoch 25 train loss 3.9231300354003906\n",
      "val loss 3.647230863571167\n",
      "______________\n",
      "epoch 26 train loss 3.8656165599823\n",
      "val loss 3.6015827655792236\n",
      "______________\n",
      "epoch 27 train loss 3.803894519805908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 3.5606184005737305\n",
      "______________\n",
      "epoch 28 train loss 3.780531167984009\n",
      "val loss 3.5240886211395264\n",
      "______________\n",
      "epoch 29 train loss 3.740468978881836\n",
      "val loss 3.492593288421631\n",
      "______________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 train loss 3.71014142036438\n",
      "val loss 3.4643454551696777\n",
      "______________\n",
      "epoch 31 train loss 3.6975150108337402\n",
      "val loss 3.437966823577881\n",
      "______________\n",
      "epoch 32 train loss 3.6622302532196045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 3.414724111557007\n",
      "______________\n",
      "epoch 33 train loss 3.6114768981933594\n",
      "val loss 3.393545627593994\n",
      "______________\n",
      "epoch 34 train loss 3.6216704845428467\n",
      "val loss 3.3754544258117676\n",
      "______________\n",
      "epoch 35 train loss 3.5745790004730225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 3.3602564334869385\n",
      "______________\n",
      "epoch 36 train loss 3.554840326309204\n",
      "val loss 3.3468925952911377\n",
      "______________\n",
      "epoch 37 train loss 3.5837433338165283\n",
      "val loss 3.334935426712036\n",
      "______________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 38 train loss 3.5327858924865723\n",
      "val loss 3.3237617015838623\n",
      "______________\n",
      "epoch 39 train loss 3.4894204139709473\n",
      "val loss 3.313202381134033\n",
      "______________\n",
      "epoch 40 train loss 3.495757579803467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 3.3036985397338867\n",
      "______________\n",
      "epoch 41 train loss 3.454291582107544\n",
      "val loss 3.2962098121643066\n",
      "______________\n",
      "epoch 42 train loss 3.4351892471313477\n",
      "val loss 3.2898764610290527\n",
      "______________\n",
      "epoch 43 train loss 3.3971259593963623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 3.2835781574249268\n",
      "______________\n",
      "epoch 44 train loss 3.425179958343506\n",
      "val loss 3.2778358459472656\n",
      "______________\n",
      "epoch 45 train loss 3.358964204788208\n",
      "val loss 3.2716798782348633\n",
      "______________\n",
      "epoch 46 train loss 3.3377130031585693\n",
      "val loss 3.265054225921631\n",
      "______________\n",
      "epoch 47 train loss 3.3983142375946045\n",
      "val loss 3.2590363025665283\n",
      "______________\n",
      "epoch 48 train loss 3.340468645095825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 3.252532482147217\n",
      "______________\n",
      "epoch 49 train loss 3.298657178878784\n",
      "val loss 3.2447173595428467\n",
      "______________\n",
      "epoch 50 train loss 3.2648942470550537\n",
      "val loss 3.2370893955230713\n",
      "______________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 51 train loss 3.2688615322113037\n",
      "val loss 3.228454351425171\n",
      "______________\n",
      "epoch 52 train loss 3.2721946239471436\n",
      "val loss 3.220998525619507\n",
      "______________\n",
      "epoch 53 train loss 3.2336838245391846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 3.2135908603668213\n",
      "______________\n",
      "epoch 54 train loss 3.2446439266204834\n",
      "val loss 3.2072296142578125\n",
      "______________\n",
      "epoch 55 train loss 3.2071385383605957\n",
      "val loss 3.200655698776245\n",
      "______________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 56 train loss 3.1916348934173584\n",
      "val loss 3.1949570178985596\n",
      "______________\n",
      "epoch 57 train loss 3.179058790206909\n",
      "val loss 3.189561605453491\n",
      "______________\n",
      "epoch 58 train loss 3.1939010620117188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 3.1847550868988037\n",
      "______________\n",
      "epoch 59 train loss 3.15960431098938\n",
      "val loss 3.1799092292785645\n",
      "______________\n",
      "epoch 60 train loss 3.1432671546936035\n",
      "val loss 3.173334836959839\n",
      "______________\n",
      "epoch 61 train loss 3.1289327144622803\n",
      "val loss 3.1667299270629883\n",
      "______________\n",
      "epoch 62 train loss 3.1241719722747803\n",
      "val loss 3.159519672393799\n",
      "______________\n",
      "epoch 63 train loss 3.107797861099243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 3.1519927978515625\n",
      "______________\n",
      "epoch 64 train loss 3.060866355895996\n",
      "val loss 3.145702838897705\n",
      "______________\n",
      "epoch 65 train loss 3.0811522006988525\n",
      "val loss 3.13985013961792\n",
      "______________\n",
      "epoch 66 train loss 3.052199363708496\n",
      "val loss 3.134077787399292\n",
      "______________\n",
      "epoch 67 train loss 3.0525476932525635\n",
      "val loss 3.1281967163085938\n",
      "______________\n",
      "epoch 68 train loss 3.0269322395324707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 3.1219382286071777\n",
      "______________\n",
      "epoch 69 train loss 3.012512445449829\n",
      "val loss 3.1146440505981445\n",
      "______________\n",
      "epoch 70 train loss 2.9745190143585205\n",
      "val loss 3.1087982654571533\n",
      "______________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 71 train loss 3.011258125305176\n",
      "val loss 3.103254556655884\n",
      "______________\n",
      "epoch 72 train loss 2.946201801300049\n",
      "val loss 3.097930431365967\n",
      "______________\n",
      "epoch 73 train loss 2.9810774326324463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 3.0935521125793457\n",
      "______________\n",
      "epoch 74 train loss 2.926347255706787\n",
      "val loss 3.088517427444458\n",
      "______________\n",
      "epoch 75 train loss 2.9472248554229736\n",
      "val loss 3.084005117416382\n",
      "______________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 76 train loss 2.909857749938965\n",
      "val loss 3.0795702934265137\n",
      "______________\n",
      "epoch 77 train loss 2.9466989040374756\n",
      "val loss 3.0753226280212402\n",
      "______________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 78 train loss 2.9165711402893066\n",
      "val loss 3.0711638927459717\n",
      "______________\n",
      "epoch 79 train loss 2.876617431640625\n",
      "val loss 3.0666184425354004\n",
      "______________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 80 train loss 2.9288554191589355\n",
      "val loss 3.062391519546509\n",
      "______________\n",
      "epoch 81 train loss 2.8886260986328125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 3.0587573051452637\n",
      "______________\n",
      "epoch 82 train loss 2.8618695735931396\n",
      "val loss 3.0546700954437256\n",
      "______________\n",
      "epoch 83 train loss 2.8534505367279053\n",
      "val loss 3.0496666431427\n",
      "______________\n",
      "epoch 84 train loss 2.8331031799316406\n",
      "val loss 3.0440433025360107\n",
      "______________\n",
      "epoch 85 train loss 2.837718963623047\n",
      "val loss 3.037994861602783\n",
      "______________\n",
      "epoch 86 train loss 2.8417375087738037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 3.032968282699585\n",
      "______________\n",
      "epoch 87 train loss 2.815450429916382\n",
      "val loss 3.0271992683410645\n",
      "______________\n",
      "epoch 88 train loss 2.777944326400757\n",
      "val loss 3.0211727619171143\n",
      "______________\n",
      "epoch 89 train loss 2.752603054046631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 3.0161001682281494\n",
      "______________\n",
      "epoch 90 train loss 2.7788491249084473\n",
      "val loss 3.011270046234131\n",
      "______________\n",
      "epoch 91 train loss 2.7819931507110596\n",
      "val loss 3.0066158771514893\n",
      "______________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 92 train loss 2.7573978900909424\n",
      "val loss 3.003504753112793\n",
      "______________\n",
      "epoch 93 train loss 2.749675750732422\n",
      "val loss 3.0013370513916016\n",
      "______________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 94 train loss 2.729771375656128\n",
      "val loss 2.9989120960235596\n",
      "______________\n",
      "epoch 95 train loss 2.7445452213287354\n",
      "val loss 2.9977831840515137\n",
      "______________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 96 train loss 2.7038025856018066\n",
      "val loss 2.9955661296844482\n",
      "______________\n",
      "epoch 97 train loss 2.723533868789673\n",
      "val loss 2.993093252182007\n",
      "______________\n",
      "epoch 98 train loss 2.7081336975097656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 2.9902396202087402\n",
      "______________\n",
      "epoch 99 train loss 2.683424234390259\n",
      "val loss 2.987056255340576\n",
      "______________\n",
      "epoch 100 train loss 2.707948923110962\n",
      "val loss 2.9859914779663086\n",
      "______________\n",
      "epoch 101 train loss 2.6641831398010254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 2.983588933944702\n",
      "______________\n",
      "epoch 102 train loss 2.630382537841797\n",
      "val loss 2.9819891452789307\n",
      "______________\n",
      "epoch 103 train loss 2.6635963916778564\n",
      "val loss 2.979915142059326\n",
      "______________\n",
      "epoch 104 train loss 2.6766607761383057\n",
      "val loss 2.978588342666626\n",
      "______________\n",
      "epoch 105 train loss 2.6336441040039062\n",
      "val loss 2.9774961471557617\n",
      "______________\n",
      "epoch 106 train loss 2.681339979171753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 2.9760007858276367\n",
      "______________\n",
      "epoch 107 train loss 2.577894449234009\n",
      "val loss 2.9749741554260254\n",
      "______________\n",
      "epoch 108 train loss 2.580425977706909\n",
      "val loss 2.97365140914917\n",
      "______________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 109 train loss 2.6181492805480957\n",
      "val loss 2.971081495285034\n",
      "______________\n",
      "epoch 110 train loss 2.5845093727111816\n",
      "val loss 2.9667465686798096\n",
      "______________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 111 train loss 2.5615289211273193\n",
      "val loss 2.9622650146484375\n",
      "______________\n",
      "epoch 112 train loss 2.5710842609405518\n",
      "val loss 2.9566850662231445\n",
      "______________\n",
      "epoch 113 train loss 2.559952974319458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 2.950462818145752\n",
      "______________\n",
      "epoch 114 train loss 2.5440566539764404\n",
      "val loss 2.9454121589660645\n",
      "______________\n",
      "epoch 115 train loss 2.5244078636169434\n",
      "val loss 2.9429831504821777\n",
      "______________\n",
      "epoch 116 train loss 2.530428647994995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 2.940115451812744\n",
      "______________\n",
      "epoch 117 train loss 2.534360647201538\n",
      "val loss 2.9402432441711426\n",
      "______________\n",
      "epoch 118 train loss 2.4976305961608887\n",
      "val loss 2.9414355754852295\n",
      "______________\n",
      "epoch 119 train loss 2.4785401821136475\n",
      "val loss 2.943080186843872\n",
      "______________\n",
      "epoch 120 train loss 2.477588415145874\n",
      "val loss 2.9452521800994873\n",
      "______________\n",
      "epoch 121 train loss 2.5029568672180176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 2.9485318660736084\n",
      "______________\n",
      "epoch 122 train loss 2.514151096343994\n",
      "val loss 2.952755928039551\n",
      "______________\n",
      "epoch 123 train loss 2.4670259952545166\n",
      "val loss 2.9557721614837646\n",
      "______________\n",
      "epoch 124 train loss 2.447359800338745\n",
      "val loss 2.9573276042938232\n",
      "______________\n",
      "epoch 125 train loss 2.4992177486419678\n",
      "val loss 2.958098888397217\n",
      "______________\n",
      "epoch 126 train loss 2.4381864070892334\n",
      "val loss 2.95611310005188\n",
      "______________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 127 train loss 2.4426982402801514\n",
      "val loss 2.9522035121917725\n",
      "______________\n",
      "best loss 2.940115451812744 {'pd': {'accuracy': 0.4965034965034965, 'roc_micro': 0.9470455780728317, 'roc_macro': -1}, 'nd': {'accuracy': 0.36074629977069, 'roc_micro': 0.7678779408339157, 'roc_macro': 0.6578843865490452}, 'mod': {'accuracy': 0.36074629977069, 'roc_micro': 0.7678779408339157, 'roc_macro': 0.6578843865490452}, 'dlts': {'accuracy': [0.5, 0.5, 0.5, 0.5, 1.0, 0.5, 0.5, 1.0], 'accuracy_mean': 0.625, 'auc': [0.5, 0.5, 0.5, 0.5, -1, 0.5, 0.5, -1], 'auc_mean': 0.125}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/label.py:622: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((len(y), 1), dtype=np.int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OutcomeSimulator(\n",
       "  (input_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=84, out_features=1000, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (batchnorm): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (softmax): LogSoftmax(dim=1)\n",
       "  (disease_layer): Linear(in_features=1000, out_features=3, bias=True)\n",
       "  (nodal_disease_layer): Linear(in_features=1000, out_features=3, bias=True)\n",
       "  (dlt_layers): ModuleList(\n",
       "    (0): Linear(in_features=1000, out_features=2, bias=True)\n",
       "    (1): Linear(in_features=1000, out_features=2, bias=True)\n",
       "    (2): Linear(in_features=1000, out_features=2, bias=True)\n",
       "    (3): Linear(in_features=1000, out_features=2, bias=True)\n",
       "    (4): Linear(in_features=1000, out_features=2, bias=True)\n",
       "    (5): Linear(in_features=1000, out_features=2, bias=True)\n",
       "    (6): Linear(in_features=1000, out_features=2, bias=True)\n",
       "    (7): Linear(in_features=1000, out_features=2, bias=True)\n",
       "  )\n",
       "  (treatment_layer): Linear(in_features=1000, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = train_state(state=2)\n",
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c33d908d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2197865/1992661605.py:2: DtypeWarning: Columns (55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/tmp/ipykernel_2197865/1521532340.py:4: DtypeWarning: Columns (55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_file)\n",
      "/tmp/ipykernel_2197865/1521532340.py:19: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  self.means = self.processed_df.mean(axis=0)\n",
      "/tmp/ipykernel_2197865/1521532340.py:20: FutureWarning: The default value of numeric_only in DataFrame.std is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  self.stds = self.processed_df.std(axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train loss 2.1015212535858154\n",
      "val loss 2.0966429710388184\n",
      "______________\n",
      "epoch 1 train loss 2.1059699058532715\n",
      "val loss 2.0853188037872314\n",
      "______________\n",
      "epoch 2 train loss 2.086408853530884\n",
      "val loss 2.074215888977051\n",
      "______________\n",
      "epoch 3 train loss 2.077211618423462\n",
      "val loss 2.063295364379883\n",
      "______________\n",
      "epoch 4 train loss 2.048168182373047\n",
      "val loss 2.0525667667388916\n",
      "______________\n",
      "epoch 5 train loss 2.033522129058838\n",
      "val loss 2.041977643966675\n",
      "______________\n",
      "epoch 6 train loss 2.0281002521514893\n",
      "val loss 2.0315542221069336\n",
      "______________\n",
      "epoch 7 train loss 2.0261025428771973\n",
      "val loss 2.0213098526000977\n",
      "______________\n",
      "epoch 8 train loss 2.00384521484375\n",
      "val loss 2.0112171173095703\n",
      "______________\n",
      "epoch 9 train loss 2.004908561706543\n",
      "val loss 2.001276731491089\n",
      "______________\n",
      "epoch 10 train loss 1.9926023483276367\n",
      "val loss 1.9914941787719727\n",
      "______________\n",
      "epoch 11 train loss 1.9776113033294678\n",
      "val loss 1.9818599224090576\n",
      "______________\n",
      "epoch 12 train loss 1.9772627353668213\n",
      "val loss 1.9723842144012451\n",
      "______________\n",
      "epoch 13 train loss 1.9405312538146973\n",
      "val loss 1.9630568027496338\n",
      "______________\n",
      "epoch 14 train loss 1.935889720916748\n",
      "val loss 1.9538732767105103\n",
      "______________\n",
      "epoch 15 train loss 1.922122836112976\n",
      "val loss 1.9448390007019043\n",
      "______________\n",
      "epoch 16 train loss 1.9131669998168945\n",
      "val loss 1.935962438583374\n",
      "______________\n",
      "epoch 17 train loss 1.92415189743042\n",
      "val loss 1.9272434711456299\n",
      "______________\n",
      "epoch 18 train loss 1.896903395652771\n",
      "val loss 1.9186735153198242\n",
      "______________\n",
      "epoch 19 train loss 1.8998923301696777\n",
      "val loss 1.910245656967163\n",
      "______________\n",
      "epoch 20 train loss 1.8718326091766357\n",
      "val loss 1.9019575119018555\n",
      "______________\n",
      "epoch 21 train loss 1.8666465282440186\n",
      "val loss 1.8938312530517578\n",
      "______________\n",
      "epoch 22 train loss 1.8545396327972412\n",
      "val loss 1.8858633041381836\n",
      "______________\n",
      "epoch 23 train loss 1.8200637102127075\n",
      "val loss 1.8780269622802734\n",
      "______________\n",
      "epoch 24 train loss 1.8256912231445312\n",
      "val loss 1.8703399896621704\n",
      "______________\n",
      "epoch 25 train loss 1.833072543144226\n",
      "val loss 1.8627899885177612\n",
      "______________\n",
      "epoch 26 train loss 1.810385823249817\n",
      "val loss 1.855372428894043\n",
      "______________\n",
      "epoch 27 train loss 1.800846815109253\n",
      "val loss 1.8480703830718994\n",
      "______________\n",
      "epoch 28 train loss 1.7935822010040283\n",
      "val loss 1.8408961296081543\n",
      "______________\n",
      "epoch 29 train loss 1.7925357818603516\n",
      "val loss 1.83384370803833\n",
      "______________\n",
      "epoch 30 train loss 1.777461051940918\n",
      "val loss 1.8269271850585938\n",
      "______________\n",
      "epoch 31 train loss 1.7708992958068848\n",
      "val loss 1.8201286792755127\n",
      "______________\n",
      "epoch 32 train loss 1.7687153816223145\n",
      "val loss 1.8134382963180542\n",
      "______________\n",
      "epoch 33 train loss 1.7522788047790527\n",
      "val loss 1.806854009628296\n",
      "______________\n",
      "epoch 34 train loss 1.7308800220489502\n",
      "val loss 1.8004083633422852\n",
      "______________\n",
      "epoch 35 train loss 1.7364866733551025\n",
      "val loss 1.7940723896026611\n",
      "______________\n",
      "epoch 36 train loss 1.7379753589630127\n",
      "val loss 1.787834882736206\n",
      "______________\n",
      "epoch 37 train loss 1.7112188339233398\n",
      "val loss 1.7817069292068481\n",
      "______________\n",
      "epoch 38 train loss 1.7232334613800049\n",
      "val loss 1.7756929397583008\n",
      "______________\n",
      "epoch 39 train loss 1.7142424583435059\n",
      "val loss 1.7697997093200684\n",
      "______________\n",
      "epoch 40 train loss 1.6760698556900024\n",
      "val loss 1.764007329940796\n",
      "______________\n",
      "epoch 41 train loss 1.694554090499878\n",
      "val loss 1.758302927017212\n",
      "______________\n",
      "epoch 42 train loss 1.6754264831542969\n",
      "val loss 1.7526854276657104\n",
      "______________\n",
      "epoch 43 train loss 1.680821418762207\n",
      "val loss 1.7471694946289062\n",
      "______________\n",
      "epoch 44 train loss 1.688370704650879\n",
      "val loss 1.7417302131652832\n",
      "______________\n",
      "epoch 45 train loss 1.6598657369613647\n",
      "val loss 1.736386775970459\n",
      "______________\n",
      "epoch 46 train loss 1.653802514076233\n",
      "val loss 1.7311259508132935\n",
      "______________\n",
      "epoch 47 train loss 1.657020926475525\n",
      "val loss 1.7259409427642822\n",
      "______________\n",
      "epoch 48 train loss 1.6437655687332153\n",
      "val loss 1.7208359241485596\n",
      "______________\n",
      "epoch 49 train loss 1.6433203220367432\n",
      "val loss 1.7158135175704956\n",
      "______________\n",
      "epoch 50 train loss 1.626272201538086\n",
      "val loss 1.7108551263809204\n",
      "______________\n",
      "epoch 51 train loss 1.6407356262207031\n",
      "val loss 1.7059552669525146\n",
      "______________\n",
      "epoch 52 train loss 1.6153433322906494\n",
      "val loss 1.7011258602142334\n",
      "______________\n",
      "epoch 53 train loss 1.6086533069610596\n",
      "val loss 1.6963601112365723\n",
      "______________\n",
      "epoch 54 train loss 1.5702180862426758\n",
      "val loss 1.691655158996582\n",
      "______________\n",
      "epoch 55 train loss 1.5939339399337769\n",
      "val loss 1.687016487121582\n",
      "______________\n",
      "epoch 56 train loss 1.578131914138794\n",
      "val loss 1.6824283599853516\n",
      "______________\n",
      "epoch 57 train loss 1.5830109119415283\n",
      "val loss 1.6779022216796875\n",
      "______________\n",
      "epoch 58 train loss 1.5662460327148438\n",
      "val loss 1.6734445095062256\n",
      "______________\n",
      "epoch 59 train loss 1.5539031028747559\n",
      "val loss 1.6690459251403809\n",
      "______________\n",
      "epoch 60 train loss 1.57940673828125\n",
      "val loss 1.6647083759307861\n",
      "______________\n",
      "epoch 61 train loss 1.556656837463379\n",
      "val loss 1.6604089736938477\n",
      "______________\n",
      "epoch 62 train loss 1.5363686084747314\n",
      "val loss 1.6561791896820068\n",
      "______________\n",
      "epoch 63 train loss 1.5314936637878418\n",
      "val loss 1.6519962549209595\n",
      "______________\n",
      "epoch 64 train loss 1.5415079593658447\n",
      "val loss 1.6478538513183594\n",
      "______________\n",
      "epoch 65 train loss 1.5464842319488525\n",
      "val loss 1.643770456314087\n",
      "______________\n",
      "epoch 66 train loss 1.5257822275161743\n",
      "val loss 1.6397416591644287\n",
      "______________\n",
      "epoch 67 train loss 1.5210052728652954\n",
      "val loss 1.6357392072677612\n",
      "______________\n",
      "epoch 68 train loss 1.5295779705047607\n",
      "val loss 1.6317636966705322\n",
      "______________\n",
      "epoch 69 train loss 1.5281212329864502\n",
      "val loss 1.6278324127197266\n",
      "______________\n",
      "epoch 70 train loss 1.5050692558288574\n",
      "val loss 1.6239469051361084\n",
      "______________\n",
      "epoch 71 train loss 1.5007786750793457\n",
      "val loss 1.6201013326644897\n",
      "______________\n",
      "epoch 72 train loss 1.5025497674942017\n",
      "val loss 1.616290807723999\n",
      "______________\n",
      "epoch 73 train loss 1.5013775825500488\n",
      "val loss 1.6125181913375854\n",
      "______________\n",
      "epoch 74 train loss 1.4840437173843384\n",
      "val loss 1.6087663173675537\n",
      "______________\n",
      "epoch 75 train loss 1.4785070419311523\n",
      "val loss 1.6050580739974976\n",
      "______________\n",
      "epoch 76 train loss 1.4796414375305176\n",
      "val loss 1.6013693809509277\n",
      "______________\n",
      "epoch 77 train loss 1.468317985534668\n",
      "val loss 1.5977108478546143\n",
      "______________\n",
      "epoch 78 train loss 1.4569857120513916\n",
      "val loss 1.594090223312378\n",
      "______________\n",
      "epoch 79 train loss 1.450575828552246\n",
      "val loss 1.5905030965805054\n",
      "______________\n",
      "epoch 80 train loss 1.4644367694854736\n",
      "val loss 1.586935043334961\n",
      "______________\n",
      "epoch 81 train loss 1.4408485889434814\n",
      "val loss 1.583391785621643\n",
      "______________\n",
      "epoch 82 train loss 1.4338922500610352\n",
      "val loss 1.5798752307891846\n",
      "______________\n",
      "epoch 83 train loss 1.4501852989196777\n",
      "val loss 1.5763683319091797\n",
      "______________\n",
      "epoch 84 train loss 1.4336509704589844\n",
      "val loss 1.5728826522827148\n",
      "______________\n",
      "epoch 85 train loss 1.416170358657837\n",
      "val loss 1.5694081783294678\n",
      "______________\n",
      "epoch 86 train loss 1.4132269620895386\n",
      "val loss 1.5659434795379639\n",
      "______________\n",
      "epoch 87 train loss 1.4157531261444092\n",
      "val loss 1.5625035762786865\n",
      "______________\n",
      "epoch 88 train loss 1.4262927770614624\n",
      "val loss 1.5590901374816895\n",
      "______________\n",
      "epoch 89 train loss 1.4101216793060303\n",
      "val loss 1.5556950569152832\n",
      "______________\n",
      "epoch 90 train loss 1.4010324478149414\n",
      "val loss 1.5523204803466797\n",
      "______________\n",
      "epoch 91 train loss 1.4029321670532227\n",
      "val loss 1.5489637851715088\n",
      "______________\n",
      "epoch 92 train loss 1.4094617366790771\n",
      "val loss 1.5456175804138184\n",
      "______________\n",
      "epoch 93 train loss 1.3845945596694946\n",
      "val loss 1.542283296585083\n",
      "______________\n",
      "epoch 94 train loss 1.3980870246887207\n",
      "val loss 1.5389652252197266\n",
      "______________\n",
      "epoch 95 train loss 1.3978108167648315\n",
      "val loss 1.53566312789917\n",
      "______________\n",
      "epoch 96 train loss 1.4008350372314453\n",
      "val loss 1.532395362854004\n",
      "______________\n",
      "epoch 97 train loss 1.3715401887893677\n",
      "val loss 1.5291423797607422\n",
      "______________\n",
      "epoch 98 train loss 1.3737847805023193\n",
      "val loss 1.5259151458740234\n",
      "______________\n",
      "epoch 99 train loss 1.3668537139892578\n",
      "val loss 1.522698163986206\n",
      "______________\n",
      "epoch 100 train loss 1.380353331565857\n",
      "val loss 1.519502878189087\n",
      "______________\n",
      "epoch 101 train loss 1.3575602769851685\n",
      "val loss 1.5163252353668213\n",
      "______________\n",
      "epoch 102 train loss 1.3423444032669067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 1.5131748914718628\n",
      "______________\n",
      "epoch 103 train loss 1.364267110824585\n",
      "val loss 1.5100288391113281\n",
      "______________\n",
      "epoch 104 train loss 1.3561034202575684\n",
      "val loss 1.506890058517456\n",
      "______________\n",
      "epoch 105 train loss 1.338455319404602\n",
      "val loss 1.503773808479309\n",
      "______________\n",
      "epoch 106 train loss 1.337105631828308\n",
      "val loss 1.500678539276123\n",
      "______________\n",
      "epoch 107 train loss 1.3370769023895264\n",
      "val loss 1.497603416442871\n",
      "______________\n",
      "epoch 108 train loss 1.343722939491272\n",
      "val loss 1.4945294857025146\n",
      "______________\n",
      "epoch 109 train loss 1.3152530193328857\n",
      "val loss 1.4914586544036865\n",
      "______________\n",
      "epoch 110 train loss 1.3143858909606934\n",
      "val loss 1.4883888959884644\n",
      "______________\n",
      "epoch 111 train loss 1.3281302452087402\n",
      "val loss 1.4853273630142212\n",
      "______________\n",
      "epoch 112 train loss 1.3213024139404297\n",
      "val loss 1.4822919368743896\n",
      "______________\n",
      "epoch 113 train loss 1.310210108757019\n",
      "val loss 1.4792516231536865\n",
      "______________\n",
      "epoch 114 train loss 1.2941986322402954\n",
      "val loss 1.476216435432434\n",
      "______________\n",
      "epoch 115 train loss 1.3130841255187988\n",
      "val loss 1.4731967449188232\n",
      "______________\n",
      "epoch 116 train loss 1.294319748878479\n",
      "val loss 1.470172643661499\n",
      "______________\n",
      "epoch 117 train loss 1.2753937244415283\n",
      "val loss 1.4671626091003418\n",
      "______________\n",
      "epoch 118 train loss 1.277247667312622\n",
      "val loss 1.4641462564468384\n",
      "______________\n",
      "epoch 119 train loss 1.288403868675232\n",
      "val loss 1.461130976676941\n",
      "______________\n",
      "epoch 120 train loss 1.2701977491378784\n",
      "val loss 1.4581165313720703\n",
      "______________\n",
      "epoch 121 train loss 1.274591326713562\n",
      "val loss 1.4551039934158325\n",
      "______________\n",
      "epoch 122 train loss 1.2593969106674194\n",
      "val loss 1.4521076679229736\n",
      "______________\n",
      "epoch 123 train loss 1.2669343948364258\n",
      "val loss 1.4491121768951416\n",
      "______________\n",
      "epoch 124 train loss 1.2693310976028442\n",
      "val loss 1.4461334943771362\n",
      "______________\n",
      "epoch 125 train loss 1.2602713108062744\n",
      "val loss 1.4431712627410889\n",
      "______________\n",
      "epoch 126 train loss 1.2499245405197144\n",
      "val loss 1.440220832824707\n",
      "______________\n",
      "epoch 127 train loss 1.247666835784912\n",
      "val loss 1.4372801780700684\n",
      "______________\n",
      "epoch 128 train loss 1.2556452751159668\n",
      "val loss 1.4343531131744385\n",
      "______________\n",
      "epoch 129 train loss 1.2563717365264893\n",
      "val loss 1.431435227394104\n",
      "______________\n",
      "epoch 130 train loss 1.252966284751892\n",
      "val loss 1.4285314083099365\n",
      "______________\n",
      "epoch 131 train loss 1.2455861568450928\n",
      "val loss 1.4256367683410645\n",
      "______________\n",
      "epoch 132 train loss 1.238594889640808\n",
      "val loss 1.422756552696228\n",
      "______________\n",
      "epoch 133 train loss 1.2374160289764404\n",
      "val loss 1.4198694229125977\n",
      "______________\n",
      "epoch 134 train loss 1.2175846099853516\n",
      "val loss 1.416991949081421\n",
      "______________\n",
      "epoch 135 train loss 1.2322301864624023\n",
      "val loss 1.4141311645507812\n",
      "______________\n",
      "epoch 136 train loss 1.2326831817626953\n",
      "val loss 1.4112615585327148\n",
      "______________\n",
      "epoch 137 train loss 1.2291207313537598\n",
      "val loss 1.408409595489502\n",
      "______________\n",
      "epoch 138 train loss 1.2112090587615967\n",
      "val loss 1.4055705070495605\n",
      "______________\n",
      "epoch 139 train loss 1.215916395187378\n",
      "val loss 1.402729868888855\n",
      "______________\n",
      "epoch 140 train loss 1.2054524421691895\n",
      "val loss 1.3998991250991821\n",
      "______________\n",
      "epoch 141 train loss 1.2189879417419434\n",
      "val loss 1.3970675468444824\n",
      "______________\n",
      "epoch 142 train loss 1.2156128883361816\n",
      "val loss 1.3942267894744873\n",
      "______________\n",
      "epoch 143 train loss 1.2118171453475952\n",
      "val loss 1.391379952430725\n",
      "______________\n",
      "epoch 144 train loss 1.1759395599365234\n",
      "val loss 1.388545036315918\n",
      "______________\n",
      "epoch 145 train loss 1.1876708269119263\n",
      "val loss 1.3857263326644897\n",
      "______________\n",
      "epoch 146 train loss 1.1811906099319458\n",
      "val loss 1.3829032182693481\n",
      "______________\n",
      "epoch 147 train loss 1.2021408081054688\n",
      "val loss 1.3800899982452393\n",
      "______________\n",
      "epoch 148 train loss 1.210569143295288\n",
      "val loss 1.3772802352905273\n",
      "______________\n",
      "epoch 149 train loss 1.1876356601715088\n",
      "val loss 1.374471664428711\n",
      "______________\n",
      "epoch 150 train loss 1.1739484071731567\n",
      "val loss 1.3716644048690796\n",
      "______________\n",
      "epoch 151 train loss 1.1663898229599\n",
      "val loss 1.3688502311706543\n",
      "______________\n",
      "epoch 152 train loss 1.1747262477874756\n",
      "val loss 1.366029977798462\n",
      "______________\n",
      "epoch 153 train loss 1.1778154373168945\n",
      "val loss 1.3631987571716309\n",
      "______________\n",
      "epoch 154 train loss 1.1690311431884766\n",
      "val loss 1.3603829145431519\n",
      "______________\n",
      "epoch 155 train loss 1.1748547554016113\n",
      "val loss 1.3575810194015503\n",
      "______________\n",
      "epoch 156 train loss 1.1625444889068604\n",
      "val loss 1.3547827005386353\n",
      "______________\n",
      "epoch 157 train loss 1.1449575424194336\n",
      "val loss 1.351990818977356\n",
      "______________\n",
      "epoch 158 train loss 1.1344972848892212\n",
      "val loss 1.3492062091827393\n",
      "______________\n",
      "epoch 159 train loss 1.1471540927886963\n",
      "val loss 1.3464187383651733\n",
      "______________\n",
      "epoch 160 train loss 1.1412416696548462\n",
      "val loss 1.3436279296875\n",
      "______________\n",
      "epoch 161 train loss 1.1310527324676514\n",
      "val loss 1.3408493995666504\n",
      "______________\n",
      "epoch 162 train loss 1.1229541301727295\n",
      "val loss 1.3380730152130127\n",
      "______________\n",
      "epoch 163 train loss 1.1426537036895752\n",
      "val loss 1.3352899551391602\n",
      "______________\n",
      "epoch 164 train loss 1.1345759630203247\n",
      "val loss 1.3325104713439941\n",
      "______________\n",
      "epoch 165 train loss 1.1013559103012085\n",
      "val loss 1.3297483921051025\n",
      "______________\n",
      "epoch 166 train loss 1.1275949478149414\n",
      "val loss 1.3269951343536377\n",
      "______________\n",
      "epoch 167 train loss 1.1058101654052734\n",
      "val loss 1.3242402076721191\n",
      "______________\n",
      "epoch 168 train loss 1.1273002624511719\n",
      "val loss 1.321476697921753\n",
      "______________\n",
      "epoch 169 train loss 1.122990608215332\n",
      "val loss 1.3187191486358643\n",
      "______________\n",
      "epoch 170 train loss 1.10783851146698\n",
      "val loss 1.3159525394439697\n",
      "______________\n",
      "epoch 171 train loss 1.1081774234771729\n",
      "val loss 1.3131933212280273\n",
      "______________\n",
      "epoch 172 train loss 1.1015937328338623\n",
      "val loss 1.310434103012085\n",
      "______________\n",
      "epoch 173 train loss 1.1053352355957031\n",
      "val loss 1.3076903820037842\n",
      "______________\n",
      "epoch 174 train loss 1.088526964187622\n",
      "val loss 1.3049519062042236\n",
      "______________\n",
      "epoch 175 train loss 1.077754259109497\n",
      "val loss 1.3021917343139648\n",
      "______________\n",
      "epoch 176 train loss 1.0951544046401978\n",
      "val loss 1.2994379997253418\n",
      "______________\n",
      "epoch 177 train loss 1.0784754753112793\n",
      "val loss 1.2966816425323486\n",
      "______________\n",
      "epoch 178 train loss 1.0891671180725098\n",
      "val loss 1.2939274311065674\n",
      "______________\n",
      "epoch 179 train loss 1.0913938283920288\n",
      "val loss 1.2911673784255981\n",
      "______________\n",
      "epoch 180 train loss 1.0630396604537964\n",
      "val loss 1.2884068489074707\n",
      "______________\n",
      "epoch 181 train loss 1.0889499187469482\n",
      "val loss 1.285658597946167\n",
      "______________\n",
      "epoch 182 train loss 1.0626400709152222\n",
      "val loss 1.282912254333496\n",
      "______________\n",
      "epoch 183 train loss 1.068673849105835\n",
      "val loss 1.280171275138855\n",
      "______________\n",
      "epoch 184 train loss 1.0626294612884521\n",
      "val loss 1.277431607246399\n",
      "______________\n",
      "epoch 185 train loss 1.0745620727539062\n",
      "val loss 1.2746962308883667\n",
      "______________\n",
      "epoch 186 train loss 1.0655977725982666\n",
      "val loss 1.2719625234603882\n",
      "______________\n",
      "epoch 187 train loss 1.0619145631790161\n",
      "val loss 1.269242763519287\n",
      "______________\n",
      "epoch 188 train loss 1.0720211267471313\n",
      "val loss 1.2665190696716309\n",
      "______________\n",
      "epoch 189 train loss 1.0568350553512573\n",
      "val loss 1.2637953758239746\n",
      "______________\n",
      "epoch 190 train loss 1.0502800941467285\n",
      "val loss 1.2610584497451782\n",
      "______________\n",
      "epoch 191 train loss 1.0428566932678223\n",
      "val loss 1.258339762687683\n",
      "______________\n",
      "epoch 192 train loss 1.0521703958511353\n",
      "val loss 1.2556301355361938\n",
      "______________\n",
      "epoch 193 train loss 1.0406131744384766\n",
      "val loss 1.2529222965240479\n",
      "______________\n",
      "epoch 194 train loss 1.032301664352417\n",
      "val loss 1.2502222061157227\n",
      "______________\n",
      "epoch 195 train loss 1.0482616424560547\n",
      "val loss 1.2475180625915527\n",
      "______________\n",
      "epoch 196 train loss 1.0404889583587646\n",
      "val loss 1.2447905540466309\n",
      "______________\n",
      "epoch 197 train loss 1.0188339948654175\n",
      "val loss 1.2420634031295776\n",
      "______________\n",
      "epoch 198 train loss 1.0318725109100342\n",
      "val loss 1.2393256425857544\n",
      "______________\n",
      "epoch 199 train loss 1.0244438648223877\n",
      "val loss 1.2365972995758057\n",
      "______________\n",
      "epoch 200 train loss 1.0095869302749634\n",
      "val loss 1.2338807582855225\n",
      "______________\n",
      "epoch 201 train loss 1.0079867839813232\n",
      "val loss 1.2311832904815674\n",
      "______________\n",
      "epoch 202 train loss 1.0042072534561157\n",
      "val loss 1.2285021543502808\n",
      "______________\n",
      "epoch 203 train loss 1.0129904747009277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 1.2258294820785522\n",
      "______________\n",
      "epoch 204 train loss 1.0298409461975098\n",
      "val loss 1.2231554985046387\n",
      "______________\n",
      "epoch 205 train loss 1.0041135549545288\n",
      "val loss 1.2204821109771729\n",
      "______________\n",
      "epoch 206 train loss 1.012751579284668\n",
      "val loss 1.2178139686584473\n",
      "______________\n",
      "epoch 207 train loss 0.9914513230323792\n",
      "val loss 1.2151459455490112\n",
      "______________\n",
      "epoch 208 train loss 0.9986729621887207\n",
      "val loss 1.2124624252319336\n",
      "______________\n",
      "epoch 209 train loss 0.9916490316390991\n",
      "val loss 1.2097591161727905\n",
      "______________\n",
      "epoch 210 train loss 0.9785895347595215\n",
      "val loss 1.207074761390686\n",
      "______________\n",
      "epoch 211 train loss 0.9941638708114624\n",
      "val loss 1.2043933868408203\n",
      "______________\n",
      "epoch 212 train loss 0.9614653587341309\n",
      "val loss 1.2017185688018799\n",
      "______________\n",
      "epoch 213 train loss 0.9748309254646301\n",
      "val loss 1.1990429162979126\n",
      "______________\n",
      "epoch 214 train loss 0.9763867855072021\n",
      "val loss 1.1963834762573242\n",
      "______________\n",
      "epoch 215 train loss 0.9727275371551514\n",
      "val loss 1.1937134265899658\n",
      "______________\n",
      "epoch 216 train loss 0.9690458178520203\n",
      "val loss 1.1910347938537598\n",
      "______________\n",
      "epoch 217 train loss 0.9636595249176025\n",
      "val loss 1.1883692741394043\n",
      "______________\n",
      "epoch 218 train loss 0.9523735642433167\n",
      "val loss 1.1856889724731445\n",
      "______________\n",
      "epoch 219 train loss 0.9792542457580566\n",
      "val loss 1.1830090284347534\n",
      "______________\n",
      "epoch 220 train loss 0.9589025974273682\n",
      "val loss 1.1803359985351562\n",
      "______________\n",
      "epoch 221 train loss 0.9436901211738586\n",
      "val loss 1.1776660680770874\n",
      "______________\n",
      "epoch 222 train loss 0.9613354206085205\n",
      "val loss 1.1749991178512573\n",
      "______________\n",
      "epoch 223 train loss 0.9485387802124023\n",
      "val loss 1.1723251342773438\n",
      "______________\n",
      "epoch 224 train loss 0.9463178515434265\n",
      "val loss 1.1696453094482422\n",
      "______________\n",
      "epoch 225 train loss 0.9453150033950806\n",
      "val loss 1.166982889175415\n",
      "______________\n",
      "epoch 226 train loss 0.9486669898033142\n",
      "val loss 1.1643198728561401\n",
      "______________\n",
      "epoch 227 train loss 0.9536815285682678\n",
      "val loss 1.1616718769073486\n",
      "______________\n",
      "epoch 228 train loss 0.9216392040252686\n",
      "val loss 1.1590486764907837\n",
      "______________\n",
      "epoch 229 train loss 0.936939001083374\n",
      "val loss 1.1564545631408691\n",
      "______________\n",
      "epoch 230 train loss 0.9447054862976074\n",
      "val loss 1.1538633108139038\n",
      "______________\n",
      "epoch 231 train loss 0.9352344870567322\n",
      "val loss 1.1512820720672607\n",
      "______________\n",
      "epoch 232 train loss 0.9272258877754211\n",
      "val loss 1.148710012435913\n",
      "______________\n",
      "epoch 233 train loss 0.9438384771347046\n",
      "val loss 1.1461340188980103\n",
      "______________\n",
      "epoch 234 train loss 0.9214624166488647\n",
      "val loss 1.143580675125122\n",
      "______________\n",
      "epoch 235 train loss 0.9321994781494141\n",
      "val loss 1.1409879922866821\n",
      "______________\n",
      "epoch 236 train loss 0.913200855255127\n",
      "val loss 1.1384104490280151\n",
      "______________\n",
      "epoch 237 train loss 0.9202430248260498\n",
      "val loss 1.1358551979064941\n",
      "______________\n",
      "epoch 238 train loss 0.9202840328216553\n",
      "val loss 1.1333014965057373\n",
      "______________\n",
      "epoch 239 train loss 0.9149991273880005\n",
      "val loss 1.1307331323623657\n",
      "______________\n",
      "epoch 240 train loss 0.9072579145431519\n",
      "val loss 1.1281718015670776\n",
      "______________\n",
      "epoch 241 train loss 0.9147293567657471\n",
      "val loss 1.1256076097488403\n",
      "______________\n",
      "epoch 242 train loss 0.8883140087127686\n",
      "val loss 1.123049020767212\n",
      "______________\n",
      "epoch 243 train loss 0.8931691646575928\n",
      "val loss 1.1204828023910522\n",
      "______________\n",
      "epoch 244 train loss 0.8853551149368286\n",
      "val loss 1.1179463863372803\n",
      "______________\n",
      "epoch 245 train loss 0.903057873249054\n",
      "val loss 1.1154136657714844\n",
      "______________\n",
      "epoch 246 train loss 0.889206051826477\n",
      "val loss 1.112891674041748\n",
      "______________\n",
      "epoch 247 train loss 0.8952211737632751\n",
      "val loss 1.1103752851486206\n",
      "______________\n",
      "epoch 248 train loss 0.8893243074417114\n",
      "val loss 1.107856273651123\n",
      "______________\n",
      "epoch 249 train loss 0.8863364458084106\n",
      "val loss 1.1053361892700195\n",
      "______________\n",
      "epoch 250 train loss 0.868084192276001\n",
      "val loss 1.102820634841919\n",
      "______________\n",
      "epoch 251 train loss 0.8904757499694824\n",
      "val loss 1.1003152132034302\n",
      "______________\n",
      "epoch 252 train loss 0.8757768869400024\n",
      "val loss 1.0978186130523682\n",
      "______________\n",
      "epoch 253 train loss 0.8725593090057373\n",
      "val loss 1.095336675643921\n",
      "______________\n",
      "epoch 254 train loss 0.860596239566803\n",
      "val loss 1.092858076095581\n",
      "______________\n",
      "epoch 255 train loss 0.8744341135025024\n",
      "val loss 1.0903786420822144\n",
      "______________\n",
      "epoch 256 train loss 0.8586020469665527\n",
      "val loss 1.087890625\n",
      "______________\n",
      "epoch 257 train loss 0.8369835019111633\n",
      "val loss 1.0854175090789795\n",
      "______________\n",
      "epoch 258 train loss 0.8774601817131042\n",
      "val loss 1.0829377174377441\n",
      "______________\n",
      "epoch 259 train loss 0.8611356019973755\n",
      "val loss 1.0804580450057983\n",
      "______________\n",
      "epoch 260 train loss 0.8654783964157104\n",
      "val loss 1.078000545501709\n",
      "______________\n",
      "epoch 261 train loss 0.8515604734420776\n",
      "val loss 1.0755671262741089\n",
      "______________\n",
      "epoch 262 train loss 0.8595260977745056\n",
      "val loss 1.073133945465088\n",
      "______________\n",
      "epoch 263 train loss 0.8711374998092651\n",
      "val loss 1.0707333087921143\n",
      "______________\n",
      "epoch 264 train loss 0.8614941835403442\n",
      "val loss 1.0683449506759644\n",
      "______________\n",
      "epoch 265 train loss 0.8465720415115356\n",
      "val loss 1.065950870513916\n",
      "______________\n",
      "epoch 266 train loss 0.837785005569458\n",
      "val loss 1.063576579093933\n",
      "______________\n",
      "epoch 267 train loss 0.8421672582626343\n",
      "val loss 1.0612118244171143\n",
      "______________\n",
      "epoch 268 train loss 0.8413733243942261\n",
      "val loss 1.058835506439209\n",
      "______________\n",
      "epoch 269 train loss 0.8367557525634766\n",
      "val loss 1.056467890739441\n",
      "______________\n",
      "epoch 270 train loss 0.8406171798706055\n",
      "val loss 1.0541139841079712\n",
      "______________\n",
      "epoch 271 train loss 0.8311486840248108\n",
      "val loss 1.0517582893371582\n",
      "______________\n",
      "epoch 272 train loss 0.8366013765335083\n",
      "val loss 1.049412727355957\n",
      "______________\n",
      "epoch 273 train loss 0.8239300847053528\n",
      "val loss 1.0470638275146484\n",
      "______________\n",
      "epoch 274 train loss 0.8240302801132202\n",
      "val loss 1.0447232723236084\n",
      "______________\n",
      "epoch 275 train loss 0.8191704750061035\n",
      "val loss 1.0423824787139893\n",
      "______________\n",
      "epoch 276 train loss 0.8122780323028564\n",
      "val loss 1.0400254726409912\n",
      "______________\n",
      "epoch 277 train loss 0.8005581498146057\n",
      "val loss 1.0376665592193604\n",
      "______________\n",
      "epoch 278 train loss 0.7971628904342651\n",
      "val loss 1.0353271961212158\n",
      "______________\n",
      "epoch 279 train loss 0.790037989616394\n",
      "val loss 1.0329921245574951\n",
      "______________\n",
      "epoch 280 train loss 0.8024179935455322\n",
      "val loss 1.0306532382965088\n",
      "______________\n",
      "epoch 281 train loss 0.8157621622085571\n",
      "val loss 1.0283399820327759\n",
      "______________\n",
      "epoch 282 train loss 0.7903667688369751\n",
      "val loss 1.0260220766067505\n",
      "______________\n",
      "epoch 283 train loss 0.8179377317428589\n",
      "val loss 1.0237056016921997\n",
      "______________\n",
      "epoch 284 train loss 0.796043872833252\n",
      "val loss 1.021389126777649\n",
      "______________\n",
      "epoch 285 train loss 0.8055919408798218\n",
      "val loss 1.0190722942352295\n",
      "______________\n",
      "epoch 286 train loss 0.7883540391921997\n",
      "val loss 1.0167596340179443\n",
      "______________\n",
      "epoch 287 train loss 0.7842116951942444\n",
      "val loss 1.0144522190093994\n",
      "______________\n",
      "epoch 288 train loss 0.7831254005432129\n",
      "val loss 1.0121545791625977\n",
      "______________\n",
      "epoch 289 train loss 0.7706325054168701\n",
      "val loss 1.0098791122436523\n",
      "______________\n",
      "epoch 290 train loss 0.7800389528274536\n",
      "val loss 1.007617473602295\n",
      "______________\n",
      "epoch 291 train loss 0.7536711096763611\n",
      "val loss 1.0053696632385254\n",
      "______________\n",
      "epoch 292 train loss 0.7850253582000732\n",
      "val loss 1.003119707107544\n",
      "______________\n",
      "epoch 293 train loss 0.7834066152572632\n",
      "val loss 1.00088369846344\n",
      "______________\n",
      "epoch 294 train loss 0.7586405277252197\n",
      "val loss 0.9986590147018433\n",
      "______________\n",
      "epoch 295 train loss 0.7721341848373413\n",
      "val loss 0.9964570999145508\n",
      "______________\n",
      "epoch 296 train loss 0.7652919888496399\n",
      "val loss 0.9942642450332642\n",
      "______________\n",
      "epoch 297 train loss 0.7665342092514038\n",
      "val loss 0.9920838475227356\n",
      "______________\n",
      "epoch 298 train loss 0.770180344581604\n",
      "val loss 0.9899171590805054\n",
      "______________\n",
      "epoch 299 train loss 0.7840836048126221\n",
      "val loss 0.9877512454986572\n",
      "______________\n",
      "epoch 300 train loss 0.7714970111846924\n",
      "val loss 0.985561728477478\n",
      "______________\n",
      "epoch 301 train loss 0.7757073640823364\n",
      "val loss 0.9833604097366333\n",
      "______________\n",
      "epoch 302 train loss 0.7484414577484131\n",
      "val loss 0.9811800122261047\n",
      "______________\n",
      "epoch 303 train loss 0.7418984174728394\n",
      "val loss 0.978996992111206\n",
      "______________\n",
      "epoch 304 train loss 0.7574591636657715\n",
      "val loss 0.9767943024635315\n",
      "______________\n",
      "epoch 305 train loss 0.7652265429496765\n",
      "val loss 0.9745900630950928\n",
      "______________\n",
      "epoch 306 train loss 0.7383860349655151\n",
      "val loss 0.9724035263061523\n",
      "______________\n",
      "epoch 307 train loss 0.75480055809021\n",
      "val loss 0.97022545337677\n",
      "______________\n",
      "epoch 308 train loss 0.7416211366653442\n",
      "val loss 0.9680664539337158\n",
      "______________\n",
      "epoch 309 train loss 0.7494204044342041\n",
      "val loss 0.9658886194229126\n",
      "______________\n",
      "epoch 310 train loss 0.7396785020828247\n",
      "val loss 0.9637095928192139\n",
      "______________\n",
      "epoch 311 train loss 0.7361172437667847\n",
      "val loss 0.9615408778190613\n",
      "______________\n",
      "epoch 312 train loss 0.7262918949127197\n",
      "val loss 0.959356963634491\n",
      "______________\n",
      "epoch 313 train loss 0.7301481366157532\n",
      "val loss 0.9571741819381714\n",
      "______________\n",
      "epoch 314 train loss 0.7356694936752319\n",
      "val loss 0.9549800157546997\n",
      "______________\n",
      "epoch 315 train loss 0.7415361404418945\n",
      "val loss 0.9527956247329712\n",
      "______________\n",
      "epoch 316 train loss 0.7116920351982117\n",
      "val loss 0.9506269693374634\n",
      "______________\n",
      "epoch 317 train loss 0.7351272702217102\n",
      "val loss 0.948462963104248\n",
      "______________\n",
      "epoch 318 train loss 0.7286393642425537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 0.9463107585906982\n",
      "______________\n",
      "epoch 319 train loss 0.7253338098526001\n",
      "val loss 0.9441831111907959\n",
      "______________\n",
      "epoch 320 train loss 0.7140083312988281\n",
      "val loss 0.9420498013496399\n",
      "______________\n",
      "epoch 321 train loss 0.7160630226135254\n",
      "val loss 0.9399226903915405\n",
      "______________\n",
      "epoch 322 train loss 0.7212742567062378\n",
      "val loss 0.9377880096435547\n",
      "______________\n",
      "epoch 323 train loss 0.7199103236198425\n",
      "val loss 0.9356637001037598\n",
      "______________\n",
      "epoch 324 train loss 0.7109595537185669\n",
      "val loss 0.9335514307022095\n",
      "______________\n",
      "epoch 325 train loss 0.7064943313598633\n",
      "val loss 0.9314417839050293\n",
      "______________\n",
      "epoch 326 train loss 0.7031735181808472\n",
      "val loss 0.9293434619903564\n",
      "______________\n",
      "epoch 327 train loss 0.7207792401313782\n",
      "val loss 0.9272761344909668\n",
      "______________\n",
      "epoch 328 train loss 0.7099193334579468\n",
      "val loss 0.9252252578735352\n",
      "______________\n",
      "epoch 329 train loss 0.7070194482803345\n",
      "val loss 0.9231759309768677\n",
      "______________\n",
      "epoch 330 train loss 0.693734347820282\n",
      "val loss 0.9211431741714478\n",
      "______________\n",
      "epoch 331 train loss 0.7180927991867065\n",
      "val loss 0.9191088676452637\n",
      "______________\n",
      "epoch 332 train loss 0.6921467781066895\n",
      "val loss 0.9170920252799988\n",
      "______________\n",
      "epoch 333 train loss 0.6907457113265991\n",
      "val loss 0.9150611162185669\n",
      "______________\n",
      "epoch 334 train loss 0.7178037166595459\n",
      "val loss 0.9130381345748901\n",
      "______________\n",
      "epoch 335 train loss 0.6947550177574158\n",
      "val loss 0.9110186696052551\n",
      "______________\n",
      "epoch 336 train loss 0.6826398968696594\n",
      "val loss 0.9090002775192261\n",
      "______________\n",
      "epoch 337 train loss 0.6857718229293823\n",
      "val loss 0.906996488571167\n",
      "______________\n",
      "epoch 338 train loss 0.6956566572189331\n",
      "val loss 0.9049967527389526\n",
      "______________\n",
      "epoch 339 train loss 0.6953269243240356\n",
      "val loss 0.9030075073242188\n",
      "______________\n",
      "epoch 340 train loss 0.6842237114906311\n",
      "val loss 0.9010069370269775\n",
      "______________\n",
      "epoch 341 train loss 0.676145613193512\n",
      "val loss 0.8989942073822021\n",
      "______________\n",
      "epoch 342 train loss 0.6641316413879395\n",
      "val loss 0.8969898223876953\n",
      "______________\n",
      "epoch 343 train loss 0.6954824328422546\n",
      "val loss 0.8949788808822632\n",
      "______________\n",
      "epoch 344 train loss 0.684937596321106\n",
      "val loss 0.8929497003555298\n",
      "______________\n",
      "epoch 345 train loss 0.6635361313819885\n",
      "val loss 0.8909119367599487\n",
      "______________\n",
      "epoch 346 train loss 0.6816421747207642\n",
      "val loss 0.8888808488845825\n",
      "______________\n",
      "epoch 347 train loss 0.6619252562522888\n",
      "val loss 0.8868524432182312\n",
      "______________\n",
      "epoch 348 train loss 0.691409707069397\n",
      "val loss 0.8848234415054321\n",
      "______________\n",
      "epoch 349 train loss 0.6537570953369141\n",
      "val loss 0.8828121423721313\n",
      "______________\n",
      "epoch 350 train loss 0.665212869644165\n",
      "val loss 0.8808152079582214\n",
      "______________\n",
      "epoch 351 train loss 0.6622722148895264\n",
      "val loss 0.8788210153579712\n",
      "______________\n",
      "epoch 352 train loss 0.6682109832763672\n",
      "val loss 0.876824140548706\n",
      "______________\n",
      "epoch 353 train loss 0.6662024259567261\n",
      "val loss 0.8748482465744019\n",
      "______________\n",
      "epoch 354 train loss 0.663933515548706\n",
      "val loss 0.8728781938552856\n",
      "______________\n",
      "epoch 355 train loss 0.657977819442749\n",
      "val loss 0.8709091544151306\n",
      "______________\n",
      "epoch 356 train loss 0.6522406339645386\n",
      "val loss 0.8689658641815186\n",
      "______________\n",
      "epoch 357 train loss 0.6488816142082214\n",
      "val loss 0.8670233488082886\n",
      "______________\n",
      "epoch 358 train loss 0.6542880535125732\n",
      "val loss 0.8650791645050049\n",
      "______________\n",
      "epoch 359 train loss 0.6399892568588257\n",
      "val loss 0.863150954246521\n",
      "______________\n",
      "epoch 360 train loss 0.6255210638046265\n",
      "val loss 0.8612531423568726\n",
      "______________\n",
      "epoch 361 train loss 0.654274582862854\n",
      "val loss 0.8593647480010986\n",
      "______________\n",
      "epoch 362 train loss 0.6323617696762085\n",
      "val loss 0.857507586479187\n",
      "______________\n",
      "epoch 363 train loss 0.6293531060218811\n",
      "val loss 0.8556458950042725\n",
      "______________\n",
      "epoch 364 train loss 0.6648669242858887\n",
      "val loss 0.8537747859954834\n",
      "______________\n",
      "epoch 365 train loss 0.6163512468338013\n",
      "val loss 0.8518928289413452\n",
      "______________\n",
      "epoch 366 train loss 0.6352181434631348\n",
      "val loss 0.850031852722168\n",
      "______________\n",
      "epoch 367 train loss 0.6353982090950012\n",
      "val loss 0.8481945991516113\n",
      "______________\n",
      "epoch 368 train loss 0.6472398042678833\n",
      "val loss 0.8463644981384277\n",
      "______________\n",
      "epoch 369 train loss 0.6189171671867371\n",
      "val loss 0.8445374965667725\n",
      "______________\n",
      "epoch 370 train loss 0.6340488195419312\n",
      "val loss 0.8427037596702576\n",
      "______________\n",
      "epoch 371 train loss 0.6218973994255066\n",
      "val loss 0.840859055519104\n",
      "______________\n",
      "epoch 372 train loss 0.6373727321624756\n",
      "val loss 0.8390401601791382\n",
      "______________\n",
      "epoch 373 train loss 0.6135659217834473\n",
      "val loss 0.8372005224227905\n",
      "______________\n",
      "epoch 374 train loss 0.6220364570617676\n",
      "val loss 0.8353824615478516\n",
      "______________\n",
      "epoch 375 train loss 0.6182442307472229\n",
      "val loss 0.8335829973220825\n",
      "______________\n",
      "epoch 376 train loss 0.6323584318161011\n",
      "val loss 0.8317937254905701\n",
      "______________\n",
      "epoch 377 train loss 0.5930159091949463\n",
      "val loss 0.8300080299377441\n",
      "______________\n",
      "epoch 378 train loss 0.6034345626831055\n",
      "val loss 0.82823646068573\n",
      "______________\n",
      "epoch 379 train loss 0.6095109581947327\n",
      "val loss 0.8264385461807251\n",
      "______________\n",
      "epoch 380 train loss 0.5949931740760803\n",
      "val loss 0.8246463537216187\n",
      "______________\n",
      "epoch 381 train loss 0.6052333116531372\n",
      "val loss 0.8228629231452942\n",
      "______________\n",
      "epoch 382 train loss 0.5904867053031921\n",
      "val loss 0.821068525314331\n",
      "______________\n",
      "epoch 383 train loss 0.6128109693527222\n",
      "val loss 0.8192934989929199\n",
      "______________\n",
      "epoch 384 train loss 0.5911062359809875\n",
      "val loss 0.8175317645072937\n",
      "______________\n",
      "epoch 385 train loss 0.621134877204895\n",
      "val loss 0.8157726526260376\n",
      "______________\n",
      "epoch 386 train loss 0.5896713733673096\n",
      "val loss 0.8140233755111694\n",
      "______________\n",
      "epoch 387 train loss 0.5920603275299072\n",
      "val loss 0.8122763633728027\n",
      "______________\n",
      "epoch 388 train loss 0.5815640687942505\n",
      "val loss 0.8105384111404419\n",
      "______________\n",
      "epoch 389 train loss 0.5999001860618591\n",
      "val loss 0.8088219165802002\n",
      "______________\n",
      "epoch 390 train loss 0.591925323009491\n",
      "val loss 0.8071116209030151\n",
      "______________\n",
      "epoch 391 train loss 0.5787799954414368\n",
      "val loss 0.8053925633430481\n",
      "______________\n",
      "epoch 392 train loss 0.5996606349945068\n",
      "val loss 0.8036892414093018\n",
      "______________\n",
      "epoch 393 train loss 0.5915882587432861\n",
      "val loss 0.8019819855690002\n",
      "______________\n",
      "epoch 394 train loss 0.5875228047370911\n",
      "val loss 0.8002629280090332\n",
      "______________\n",
      "epoch 395 train loss 0.6060795783996582\n",
      "val loss 0.7985464334487915\n",
      "______________\n",
      "epoch 396 train loss 0.5790486335754395\n",
      "val loss 0.7968387603759766\n",
      "______________\n",
      "epoch 397 train loss 0.5624262690544128\n",
      "val loss 0.7951229810714722\n",
      "______________\n",
      "epoch 398 train loss 0.5724354386329651\n",
      "val loss 0.7934510707855225\n",
      "______________\n",
      "epoch 399 train loss 0.5807921886444092\n",
      "val loss 0.7918151617050171\n",
      "______________\n",
      "epoch 400 train loss 0.5722586512565613\n",
      "val loss 0.7901821136474609\n",
      "______________\n",
      "epoch 401 train loss 0.5705960988998413\n",
      "val loss 0.7885499596595764\n",
      "______________\n",
      "epoch 402 train loss 0.5889877676963806\n",
      "val loss 0.7869238257408142\n",
      "______________\n",
      "epoch 403 train loss 0.5714374780654907\n",
      "val loss 0.785325288772583\n",
      "______________\n",
      "epoch 404 train loss 0.5416695475578308\n",
      "val loss 0.7837066650390625\n",
      "______________\n",
      "epoch 405 train loss 0.5720546245574951\n",
      "val loss 0.7821056842803955\n",
      "______________\n",
      "epoch 406 train loss 0.5483731031417847\n",
      "val loss 0.780489444732666\n",
      "______________\n",
      "epoch 407 train loss 0.5892025232315063\n",
      "val loss 0.7788677215576172\n",
      "______________\n",
      "epoch 408 train loss 0.5647495985031128\n",
      "val loss 0.777274489402771\n",
      "______________\n",
      "epoch 409 train loss 0.5584791898727417\n",
      "val loss 0.7756940126419067\n",
      "______________\n",
      "epoch 410 train loss 0.5702370405197144\n",
      "val loss 0.7741004228591919\n",
      "______________\n",
      "epoch 411 train loss 0.5591208934783936\n",
      "val loss 0.7724997997283936\n",
      "______________\n",
      "epoch 412 train loss 0.5381545424461365\n",
      "val loss 0.7708847522735596\n",
      "______________\n",
      "epoch 413 train loss 0.5663703680038452\n",
      "val loss 0.7692770957946777\n",
      "______________\n",
      "epoch 414 train loss 0.5470842123031616\n",
      "val loss 0.767686128616333\n",
      "______________\n",
      "epoch 415 train loss 0.5612988471984863\n",
      "val loss 0.7660893201828003\n",
      "______________\n",
      "epoch 416 train loss 0.5458322167396545\n",
      "val loss 0.7645173072814941\n",
      "______________\n",
      "epoch 417 train loss 0.5516018271446228\n",
      "val loss 0.7629406452178955\n",
      "______________\n",
      "epoch 418 train loss 0.5434095859527588\n",
      "val loss 0.7613691091537476\n",
      "______________\n",
      "epoch 419 train loss 0.5737938284873962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 0.7597965002059937\n",
      "______________\n",
      "epoch 420 train loss 0.5548861026763916\n",
      "val loss 0.7582204341888428\n",
      "______________\n",
      "epoch 421 train loss 0.560340166091919\n",
      "val loss 0.7566412687301636\n",
      "______________\n",
      "epoch 422 train loss 0.5415385961532593\n",
      "val loss 0.7550570964813232\n",
      "______________\n",
      "epoch 423 train loss 0.5503355264663696\n",
      "val loss 0.7534670233726501\n",
      "______________\n",
      "epoch 424 train loss 0.5547986030578613\n",
      "val loss 0.7518699169158936\n",
      "______________\n",
      "epoch 425 train loss 0.5347933769226074\n",
      "val loss 0.7502496242523193\n",
      "______________\n",
      "epoch 426 train loss 0.5352985858917236\n",
      "val loss 0.7486287355422974\n",
      "______________\n",
      "epoch 427 train loss 0.5358232259750366\n",
      "val loss 0.7470083236694336\n",
      "______________\n",
      "epoch 428 train loss 0.5567042231559753\n",
      "val loss 0.7453973293304443\n",
      "______________\n",
      "epoch 429 train loss 0.5340100526809692\n",
      "val loss 0.7438057661056519\n",
      "______________\n",
      "epoch 430 train loss 0.5202455520629883\n",
      "val loss 0.7422386407852173\n",
      "______________\n",
      "epoch 431 train loss 0.5394269824028015\n",
      "val loss 0.7406763434410095\n",
      "______________\n",
      "epoch 432 train loss 0.5370583534240723\n",
      "val loss 0.739111065864563\n",
      "______________\n",
      "epoch 433 train loss 0.5424150228500366\n",
      "val loss 0.7375468015670776\n",
      "______________\n",
      "epoch 434 train loss 0.5307945609092712\n",
      "val loss 0.7359811663627625\n",
      "______________\n",
      "epoch 435 train loss 0.5269495248794556\n",
      "val loss 0.7343965768814087\n",
      "______________\n",
      "epoch 436 train loss 0.5420024991035461\n",
      "val loss 0.7328478097915649\n",
      "______________\n",
      "epoch 437 train loss 0.5184898972511292\n",
      "val loss 0.7313165664672852\n",
      "______________\n",
      "epoch 438 train loss 0.5068071484565735\n",
      "val loss 0.7298033833503723\n",
      "______________\n",
      "epoch 439 train loss 0.5317053198814392\n",
      "val loss 0.7283098697662354\n",
      "______________\n",
      "epoch 440 train loss 0.4949876070022583\n",
      "val loss 0.7268192768096924\n",
      "______________\n",
      "epoch 441 train loss 0.5152989625930786\n",
      "val loss 0.7253345251083374\n",
      "______________\n",
      "epoch 442 train loss 0.5177673697471619\n",
      "val loss 0.7238446474075317\n",
      "______________\n",
      "epoch 443 train loss 0.5011364817619324\n",
      "val loss 0.722321093082428\n",
      "______________\n",
      "epoch 444 train loss 0.5067934393882751\n",
      "val loss 0.7207702994346619\n",
      "______________\n",
      "epoch 445 train loss 0.5111393332481384\n",
      "val loss 0.7192371487617493\n",
      "______________\n",
      "epoch 446 train loss 0.5053790807723999\n",
      "val loss 0.7177234888076782\n",
      "______________\n",
      "epoch 447 train loss 0.5220910906791687\n",
      "val loss 0.716215968132019\n",
      "______________\n",
      "epoch 448 train loss 0.49513429403305054\n",
      "val loss 0.7147486209869385\n",
      "______________\n",
      "epoch 449 train loss 0.5083458423614502\n",
      "val loss 0.7132865190505981\n",
      "______________\n",
      "epoch 450 train loss 0.5080892443656921\n",
      "val loss 0.7118223905563354\n",
      "______________\n",
      "epoch 451 train loss 0.504306435585022\n",
      "val loss 0.7103484869003296\n",
      "______________\n",
      "epoch 452 train loss 0.5076010227203369\n",
      "val loss 0.7088806629180908\n",
      "______________\n",
      "epoch 453 train loss 0.5178629159927368\n",
      "val loss 0.7074460983276367\n",
      "______________\n",
      "epoch 454 train loss 0.4881863594055176\n",
      "val loss 0.7060021162033081\n",
      "______________\n",
      "epoch 455 train loss 0.5089917778968811\n",
      "val loss 0.704537570476532\n",
      "______________\n",
      "epoch 456 train loss 0.49111735820770264\n",
      "val loss 0.703109860420227\n",
      "______________\n",
      "epoch 457 train loss 0.48518556356430054\n",
      "val loss 0.701656699180603\n",
      "______________\n",
      "epoch 458 train loss 0.4876280426979065\n",
      "val loss 0.7001991868019104\n",
      "______________\n",
      "epoch 459 train loss 0.515085756778717\n",
      "val loss 0.6987496614456177\n",
      "______________\n",
      "epoch 460 train loss 0.4814195930957794\n",
      "val loss 0.6973012685775757\n",
      "______________\n",
      "epoch 461 train loss 0.49785953760147095\n",
      "val loss 0.6958679556846619\n",
      "______________\n",
      "epoch 462 train loss 0.49398496747016907\n",
      "val loss 0.6944326162338257\n",
      "______________\n",
      "epoch 463 train loss 0.48553383350372314\n",
      "val loss 0.6930043697357178\n",
      "______________\n",
      "epoch 464 train loss 0.4986325204372406\n",
      "val loss 0.6915696859359741\n",
      "______________\n",
      "epoch 465 train loss 0.48606112599372864\n",
      "val loss 0.6901345252990723\n",
      "______________\n",
      "epoch 466 train loss 0.466698557138443\n",
      "val loss 0.6887277364730835\n",
      "______________\n",
      "epoch 467 train loss 0.48969563841819763\n",
      "val loss 0.6873289346694946\n",
      "______________\n",
      "epoch 468 train loss 0.4972333312034607\n",
      "val loss 0.6859073042869568\n",
      "______________\n",
      "epoch 469 train loss 0.47974634170532227\n",
      "val loss 0.684504508972168\n",
      "______________\n",
      "epoch 470 train loss 0.4769923985004425\n",
      "val loss 0.683111310005188\n",
      "______________\n",
      "epoch 471 train loss 0.47089684009552\n",
      "val loss 0.6817432045936584\n",
      "______________\n",
      "epoch 472 train loss 0.47959309816360474\n",
      "val loss 0.6804032921791077\n",
      "______________\n",
      "epoch 473 train loss 0.4780397415161133\n",
      "val loss 0.6790568828582764\n",
      "______________\n",
      "epoch 474 train loss 0.47376012802124023\n",
      "val loss 0.677722692489624\n",
      "______________\n",
      "epoch 475 train loss 0.4847562313079834\n",
      "val loss 0.6764101982116699\n",
      "______________\n",
      "epoch 476 train loss 0.4831315875053406\n",
      "val loss 0.6751033067703247\n",
      "______________\n",
      "epoch 477 train loss 0.48118388652801514\n",
      "val loss 0.6738014221191406\n",
      "______________\n",
      "epoch 478 train loss 0.4833380877971649\n",
      "val loss 0.672498345375061\n",
      "______________\n",
      "epoch 479 train loss 0.4831104874610901\n",
      "val loss 0.6712111234664917\n",
      "______________\n",
      "epoch 480 train loss 0.4636974334716797\n",
      "val loss 0.6699135899543762\n",
      "______________\n",
      "epoch 481 train loss 0.4598560929298401\n",
      "val loss 0.6686252355575562\n",
      "______________\n",
      "epoch 482 train loss 0.47512680292129517\n",
      "val loss 0.6673334836959839\n",
      "______________\n",
      "epoch 483 train loss 0.4716528058052063\n",
      "val loss 0.6660295128822327\n",
      "______________\n",
      "epoch 484 train loss 0.4795048236846924\n",
      "val loss 0.6647093296051025\n",
      "______________\n",
      "epoch 485 train loss 0.5021121501922607\n",
      "val loss 0.6633790731430054\n",
      "______________\n",
      "epoch 486 train loss 0.456643283367157\n",
      "val loss 0.6620635390281677\n",
      "______________\n",
      "epoch 487 train loss 0.47120338678359985\n",
      "val loss 0.6607306003570557\n",
      "______________\n",
      "epoch 488 train loss 0.4839338958263397\n",
      "val loss 0.6594152450561523\n",
      "______________\n",
      "epoch 489 train loss 0.4586690664291382\n",
      "val loss 0.658098578453064\n",
      "______________\n",
      "epoch 490 train loss 0.46330752968788147\n",
      "val loss 0.6567926406860352\n",
      "______________\n",
      "epoch 491 train loss 0.46018004417419434\n",
      "val loss 0.655454158782959\n",
      "______________\n",
      "epoch 492 train loss 0.4579562246799469\n",
      "val loss 0.6541479229927063\n",
      "______________\n",
      "epoch 493 train loss 0.4734373688697815\n",
      "val loss 0.6528733968734741\n",
      "______________\n",
      "epoch 494 train loss 0.4368753433227539\n",
      "val loss 0.6516051292419434\n",
      "______________\n",
      "epoch 495 train loss 0.42845627665519714\n",
      "val loss 0.650344729423523\n",
      "______________\n",
      "epoch 496 train loss 0.4522935152053833\n",
      "val loss 0.649067759513855\n",
      "______________\n",
      "epoch 497 train loss 0.45387911796569824\n",
      "val loss 0.6477844715118408\n",
      "______________\n",
      "epoch 498 train loss 0.4443908929824829\n",
      "val loss 0.6465184688568115\n",
      "______________\n",
      "epoch 499 train loss 0.4618787169456482\n",
      "val loss 0.6452344655990601\n",
      "______________\n",
      "epoch 500 train loss 0.4510316252708435\n",
      "val loss 0.6439211964607239\n",
      "______________\n",
      "epoch 501 train loss 0.4491623342037201\n",
      "val loss 0.6426173448562622\n",
      "______________\n",
      "epoch 502 train loss 0.4361291229724884\n",
      "val loss 0.6412934064865112\n",
      "______________\n",
      "epoch 503 train loss 0.4281798005104065\n",
      "val loss 0.6399827003479004\n",
      "______________\n",
      "epoch 504 train loss 0.43691378831863403\n",
      "val loss 0.6386955380439758\n",
      "______________\n",
      "epoch 505 train loss 0.4183701276779175\n",
      "val loss 0.6374407410621643\n",
      "______________\n",
      "epoch 506 train loss 0.46869397163391113\n",
      "val loss 0.6361827850341797\n",
      "______________\n",
      "epoch 507 train loss 0.43708813190460205\n",
      "val loss 0.6349484920501709\n",
      "______________\n",
      "epoch 508 train loss 0.4360666573047638\n",
      "val loss 0.6337243318557739\n",
      "______________\n",
      "epoch 509 train loss 0.45575881004333496\n",
      "val loss 0.6324939727783203\n",
      "______________\n",
      "epoch 510 train loss 0.4334275722503662\n",
      "val loss 0.6312808990478516\n",
      "______________\n",
      "epoch 511 train loss 0.4427158832550049\n",
      "val loss 0.6300665736198425\n",
      "______________\n",
      "epoch 512 train loss 0.4138863980770111\n",
      "val loss 0.6288418769836426\n",
      "______________\n",
      "epoch 513 train loss 0.41067731380462646\n",
      "val loss 0.6276193857192993\n",
      "______________\n",
      "epoch 514 train loss 0.42077380418777466\n",
      "val loss 0.6263935565948486\n",
      "______________\n",
      "epoch 515 train loss 0.4523885250091553\n",
      "val loss 0.6251667141914368\n",
      "______________\n",
      "epoch 516 train loss 0.4267837405204773\n",
      "val loss 0.6239393949508667\n",
      "______________\n",
      "epoch 517 train loss 0.43970951437950134\n",
      "val loss 0.6226975917816162\n",
      "______________\n",
      "epoch 518 train loss 0.42403069138526917\n",
      "val loss 0.6214680671691895\n",
      "______________\n",
      "epoch 519 train loss 0.4283754229545593\n",
      "val loss 0.6202279925346375\n",
      "______________\n",
      "epoch 520 train loss 0.4314335584640503\n",
      "val loss 0.6189749240875244\n",
      "______________\n",
      "epoch 521 train loss 0.42474260926246643\n",
      "val loss 0.6177495121955872\n",
      "______________\n",
      "epoch 522 train loss 0.45706889033317566\n",
      "val loss 0.616542398929596\n",
      "______________\n",
      "epoch 523 train loss 0.4427781403064728\n",
      "val loss 0.6153218746185303\n",
      "______________\n",
      "epoch 524 train loss 0.42408835887908936\n",
      "val loss 0.6141262054443359\n",
      "______________\n",
      "epoch 525 train loss 0.4361898899078369\n",
      "val loss 0.6129321455955505\n",
      "______________\n",
      "epoch 526 train loss 0.3968047797679901\n",
      "val loss 0.6117286682128906\n",
      "______________\n",
      "epoch 527 train loss 0.40159717202186584\n",
      "val loss 0.6105859279632568\n",
      "______________\n",
      "epoch 528 train loss 0.4271606504917145\n",
      "val loss 0.6094487309455872\n",
      "______________\n",
      "epoch 529 train loss 0.40997856855392456\n",
      "val loss 0.6083096265792847\n",
      "______________\n",
      "epoch 530 train loss 0.41556984186172485\n",
      "val loss 0.6071755290031433\n",
      "______________\n",
      "epoch 531 train loss 0.42233431339263916\n",
      "val loss 0.6060540676116943\n",
      "______________\n",
      "epoch 532 train loss 0.402587890625\n",
      "val loss 0.6049457788467407\n",
      "______________\n",
      "epoch 533 train loss 0.42320817708969116\n",
      "val loss 0.6038386821746826\n",
      "______________\n",
      "epoch 534 train loss 0.41217637062072754\n",
      "val loss 0.6027560234069824\n",
      "______________\n",
      "epoch 535 train loss 0.40581852197647095\n",
      "val loss 0.6016701459884644\n",
      "______________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 536 train loss 0.39350858330726624\n",
      "val loss 0.6005626916885376\n",
      "______________\n",
      "epoch 537 train loss 0.41144222021102905\n",
      "val loss 0.5994390249252319\n",
      "______________\n",
      "epoch 538 train loss 0.39379754662513733\n",
      "val loss 0.5983330011367798\n",
      "______________\n",
      "epoch 539 train loss 0.41423487663269043\n",
      "val loss 0.5972167253494263\n",
      "______________\n",
      "epoch 540 train loss 0.4106537699699402\n",
      "val loss 0.5961236953735352\n",
      "______________\n",
      "epoch 541 train loss 0.3880569338798523\n",
      "val loss 0.5950787663459778\n",
      "______________\n",
      "epoch 542 train loss 0.3963806629180908\n",
      "val loss 0.5940229296684265\n",
      "______________\n",
      "epoch 543 train loss 0.398185133934021\n",
      "val loss 0.5929632186889648\n",
      "______________\n",
      "epoch 544 train loss 0.40398305654525757\n",
      "val loss 0.591899037361145\n",
      "______________\n",
      "epoch 545 train loss 0.39435628056526184\n",
      "val loss 0.5908199548721313\n",
      "______________\n",
      "epoch 546 train loss 0.4140913486480713\n",
      "val loss 0.5897439122200012\n",
      "______________\n",
      "epoch 547 train loss 0.41953590512275696\n",
      "val loss 0.5886635780334473\n",
      "______________\n",
      "epoch 548 train loss 0.4099142849445343\n",
      "val loss 0.5875733494758606\n",
      "______________\n",
      "epoch 549 train loss 0.3992268145084381\n",
      "val loss 0.586453914642334\n",
      "______________\n",
      "epoch 550 train loss 0.40165799856185913\n",
      "val loss 0.5853244066238403\n",
      "______________\n",
      "epoch 551 train loss 0.3842006325721741\n",
      "val loss 0.5841975808143616\n",
      "______________\n",
      "epoch 552 train loss 0.3860129117965698\n",
      "val loss 0.5830612182617188\n",
      "______________\n",
      "epoch 553 train loss 0.3818816542625427\n",
      "val loss 0.5819419622421265\n",
      "______________\n",
      "epoch 554 train loss 0.36942797899246216\n",
      "val loss 0.5808296203613281\n",
      "______________\n",
      "epoch 555 train loss 0.39520204067230225\n",
      "val loss 0.5797274112701416\n",
      "______________\n",
      "epoch 556 train loss 0.4004538655281067\n",
      "val loss 0.578654408454895\n",
      "______________\n",
      "epoch 557 train loss 0.3880988359451294\n",
      "val loss 0.5775821208953857\n",
      "______________\n",
      "epoch 558 train loss 0.3776214122772217\n",
      "val loss 0.576499342918396\n",
      "______________\n",
      "epoch 559 train loss 0.397087424993515\n",
      "val loss 0.5754009485244751\n",
      "______________\n",
      "epoch 560 train loss 0.4046909213066101\n",
      "val loss 0.5743011236190796\n",
      "______________\n",
      "epoch 561 train loss 0.3770330548286438\n",
      "val loss 0.5732245445251465\n",
      "______________\n",
      "epoch 562 train loss 0.38017624616622925\n",
      "val loss 0.572161853313446\n",
      "______________\n",
      "epoch 563 train loss 0.3693612515926361\n",
      "val loss 0.5711126923561096\n",
      "______________\n",
      "epoch 564 train loss 0.37709134817123413\n",
      "val loss 0.5700672268867493\n",
      "______________\n",
      "epoch 565 train loss 0.38796406984329224\n",
      "val loss 0.5690449476242065\n",
      "______________\n",
      "epoch 566 train loss 0.3692343831062317\n",
      "val loss 0.5680338740348816\n",
      "______________\n",
      "epoch 567 train loss 0.3441055417060852\n",
      "val loss 0.5670279860496521\n",
      "______________\n",
      "epoch 568 train loss 0.38443523645401\n",
      "val loss 0.5660288333892822\n",
      "______________\n",
      "epoch 569 train loss 0.38330599665641785\n",
      "val loss 0.5650220513343811\n",
      "______________\n",
      "epoch 570 train loss 0.37090128660202026\n",
      "val loss 0.5639818906784058\n",
      "______________\n",
      "epoch 571 train loss 0.374358206987381\n",
      "val loss 0.5629700422286987\n",
      "______________\n",
      "epoch 572 train loss 0.353765070438385\n",
      "val loss 0.5619759559631348\n",
      "______________\n",
      "epoch 573 train loss 0.3835793137550354\n",
      "val loss 0.5609707832336426\n",
      "______________\n",
      "epoch 574 train loss 0.36277124285697937\n",
      "val loss 0.5599570870399475\n",
      "______________\n",
      "epoch 575 train loss 0.3712161183357239\n",
      "val loss 0.5589345693588257\n",
      "______________\n",
      "epoch 576 train loss 0.3791317939758301\n",
      "val loss 0.5579208731651306\n",
      "______________\n",
      "epoch 577 train loss 0.34927600622177124\n",
      "val loss 0.5569266080856323\n",
      "______________\n",
      "epoch 578 train loss 0.3587074875831604\n",
      "val loss 0.5559331774711609\n",
      "______________\n",
      "epoch 579 train loss 0.36917150020599365\n",
      "val loss 0.5549160838127136\n",
      "______________\n",
      "epoch 580 train loss 0.37717920541763306\n",
      "val loss 0.5538936853408813\n",
      "______________\n",
      "epoch 581 train loss 0.3719339966773987\n",
      "val loss 0.5528770089149475\n",
      "______________\n",
      "epoch 582 train loss 0.3588300347328186\n",
      "val loss 0.5518655776977539\n",
      "______________\n",
      "epoch 583 train loss 0.34164923429489136\n",
      "val loss 0.55086350440979\n",
      "______________\n",
      "epoch 584 train loss 0.35803908109664917\n",
      "val loss 0.5498542785644531\n",
      "______________\n",
      "epoch 585 train loss 0.3658260703086853\n",
      "val loss 0.5488367080688477\n",
      "______________\n",
      "epoch 586 train loss 0.3538101315498352\n",
      "val loss 0.5478290319442749\n",
      "______________\n",
      "epoch 587 train loss 0.3554028570652008\n",
      "val loss 0.5468525886535645\n",
      "______________\n",
      "epoch 588 train loss 0.3462946116924286\n",
      "val loss 0.5458683967590332\n",
      "______________\n",
      "epoch 589 train loss 0.35721415281295776\n",
      "val loss 0.5448830723762512\n",
      "______________\n",
      "epoch 590 train loss 0.39201968908309937\n",
      "val loss 0.5438809990882874\n",
      "______________\n",
      "epoch 591 train loss 0.3636464774608612\n",
      "val loss 0.5428813695907593\n",
      "______________\n",
      "epoch 592 train loss 0.38026759028434753\n",
      "val loss 0.5418519973754883\n",
      "______________\n",
      "epoch 593 train loss 0.3602929711341858\n",
      "val loss 0.5407966375350952\n",
      "______________\n",
      "epoch 594 train loss 0.372680127620697\n",
      "val loss 0.5397754907608032\n",
      "______________\n",
      "epoch 595 train loss 0.35505884885787964\n",
      "val loss 0.5387547016143799\n",
      "______________\n",
      "epoch 596 train loss 0.3488146662712097\n",
      "val loss 0.5377449989318848\n",
      "______________\n",
      "epoch 597 train loss 0.34619295597076416\n",
      "val loss 0.5367254018783569\n",
      "______________\n",
      "epoch 598 train loss 0.36757418513298035\n",
      "val loss 0.5357085466384888\n",
      "______________\n",
      "epoch 599 train loss 0.36405736207962036\n",
      "val loss 0.5346760153770447\n",
      "______________\n",
      "epoch 600 train loss 0.3437763452529907\n",
      "val loss 0.5336651802062988\n",
      "______________\n",
      "epoch 601 train loss 0.3339717984199524\n",
      "val loss 0.5326817035675049\n",
      "______________\n",
      "epoch 602 train loss 0.3383881747722626\n",
      "val loss 0.5317407846450806\n",
      "______________\n",
      "epoch 603 train loss 0.35080140829086304\n",
      "val loss 0.5308184027671814\n",
      "______________\n",
      "epoch 604 train loss 0.36232396960258484\n",
      "val loss 0.5298784971237183\n",
      "______________\n",
      "epoch 605 train loss 0.317846417427063\n",
      "val loss 0.5289618968963623\n",
      "______________\n",
      "epoch 606 train loss 0.34211623668670654\n",
      "val loss 0.5280435085296631\n",
      "______________\n",
      "epoch 607 train loss 0.3526095449924469\n",
      "val loss 0.5271246433258057\n",
      "______________\n",
      "epoch 608 train loss 0.32529690861701965\n",
      "val loss 0.526204526424408\n",
      "______________\n",
      "epoch 609 train loss 0.32847219705581665\n",
      "val loss 0.5252851247787476\n",
      "______________\n",
      "epoch 610 train loss 0.3311019241809845\n",
      "val loss 0.5243797898292542\n",
      "______________\n",
      "epoch 611 train loss 0.3157612085342407\n",
      "val loss 0.5234946608543396\n",
      "______________\n",
      "epoch 612 train loss 0.32948991656303406\n",
      "val loss 0.5226514339447021\n",
      "______________\n",
      "epoch 613 train loss 0.35093164443969727\n",
      "val loss 0.5217932462692261\n",
      "______________\n",
      "epoch 614 train loss 0.3555065095424652\n",
      "val loss 0.5209234356880188\n",
      "______________\n",
      "epoch 615 train loss 0.33602795004844666\n",
      "val loss 0.5200563669204712\n",
      "______________\n",
      "epoch 616 train loss 0.3806883692741394\n",
      "val loss 0.5191285610198975\n",
      "______________\n",
      "epoch 617 train loss 0.3409310281276703\n",
      "val loss 0.5182175040245056\n",
      "______________\n",
      "epoch 618 train loss 0.36701104044914246\n",
      "val loss 0.5173108577728271\n",
      "______________\n",
      "epoch 619 train loss 0.3250385522842407\n",
      "val loss 0.5164049863815308\n",
      "______________\n",
      "epoch 620 train loss 0.3029254674911499\n",
      "val loss 0.5155470371246338\n",
      "______________\n",
      "epoch 621 train loss 0.3401895761489868\n",
      "val loss 0.5147213935852051\n",
      "______________\n",
      "epoch 622 train loss 0.3281055688858032\n",
      "val loss 0.5138771533966064\n",
      "______________\n",
      "epoch 623 train loss 0.3228035569190979\n",
      "val loss 0.5130702257156372\n",
      "______________\n",
      "epoch 624 train loss 0.32729214429855347\n",
      "val loss 0.5122423768043518\n",
      "______________\n",
      "epoch 625 train loss 0.33152878284454346\n",
      "val loss 0.5114119052886963\n",
      "______________\n",
      "epoch 626 train loss 0.32586872577667236\n",
      "val loss 0.5105702877044678\n",
      "______________\n",
      "epoch 627 train loss 0.3275274932384491\n",
      "val loss 0.5097026824951172\n",
      "______________\n",
      "epoch 628 train loss 0.32400503754615784\n",
      "val loss 0.5088798999786377\n",
      "______________\n",
      "epoch 629 train loss 0.33156877756118774\n",
      "val loss 0.508010745048523\n",
      "______________\n",
      "epoch 630 train loss 0.31076836585998535\n",
      "val loss 0.5071311593055725\n",
      "______________\n",
      "epoch 631 train loss 0.31364572048187256\n",
      "val loss 0.5062402486801147\n",
      "______________\n",
      "epoch 632 train loss 0.3289203345775604\n",
      "val loss 0.5053609013557434\n",
      "______________\n",
      "epoch 633 train loss 0.3430618643760681\n",
      "val loss 0.5044931173324585\n",
      "______________\n",
      "epoch 634 train loss 0.3359048664569855\n",
      "val loss 0.5036253929138184\n",
      "______________\n",
      "epoch 635 train loss 0.32756710052490234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 0.5027691721916199\n",
      "______________\n",
      "epoch 636 train loss 0.31435999274253845\n",
      "val loss 0.5019323825836182\n",
      "______________\n",
      "epoch 637 train loss 0.3071950674057007\n",
      "val loss 0.5010979175567627\n",
      "______________\n",
      "epoch 638 train loss 0.32649531960487366\n",
      "val loss 0.5002340078353882\n",
      "______________\n",
      "epoch 639 train loss 0.31934690475463867\n",
      "val loss 0.49938374757766724\n",
      "______________\n",
      "epoch 640 train loss 0.33262312412261963\n",
      "val loss 0.49851590394973755\n",
      "______________\n",
      "epoch 641 train loss 0.31769776344299316\n",
      "val loss 0.4976823329925537\n",
      "______________\n",
      "epoch 642 train loss 0.321165531873703\n",
      "val loss 0.49682676792144775\n",
      "______________\n",
      "epoch 643 train loss 0.3004528880119324\n",
      "val loss 0.49598556756973267\n",
      "______________\n",
      "epoch 644 train loss 0.3061668872833252\n",
      "val loss 0.4951556324958801\n",
      "______________\n",
      "epoch 645 train loss 0.30596888065338135\n",
      "val loss 0.4943065941333771\n",
      "______________\n",
      "epoch 646 train loss 0.31751203536987305\n",
      "val loss 0.49345898628234863\n",
      "______________\n",
      "epoch 647 train loss 0.32956963777542114\n",
      "val loss 0.4926028847694397\n",
      "______________\n",
      "epoch 648 train loss 0.3022698163986206\n",
      "val loss 0.4917614459991455\n",
      "______________\n",
      "epoch 649 train loss 0.3013702630996704\n",
      "val loss 0.4909287095069885\n",
      "______________\n",
      "epoch 650 train loss 0.32064664363861084\n",
      "val loss 0.4900728464126587\n",
      "______________\n",
      "epoch 651 train loss 0.3068581223487854\n",
      "val loss 0.4892016649246216\n",
      "______________\n",
      "epoch 652 train loss 0.32887664437294006\n",
      "val loss 0.48835885524749756\n",
      "______________\n",
      "epoch 653 train loss 0.3121166229248047\n",
      "val loss 0.48749396204948425\n",
      "______________\n",
      "epoch 654 train loss 0.31580495834350586\n",
      "val loss 0.4866185188293457\n",
      "______________\n",
      "epoch 655 train loss 0.3231015205383301\n",
      "val loss 0.4857807755470276\n",
      "______________\n",
      "epoch 656 train loss 0.30193644762039185\n",
      "val loss 0.4849562644958496\n",
      "______________\n",
      "epoch 657 train loss 0.32168808579444885\n",
      "val loss 0.48411619663238525\n",
      "______________\n",
      "epoch 658 train loss 0.3037255108356476\n",
      "val loss 0.4832944869995117\n",
      "______________\n",
      "epoch 659 train loss 0.33850300312042236\n",
      "val loss 0.4824536442756653\n",
      "______________\n",
      "epoch 660 train loss 0.3195796012878418\n",
      "val loss 0.48157739639282227\n",
      "______________\n",
      "epoch 661 train loss 0.314240038394928\n",
      "val loss 0.48068252205848694\n",
      "______________\n",
      "epoch 662 train loss 0.3167453706264496\n",
      "val loss 0.4797908067703247\n",
      "______________\n",
      "epoch 663 train loss 0.3027665317058563\n",
      "val loss 0.4789319932460785\n",
      "______________\n",
      "epoch 664 train loss 0.31293362379074097\n",
      "val loss 0.47806504368782043\n",
      "______________\n",
      "epoch 665 train loss 0.31043320894241333\n",
      "val loss 0.47721654176712036\n",
      "______________\n",
      "epoch 666 train loss 0.2891482412815094\n",
      "val loss 0.4763769507408142\n",
      "______________\n",
      "epoch 667 train loss 0.2881908416748047\n",
      "val loss 0.47553691267967224\n",
      "______________\n",
      "epoch 668 train loss 0.2798408269882202\n",
      "val loss 0.4747159481048584\n",
      "______________\n",
      "epoch 669 train loss 0.3041502833366394\n",
      "val loss 0.47390323877334595\n",
      "______________\n",
      "epoch 670 train loss 0.31044191122055054\n",
      "val loss 0.4731147289276123\n",
      "______________\n",
      "epoch 671 train loss 0.2815109193325043\n",
      "val loss 0.47233065962791443\n",
      "______________\n",
      "epoch 672 train loss 0.29050683975219727\n",
      "val loss 0.47155457735061646\n",
      "______________\n",
      "epoch 673 train loss 0.3066180646419525\n",
      "val loss 0.47078126668930054\n",
      "______________\n",
      "epoch 674 train loss 0.28730839490890503\n",
      "val loss 0.47000861167907715\n",
      "______________\n",
      "epoch 675 train loss 0.29472246766090393\n",
      "val loss 0.46925392746925354\n",
      "______________\n",
      "epoch 676 train loss 0.2823975086212158\n",
      "val loss 0.4685019850730896\n",
      "______________\n",
      "epoch 677 train loss 0.282734751701355\n",
      "val loss 0.4677395522594452\n",
      "______________\n",
      "epoch 678 train loss 0.2954939603805542\n",
      "val loss 0.46700283885002136\n",
      "______________\n",
      "epoch 679 train loss 0.27921515703201294\n",
      "val loss 0.46626386046409607\n",
      "______________\n",
      "epoch 680 train loss 0.3161899745464325\n",
      "val loss 0.46550002694129944\n",
      "______________\n",
      "epoch 681 train loss 0.28335297107696533\n",
      "val loss 0.4647448658943176\n",
      "______________\n",
      "epoch 682 train loss 0.2923333942890167\n",
      "val loss 0.4640258848667145\n",
      "______________\n",
      "epoch 683 train loss 0.28440743684768677\n",
      "val loss 0.46331530809402466\n",
      "______________\n",
      "epoch 684 train loss 0.28361567854881287\n",
      "val loss 0.4626040458679199\n",
      "______________\n",
      "epoch 685 train loss 0.2994610369205475\n",
      "val loss 0.461895614862442\n",
      "______________\n",
      "epoch 686 train loss 0.27518317103385925\n",
      "val loss 0.4611859917640686\n",
      "______________\n",
      "epoch 687 train loss 0.27380621433258057\n",
      "val loss 0.4604721665382385\n",
      "______________\n",
      "epoch 688 train loss 0.2879968285560608\n",
      "val loss 0.4597504436969757\n",
      "______________\n",
      "epoch 689 train loss 0.28917187452316284\n",
      "val loss 0.45901134610176086\n",
      "______________\n",
      "epoch 690 train loss 0.2959026098251343\n",
      "val loss 0.4582730233669281\n",
      "______________\n",
      "epoch 691 train loss 0.28046298027038574\n",
      "val loss 0.45752769708633423\n",
      "______________\n",
      "epoch 692 train loss 0.2892293334007263\n",
      "val loss 0.4567793607711792\n",
      "______________\n",
      "epoch 693 train loss 0.29041582345962524\n",
      "val loss 0.45605170726776123\n",
      "______________\n",
      "epoch 694 train loss 0.2539236545562744\n",
      "val loss 0.45533475279808044\n",
      "______________\n",
      "epoch 695 train loss 0.29399168491363525\n",
      "val loss 0.45458167791366577\n",
      "______________\n",
      "epoch 696 train loss 0.28617754578590393\n",
      "val loss 0.4538346529006958\n",
      "______________\n",
      "epoch 697 train loss 0.29101496934890747\n",
      "val loss 0.45307305455207825\n",
      "______________\n",
      "epoch 698 train loss 0.27984943985939026\n",
      "val loss 0.4523180425167084\n",
      "______________\n",
      "epoch 699 train loss 0.29028308391571045\n",
      "val loss 0.45154815912246704\n",
      "______________\n",
      "epoch 700 train loss 0.2682775557041168\n",
      "val loss 0.4508371949195862\n",
      "______________\n",
      "epoch 701 train loss 0.26502445340156555\n",
      "val loss 0.4501107335090637\n",
      "______________\n",
      "epoch 702 train loss 0.2977101504802704\n",
      "val loss 0.44941991567611694\n",
      "______________\n",
      "epoch 703 train loss 0.2747597098350525\n",
      "val loss 0.4487142562866211\n",
      "______________\n",
      "epoch 704 train loss 0.27201902866363525\n",
      "val loss 0.4480089843273163\n",
      "______________\n",
      "epoch 705 train loss 0.2875399887561798\n",
      "val loss 0.4472956657409668\n",
      "______________\n",
      "epoch 706 train loss 0.27022480964660645\n",
      "val loss 0.4466039538383484\n",
      "______________\n",
      "epoch 707 train loss 0.29709720611572266\n",
      "val loss 0.4458947777748108\n",
      "______________\n",
      "epoch 708 train loss 0.28553369641304016\n",
      "val loss 0.44519078731536865\n",
      "______________\n",
      "epoch 709 train loss 0.2861744463443756\n",
      "val loss 0.44449734687805176\n",
      "______________\n",
      "epoch 710 train loss 0.2873961329460144\n",
      "val loss 0.4437936246395111\n",
      "______________\n",
      "epoch 711 train loss 0.27852997183799744\n",
      "val loss 0.44311243295669556\n",
      "______________\n",
      "epoch 712 train loss 0.2622814178466797\n",
      "val loss 0.44244861602783203\n",
      "______________\n",
      "epoch 713 train loss 0.27697092294692993\n",
      "val loss 0.44177716970443726\n",
      "______________\n",
      "epoch 714 train loss 0.2645520865917206\n",
      "val loss 0.44106149673461914\n",
      "______________\n",
      "epoch 715 train loss 0.29895633459091187\n",
      "val loss 0.44034069776535034\n",
      "______________\n",
      "epoch 716 train loss 0.2815163731575012\n",
      "val loss 0.43959367275238037\n",
      "______________\n",
      "epoch 717 train loss 0.24547892808914185\n",
      "val loss 0.43883824348449707\n",
      "______________\n",
      "epoch 718 train loss 0.27326563000679016\n",
      "val loss 0.4380338490009308\n",
      "______________\n",
      "epoch 719 train loss 0.2681698799133301\n",
      "val loss 0.4372413754463196\n",
      "______________\n",
      "epoch 720 train loss 0.28490763902664185\n",
      "val loss 0.4364505410194397\n",
      "______________\n",
      "epoch 721 train loss 0.256615549325943\n",
      "val loss 0.43564608693122864\n",
      "______________\n",
      "epoch 722 train loss 0.25222307443618774\n",
      "val loss 0.4348627030849457\n",
      "______________\n",
      "epoch 723 train loss 0.2580173909664154\n",
      "val loss 0.4340626001358032\n",
      "______________\n",
      "epoch 724 train loss 0.25567424297332764\n",
      "val loss 0.4332425594329834\n",
      "______________\n",
      "epoch 725 train loss 0.2628979980945587\n",
      "val loss 0.4324181079864502\n",
      "______________\n",
      "epoch 726 train loss 0.2531563341617584\n",
      "val loss 0.431618869304657\n",
      "______________\n",
      "epoch 727 train loss 0.27821213006973267\n",
      "val loss 0.43084192276000977\n",
      "______________\n",
      "epoch 728 train loss 0.2768486738204956\n",
      "val loss 0.43005841970443726\n",
      "______________\n",
      "epoch 729 train loss 0.27821820974349976\n",
      "val loss 0.42927759885787964\n",
      "______________\n",
      "epoch 730 train loss 0.2548834979534149\n",
      "val loss 0.4285283088684082\n",
      "______________\n",
      "epoch 731 train loss 0.25240811705589294\n",
      "val loss 0.4277845621109009\n",
      "______________\n",
      "epoch 732 train loss 0.2614317536354065\n",
      "val loss 0.427050918340683\n",
      "______________\n",
      "epoch 733 train loss 0.2780345678329468\n",
      "val loss 0.42630505561828613\n",
      "______________\n",
      "epoch 734 train loss 0.25448691844940186\n",
      "val loss 0.42554399371147156\n",
      "______________\n",
      "epoch 735 train loss 0.281660258769989\n",
      "val loss 0.4247667193412781\n",
      "______________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 736 train loss 0.25470805168151855\n",
      "val loss 0.42401042580604553\n",
      "______________\n",
      "epoch 737 train loss 0.2568811774253845\n",
      "val loss 0.4232921004295349\n",
      "______________\n",
      "epoch 738 train loss 0.2459629774093628\n",
      "val loss 0.4226069450378418\n",
      "______________\n",
      "epoch 739 train loss 0.2656131386756897\n",
      "val loss 0.42190104722976685\n",
      "______________\n",
      "epoch 740 train loss 0.29209452867507935\n",
      "val loss 0.4211850166320801\n",
      "______________\n",
      "epoch 741 train loss 0.2532423138618469\n",
      "val loss 0.4204750061035156\n",
      "______________\n",
      "epoch 742 train loss 0.2714530825614929\n",
      "val loss 0.4197287857532501\n",
      "______________\n",
      "epoch 743 train loss 0.2660319209098816\n",
      "val loss 0.4189770519733429\n",
      "______________\n",
      "epoch 744 train loss 0.266239196062088\n",
      "val loss 0.4182116687297821\n",
      "______________\n",
      "epoch 745 train loss 0.2628461718559265\n",
      "val loss 0.4174472689628601\n",
      "______________\n",
      "epoch 746 train loss 0.25955793261528015\n",
      "val loss 0.416687548160553\n",
      "______________\n",
      "epoch 747 train loss 0.27983230352401733\n",
      "val loss 0.4158672094345093\n",
      "______________\n",
      "epoch 748 train loss 0.26216578483581543\n",
      "val loss 0.41506272554397583\n",
      "______________\n",
      "epoch 749 train loss 0.252255916595459\n",
      "val loss 0.4142867624759674\n",
      "______________\n",
      "epoch 750 train loss 0.23829278349876404\n",
      "val loss 0.4135172963142395\n",
      "______________\n",
      "epoch 751 train loss 0.23609942197799683\n",
      "val loss 0.4127722978591919\n",
      "______________\n",
      "epoch 752 train loss 0.24277010560035706\n",
      "val loss 0.4120329022407532\n",
      "______________\n",
      "epoch 753 train loss 0.2628299593925476\n",
      "val loss 0.4112812876701355\n",
      "______________\n",
      "epoch 754 train loss 0.26729923486709595\n",
      "val loss 0.4105495810508728\n",
      "______________\n",
      "epoch 755 train loss 0.23261608183383942\n",
      "val loss 0.40986108779907227\n",
      "______________\n",
      "epoch 756 train loss 0.2561926543712616\n",
      "val loss 0.40915828943252563\n",
      "______________\n",
      "epoch 757 train loss 0.2527913749217987\n",
      "val loss 0.40847447514533997\n",
      "______________\n",
      "epoch 758 train loss 0.2562654912471771\n",
      "val loss 0.4077998101711273\n",
      "______________\n",
      "epoch 759 train loss 0.24424932897090912\n",
      "val loss 0.40714961290359497\n",
      "______________\n",
      "epoch 760 train loss 0.25535398721694946\n",
      "val loss 0.4065169095993042\n",
      "______________\n",
      "epoch 761 train loss 0.252499520778656\n",
      "val loss 0.4059118628501892\n",
      "______________\n",
      "epoch 762 train loss 0.2361215502023697\n",
      "val loss 0.4053077697753906\n",
      "______________\n",
      "epoch 763 train loss 0.24301911890506744\n",
      "val loss 0.4046647548675537\n",
      "______________\n",
      "epoch 764 train loss 0.2569483518600464\n",
      "val loss 0.40405452251434326\n",
      "______________\n",
      "epoch 765 train loss 0.25265946984291077\n",
      "val loss 0.4034408926963806\n",
      "______________\n",
      "epoch 766 train loss 0.23708561062812805\n",
      "val loss 0.40285545587539673\n",
      "______________\n",
      "epoch 767 train loss 0.25638043880462646\n",
      "val loss 0.40228137373924255\n",
      "______________\n",
      "epoch 768 train loss 0.25052911043167114\n",
      "val loss 0.40171122550964355\n",
      "______________\n",
      "epoch 769 train loss 0.24647819995880127\n",
      "val loss 0.40114593505859375\n",
      "______________\n",
      "epoch 770 train loss 0.24647176265716553\n",
      "val loss 0.40060627460479736\n",
      "______________\n",
      "epoch 771 train loss 0.23737135529518127\n",
      "val loss 0.400112122297287\n",
      "______________\n",
      "epoch 772 train loss 0.2394282966852188\n",
      "val loss 0.39964139461517334\n",
      "______________\n",
      "epoch 773 train loss 0.24565187096595764\n",
      "val loss 0.3991811275482178\n",
      "______________\n",
      "epoch 774 train loss 0.24438774585723877\n",
      "val loss 0.3987049460411072\n",
      "______________\n",
      "epoch 775 train loss 0.23202306032180786\n",
      "val loss 0.3982163071632385\n",
      "______________\n",
      "epoch 776 train loss 0.2683878540992737\n",
      "val loss 0.3977241516113281\n",
      "______________\n",
      "epoch 777 train loss 0.2462977170944214\n",
      "val loss 0.3972143232822418\n",
      "______________\n",
      "epoch 778 train loss 0.2314365804195404\n",
      "val loss 0.3967266082763672\n",
      "______________\n",
      "epoch 779 train loss 0.24607175588607788\n",
      "val loss 0.3962286114692688\n",
      "______________\n",
      "epoch 780 train loss 0.23532892763614655\n",
      "val loss 0.39567381143569946\n",
      "______________\n",
      "epoch 781 train loss 0.243183434009552\n",
      "val loss 0.39511120319366455\n",
      "______________\n",
      "epoch 782 train loss 0.23289033770561218\n",
      "val loss 0.3945278525352478\n",
      "______________\n",
      "epoch 783 train loss 0.24092349410057068\n",
      "val loss 0.3939482271671295\n",
      "______________\n",
      "epoch 784 train loss 0.246279776096344\n",
      "val loss 0.3933366537094116\n",
      "______________\n",
      "epoch 785 train loss 0.2591649293899536\n",
      "val loss 0.39268940687179565\n",
      "______________\n",
      "epoch 786 train loss 0.23700442910194397\n",
      "val loss 0.39203768968582153\n",
      "______________\n",
      "epoch 787 train loss 0.24451223015785217\n",
      "val loss 0.3913993835449219\n",
      "______________\n",
      "epoch 788 train loss 0.23044990003108978\n",
      "val loss 0.3908005952835083\n",
      "______________\n",
      "epoch 789 train loss 0.2575483024120331\n",
      "val loss 0.3901447653770447\n",
      "______________\n",
      "epoch 790 train loss 0.2617660164833069\n",
      "val loss 0.3894941508769989\n",
      "______________\n",
      "epoch 791 train loss 0.2595524191856384\n",
      "val loss 0.3887947201728821\n",
      "______________\n",
      "epoch 792 train loss 0.222789466381073\n",
      "val loss 0.38811230659484863\n",
      "______________\n",
      "epoch 793 train loss 0.24592959880828857\n",
      "val loss 0.3874303102493286\n",
      "______________\n",
      "epoch 794 train loss 0.2621157169342041\n",
      "val loss 0.3867495656013489\n",
      "______________\n",
      "epoch 795 train loss 0.2259921431541443\n",
      "val loss 0.3860814571380615\n",
      "______________\n",
      "epoch 796 train loss 0.2250359207391739\n",
      "val loss 0.3854135274887085\n",
      "______________\n",
      "epoch 797 train loss 0.26129966974258423\n",
      "val loss 0.3847495913505554\n",
      "______________\n",
      "epoch 798 train loss 0.21525952219963074\n",
      "val loss 0.38412487506866455\n",
      "______________\n",
      "epoch 799 train loss 0.2502012550830841\n",
      "val loss 0.383527010679245\n",
      "______________\n",
      "epoch 800 train loss 0.2222028374671936\n",
      "val loss 0.38291990756988525\n",
      "______________\n",
      "epoch 801 train loss 0.23110055923461914\n",
      "val loss 0.3823051154613495\n",
      "______________\n",
      "epoch 802 train loss 0.2391756922006607\n",
      "val loss 0.3817014992237091\n",
      "______________\n",
      "epoch 803 train loss 0.2512133717536926\n",
      "val loss 0.38111409544944763\n",
      "______________\n",
      "epoch 804 train loss 0.24556446075439453\n",
      "val loss 0.38051170110702515\n",
      "______________\n",
      "epoch 805 train loss 0.23314009606838226\n",
      "val loss 0.37989184260368347\n",
      "______________\n",
      "epoch 806 train loss 0.21890032291412354\n",
      "val loss 0.3792935609817505\n",
      "______________\n",
      "epoch 807 train loss 0.21001583337783813\n",
      "val loss 0.378726065158844\n",
      "______________\n",
      "epoch 808 train loss 0.23032623529434204\n",
      "val loss 0.3781628906726837\n",
      "______________\n",
      "epoch 809 train loss 0.258217990398407\n",
      "val loss 0.3775407075881958\n",
      "______________\n",
      "epoch 810 train loss 0.23397332429885864\n",
      "val loss 0.3769189119338989\n",
      "______________\n",
      "epoch 811 train loss 0.2476297914981842\n",
      "val loss 0.376280814409256\n",
      "______________\n",
      "epoch 812 train loss 0.25077304244041443\n",
      "val loss 0.37566590309143066\n",
      "______________\n",
      "epoch 813 train loss 0.22559478878974915\n",
      "val loss 0.37509530782699585\n",
      "______________\n",
      "epoch 814 train loss 0.20156198740005493\n",
      "val loss 0.3745245337486267\n",
      "______________\n",
      "epoch 815 train loss 0.19858485460281372\n",
      "val loss 0.37394171953201294\n",
      "______________\n",
      "epoch 816 train loss 0.2071044147014618\n",
      "val loss 0.3733694553375244\n",
      "______________\n",
      "epoch 817 train loss 0.21645498275756836\n",
      "val loss 0.3727937340736389\n",
      "______________\n",
      "epoch 818 train loss 0.22442016005516052\n",
      "val loss 0.3722517192363739\n",
      "______________\n",
      "epoch 819 train loss 0.2310459315776825\n",
      "val loss 0.3717046082019806\n",
      "______________\n",
      "epoch 820 train loss 0.2028983235359192\n",
      "val loss 0.37113070487976074\n",
      "______________\n",
      "epoch 821 train loss 0.2265467643737793\n",
      "val loss 0.37055909633636475\n",
      "______________\n",
      "epoch 822 train loss 0.2139720916748047\n",
      "val loss 0.3699806332588196\n",
      "______________\n",
      "epoch 823 train loss 0.20294694602489471\n",
      "val loss 0.3694148063659668\n",
      "______________\n",
      "epoch 824 train loss 0.2608417272567749\n",
      "val loss 0.36882275342941284\n",
      "______________\n",
      "epoch 825 train loss 0.19936007261276245\n",
      "val loss 0.3682315945625305\n",
      "______________\n",
      "epoch 826 train loss 0.20753836631774902\n",
      "val loss 0.36765986680984497\n",
      "______________\n",
      "epoch 827 train loss 0.20528210699558258\n",
      "val loss 0.36712029576301575\n",
      "______________\n",
      "epoch 828 train loss 0.23455311357975006\n",
      "val loss 0.3665933609008789\n",
      "______________\n",
      "epoch 829 train loss 0.20098771154880524\n",
      "val loss 0.3661034107208252\n",
      "______________\n",
      "epoch 830 train loss 0.22160091996192932\n",
      "val loss 0.36563366651535034\n",
      "______________\n",
      "epoch 831 train loss 0.19666269421577454\n",
      "val loss 0.36519357562065125\n",
      "______________\n",
      "epoch 832 train loss 0.2320634126663208\n",
      "val loss 0.36473149061203003\n",
      "______________\n",
      "epoch 833 train loss 0.19431956112384796\n",
      "val loss 0.36428120732307434\n",
      "______________\n",
      "epoch 834 train loss 0.2055458128452301\n",
      "val loss 0.3637799620628357\n",
      "______________\n",
      "epoch 835 train loss 0.2186579704284668\n",
      "val loss 0.36329084634780884\n",
      "______________\n",
      "epoch 836 train loss 0.21084633469581604\n",
      "val loss 0.36281341314315796\n",
      "______________\n",
      "epoch 837 train loss 0.21494561433792114\n",
      "val loss 0.36234334111213684\n",
      "______________\n",
      "epoch 838 train loss 0.2036079615354538\n",
      "val loss 0.3619205057621002\n",
      "______________\n",
      "epoch 839 train loss 0.21289978921413422\n",
      "val loss 0.3614846467971802\n",
      "______________\n",
      "epoch 840 train loss 0.2335624098777771\n",
      "val loss 0.3610076308250427\n",
      "______________\n",
      "epoch 841 train loss 0.21899662911891937\n",
      "val loss 0.36050552129745483\n",
      "______________\n",
      "epoch 842 train loss 0.21741598844528198\n",
      "val loss 0.35999011993408203\n",
      "______________\n",
      "epoch 843 train loss 0.21986734867095947\n",
      "val loss 0.35947874188423157\n",
      "______________\n",
      "epoch 844 train loss 0.21746468544006348\n",
      "val loss 0.3589418828487396\n",
      "______________\n",
      "epoch 845 train loss 0.2053213119506836\n",
      "val loss 0.35841721296310425\n",
      "______________\n",
      "epoch 846 train loss 0.23457679152488708\n",
      "val loss 0.3578844666481018\n",
      "______________\n",
      "epoch 847 train loss 0.19331885874271393\n",
      "val loss 0.3573598861694336\n",
      "______________\n",
      "epoch 848 train loss 0.22822532057762146\n",
      "val loss 0.35678333044052124\n",
      "______________\n",
      "epoch 849 train loss 0.20058338344097137\n",
      "val loss 0.356214314699173\n",
      "______________\n",
      "epoch 850 train loss 0.2091192603111267\n",
      "val loss 0.3556482791900635\n",
      "______________\n",
      "epoch 851 train loss 0.20381483435630798\n",
      "val loss 0.35509759187698364\n",
      "______________\n",
      "epoch 852 train loss 0.21422889828681946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 0.35454076528549194\n",
      "______________\n",
      "epoch 853 train loss 0.20980745553970337\n",
      "val loss 0.3539932370185852\n",
      "______________\n",
      "epoch 854 train loss 0.2356913536787033\n",
      "val loss 0.35344475507736206\n",
      "______________\n",
      "epoch 855 train loss 0.20582038164138794\n",
      "val loss 0.35293376445770264\n",
      "______________\n",
      "epoch 856 train loss 0.23354794085025787\n",
      "val loss 0.3524025082588196\n",
      "______________\n",
      "epoch 857 train loss 0.19390860199928284\n",
      "val loss 0.3518618941307068\n",
      "______________\n",
      "epoch 858 train loss 0.2225935459136963\n",
      "val loss 0.3513113856315613\n",
      "______________\n",
      "epoch 859 train loss 0.19428715109825134\n",
      "val loss 0.35076504945755005\n",
      "______________\n",
      "epoch 860 train loss 0.20874634385108948\n",
      "val loss 0.35022711753845215\n",
      "______________\n",
      "epoch 861 train loss 0.1975315511226654\n",
      "val loss 0.3496742844581604\n",
      "______________\n",
      "epoch 862 train loss 0.2117052972316742\n",
      "val loss 0.3490836024284363\n",
      "______________\n",
      "epoch 863 train loss 0.2067120373249054\n",
      "val loss 0.34848925471305847\n",
      "______________\n",
      "epoch 864 train loss 0.20639057457447052\n",
      "val loss 0.34791988134384155\n",
      "______________\n",
      "epoch 865 train loss 0.1907951682806015\n",
      "val loss 0.34735745191574097\n",
      "______________\n",
      "epoch 866 train loss 0.2045774906873703\n",
      "val loss 0.3468116521835327\n",
      "______________\n",
      "epoch 867 train loss 0.1858268678188324\n",
      "val loss 0.34628504514694214\n",
      "______________\n",
      "epoch 868 train loss 0.21134479343891144\n",
      "val loss 0.3457337021827698\n",
      "______________\n",
      "epoch 869 train loss 0.1867729276418686\n",
      "val loss 0.3451836109161377\n",
      "______________\n",
      "epoch 870 train loss 0.20124728977680206\n",
      "val loss 0.3446178734302521\n",
      "______________\n",
      "epoch 871 train loss 0.20650094747543335\n",
      "val loss 0.34406405687332153\n",
      "______________\n",
      "epoch 872 train loss 0.20169158279895782\n",
      "val loss 0.3434971570968628\n",
      "______________\n",
      "epoch 873 train loss 0.18999654054641724\n",
      "val loss 0.342966228723526\n",
      "______________\n",
      "epoch 874 train loss 0.21456001698970795\n",
      "val loss 0.3424415588378906\n",
      "______________\n",
      "epoch 875 train loss 0.2216932624578476\n",
      "val loss 0.3418954610824585\n",
      "______________\n",
      "epoch 876 train loss 0.2129162698984146\n",
      "val loss 0.3413403332233429\n",
      "______________\n",
      "epoch 877 train loss 0.18992337584495544\n",
      "val loss 0.34081870317459106\n",
      "______________\n",
      "epoch 878 train loss 0.21231086552143097\n",
      "val loss 0.3402983546257019\n",
      "______________\n",
      "epoch 879 train loss 0.2097797393798828\n",
      "val loss 0.33978021144866943\n",
      "______________\n",
      "epoch 880 train loss 0.20540925860404968\n",
      "val loss 0.3392598628997803\n",
      "______________\n",
      "epoch 881 train loss 0.20393770933151245\n",
      "val loss 0.33874669671058655\n",
      "______________\n",
      "epoch 882 train loss 0.22015076875686646\n",
      "val loss 0.33821266889572144\n",
      "______________\n",
      "epoch 883 train loss 0.21587783098220825\n",
      "val loss 0.33764874935150146\n",
      "______________\n",
      "epoch 884 train loss 0.1844397783279419\n",
      "val loss 0.33710581064224243\n",
      "______________\n",
      "epoch 885 train loss 0.17332185804843903\n",
      "val loss 0.33659085631370544\n",
      "______________\n",
      "epoch 886 train loss 0.20306992530822754\n",
      "val loss 0.3360387682914734\n",
      "______________\n",
      "epoch 887 train loss 0.18754535913467407\n",
      "val loss 0.33546000719070435\n",
      "______________\n",
      "epoch 888 train loss 0.202098548412323\n",
      "val loss 0.33488035202026367\n",
      "______________\n",
      "epoch 889 train loss 0.16499148309230804\n",
      "val loss 0.3343573212623596\n",
      "______________\n",
      "epoch 890 train loss 0.20818361639976501\n",
      "val loss 0.3338489532470703\n",
      "______________\n",
      "epoch 891 train loss 0.20258969068527222\n",
      "val loss 0.3333391845226288\n",
      "______________\n",
      "epoch 892 train loss 0.19682209193706512\n",
      "val loss 0.33281978964805603\n",
      "______________\n",
      "epoch 893 train loss 0.2145504355430603\n",
      "val loss 0.33228349685668945\n",
      "______________\n",
      "epoch 894 train loss 0.18077613413333893\n",
      "val loss 0.331764280796051\n",
      "______________\n",
      "epoch 895 train loss 0.18834896385669708\n",
      "val loss 0.3312417268753052\n",
      "______________\n",
      "epoch 896 train loss 0.1854645311832428\n",
      "val loss 0.33074599504470825\n",
      "______________\n",
      "epoch 897 train loss 0.19972412288188934\n",
      "val loss 0.33023449778556824\n",
      "______________\n",
      "epoch 898 train loss 0.18967415392398834\n",
      "val loss 0.3297337293624878\n",
      "______________\n",
      "epoch 899 train loss 0.2160944938659668\n",
      "val loss 0.3292277455329895\n",
      "______________\n",
      "epoch 900 train loss 0.21182408928871155\n",
      "val loss 0.32869988679885864\n",
      "______________\n",
      "epoch 901 train loss 0.18673577904701233\n",
      "val loss 0.3281670808792114\n",
      "______________\n",
      "epoch 902 train loss 0.21524371206760406\n",
      "val loss 0.3276670575141907\n",
      "______________\n",
      "epoch 903 train loss 0.18657949566841125\n",
      "val loss 0.3271794319152832\n",
      "______________\n",
      "epoch 904 train loss 0.21478226780891418\n",
      "val loss 0.3266947269439697\n",
      "______________\n",
      "epoch 905 train loss 0.16935166716575623\n",
      "val loss 0.3262178301811218\n",
      "______________\n",
      "epoch 906 train loss 0.19227246940135956\n",
      "val loss 0.3257509768009186\n",
      "______________\n",
      "epoch 907 train loss 0.19438880681991577\n",
      "val loss 0.325245201587677\n",
      "______________\n",
      "epoch 908 train loss 0.17626413702964783\n",
      "val loss 0.3247469663619995\n",
      "______________\n",
      "epoch 909 train loss 0.18942707777023315\n",
      "val loss 0.3242834806442261\n",
      "______________\n",
      "epoch 910 train loss 0.19214953482151031\n",
      "val loss 0.32378873229026794\n",
      "______________\n",
      "epoch 911 train loss 0.2039358913898468\n",
      "val loss 0.3233024775981903\n",
      "______________\n",
      "epoch 912 train loss 0.18437838554382324\n",
      "val loss 0.322811484336853\n",
      "______________\n",
      "epoch 913 train loss 0.2080289125442505\n",
      "val loss 0.3222890794277191\n",
      "______________\n",
      "epoch 914 train loss 0.18697525560855865\n",
      "val loss 0.3217720687389374\n",
      "______________\n",
      "epoch 915 train loss 0.19445227086544037\n",
      "val loss 0.3212546706199646\n",
      "______________\n",
      "epoch 916 train loss 0.20388329029083252\n",
      "val loss 0.32073020935058594\n",
      "______________\n",
      "epoch 917 train loss 0.18401621282100677\n",
      "val loss 0.3202171325683594\n",
      "______________\n",
      "epoch 918 train loss 0.21093501150608063\n",
      "val loss 0.3196623921394348\n",
      "______________\n",
      "epoch 919 train loss 0.1909354031085968\n",
      "val loss 0.3190946877002716\n",
      "______________\n",
      "epoch 920 train loss 0.19589738547801971\n",
      "val loss 0.31852129101753235\n",
      "______________\n",
      "epoch 921 train loss 0.18298551440238953\n",
      "val loss 0.3179952800273895\n",
      "______________\n",
      "epoch 922 train loss 0.16952857375144958\n",
      "val loss 0.31745731830596924\n",
      "______________\n",
      "epoch 923 train loss 0.18107819557189941\n",
      "val loss 0.3169269263744354\n",
      "______________\n",
      "epoch 924 train loss 0.21881306171417236\n",
      "val loss 0.3163885176181793\n",
      "______________\n",
      "epoch 925 train loss 0.17304325103759766\n",
      "val loss 0.315860390663147\n",
      "______________\n",
      "epoch 926 train loss 0.18491734564304352\n",
      "val loss 0.3153167963027954\n",
      "______________\n",
      "epoch 927 train loss 0.18863606452941895\n",
      "val loss 0.3147851228713989\n",
      "______________\n",
      "epoch 928 train loss 0.18962153792381287\n",
      "val loss 0.3142661452293396\n",
      "______________\n",
      "epoch 929 train loss 0.17702266573905945\n",
      "val loss 0.313784658908844\n",
      "______________\n",
      "epoch 930 train loss 0.20450490713119507\n",
      "val loss 0.31332629919052124\n",
      "______________\n",
      "epoch 931 train loss 0.20419472455978394\n",
      "val loss 0.312866747379303\n",
      "______________\n",
      "epoch 932 train loss 0.18160787224769592\n",
      "val loss 0.31240665912628174\n",
      "______________\n",
      "epoch 933 train loss 0.19949185848236084\n",
      "val loss 0.3119785785675049\n",
      "______________\n",
      "epoch 934 train loss 0.17390045523643494\n",
      "val loss 0.3115609884262085\n",
      "______________\n",
      "epoch 935 train loss 0.1746862232685089\n",
      "val loss 0.311151385307312\n",
      "______________\n",
      "epoch 936 train loss 0.18709857761859894\n",
      "val loss 0.3107454776763916\n",
      "______________\n",
      "epoch 937 train loss 0.20700405538082123\n",
      "val loss 0.31036362051963806\n",
      "______________\n",
      "epoch 938 train loss 0.2062489092350006\n",
      "val loss 0.3100093603134155\n",
      "______________\n",
      "epoch 939 train loss 0.19167280197143555\n",
      "val loss 0.3096389174461365\n",
      "______________\n",
      "epoch 940 train loss 0.18870759010314941\n",
      "val loss 0.3092675507068634\n",
      "______________\n",
      "epoch 941 train loss 0.17210565507411957\n",
      "val loss 0.30888432264328003\n",
      "______________\n",
      "epoch 942 train loss 0.19120626151561737\n",
      "val loss 0.30847418308258057\n",
      "______________\n",
      "epoch 943 train loss 0.16485613584518433\n",
      "val loss 0.3080894649028778\n",
      "______________\n",
      "epoch 944 train loss 0.1770215630531311\n",
      "val loss 0.30771851539611816\n",
      "______________\n",
      "epoch 945 train loss 0.19952934980392456\n",
      "val loss 0.3073561489582062\n",
      "______________\n",
      "epoch 946 train loss 0.1989380121231079\n",
      "val loss 0.3069841265678406\n",
      "______________\n",
      "epoch 947 train loss 0.19776731729507446\n",
      "val loss 0.30663174390792847\n",
      "______________\n",
      "epoch 948 train loss 0.16330558061599731\n",
      "val loss 0.30628544092178345\n",
      "______________\n",
      "epoch 949 train loss 0.1839122176170349\n",
      "val loss 0.30593565106391907\n",
      "______________\n",
      "epoch 950 train loss 0.20470920205116272\n",
      "val loss 0.30555206537246704\n",
      "______________\n",
      "epoch 951 train loss 0.20020610094070435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 0.3051312565803528\n",
      "______________\n",
      "epoch 952 train loss 0.17744839191436768\n",
      "val loss 0.30468982458114624\n",
      "______________\n",
      "epoch 953 train loss 0.17522652447223663\n",
      "val loss 0.304268479347229\n",
      "______________\n",
      "epoch 954 train loss 0.1836375892162323\n",
      "val loss 0.30385905504226685\n",
      "______________\n",
      "epoch 955 train loss 0.16385847330093384\n",
      "val loss 0.3034534454345703\n",
      "______________\n",
      "epoch 956 train loss 0.16314569115638733\n",
      "val loss 0.3030148148536682\n",
      "______________\n",
      "epoch 957 train loss 0.16750836372375488\n",
      "val loss 0.30260777473449707\n",
      "______________\n",
      "epoch 958 train loss 0.1871088743209839\n",
      "val loss 0.3021909296512604\n",
      "______________\n",
      "epoch 959 train loss 0.16418002545833588\n",
      "val loss 0.3017989993095398\n",
      "______________\n",
      "epoch 960 train loss 0.19186033308506012\n",
      "val loss 0.301398366689682\n",
      "______________\n",
      "epoch 961 train loss 0.15912549197673798\n",
      "val loss 0.3010390102863312\n",
      "______________\n",
      "epoch 962 train loss 0.18880274891853333\n",
      "val loss 0.30069097876548767\n",
      "______________\n",
      "epoch 963 train loss 0.18409186601638794\n",
      "val loss 0.30033886432647705\n",
      "______________\n",
      "epoch 964 train loss 0.17450058460235596\n",
      "val loss 0.2999950647354126\n",
      "______________\n",
      "epoch 965 train loss 0.17574626207351685\n",
      "val loss 0.2995983362197876\n",
      "______________\n",
      "epoch 966 train loss 0.1847083568572998\n",
      "val loss 0.29916125535964966\n",
      "______________\n",
      "epoch 967 train loss 0.17985689640045166\n",
      "val loss 0.29871976375579834\n",
      "______________\n",
      "epoch 968 train loss 0.20212134718894958\n",
      "val loss 0.29826486110687256\n",
      "______________\n",
      "epoch 969 train loss 0.17378553748130798\n",
      "val loss 0.297835111618042\n",
      "______________\n",
      "epoch 970 train loss 0.2010086178779602\n",
      "val loss 0.2974226176738739\n",
      "______________\n",
      "epoch 971 train loss 0.16240766644477844\n",
      "val loss 0.29702872037887573\n",
      "______________\n",
      "epoch 972 train loss 0.171370267868042\n",
      "val loss 0.29664722084999084\n",
      "______________\n",
      "epoch 973 train loss 0.19143953919410706\n",
      "val loss 0.2962586283683777\n",
      "______________\n",
      "epoch 974 train loss 0.1900758147239685\n",
      "val loss 0.29584407806396484\n",
      "______________\n",
      "epoch 975 train loss 0.1627938598394394\n",
      "val loss 0.2954564094543457\n",
      "______________\n",
      "epoch 976 train loss 0.17542770504951477\n",
      "val loss 0.2950577139854431\n",
      "______________\n",
      "epoch 977 train loss 0.16451585292816162\n",
      "val loss 0.29465222358703613\n",
      "______________\n",
      "epoch 978 train loss 0.15348881483078003\n",
      "val loss 0.29427385330200195\n",
      "______________\n",
      "epoch 979 train loss 0.1702597737312317\n",
      "val loss 0.2938825488090515\n",
      "______________\n",
      "epoch 980 train loss 0.16209764778614044\n",
      "val loss 0.29351961612701416\n",
      "______________\n",
      "epoch 981 train loss 0.16853535175323486\n",
      "val loss 0.2931337356567383\n",
      "______________\n",
      "epoch 982 train loss 0.1832103282213211\n",
      "val loss 0.29276567697525024\n",
      "______________\n",
      "epoch 983 train loss 0.16019801795482635\n",
      "val loss 0.2924045920372009\n",
      "______________\n",
      "epoch 984 train loss 0.1814761757850647\n",
      "val loss 0.2920408546924591\n",
      "______________\n",
      "epoch 985 train loss 0.1724914014339447\n",
      "val loss 0.29167330265045166\n",
      "______________\n",
      "epoch 986 train loss 0.19387710094451904\n",
      "val loss 0.29131072759628296\n",
      "______________\n",
      "epoch 987 train loss 0.16289804875850677\n",
      "val loss 0.2909674644470215\n",
      "______________\n",
      "epoch 988 train loss 0.20084071159362793\n",
      "val loss 0.2905966639518738\n",
      "______________\n",
      "epoch 989 train loss 0.1806999295949936\n",
      "val loss 0.29020458459854126\n",
      "______________\n",
      "epoch 990 train loss 0.15580478310585022\n",
      "val loss 0.2898414731025696\n",
      "______________\n",
      "epoch 991 train loss 0.17680227756500244\n",
      "val loss 0.28948503732681274\n",
      "______________\n",
      "epoch 992 train loss 0.16987596452236176\n",
      "val loss 0.2891108989715576\n",
      "______________\n",
      "epoch 993 train loss 0.18150827288627625\n",
      "val loss 0.2887326776981354\n",
      "______________\n",
      "epoch 994 train loss 0.18104952573776245\n",
      "val loss 0.2883400321006775\n",
      "______________\n",
      "epoch 995 train loss 0.17450286448001862\n",
      "val loss 0.2879558801651001\n",
      "______________\n",
      "epoch 996 train loss 0.1570664793252945\n",
      "val loss 0.28759413957595825\n",
      "______________\n",
      "epoch 997 train loss 0.15005898475646973\n",
      "val loss 0.28722137212753296\n",
      "______________\n",
      "epoch 998 train loss 0.14661724865436554\n",
      "val loss 0.28686174750328064\n",
      "______________\n",
      "epoch 999 train loss 0.17270395159721375\n",
      "val loss 0.2864771783351898\n",
      "______________\n",
      "epoch 1000 train loss 0.206228107213974\n",
      "val loss 0.28606274724006653\n",
      "______________\n",
      "epoch 1001 train loss 0.16289398074150085\n",
      "val loss 0.2856849133968353\n",
      "______________\n",
      "epoch 1002 train loss 0.15248925983905792\n",
      "val loss 0.28533464670181274\n",
      "______________\n",
      "epoch 1003 train loss 0.1618364155292511\n",
      "val loss 0.28495287895202637\n",
      "______________\n",
      "epoch 1004 train loss 0.16806530952453613\n",
      "val loss 0.2845602333545685\n",
      "______________\n",
      "epoch 1005 train loss 0.17141059041023254\n",
      "val loss 0.28418710827827454\n",
      "______________\n",
      "epoch 1006 train loss 0.17195719480514526\n",
      "val loss 0.28382501006126404\n",
      "______________\n",
      "epoch 1007 train loss 0.17395666241645813\n",
      "val loss 0.28346267342567444\n",
      "______________\n",
      "epoch 1008 train loss 0.18120422959327698\n",
      "val loss 0.2830882668495178\n",
      "______________\n",
      "epoch 1009 train loss 0.17214885354042053\n",
      "val loss 0.2827026844024658\n",
      "______________\n",
      "epoch 1010 train loss 0.1654474139213562\n",
      "val loss 0.2823134660720825\n",
      "______________\n",
      "epoch 1011 train loss 0.1724518984556198\n",
      "val loss 0.28193992376327515\n",
      "______________\n",
      "epoch 1012 train loss 0.17523446679115295\n",
      "val loss 0.2815646827220917\n",
      "______________\n",
      "epoch 1013 train loss 0.1920449137687683\n",
      "val loss 0.2811563014984131\n",
      "______________\n",
      "epoch 1014 train loss 0.1595788300037384\n",
      "val loss 0.28078991174697876\n",
      "______________\n",
      "epoch 1015 train loss 0.1678592562675476\n",
      "val loss 0.2804325520992279\n",
      "______________\n",
      "epoch 1016 train loss 0.1616976112127304\n",
      "val loss 0.28008732199668884\n",
      "______________\n",
      "epoch 1017 train loss 0.14053520560264587\n",
      "val loss 0.2797408699989319\n",
      "______________\n",
      "epoch 1018 train loss 0.142084538936615\n",
      "val loss 0.2793610394001007\n",
      "______________\n",
      "epoch 1019 train loss 0.15210096538066864\n",
      "val loss 0.2789735496044159\n",
      "______________\n",
      "epoch 1020 train loss 0.1401156783103943\n",
      "val loss 0.27864906191825867\n",
      "______________\n",
      "epoch 1021 train loss 0.17108123004436493\n",
      "val loss 0.2783151865005493\n",
      "______________\n",
      "epoch 1022 train loss 0.15958762168884277\n",
      "val loss 0.2779807448387146\n",
      "______________\n",
      "epoch 1023 train loss 0.18252387642860413\n",
      "val loss 0.2776174247264862\n",
      "______________\n",
      "epoch 1024 train loss 0.1420329213142395\n",
      "val loss 0.2772704064846039\n",
      "______________\n",
      "epoch 1025 train loss 0.1693674921989441\n",
      "val loss 0.276901513338089\n",
      "______________\n",
      "epoch 1026 train loss 0.1449122428894043\n",
      "val loss 0.27656441926956177\n",
      "______________\n",
      "epoch 1027 train loss 0.1355629861354828\n",
      "val loss 0.2762366235256195\n",
      "______________\n",
      "epoch 1028 train loss 0.21065261960029602\n",
      "val loss 0.27583348751068115\n",
      "______________\n",
      "epoch 1029 train loss 0.1777898371219635\n",
      "val loss 0.2754102945327759\n",
      "______________\n",
      "epoch 1030 train loss 0.14810249209403992\n",
      "val loss 0.27501368522644043\n",
      "______________\n",
      "epoch 1031 train loss 0.16595223546028137\n",
      "val loss 0.2746204137802124\n",
      "______________\n",
      "epoch 1032 train loss 0.1660512089729309\n",
      "val loss 0.27420538663864136\n",
      "______________\n",
      "epoch 1033 train loss 0.15998128056526184\n",
      "val loss 0.2737840414047241\n",
      "______________\n",
      "epoch 1034 train loss 0.1580878645181656\n",
      "val loss 0.27339357137680054\n",
      "______________\n",
      "epoch 1035 train loss 0.15775474905967712\n",
      "val loss 0.27298176288604736\n",
      "______________\n",
      "epoch 1036 train loss 0.1678168773651123\n",
      "val loss 0.2725415527820587\n",
      "______________\n",
      "epoch 1037 train loss 0.14074081182479858\n",
      "val loss 0.2720986604690552\n",
      "______________\n",
      "epoch 1038 train loss 0.17825956642627716\n",
      "val loss 0.27168145775794983\n",
      "______________\n",
      "epoch 1039 train loss 0.17159375548362732\n",
      "val loss 0.2712470293045044\n",
      "______________\n",
      "epoch 1040 train loss 0.1701304316520691\n",
      "val loss 0.2708158493041992\n",
      "______________\n",
      "epoch 1041 train loss 0.14812666177749634\n",
      "val loss 0.27038148045539856\n",
      "______________\n",
      "epoch 1042 train loss 0.17953093349933624\n",
      "val loss 0.26995086669921875\n",
      "______________\n",
      "epoch 1043 train loss 0.1725170910358429\n",
      "val loss 0.2695166766643524\n",
      "______________\n",
      "epoch 1044 train loss 0.15574738383293152\n",
      "val loss 0.2690877318382263\n",
      "______________\n",
      "epoch 1045 train loss 0.16670426726341248\n",
      "val loss 0.26862841844558716\n",
      "______________\n",
      "epoch 1046 train loss 0.17885169386863708\n",
      "val loss 0.26819491386413574\n",
      "______________\n",
      "epoch 1047 train loss 0.15049567818641663\n",
      "val loss 0.2677879333496094\n",
      "______________\n",
      "epoch 1048 train loss 0.18335428833961487\n",
      "val loss 0.26733675599098206\n",
      "______________\n",
      "epoch 1049 train loss 0.16265614330768585\n",
      "val loss 0.2668909430503845\n",
      "______________\n",
      "epoch 1050 train loss 0.16099205613136292\n",
      "val loss 0.2664337456226349\n",
      "______________\n",
      "epoch 1051 train loss 0.16668665409088135\n",
      "val loss 0.26600196957588196\n",
      "______________\n",
      "epoch 1052 train loss 0.15380001068115234\n",
      "val loss 0.2656160593032837\n",
      "______________\n",
      "epoch 1053 train loss 0.1507006287574768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 0.2652358412742615\n",
      "______________\n",
      "epoch 1054 train loss 0.17326992750167847\n",
      "val loss 0.2648453712463379\n",
      "______________\n",
      "epoch 1055 train loss 0.1465451866388321\n",
      "val loss 0.2644449472427368\n",
      "______________\n",
      "epoch 1056 train loss 0.14691162109375\n",
      "val loss 0.2640461027622223\n",
      "______________\n",
      "epoch 1057 train loss 0.1537531614303589\n",
      "val loss 0.2636430859565735\n",
      "______________\n",
      "epoch 1058 train loss 0.14364194869995117\n",
      "val loss 0.26325279474258423\n",
      "______________\n",
      "epoch 1059 train loss 0.1494346261024475\n",
      "val loss 0.26285520195961\n",
      "______________\n",
      "epoch 1060 train loss 0.15195631980895996\n",
      "val loss 0.26252442598342896\n",
      "______________\n",
      "epoch 1061 train loss 0.16855791211128235\n",
      "val loss 0.26218289136886597\n",
      "______________\n",
      "epoch 1062 train loss 0.17471176385879517\n",
      "val loss 0.2618129253387451\n",
      "______________\n",
      "epoch 1063 train loss 0.15344740450382233\n",
      "val loss 0.26145485043525696\n",
      "______________\n",
      "epoch 1064 train loss 0.15068957209587097\n",
      "val loss 0.2611173689365387\n",
      "______________\n",
      "epoch 1065 train loss 0.16591086983680725\n",
      "val loss 0.26076439023017883\n",
      "______________\n",
      "epoch 1066 train loss 0.1660807728767395\n",
      "val loss 0.2604253888130188\n",
      "______________\n",
      "epoch 1067 train loss 0.17581568658351898\n",
      "val loss 0.260055273771286\n",
      "______________\n",
      "epoch 1068 train loss 0.15947388112545013\n",
      "val loss 0.25968343019485474\n",
      "______________\n",
      "epoch 1069 train loss 0.16412249207496643\n",
      "val loss 0.2593553066253662\n",
      "______________\n",
      "epoch 1070 train loss 0.15039493143558502\n",
      "val loss 0.25901222229003906\n",
      "______________\n",
      "epoch 1071 train loss 0.14958816766738892\n",
      "val loss 0.2586970627307892\n",
      "______________\n",
      "epoch 1072 train loss 0.1305251121520996\n",
      "val loss 0.25839269161224365\n",
      "______________\n",
      "epoch 1073 train loss 0.17055800557136536\n",
      "val loss 0.2580590546131134\n",
      "______________\n",
      "epoch 1074 train loss 0.1645991951227188\n",
      "val loss 0.2577074468135834\n",
      "______________\n",
      "epoch 1075 train loss 0.13700473308563232\n",
      "val loss 0.2574041187763214\n",
      "______________\n",
      "epoch 1076 train loss 0.16619914770126343\n",
      "val loss 0.2570798695087433\n",
      "______________\n",
      "epoch 1077 train loss 0.18566161394119263\n",
      "val loss 0.25673526525497437\n",
      "______________\n",
      "epoch 1078 train loss 0.15819866955280304\n",
      "val loss 0.25638142228126526\n",
      "______________\n",
      "epoch 1079 train loss 0.1664501279592514\n",
      "val loss 0.2559913694858551\n",
      "______________\n",
      "epoch 1080 train loss 0.13971135020256042\n",
      "val loss 0.25558948516845703\n",
      "______________\n",
      "epoch 1081 train loss 0.16347463428974152\n",
      "val loss 0.2551698088645935\n",
      "______________\n",
      "epoch 1082 train loss 0.139834463596344\n",
      "val loss 0.25480180978775024\n",
      "______________\n",
      "epoch 1083 train loss 0.16699962317943573\n",
      "val loss 0.25445058941841125\n",
      "______________\n",
      "epoch 1084 train loss 0.1391196995973587\n",
      "val loss 0.254133939743042\n",
      "______________\n",
      "epoch 1085 train loss 0.14529143273830414\n",
      "val loss 0.25378406047821045\n",
      "______________\n",
      "epoch 1086 train loss 0.1638423502445221\n",
      "val loss 0.2534695565700531\n",
      "______________\n",
      "epoch 1087 train loss 0.16197514533996582\n",
      "val loss 0.2531927227973938\n",
      "______________\n",
      "epoch 1088 train loss 0.1519811600446701\n",
      "val loss 0.2528848946094513\n",
      "______________\n",
      "epoch 1089 train loss 0.15353333950042725\n",
      "val loss 0.2525663375854492\n",
      "______________\n",
      "epoch 1090 train loss 0.14950713515281677\n",
      "val loss 0.25226688385009766\n",
      "______________\n",
      "epoch 1091 train loss 0.14333316683769226\n",
      "val loss 0.25200825929641724\n",
      "______________\n",
      "epoch 1092 train loss 0.15270833671092987\n",
      "val loss 0.25174641609191895\n",
      "______________\n",
      "epoch 1093 train loss 0.147737517952919\n",
      "val loss 0.2514813542366028\n",
      "______________\n",
      "epoch 1094 train loss 0.14252811670303345\n",
      "val loss 0.251201331615448\n",
      "______________\n",
      "epoch 1095 train loss 0.13812541961669922\n",
      "val loss 0.25091299414634705\n",
      "______________\n",
      "epoch 1096 train loss 0.13003389537334442\n",
      "val loss 0.25065356492996216\n",
      "______________\n",
      "epoch 1097 train loss 0.15254206955432892\n",
      "val loss 0.25039196014404297\n",
      "______________\n",
      "epoch 1098 train loss 0.153092160820961\n",
      "val loss 0.250151127576828\n",
      "______________\n",
      "epoch 1099 train loss 0.1408834606409073\n",
      "val loss 0.2498893141746521\n",
      "______________\n",
      "epoch 1100 train loss 0.12438725680112839\n",
      "val loss 0.24963369965553284\n",
      "______________\n",
      "epoch 1101 train loss 0.16274133324623108\n",
      "val loss 0.24934986233711243\n",
      "______________\n",
      "epoch 1102 train loss 0.18809980154037476\n",
      "val loss 0.24901306629180908\n",
      "______________\n",
      "epoch 1103 train loss 0.17691443860530853\n",
      "val loss 0.24865126609802246\n",
      "______________\n",
      "epoch 1104 train loss 0.14245054125785828\n",
      "val loss 0.2483072131872177\n",
      "______________\n",
      "epoch 1105 train loss 0.14739559590816498\n",
      "val loss 0.24796825647354126\n",
      "______________\n",
      "epoch 1106 train loss 0.13414964079856873\n",
      "val loss 0.24765434861183167\n",
      "______________\n",
      "epoch 1107 train loss 0.1463484764099121\n",
      "val loss 0.24732889235019684\n",
      "______________\n",
      "epoch 1108 train loss 0.14551842212677002\n",
      "val loss 0.24703815579414368\n",
      "______________\n",
      "epoch 1109 train loss 0.14352288842201233\n",
      "val loss 0.24672822654247284\n",
      "______________\n",
      "epoch 1110 train loss 0.14170578122138977\n",
      "val loss 0.24643942713737488\n",
      "______________\n",
      "epoch 1111 train loss 0.15046574175357819\n",
      "val loss 0.24616652727127075\n",
      "______________\n",
      "epoch 1112 train loss 0.16233184933662415\n",
      "val loss 0.24584336578845978\n",
      "______________\n",
      "epoch 1113 train loss 0.12358339130878448\n",
      "val loss 0.24554896354675293\n",
      "______________\n",
      "epoch 1114 train loss 0.13597357273101807\n",
      "val loss 0.2452729344367981\n",
      "______________\n",
      "epoch 1115 train loss 0.12658095359802246\n",
      "val loss 0.24502432346343994\n",
      "______________\n",
      "epoch 1116 train loss 0.13880211114883423\n",
      "val loss 0.24478328227996826\n",
      "______________\n",
      "epoch 1117 train loss 0.13372820615768433\n",
      "val loss 0.24454239010810852\n",
      "______________\n",
      "epoch 1118 train loss 0.1399083435535431\n",
      "val loss 0.24428072571754456\n",
      "______________\n",
      "epoch 1119 train loss 0.15644092857837677\n",
      "val loss 0.2439941018819809\n",
      "______________\n",
      "epoch 1120 train loss 0.12958499789237976\n",
      "val loss 0.2437259554862976\n",
      "______________\n",
      "epoch 1121 train loss 0.15499576926231384\n",
      "val loss 0.24342703819274902\n",
      "______________\n",
      "epoch 1122 train loss 0.15642258524894714\n",
      "val loss 0.24313688278198242\n",
      "______________\n",
      "epoch 1123 train loss 0.11701361835002899\n",
      "val loss 0.2428831160068512\n",
      "______________\n",
      "epoch 1124 train loss 0.15188921988010406\n",
      "val loss 0.2425796538591385\n",
      "______________\n",
      "epoch 1125 train loss 0.13315409421920776\n",
      "val loss 0.24229441583156586\n",
      "______________\n",
      "epoch 1126 train loss 0.1439797282218933\n",
      "val loss 0.24202825129032135\n",
      "______________\n",
      "epoch 1127 train loss 0.15276002883911133\n",
      "val loss 0.24176691472530365\n",
      "______________\n",
      "epoch 1128 train loss 0.12226779758930206\n",
      "val loss 0.241530179977417\n",
      "______________\n",
      "epoch 1129 train loss 0.15561801195144653\n",
      "val loss 0.24126726388931274\n",
      "______________\n",
      "epoch 1130 train loss 0.13134607672691345\n",
      "val loss 0.24102288484573364\n",
      "______________\n",
      "epoch 1131 train loss 0.16443827748298645\n",
      "val loss 0.24071791768074036\n",
      "______________\n",
      "epoch 1132 train loss 0.12867359817028046\n",
      "val loss 0.24042901396751404\n",
      "______________\n",
      "epoch 1133 train loss 0.1664600372314453\n",
      "val loss 0.24012771248817444\n",
      "______________\n",
      "epoch 1134 train loss 0.15094834566116333\n",
      "val loss 0.23982274532318115\n",
      "______________\n",
      "epoch 1135 train loss 0.13786658644676208\n",
      "val loss 0.23954829573631287\n",
      "______________\n",
      "epoch 1136 train loss 0.13410969078540802\n",
      "val loss 0.2392735630273819\n",
      "______________\n",
      "epoch 1137 train loss 0.15148228406906128\n",
      "val loss 0.2390279769897461\n",
      "______________\n",
      "epoch 1138 train loss 0.13941490650177002\n",
      "val loss 0.23876836895942688\n",
      "______________\n",
      "epoch 1139 train loss 0.14730140566825867\n",
      "val loss 0.23850451409816742\n",
      "______________\n",
      "epoch 1140 train loss 0.14345023036003113\n",
      "val loss 0.23821115493774414\n",
      "______________\n",
      "epoch 1141 train loss 0.13179467618465424\n",
      "val loss 0.23792582750320435\n",
      "______________\n",
      "epoch 1142 train loss 0.1586066484451294\n",
      "val loss 0.23762300610542297\n",
      "______________\n",
      "epoch 1143 train loss 0.11141949146986008\n",
      "val loss 0.23734919726848602\n",
      "______________\n",
      "epoch 1144 train loss 0.15662796795368195\n",
      "val loss 0.2370491474866867\n",
      "______________\n",
      "epoch 1145 train loss 0.11368274688720703\n",
      "val loss 0.23680175840854645\n",
      "______________\n",
      "epoch 1146 train loss 0.16785046458244324\n",
      "val loss 0.23653843998908997\n",
      "______________\n",
      "epoch 1147 train loss 0.1488218903541565\n",
      "val loss 0.2362874448299408\n",
      "______________\n",
      "epoch 1148 train loss 0.19091705977916718\n",
      "val loss 0.23595300316810608\n",
      "______________\n",
      "epoch 1149 train loss 0.17421294748783112\n",
      "val loss 0.23557549715042114\n",
      "______________\n",
      "epoch 1150 train loss 0.15583913028240204\n",
      "val loss 0.23518937826156616\n",
      "______________\n",
      "epoch 1151 train loss 0.15271779894828796\n",
      "val loss 0.2348005175590515\n",
      "______________\n",
      "epoch 1152 train loss 0.12705513834953308\n",
      "val loss 0.23444311320781708\n",
      "______________\n",
      "epoch 1153 train loss 0.13752058148384094\n",
      "val loss 0.234100341796875\n",
      "______________\n",
      "epoch 1154 train loss 0.13433191180229187\n",
      "val loss 0.2337627410888672\n",
      "______________\n",
      "epoch 1155 train loss 0.15101759135723114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 0.23342807590961456\n",
      "______________\n",
      "epoch 1156 train loss 0.12820357084274292\n",
      "val loss 0.23309406638145447\n",
      "______________\n",
      "epoch 1157 train loss 0.13428595662117004\n",
      "val loss 0.2327612340450287\n",
      "______________\n",
      "epoch 1158 train loss 0.12364634871482849\n",
      "val loss 0.23244836926460266\n",
      "______________\n",
      "epoch 1159 train loss 0.13025644421577454\n",
      "val loss 0.2321535050868988\n",
      "______________\n",
      "epoch 1160 train loss 0.13248591125011444\n",
      "val loss 0.23187649250030518\n",
      "______________\n",
      "epoch 1161 train loss 0.14368698000907898\n",
      "val loss 0.23158834874629974\n",
      "______________\n",
      "epoch 1162 train loss 0.14191806316375732\n",
      "val loss 0.23129770159721375\n",
      "______________\n",
      "epoch 1163 train loss 0.11413101851940155\n",
      "val loss 0.23104722797870636\n",
      "______________\n",
      "epoch 1164 train loss 0.16127288341522217\n",
      "val loss 0.2307988405227661\n",
      "______________\n",
      "epoch 1165 train loss 0.11613081395626068\n",
      "val loss 0.23056258261203766\n",
      "______________\n",
      "epoch 1166 train loss 0.14574715495109558\n",
      "val loss 0.23031604290008545\n",
      "______________\n",
      "epoch 1167 train loss 0.12851916253566742\n",
      "val loss 0.2300841361284256\n",
      "______________\n",
      "epoch 1168 train loss 0.15016447007656097\n",
      "val loss 0.2298106998205185\n",
      "______________\n",
      "epoch 1169 train loss 0.11561408638954163\n",
      "val loss 0.22958451509475708\n",
      "______________\n",
      "epoch 1170 train loss 0.13623544573783875\n",
      "val loss 0.22939996421337128\n",
      "______________\n",
      "epoch 1171 train loss 0.12548398971557617\n",
      "val loss 0.2292344868183136\n",
      "______________\n",
      "epoch 1172 train loss 0.12291938066482544\n",
      "val loss 0.22908279299736023\n",
      "______________\n",
      "epoch 1173 train loss 0.16817085444927216\n",
      "val loss 0.22891324758529663\n",
      "______________\n",
      "epoch 1174 train loss 0.14843204617500305\n",
      "val loss 0.2286950647830963\n",
      "______________\n",
      "epoch 1175 train loss 0.12010269612073898\n",
      "val loss 0.2284788191318512\n",
      "______________\n",
      "epoch 1176 train loss 0.1352807879447937\n",
      "val loss 0.22824741899967194\n",
      "______________\n",
      "epoch 1177 train loss 0.13103923201560974\n",
      "val loss 0.22800543904304504\n",
      "______________\n",
      "epoch 1178 train loss 0.14185424149036407\n",
      "val loss 0.2277921885251999\n",
      "______________\n",
      "epoch 1179 train loss 0.14131656289100647\n",
      "val loss 0.22752505540847778\n",
      "______________\n",
      "epoch 1180 train loss 0.12172780930995941\n",
      "val loss 0.22724422812461853\n",
      "______________\n",
      "epoch 1181 train loss 0.13005168735980988\n",
      "val loss 0.227005273103714\n",
      "______________\n",
      "epoch 1182 train loss 0.11620650440454483\n",
      "val loss 0.22675731778144836\n",
      "______________\n",
      "epoch 1183 train loss 0.12038296461105347\n",
      "val loss 0.22651232779026031\n",
      "______________\n",
      "epoch 1184 train loss 0.13820764422416687\n",
      "val loss 0.22625888884067535\n",
      "______________\n",
      "epoch 1185 train loss 0.13049235939979553\n",
      "val loss 0.22603058815002441\n",
      "______________\n",
      "epoch 1186 train loss 0.17408908903598785\n",
      "val loss 0.2257312536239624\n",
      "______________\n",
      "epoch 1187 train loss 0.12971210479736328\n",
      "val loss 0.22543640434741974\n",
      "______________\n",
      "epoch 1188 train loss 0.14536063373088837\n",
      "val loss 0.2251284271478653\n",
      "______________\n",
      "epoch 1189 train loss 0.13478878140449524\n",
      "val loss 0.22484517097473145\n",
      "______________\n",
      "epoch 1190 train loss 0.12902528047561646\n",
      "val loss 0.2246328592300415\n",
      "______________\n",
      "epoch 1191 train loss 0.1383572816848755\n",
      "val loss 0.22439485788345337\n",
      "______________\n",
      "epoch 1192 train loss 0.12409967184066772\n",
      "val loss 0.22418299317359924\n",
      "______________\n",
      "epoch 1193 train loss 0.1315295398235321\n",
      "val loss 0.22397533059120178\n",
      "______________\n",
      "epoch 1194 train loss 0.131455659866333\n",
      "val loss 0.2237405776977539\n",
      "______________\n",
      "epoch 1195 train loss 0.11401502788066864\n",
      "val loss 0.2235102653503418\n",
      "______________\n",
      "epoch 1196 train loss 0.12175242602825165\n",
      "val loss 0.2232973277568817\n",
      "______________\n",
      "epoch 1197 train loss 0.11141632497310638\n",
      "val loss 0.22311675548553467\n",
      "______________\n",
      "epoch 1198 train loss 0.1120593398809433\n",
      "val loss 0.2229190617799759\n",
      "______________\n",
      "epoch 1199 train loss 0.12213670462369919\n",
      "val loss 0.22275620698928833\n",
      "______________\n",
      "epoch 1200 train loss 0.13056319952011108\n",
      "val loss 0.22259727120399475\n",
      "______________\n",
      "epoch 1201 train loss 0.16628721356391907\n",
      "val loss 0.22241908311843872\n",
      "______________\n",
      "epoch 1202 train loss 0.11342576891183853\n",
      "val loss 0.22224147617816925\n",
      "______________\n",
      "epoch 1203 train loss 0.1415529102087021\n",
      "val loss 0.22208523750305176\n",
      "______________\n",
      "epoch 1204 train loss 0.15475161373615265\n",
      "val loss 0.22189772129058838\n",
      "______________\n",
      "epoch 1205 train loss 0.12191572040319443\n",
      "val loss 0.22170168161392212\n",
      "______________\n",
      "epoch 1206 train loss 0.10587029904127121\n",
      "val loss 0.22150252759456635\n",
      "______________\n",
      "epoch 1207 train loss 0.1274033635854721\n",
      "val loss 0.221303790807724\n",
      "______________\n",
      "epoch 1208 train loss 0.13721151649951935\n",
      "val loss 0.22106823325157166\n",
      "______________\n",
      "epoch 1209 train loss 0.13587695360183716\n",
      "val loss 0.220808744430542\n",
      "______________\n",
      "epoch 1210 train loss 0.10977006703615189\n",
      "val loss 0.22056114673614502\n",
      "______________\n",
      "epoch 1211 train loss 0.11413213610649109\n",
      "val loss 0.22028616070747375\n",
      "______________\n",
      "epoch 1212 train loss 0.1243872344493866\n",
      "val loss 0.2200174331665039\n",
      "______________\n",
      "epoch 1213 train loss 0.13660427927970886\n",
      "val loss 0.2197372317314148\n",
      "______________\n",
      "epoch 1214 train loss 0.13973036408424377\n",
      "val loss 0.21941274404525757\n",
      "______________\n",
      "epoch 1215 train loss 0.13393867015838623\n",
      "val loss 0.21907342970371246\n",
      "______________\n",
      "epoch 1216 train loss 0.16141536831855774\n",
      "val loss 0.21868236362934113\n",
      "______________\n",
      "epoch 1217 train loss 0.12870654463768005\n",
      "val loss 0.21832892298698425\n",
      "______________\n",
      "epoch 1218 train loss 0.09637703001499176\n",
      "val loss 0.21802273392677307\n",
      "______________\n",
      "epoch 1219 train loss 0.13280577957630157\n",
      "val loss 0.21771138906478882\n",
      "______________\n",
      "epoch 1220 train loss 0.12555980682373047\n",
      "val loss 0.2174052894115448\n",
      "______________\n",
      "epoch 1221 train loss 0.11771731078624725\n",
      "val loss 0.21711669862270355\n",
      "______________\n",
      "epoch 1222 train loss 0.11892545223236084\n",
      "val loss 0.21682113409042358\n",
      "______________\n",
      "epoch 1223 train loss 0.14192217588424683\n",
      "val loss 0.21652698516845703\n",
      "______________\n",
      "epoch 1224 train loss 0.13916227221488953\n",
      "val loss 0.21622627973556519\n",
      "______________\n",
      "epoch 1225 train loss 0.12151405960321426\n",
      "val loss 0.215952530503273\n",
      "______________\n",
      "epoch 1226 train loss 0.15124282240867615\n",
      "val loss 0.21566805243492126\n",
      "______________\n",
      "epoch 1227 train loss 0.1352779120206833\n",
      "val loss 0.21537861227989197\n",
      "______________\n",
      "epoch 1228 train loss 0.13788321614265442\n",
      "val loss 0.21507450938224792\n",
      "______________\n",
      "epoch 1229 train loss 0.12247122824192047\n",
      "val loss 0.21473145484924316\n",
      "______________\n",
      "epoch 1230 train loss 0.11467800289392471\n",
      "val loss 0.21439749002456665\n",
      "______________\n",
      "epoch 1231 train loss 0.10276924818754196\n",
      "val loss 0.2140926867723465\n",
      "______________\n",
      "epoch 1232 train loss 0.14599324762821198\n",
      "val loss 0.21379145979881287\n",
      "______________\n",
      "epoch 1233 train loss 0.131307452917099\n",
      "val loss 0.21352708339691162\n",
      "______________\n",
      "epoch 1234 train loss 0.13245615363121033\n",
      "val loss 0.21328820288181305\n",
      "______________\n",
      "epoch 1235 train loss 0.14225547015666962\n",
      "val loss 0.21303337812423706\n",
      "______________\n",
      "epoch 1236 train loss 0.11883501708507538\n",
      "val loss 0.21277394890785217\n",
      "______________\n",
      "epoch 1237 train loss 0.11962361633777618\n",
      "val loss 0.21253702044487\n",
      "______________\n",
      "epoch 1238 train loss 0.1480504274368286\n",
      "val loss 0.2122541069984436\n",
      "______________\n",
      "epoch 1239 train loss 0.14848677814006805\n",
      "val loss 0.21195997297763824\n",
      "______________\n",
      "epoch 1240 train loss 0.11702987551689148\n",
      "val loss 0.21168330311775208\n",
      "______________\n",
      "epoch 1241 train loss 0.13020263612270355\n",
      "val loss 0.21143199503421783\n",
      "______________\n",
      "epoch 1242 train loss 0.1149279773235321\n",
      "val loss 0.21121671795845032\n",
      "______________\n",
      "epoch 1243 train loss 0.11910983920097351\n",
      "val loss 0.21103428304195404\n",
      "______________\n",
      "epoch 1244 train loss 0.11728024482727051\n",
      "val loss 0.2108544409275055\n",
      "______________\n",
      "epoch 1245 train loss 0.1360693871974945\n",
      "val loss 0.21066996455192566\n",
      "______________\n",
      "epoch 1246 train loss 0.12185882031917572\n",
      "val loss 0.21048957109451294\n",
      "______________\n",
      "epoch 1247 train loss 0.1153399720788002\n",
      "val loss 0.21029150485992432\n",
      "______________\n",
      "epoch 1248 train loss 0.1215742900967598\n",
      "val loss 0.21010838449001312\n",
      "______________\n",
      "epoch 1249 train loss 0.12255772203207016\n",
      "val loss 0.20988857746124268\n",
      "______________\n",
      "epoch 1250 train loss 0.13238662481307983\n",
      "val loss 0.20969946682453156\n",
      "______________\n",
      "epoch 1251 train loss 0.11063343286514282\n",
      "val loss 0.2095310091972351\n",
      "______________\n",
      "epoch 1252 train loss 0.12370170652866364\n",
      "val loss 0.20936068892478943\n",
      "______________\n",
      "epoch 1253 train loss 0.1280285120010376\n",
      "val loss 0.20917245745658875\n",
      "______________\n",
      "epoch 1254 train loss 0.1352173089981079\n",
      "val loss 0.2090146541595459\n",
      "______________\n",
      "epoch 1255 train loss 0.09184259176254272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 0.20887017250061035\n",
      "______________\n",
      "epoch 1256 train loss 0.11570204794406891\n",
      "val loss 0.2087177038192749\n",
      "______________\n",
      "epoch 1257 train loss 0.12707604467868805\n",
      "val loss 0.208563432097435\n",
      "______________\n",
      "epoch 1258 train loss 0.11655738949775696\n",
      "val loss 0.2083624005317688\n",
      "______________\n",
      "epoch 1259 train loss 0.11846226453781128\n",
      "val loss 0.2081623673439026\n",
      "______________\n",
      "epoch 1260 train loss 0.1038283258676529\n",
      "val loss 0.20801550149917603\n",
      "______________\n",
      "epoch 1261 train loss 0.13801036775112152\n",
      "val loss 0.20787571370601654\n",
      "______________\n",
      "epoch 1262 train loss 0.13895854353904724\n",
      "val loss 0.20771439373493195\n",
      "______________\n",
      "epoch 1263 train loss 0.10381878912448883\n",
      "val loss 0.207579106092453\n",
      "______________\n",
      "epoch 1264 train loss 0.12461298704147339\n",
      "val loss 0.20742595195770264\n",
      "______________\n",
      "epoch 1265 train loss 0.12666232883930206\n",
      "val loss 0.20722663402557373\n",
      "______________\n",
      "epoch 1266 train loss 0.12072121351957321\n",
      "val loss 0.2070099413394928\n",
      "______________\n",
      "epoch 1267 train loss 0.11801544576883316\n",
      "val loss 0.2067873477935791\n",
      "______________\n",
      "epoch 1268 train loss 0.13447913527488708\n",
      "val loss 0.20655348896980286\n",
      "______________\n",
      "epoch 1269 train loss 0.11777827143669128\n",
      "val loss 0.20628789067268372\n",
      "______________\n",
      "epoch 1270 train loss 0.11982988566160202\n",
      "val loss 0.20603588223457336\n",
      "______________\n",
      "epoch 1271 train loss 0.1219479963183403\n",
      "val loss 0.2057720422744751\n",
      "______________\n",
      "epoch 1272 train loss 0.1608206033706665\n",
      "val loss 0.20548510551452637\n",
      "______________\n",
      "epoch 1273 train loss 0.12392595410346985\n",
      "val loss 0.2051752656698227\n",
      "______________\n",
      "epoch 1274 train loss 0.11649593710899353\n",
      "val loss 0.20486435294151306\n",
      "______________\n",
      "epoch 1275 train loss 0.14502331614494324\n",
      "val loss 0.20451664924621582\n",
      "______________\n",
      "epoch 1276 train loss 0.13662415742874146\n",
      "val loss 0.2041688859462738\n",
      "______________\n",
      "epoch 1277 train loss 0.12845519185066223\n",
      "val loss 0.20379067957401276\n",
      "______________\n",
      "epoch 1278 train loss 0.12619759142398834\n",
      "val loss 0.2033875584602356\n",
      "______________\n",
      "epoch 1279 train loss 0.11332626640796661\n",
      "val loss 0.2030259072780609\n",
      "______________\n",
      "epoch 1280 train loss 0.11796185374259949\n",
      "val loss 0.20267963409423828\n",
      "______________\n",
      "epoch 1281 train loss 0.12279060482978821\n",
      "val loss 0.20229525864124298\n",
      "______________\n",
      "epoch 1282 train loss 0.10977977514266968\n",
      "val loss 0.20188525319099426\n",
      "______________\n",
      "epoch 1283 train loss 0.12735921144485474\n",
      "val loss 0.20148876309394836\n",
      "______________\n",
      "epoch 1284 train loss 0.08702921867370605\n",
      "val loss 0.20113804936408997\n",
      "______________\n",
      "epoch 1285 train loss 0.12050385773181915\n",
      "val loss 0.20082509517669678\n",
      "______________\n",
      "epoch 1286 train loss 0.12415625154972076\n",
      "val loss 0.20051006972789764\n",
      "______________\n",
      "epoch 1287 train loss 0.11331533640623093\n",
      "val loss 0.20023220777511597\n",
      "______________\n",
      "epoch 1288 train loss 0.1111283078789711\n",
      "val loss 0.1999625265598297\n",
      "______________\n",
      "epoch 1289 train loss 0.10494932532310486\n",
      "val loss 0.1996956169605255\n",
      "______________\n",
      "epoch 1290 train loss 0.11396249383687973\n",
      "val loss 0.19946891069412231\n",
      "______________\n",
      "epoch 1291 train loss 0.17559081315994263\n",
      "val loss 0.1991758644580841\n",
      "______________\n",
      "epoch 1292 train loss 0.13660886883735657\n",
      "val loss 0.19888974726200104\n",
      "______________\n",
      "epoch 1293 train loss 0.1436677873134613\n",
      "val loss 0.19857287406921387\n",
      "______________\n",
      "epoch 1294 train loss 0.1297115981578827\n",
      "val loss 0.19825106859207153\n",
      "______________\n",
      "epoch 1295 train loss 0.13308176398277283\n",
      "val loss 0.19789935648441315\n",
      "______________\n",
      "epoch 1296 train loss 0.09911205619573593\n",
      "val loss 0.19757825136184692\n",
      "______________\n",
      "epoch 1297 train loss 0.11604803800582886\n",
      "val loss 0.19727984070777893\n",
      "______________\n",
      "epoch 1298 train loss 0.11533889919519424\n",
      "val loss 0.1970175802707672\n",
      "______________\n",
      "epoch 1299 train loss 0.11028939485549927\n",
      "val loss 0.19679728150367737\n",
      "______________\n",
      "epoch 1300 train loss 0.09783987700939178\n",
      "val loss 0.19658704102039337\n",
      "______________\n",
      "epoch 1301 train loss 0.13582205772399902\n",
      "val loss 0.19634459912776947\n",
      "______________\n",
      "epoch 1302 train loss 0.09126655757427216\n",
      "val loss 0.1961301863193512\n",
      "______________\n",
      "epoch 1303 train loss 0.11244435608386993\n",
      "val loss 0.19587284326553345\n",
      "______________\n",
      "epoch 1304 train loss 0.09946928918361664\n",
      "val loss 0.1956252157688141\n",
      "______________\n",
      "epoch 1305 train loss 0.13266292214393616\n",
      "val loss 0.1953614056110382\n",
      "______________\n",
      "epoch 1306 train loss 0.10441957414150238\n",
      "val loss 0.19514057040214539\n",
      "______________\n",
      "epoch 1307 train loss 0.1261725276708603\n",
      "val loss 0.1949160248041153\n",
      "______________\n",
      "epoch 1308 train loss 0.1323574185371399\n",
      "val loss 0.19468571245670319\n",
      "______________\n",
      "epoch 1309 train loss 0.13478687405586243\n",
      "val loss 0.19446969032287598\n",
      "______________\n",
      "epoch 1310 train loss 0.11372402310371399\n",
      "val loss 0.19423837959766388\n",
      "______________\n",
      "epoch 1311 train loss 0.11697651445865631\n",
      "val loss 0.19401630759239197\n",
      "______________\n",
      "epoch 1312 train loss 0.10678261518478394\n",
      "val loss 0.19379009306430817\n",
      "______________\n",
      "epoch 1313 train loss 0.10284029692411423\n",
      "val loss 0.1935867965221405\n",
      "______________\n",
      "epoch 1314 train loss 0.12782803177833557\n",
      "val loss 0.1933329701423645\n",
      "______________\n",
      "epoch 1315 train loss 0.14677506685256958\n",
      "val loss 0.1930752694606781\n",
      "______________\n",
      "epoch 1316 train loss 0.1169210746884346\n",
      "val loss 0.1928180754184723\n",
      "______________\n",
      "epoch 1317 train loss 0.1188531294465065\n",
      "val loss 0.19255492091178894\n",
      "______________\n",
      "epoch 1318 train loss 0.12961584329605103\n",
      "val loss 0.19229552149772644\n",
      "______________\n",
      "epoch 1319 train loss 0.11157040297985077\n",
      "val loss 0.1920628398656845\n",
      "______________\n",
      "epoch 1320 train loss 0.1189449280500412\n",
      "val loss 0.19184428453445435\n",
      "______________\n",
      "epoch 1321 train loss 0.09637308865785599\n",
      "val loss 0.1916588544845581\n",
      "______________\n",
      "epoch 1322 train loss 0.15040111541748047\n",
      "val loss 0.19147399067878723\n",
      "______________\n",
      "epoch 1323 train loss 0.11815624684095383\n",
      "val loss 0.19129368662834167\n",
      "______________\n",
      "epoch 1324 train loss 0.11116084456443787\n",
      "val loss 0.19111239910125732\n",
      "______________\n",
      "epoch 1325 train loss 0.11466371268033981\n",
      "val loss 0.19091740250587463\n",
      "______________\n",
      "epoch 1326 train loss 0.10496509820222855\n",
      "val loss 0.19074830412864685\n",
      "______________\n",
      "epoch 1327 train loss 0.14063897728919983\n",
      "val loss 0.190545916557312\n",
      "______________\n",
      "epoch 1328 train loss 0.12046423554420471\n",
      "val loss 0.19037401676177979\n",
      "______________\n",
      "epoch 1329 train loss 0.15552715957164764\n",
      "val loss 0.1901894509792328\n",
      "______________\n",
      "epoch 1330 train loss 0.09770653396844864\n",
      "val loss 0.1900200992822647\n",
      "______________\n",
      "epoch 1331 train loss 0.12560346722602844\n",
      "val loss 0.18984118103981018\n",
      "______________\n",
      "epoch 1332 train loss 0.126691997051239\n",
      "val loss 0.18967297673225403\n",
      "______________\n",
      "epoch 1333 train loss 0.10828731209039688\n",
      "val loss 0.18951670825481415\n",
      "______________\n",
      "epoch 1334 train loss 0.10246336460113525\n",
      "val loss 0.18938125669956207\n",
      "______________\n",
      "epoch 1335 train loss 0.11226630210876465\n",
      "val loss 0.18926364183425903\n",
      "______________\n",
      "epoch 1336 train loss 0.12986557185649872\n",
      "val loss 0.1891322284936905\n",
      "______________\n",
      "epoch 1337 train loss 0.10673349350690842\n",
      "val loss 0.1890251785516739\n",
      "______________\n",
      "epoch 1338 train loss 0.1139126569032669\n",
      "val loss 0.18890893459320068\n",
      "______________\n",
      "epoch 1339 train loss 0.10910989344120026\n",
      "val loss 0.1887960433959961\n",
      "______________\n",
      "epoch 1340 train loss 0.12041620910167694\n",
      "val loss 0.1887182593345642\n",
      "______________\n",
      "epoch 1341 train loss 0.12369270622730255\n",
      "val loss 0.18863564729690552\n",
      "______________\n",
      "epoch 1342 train loss 0.11873291432857513\n",
      "val loss 0.18851003050804138\n",
      "______________\n",
      "epoch 1343 train loss 0.13095219433307648\n",
      "val loss 0.18835100531578064\n",
      "______________\n",
      "epoch 1344 train loss 0.0850134789943695\n",
      "val loss 0.1882191002368927\n",
      "______________\n",
      "epoch 1345 train loss 0.12674590945243835\n",
      "val loss 0.18808448314666748\n",
      "______________\n",
      "epoch 1346 train loss 0.11139293015003204\n",
      "val loss 0.18794885277748108\n",
      "______________\n",
      "epoch 1347 train loss 0.11056528985500336\n",
      "val loss 0.1877906322479248\n",
      "______________\n",
      "epoch 1348 train loss 0.1120806634426117\n",
      "val loss 0.18765246868133545\n",
      "______________\n",
      "epoch 1349 train loss 0.10459260642528534\n",
      "val loss 0.1875297725200653\n",
      "______________\n",
      "epoch 1350 train loss 0.11218974739313126\n",
      "val loss 0.18739429116249084\n",
      "______________\n",
      "epoch 1351 train loss 0.11407776176929474\n",
      "val loss 0.18725748360157013\n",
      "______________\n",
      "epoch 1352 train loss 0.09522822499275208\n",
      "val loss 0.18713247776031494\n",
      "______________\n",
      "epoch 1353 train loss 0.11987689137458801\n",
      "val loss 0.1869765818119049\n",
      "______________\n",
      "epoch 1354 train loss 0.10520178824663162\n",
      "val loss 0.18683895468711853\n",
      "______________\n",
      "epoch 1355 train loss 0.1097358763217926\n",
      "val loss 0.18670199811458588\n",
      "______________\n",
      "epoch 1356 train loss 0.08950568735599518\n",
      "val loss 0.18658976256847382\n",
      "______________\n",
      "epoch 1357 train loss 0.12302155792713165\n",
      "val loss 0.1864534616470337\n",
      "______________\n",
      "epoch 1358 train loss 0.10434385389089584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 0.18631722033023834\n",
      "______________\n",
      "epoch 1359 train loss 0.1071953997015953\n",
      "val loss 0.18616081774234772\n",
      "______________\n",
      "epoch 1360 train loss 0.09649166464805603\n",
      "val loss 0.1859986037015915\n",
      "______________\n",
      "epoch 1361 train loss 0.11857417225837708\n",
      "val loss 0.1858362853527069\n",
      "______________\n",
      "epoch 1362 train loss 0.09162862598896027\n",
      "val loss 0.18569491803646088\n",
      "______________\n",
      "epoch 1363 train loss 0.10403409600257874\n",
      "val loss 0.18553954362869263\n",
      "______________\n",
      "epoch 1364 train loss 0.11686218529939651\n",
      "val loss 0.18536928296089172\n",
      "______________\n",
      "epoch 1365 train loss 0.10814688354730606\n",
      "val loss 0.18520468473434448\n",
      "______________\n",
      "epoch 1366 train loss 0.11856906116008759\n",
      "val loss 0.18498235940933228\n",
      "______________\n",
      "epoch 1367 train loss 0.09912233054637909\n",
      "val loss 0.18480348587036133\n",
      "______________\n",
      "epoch 1368 train loss 0.09842359274625778\n",
      "val loss 0.18465149402618408\n",
      "______________\n",
      "epoch 1369 train loss 0.10622565448284149\n",
      "val loss 0.1845298707485199\n",
      "______________\n",
      "epoch 1370 train loss 0.11816570162773132\n",
      "val loss 0.18440309166908264\n",
      "______________\n",
      "epoch 1371 train loss 0.12628160417079926\n",
      "val loss 0.184260755777359\n",
      "______________\n",
      "epoch 1372 train loss 0.09765233099460602\n",
      "val loss 0.1841048002243042\n",
      "______________\n",
      "epoch 1373 train loss 0.10308201611042023\n",
      "val loss 0.18394091725349426\n",
      "______________\n",
      "epoch 1374 train loss 0.1131356805562973\n",
      "val loss 0.18380725383758545\n",
      "______________\n",
      "epoch 1375 train loss 0.0943467766046524\n",
      "val loss 0.1836712658405304\n",
      "______________\n",
      "epoch 1376 train loss 0.09184686094522476\n",
      "val loss 0.1835433542728424\n",
      "______________\n",
      "epoch 1377 train loss 0.12053347378969193\n",
      "val loss 0.18341107666492462\n",
      "______________\n",
      "epoch 1378 train loss 0.1284838616847992\n",
      "val loss 0.18328151106834412\n",
      "______________\n",
      "epoch 1379 train loss 0.12210722267627716\n",
      "val loss 0.18314599990844727\n",
      "______________\n",
      "epoch 1380 train loss 0.15194419026374817\n",
      "val loss 0.18299847841262817\n",
      "______________\n",
      "epoch 1381 train loss 0.11350998282432556\n",
      "val loss 0.18285125494003296\n",
      "______________\n",
      "epoch 1382 train loss 0.10217304527759552\n",
      "val loss 0.1826988160610199\n",
      "______________\n",
      "epoch 1383 train loss 0.10180754959583282\n",
      "val loss 0.18253973126411438\n",
      "______________\n",
      "epoch 1384 train loss 0.10609371960163116\n",
      "val loss 0.18238255381584167\n",
      "______________\n",
      "epoch 1385 train loss 0.12672880291938782\n",
      "val loss 0.18219587206840515\n",
      "______________\n",
      "epoch 1386 train loss 0.13159385323524475\n",
      "val loss 0.18199720978736877\n",
      "______________\n",
      "epoch 1387 train loss 0.11025164276361465\n",
      "val loss 0.1818133294582367\n",
      "______________\n",
      "epoch 1388 train loss 0.09403099119663239\n",
      "val loss 0.1816510558128357\n",
      "______________\n",
      "epoch 1389 train loss 0.1352922022342682\n",
      "val loss 0.18146935105323792\n",
      "______________\n",
      "epoch 1390 train loss 0.10307584702968597\n",
      "val loss 0.18132372200489044\n",
      "______________\n",
      "epoch 1391 train loss 0.11730847507715225\n",
      "val loss 0.18116433918476105\n",
      "______________\n",
      "epoch 1392 train loss 0.11062256991863251\n",
      "val loss 0.1809709668159485\n",
      "______________\n",
      "epoch 1393 train loss 0.10736899077892303\n",
      "val loss 0.18076249957084656\n",
      "______________\n",
      "epoch 1394 train loss 0.12362246215343475\n",
      "val loss 0.18055853247642517\n",
      "______________\n",
      "epoch 1395 train loss 0.10080835223197937\n",
      "val loss 0.18039828538894653\n",
      "______________\n",
      "epoch 1396 train loss 0.0999215617775917\n",
      "val loss 0.18022137880325317\n",
      "______________\n",
      "epoch 1397 train loss 0.1055416464805603\n",
      "val loss 0.1800418496131897\n",
      "______________\n",
      "epoch 1398 train loss 0.11394050717353821\n",
      "val loss 0.17983880639076233\n",
      "______________\n",
      "epoch 1399 train loss 0.1286359429359436\n",
      "val loss 0.17961111664772034\n",
      "______________\n",
      "epoch 1400 train loss 0.12366386502981186\n",
      "val loss 0.1793457269668579\n",
      "______________\n",
      "epoch 1401 train loss 0.10073055326938629\n",
      "val loss 0.17915715277194977\n",
      "______________\n",
      "epoch 1402 train loss 0.09680488705635071\n",
      "val loss 0.17898067831993103\n",
      "______________\n",
      "epoch 1403 train loss 0.11015072464942932\n",
      "val loss 0.17880377173423767\n",
      "______________\n",
      "epoch 1404 train loss 0.09219043701887131\n",
      "val loss 0.17864534258842468\n",
      "______________\n",
      "epoch 1405 train loss 0.10073171555995941\n",
      "val loss 0.1785089373588562\n",
      "______________\n",
      "epoch 1406 train loss 0.09880824387073517\n",
      "val loss 0.17842286825180054\n",
      "______________\n",
      "epoch 1407 train loss 0.10396749526262283\n",
      "val loss 0.17834201455116272\n",
      "______________\n",
      "epoch 1408 train loss 0.11391742527484894\n",
      "val loss 0.178268164396286\n",
      "______________\n",
      "epoch 1409 train loss 0.0982167050242424\n",
      "val loss 0.17819178104400635\n",
      "______________\n",
      "epoch 1410 train loss 0.11344307661056519\n",
      "val loss 0.17811724543571472\n",
      "______________\n",
      "epoch 1411 train loss 0.1041886955499649\n",
      "val loss 0.17806179821491241\n",
      "______________\n",
      "epoch 1412 train loss 0.11779801547527313\n",
      "val loss 0.17794331908226013\n",
      "______________\n",
      "epoch 1413 train loss 0.1125420331954956\n",
      "val loss 0.17781387269496918\n",
      "______________\n",
      "epoch 1414 train loss 0.12479905784130096\n",
      "val loss 0.17764389514923096\n",
      "______________\n",
      "epoch 1415 train loss 0.11771930754184723\n",
      "val loss 0.17748351395130157\n",
      "______________\n",
      "epoch 1416 train loss 0.11005991697311401\n",
      "val loss 0.17733609676361084\n",
      "______________\n",
      "epoch 1417 train loss 0.09817218780517578\n",
      "val loss 0.17719495296478271\n",
      "______________\n",
      "epoch 1418 train loss 0.09489947557449341\n",
      "val loss 0.17706550657749176\n",
      "______________\n",
      "epoch 1419 train loss 0.08134965598583221\n",
      "val loss 0.17697520554065704\n",
      "______________\n",
      "epoch 1420 train loss 0.10551021993160248\n",
      "val loss 0.17684325575828552\n",
      "______________\n",
      "epoch 1421 train loss 0.11932113021612167\n",
      "val loss 0.17668165266513824\n",
      "______________\n",
      "epoch 1422 train loss 0.09603284299373627\n",
      "val loss 0.17655636370182037\n",
      "______________\n",
      "epoch 1423 train loss 0.11859268695116043\n",
      "val loss 0.17645546793937683\n",
      "______________\n",
      "epoch 1424 train loss 0.08216385543346405\n",
      "val loss 0.1763826161623001\n",
      "______________\n",
      "epoch 1425 train loss 0.1088394746184349\n",
      "val loss 0.17630887031555176\n",
      "______________\n",
      "epoch 1426 train loss 0.093726746737957\n",
      "val loss 0.17628113925457\n",
      "______________\n",
      "epoch 1427 train loss 0.1243925541639328\n",
      "val loss 0.1762389987707138\n",
      "______________\n",
      "epoch 1428 train loss 0.10594268143177032\n",
      "val loss 0.1761815845966339\n",
      "______________\n",
      "epoch 1429 train loss 0.0905320793390274\n",
      "val loss 0.17609667778015137\n",
      "______________\n",
      "epoch 1430 train loss 0.1256941556930542\n",
      "val loss 0.17597350478172302\n",
      "______________\n",
      "epoch 1431 train loss 0.1157674789428711\n",
      "val loss 0.1758010983467102\n",
      "______________\n",
      "epoch 1432 train loss 0.0922929123044014\n",
      "val loss 0.1756935715675354\n",
      "______________\n",
      "epoch 1433 train loss 0.09378153085708618\n",
      "val loss 0.17558476328849792\n",
      "______________\n",
      "epoch 1434 train loss 0.10733100771903992\n",
      "val loss 0.17548903822898865\n",
      "______________\n",
      "epoch 1435 train loss 0.10527084767818451\n",
      "val loss 0.17537519335746765\n",
      "______________\n",
      "epoch 1436 train loss 0.1029399037361145\n",
      "val loss 0.1752694547176361\n",
      "______________\n",
      "epoch 1437 train loss 0.1190711259841919\n",
      "val loss 0.17513644695281982\n",
      "______________\n",
      "epoch 1438 train loss 0.09504570066928864\n",
      "val loss 0.1750149428844452\n",
      "______________\n",
      "epoch 1439 train loss 0.11685721576213837\n",
      "val loss 0.17483529448509216\n",
      "______________\n",
      "epoch 1440 train loss 0.09552853554487228\n",
      "val loss 0.17467433214187622\n",
      "______________\n",
      "epoch 1441 train loss 0.12397484481334686\n",
      "val loss 0.17448881268501282\n",
      "______________\n",
      "epoch 1442 train loss 0.09793700277805328\n",
      "val loss 0.1743144541978836\n",
      "______________\n",
      "epoch 1443 train loss 0.10379981994628906\n",
      "val loss 0.17412951588630676\n",
      "______________\n",
      "epoch 1444 train loss 0.11295732855796814\n",
      "val loss 0.17393405735492706\n",
      "______________\n",
      "epoch 1445 train loss 0.1009863093495369\n",
      "val loss 0.17375141382217407\n",
      "______________\n",
      "epoch 1446 train loss 0.10033553838729858\n",
      "val loss 0.17355331778526306\n",
      "______________\n",
      "epoch 1447 train loss 0.09762829542160034\n",
      "val loss 0.17333567142486572\n",
      "______________\n",
      "epoch 1448 train loss 0.10166040062904358\n",
      "val loss 0.17309260368347168\n",
      "______________\n",
      "epoch 1449 train loss 0.12186014652252197\n",
      "val loss 0.172836035490036\n",
      "______________\n",
      "epoch 1450 train loss 0.09245049953460693\n",
      "val loss 0.1725834310054779\n",
      "______________\n",
      "epoch 1451 train loss 0.10730724781751633\n",
      "val loss 0.17231428623199463\n",
      "______________\n",
      "epoch 1452 train loss 0.12088453769683838\n",
      "val loss 0.17202526330947876\n",
      "______________\n",
      "epoch 1453 train loss 0.0948527380824089\n",
      "val loss 0.17174121737480164\n",
      "______________\n",
      "epoch 1454 train loss 0.11176891624927521\n",
      "val loss 0.17146211862564087\n",
      "______________\n",
      "epoch 1455 train loss 0.10855929553508759\n",
      "val loss 0.17120222747325897\n",
      "______________\n",
      "epoch 1456 train loss 0.09736450016498566\n",
      "val loss 0.17092841863632202\n",
      "______________\n",
      "epoch 1457 train loss 0.09485487639904022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 0.17064018547534943\n",
      "______________\n",
      "epoch 1458 train loss 0.13460560142993927\n",
      "val loss 0.17029544711112976\n",
      "______________\n",
      "epoch 1459 train loss 0.125975102186203\n",
      "val loss 0.16993507742881775\n",
      "______________\n",
      "epoch 1460 train loss 0.09818544238805771\n",
      "val loss 0.16961997747421265\n",
      "______________\n",
      "epoch 1461 train loss 0.10737866163253784\n",
      "val loss 0.16930973529815674\n",
      "______________\n",
      "epoch 1462 train loss 0.10181179642677307\n",
      "val loss 0.169008269906044\n",
      "______________\n",
      "epoch 1463 train loss 0.10307560861110687\n",
      "val loss 0.16875198483467102\n",
      "______________\n",
      "epoch 1464 train loss 0.12748658657073975\n",
      "val loss 0.1684880554676056\n",
      "______________\n",
      "epoch 1465 train loss 0.12258991599082947\n",
      "val loss 0.16821670532226562\n",
      "______________\n",
      "epoch 1466 train loss 0.10157199203968048\n",
      "val loss 0.16797907650470734\n",
      "______________\n",
      "epoch 1467 train loss 0.08742427825927734\n",
      "val loss 0.16777467727661133\n",
      "______________\n",
      "epoch 1468 train loss 0.1146957278251648\n",
      "val loss 0.16756537556648254\n",
      "______________\n",
      "epoch 1469 train loss 0.10434158891439438\n",
      "val loss 0.16735276579856873\n",
      "______________\n",
      "epoch 1470 train loss 0.09234687685966492\n",
      "val loss 0.16713613271713257\n",
      "______________\n",
      "epoch 1471 train loss 0.10189247131347656\n",
      "val loss 0.16692867875099182\n",
      "______________\n",
      "epoch 1472 train loss 0.09247525036334991\n",
      "val loss 0.16673626005649567\n",
      "______________\n",
      "epoch 1473 train loss 0.10110968351364136\n",
      "val loss 0.16651302576065063\n",
      "______________\n",
      "epoch 1474 train loss 0.07555266469717026\n",
      "val loss 0.16633924841880798\n",
      "______________\n",
      "epoch 1475 train loss 0.08477401733398438\n",
      "val loss 0.16617229580879211\n",
      "______________\n",
      "epoch 1476 train loss 0.10042446851730347\n",
      "val loss 0.16603663563728333\n",
      "______________\n",
      "epoch 1477 train loss 0.11414676904678345\n",
      "val loss 0.16585764288902283\n",
      "______________\n",
      "epoch 1478 train loss 0.09716805070638657\n",
      "val loss 0.16569310426712036\n",
      "______________\n",
      "epoch 1479 train loss 0.12096002697944641\n",
      "val loss 0.1655089557170868\n",
      "______________\n",
      "epoch 1480 train loss 0.11178537458181381\n",
      "val loss 0.16529160737991333\n",
      "______________\n",
      "epoch 1481 train loss 0.12102443724870682\n",
      "val loss 0.16506940126419067\n",
      "______________\n",
      "epoch 1482 train loss 0.10050736367702484\n",
      "val loss 0.16485029458999634\n",
      "______________\n",
      "epoch 1483 train loss 0.12322302162647247\n",
      "val loss 0.1646072417497635\n",
      "______________\n",
      "epoch 1484 train loss 0.111689493060112\n",
      "val loss 0.16437987983226776\n",
      "______________\n",
      "epoch 1485 train loss 0.08765114843845367\n",
      "val loss 0.16417373716831207\n",
      "______________\n",
      "epoch 1486 train loss 0.10275154560804367\n",
      "val loss 0.1639809012413025\n",
      "______________\n",
      "epoch 1487 train loss 0.10351644456386566\n",
      "val loss 0.16379177570343018\n",
      "______________\n",
      "epoch 1488 train loss 0.09225240349769592\n",
      "val loss 0.163621187210083\n",
      "______________\n",
      "epoch 1489 train loss 0.08043481409549713\n",
      "val loss 0.1634771078824997\n",
      "______________\n",
      "epoch 1490 train loss 0.09885242581367493\n",
      "val loss 0.16333816945552826\n",
      "______________\n",
      "epoch 1491 train loss 0.09703212976455688\n",
      "val loss 0.16322940587997437\n",
      "______________\n",
      "epoch 1492 train loss 0.07574106752872467\n",
      "val loss 0.16315220296382904\n",
      "______________\n",
      "epoch 1493 train loss 0.13550058007240295\n",
      "val loss 0.16305778920650482\n",
      "______________\n",
      "epoch 1494 train loss 0.09770014882087708\n",
      "val loss 0.16296294331550598\n",
      "______________\n",
      "epoch 1495 train loss 0.09028449654579163\n",
      "val loss 0.16286519169807434\n",
      "______________\n",
      "epoch 1496 train loss 0.10806974023580551\n",
      "val loss 0.16276375949382782\n",
      "______________\n",
      "epoch 1497 train loss 0.1254822313785553\n",
      "val loss 0.16268950700759888\n",
      "______________\n",
      "epoch 1498 train loss 0.08614688366651535\n",
      "val loss 0.16260525584220886\n",
      "______________\n",
      "epoch 1499 train loss 0.10174736380577087\n",
      "val loss 0.16247554123401642\n",
      "______________\n",
      "epoch 1500 train loss 0.08882903307676315\n",
      "val loss 0.16233444213867188\n",
      "______________\n",
      "epoch 1501 train loss 0.09657301753759384\n",
      "val loss 0.16220080852508545\n",
      "______________\n",
      "epoch 1502 train loss 0.091398686170578\n",
      "val loss 0.16202078759670258\n",
      "______________\n",
      "epoch 1503 train loss 0.10782378911972046\n",
      "val loss 0.16181354224681854\n",
      "______________\n",
      "epoch 1504 train loss 0.10218018293380737\n",
      "val loss 0.16160684823989868\n",
      "______________\n",
      "epoch 1505 train loss 0.10693670809268951\n",
      "val loss 0.16142261028289795\n",
      "______________\n",
      "epoch 1506 train loss 0.10355530679225922\n",
      "val loss 0.16127464175224304\n",
      "______________\n",
      "epoch 1507 train loss 0.11137503385543823\n",
      "val loss 0.16107675433158875\n",
      "______________\n",
      "epoch 1508 train loss 0.09948742389678955\n",
      "val loss 0.16086536645889282\n",
      "______________\n",
      "epoch 1509 train loss 0.10518510639667511\n",
      "val loss 0.16071480512619019\n",
      "______________\n",
      "epoch 1510 train loss 0.1257111132144928\n",
      "val loss 0.16057470440864563\n",
      "______________\n",
      "epoch 1511 train loss 0.09916377067565918\n",
      "val loss 0.1604718714952469\n",
      "______________\n",
      "epoch 1512 train loss 0.08334901928901672\n",
      "val loss 0.16036921739578247\n",
      "______________\n",
      "epoch 1513 train loss 0.08704830706119537\n",
      "val loss 0.16031014919281006\n",
      "______________\n",
      "epoch 1514 train loss 0.10165983438491821\n",
      "val loss 0.16023243963718414\n",
      "______________\n",
      "epoch 1515 train loss 0.11091572046279907\n",
      "val loss 0.16014346480369568\n",
      "______________\n",
      "epoch 1516 train loss 0.10240902006626129\n",
      "val loss 0.16004307568073273\n",
      "______________\n",
      "epoch 1517 train loss 0.11538992822170258\n",
      "val loss 0.1599545031785965\n",
      "______________\n",
      "epoch 1518 train loss 0.08622086048126221\n",
      "val loss 0.15987932682037354\n",
      "______________\n",
      "epoch 1519 train loss 0.1131587103009224\n",
      "val loss 0.15981152653694153\n",
      "______________\n",
      "epoch 1520 train loss 0.10248011350631714\n",
      "val loss 0.15972664952278137\n",
      "______________\n",
      "epoch 1521 train loss 0.08662194013595581\n",
      "val loss 0.15964312851428986\n",
      "______________\n",
      "epoch 1522 train loss 0.09289807081222534\n",
      "val loss 0.15958404541015625\n",
      "______________\n",
      "epoch 1523 train loss 0.09425046294927597\n",
      "val loss 0.1595359444618225\n",
      "______________\n",
      "epoch 1524 train loss 0.0966232419013977\n",
      "val loss 0.15947294235229492\n",
      "______________\n",
      "epoch 1525 train loss 0.10044721513986588\n",
      "val loss 0.15938320755958557\n",
      "______________\n",
      "epoch 1526 train loss 0.12046664953231812\n",
      "val loss 0.15921896696090698\n",
      "______________\n",
      "epoch 1527 train loss 0.09539015591144562\n",
      "val loss 0.15906722843647003\n",
      "______________\n",
      "epoch 1528 train loss 0.11276474595069885\n",
      "val loss 0.15889006853103638\n",
      "______________\n",
      "epoch 1529 train loss 0.08731694519519806\n",
      "val loss 0.1587265133857727\n",
      "______________\n",
      "epoch 1530 train loss 0.11451100558042526\n",
      "val loss 0.15849393606185913\n",
      "______________\n",
      "epoch 1531 train loss 0.10428288578987122\n",
      "val loss 0.15826159715652466\n",
      "______________\n",
      "epoch 1532 train loss 0.09805898368358612\n",
      "val loss 0.15806204080581665\n",
      "______________\n",
      "epoch 1533 train loss 0.11168228089809418\n",
      "val loss 0.1578701287508011\n",
      "______________\n",
      "epoch 1534 train loss 0.09780053794384003\n",
      "val loss 0.15763339400291443\n",
      "______________\n",
      "epoch 1535 train loss 0.10202805697917938\n",
      "val loss 0.1574549823999405\n",
      "______________\n",
      "epoch 1536 train loss 0.10701563954353333\n",
      "val loss 0.15724849700927734\n",
      "______________\n",
      "epoch 1537 train loss 0.10540462285280228\n",
      "val loss 0.1570584923028946\n",
      "______________\n",
      "epoch 1538 train loss 0.10391255468130112\n",
      "val loss 0.15685662627220154\n",
      "______________\n",
      "epoch 1539 train loss 0.10604110360145569\n",
      "val loss 0.15662381052970886\n",
      "______________\n",
      "epoch 1540 train loss 0.09964410960674286\n",
      "val loss 0.15638163685798645\n",
      "______________\n",
      "epoch 1541 train loss 0.09525957703590393\n",
      "val loss 0.15612417459487915\n",
      "______________\n",
      "epoch 1542 train loss 0.0917995423078537\n",
      "val loss 0.15587201714515686\n",
      "______________\n",
      "epoch 1543 train loss 0.0985308289527893\n",
      "val loss 0.1556166708469391\n",
      "______________\n",
      "epoch 1544 train loss 0.08240780234336853\n",
      "val loss 0.15536563098430634\n",
      "______________\n",
      "epoch 1545 train loss 0.12936581671237946\n",
      "val loss 0.15508969128131866\n",
      "______________\n",
      "epoch 1546 train loss 0.11641952395439148\n",
      "val loss 0.1548297107219696\n",
      "______________\n",
      "epoch 1547 train loss 0.10876161605119705\n",
      "val loss 0.1545594036579132\n",
      "______________\n",
      "epoch 1548 train loss 0.09993346780538559\n",
      "val loss 0.1543480008840561\n",
      "______________\n",
      "epoch 1549 train loss 0.09923398494720459\n",
      "val loss 0.1541319489479065\n",
      "______________\n",
      "epoch 1550 train loss 0.08834663033485413\n",
      "val loss 0.15394327044487\n",
      "______________\n",
      "epoch 1551 train loss 0.0928119346499443\n",
      "val loss 0.1537870466709137\n",
      "______________\n",
      "epoch 1552 train loss 0.10235485434532166\n",
      "val loss 0.1536509245634079\n",
      "______________\n",
      "epoch 1553 train loss 0.09947056323289871\n",
      "val loss 0.15351615846157074\n",
      "______________\n",
      "epoch 1554 train loss 0.11309826374053955\n",
      "val loss 0.15336178243160248\n",
      "______________\n",
      "epoch 1555 train loss 0.08501468598842621\n",
      "val loss 0.15326961874961853\n",
      "______________\n",
      "epoch 1556 train loss 0.10095828771591187\n",
      "val loss 0.1531963348388672\n",
      "______________\n",
      "epoch 1557 train loss 0.08885935693979263\n",
      "val loss 0.15310993790626526\n",
      "______________\n",
      "epoch 1558 train loss 0.10576322674751282\n",
      "val loss 0.15305116772651672\n",
      "______________\n",
      "epoch 1559 train loss 0.08098195493221283\n",
      "val loss 0.1530046910047531\n",
      "______________\n",
      "epoch 1560 train loss 0.10946942865848541\n",
      "val loss 0.15292981266975403\n",
      "______________\n",
      "epoch 1561 train loss 0.09406519681215286\n",
      "val loss 0.15287908911705017\n",
      "______________\n",
      "epoch 1562 train loss 0.08761537075042725\n",
      "val loss 0.15281331539154053\n",
      "______________\n",
      "epoch 1563 train loss 0.0771045982837677\n",
      "val loss 0.15275052189826965\n",
      "______________\n",
      "epoch 1564 train loss 0.09424134343862534\n",
      "val loss 0.15267601609230042\n",
      "______________\n",
      "epoch 1565 train loss 0.13789761066436768\n",
      "val loss 0.15255805850028992\n",
      "______________\n",
      "epoch 1566 train loss 0.09866447001695633\n",
      "val loss 0.15246418118476868\n",
      "______________\n",
      "epoch 1567 train loss 0.1177523285150528\n",
      "val loss 0.1523858606815338\n",
      "______________\n",
      "epoch 1568 train loss 0.10067490488290787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 0.15230952203273773\n",
      "______________\n",
      "epoch 1569 train loss 0.09713344275951385\n",
      "val loss 0.15224888920783997\n",
      "______________\n",
      "epoch 1570 train loss 0.0941154807806015\n",
      "val loss 0.15220554172992706\n",
      "______________\n",
      "epoch 1571 train loss 0.09064488112926483\n",
      "val loss 0.15217220783233643\n",
      "______________\n",
      "epoch 1572 train loss 0.1019456759095192\n",
      "val loss 0.1520928591489792\n",
      "______________\n",
      "epoch 1573 train loss 0.0724455714225769\n",
      "val loss 0.1520647406578064\n",
      "______________\n",
      "epoch 1574 train loss 0.10971759259700775\n",
      "val loss 0.15202338993549347\n",
      "______________\n",
      "epoch 1575 train loss 0.08788830041885376\n",
      "val loss 0.15198519825935364\n",
      "______________\n",
      "epoch 1576 train loss 0.10382348299026489\n",
      "val loss 0.15198427438735962\n",
      "______________\n",
      "epoch 1577 train loss 0.0906401127576828\n",
      "val loss 0.15198230743408203\n",
      "______________\n",
      "epoch 1578 train loss 0.07825427502393723\n",
      "val loss 0.15199053287506104\n",
      "______________\n",
      "epoch 1579 train loss 0.10504946857690811\n",
      "val loss 0.15200257301330566\n",
      "______________\n",
      "epoch 1580 train loss 0.12090633064508438\n",
      "val loss 0.15195074677467346\n",
      "______________\n",
      "epoch 1581 train loss 0.11208401620388031\n",
      "val loss 0.15186378359794617\n",
      "______________\n",
      "epoch 1582 train loss 0.08907131850719452\n",
      "val loss 0.15177112817764282\n",
      "______________\n",
      "epoch 1583 train loss 0.09489209949970245\n",
      "val loss 0.15165495872497559\n",
      "______________\n",
      "epoch 1584 train loss 0.10029751062393188\n",
      "val loss 0.15150250494480133\n",
      "______________\n",
      "epoch 1585 train loss 0.12756183743476868\n",
      "val loss 0.1512884497642517\n",
      "______________\n",
      "epoch 1586 train loss 0.07422548532485962\n",
      "val loss 0.15110795199871063\n",
      "______________\n",
      "epoch 1587 train loss 0.1328561156988144\n",
      "val loss 0.1508612036705017\n",
      "______________\n",
      "epoch 1588 train loss 0.07446175813674927\n",
      "val loss 0.1506291627883911\n",
      "______________\n",
      "epoch 1589 train loss 0.13533420860767365\n",
      "val loss 0.1503826081752777\n",
      "______________\n",
      "epoch 1590 train loss 0.08269510418176651\n",
      "val loss 0.15017594397068024\n",
      "______________\n",
      "epoch 1591 train loss 0.12078987807035446\n",
      "val loss 0.14999236166477203\n",
      "______________\n",
      "epoch 1592 train loss 0.10850737988948822\n",
      "val loss 0.14980100095272064\n",
      "______________\n",
      "epoch 1593 train loss 0.07286141067743301\n",
      "val loss 0.14963212609291077\n",
      "______________\n",
      "epoch 1594 train loss 0.07605962455272675\n",
      "val loss 0.14947912096977234\n",
      "______________\n",
      "epoch 1595 train loss 0.07633750140666962\n",
      "val loss 0.14936575293540955\n",
      "______________\n",
      "epoch 1596 train loss 0.0897381380200386\n",
      "val loss 0.14925536513328552\n",
      "______________\n",
      "epoch 1597 train loss 0.11838398873806\n",
      "val loss 0.14914920926094055\n",
      "______________\n",
      "epoch 1598 train loss 0.10228080302476883\n",
      "val loss 0.14902611076831818\n",
      "______________\n",
      "epoch 1599 train loss 0.08043898642063141\n",
      "val loss 0.14895124733448029\n",
      "______________\n",
      "epoch 1600 train loss 0.07140253484249115\n",
      "val loss 0.14889177680015564\n",
      "______________\n",
      "epoch 1601 train loss 0.09505591541528702\n",
      "val loss 0.14885230362415314\n",
      "______________\n",
      "epoch 1602 train loss 0.10657291114330292\n",
      "val loss 0.1487872153520584\n",
      "______________\n",
      "epoch 1603 train loss 0.09765815734863281\n",
      "val loss 0.1486944854259491\n",
      "______________\n",
      "epoch 1604 train loss 0.11837469041347504\n",
      "val loss 0.1485581398010254\n",
      "______________\n",
      "epoch 1605 train loss 0.08471231907606125\n",
      "val loss 0.14842212200164795\n",
      "______________\n",
      "epoch 1606 train loss 0.08961580693721771\n",
      "val loss 0.14828766882419586\n",
      "______________\n",
      "epoch 1607 train loss 0.10694704949855804\n",
      "val loss 0.14817029237747192\n",
      "______________\n",
      "epoch 1608 train loss 0.10535968840122223\n",
      "val loss 0.14802591502666473\n",
      "______________\n",
      "epoch 1609 train loss 0.11544054746627808\n",
      "val loss 0.1478479504585266\n",
      "______________\n",
      "epoch 1610 train loss 0.09996359050273895\n",
      "val loss 0.14763349294662476\n",
      "______________\n",
      "epoch 1611 train loss 0.08338752388954163\n",
      "val loss 0.1474473476409912\n",
      "______________\n",
      "epoch 1612 train loss 0.08203548192977905\n",
      "val loss 0.14727428555488586\n",
      "______________\n",
      "epoch 1613 train loss 0.08169619739055634\n",
      "val loss 0.14717644453048706\n",
      "______________\n",
      "epoch 1614 train loss 0.11206146329641342\n",
      "val loss 0.14707812666893005\n",
      "______________\n",
      "epoch 1615 train loss 0.10638116300106049\n",
      "val loss 0.14696551859378815\n",
      "______________\n",
      "epoch 1616 train loss 0.10297033935785294\n",
      "val loss 0.14682962000370026\n",
      "______________\n",
      "epoch 1617 train loss 0.11523944139480591\n",
      "val loss 0.14667850732803345\n",
      "______________\n",
      "epoch 1618 train loss 0.09256890416145325\n",
      "val loss 0.14654776453971863\n",
      "______________\n",
      "epoch 1619 train loss 0.08052326738834381\n",
      "val loss 0.14641982316970825\n",
      "______________\n",
      "epoch 1620 train loss 0.09097860753536224\n",
      "val loss 0.14632095396518707\n",
      "______________\n",
      "epoch 1621 train loss 0.10980229079723358\n",
      "val loss 0.14622649550437927\n",
      "______________\n",
      "epoch 1622 train loss 0.09590893983840942\n",
      "val loss 0.14614735543727875\n",
      "______________\n",
      "epoch 1623 train loss 0.08454965054988861\n",
      "val loss 0.14605316519737244\n",
      "______________\n",
      "epoch 1624 train loss 0.06191498041152954\n",
      "val loss 0.14596213400363922\n",
      "______________\n",
      "epoch 1625 train loss 0.10524430871009827\n",
      "val loss 0.14583027362823486\n",
      "______________\n",
      "epoch 1626 train loss 0.08936432003974915\n",
      "val loss 0.14568936824798584\n",
      "______________\n",
      "epoch 1627 train loss 0.09274422377347946\n",
      "val loss 0.14555780589580536\n",
      "______________\n",
      "epoch 1628 train loss 0.07418698072433472\n",
      "val loss 0.14543327689170837\n",
      "______________\n",
      "epoch 1629 train loss 0.11076377332210541\n",
      "val loss 0.14529123902320862\n",
      "______________\n",
      "epoch 1630 train loss 0.09323974698781967\n",
      "val loss 0.14512859284877777\n",
      "______________\n",
      "epoch 1631 train loss 0.0924261286854744\n",
      "val loss 0.14496967196464539\n",
      "______________\n",
      "epoch 1632 train loss 0.08508561551570892\n",
      "val loss 0.14482536911964417\n",
      "______________\n",
      "epoch 1633 train loss 0.10488638281822205\n",
      "val loss 0.14467012882232666\n",
      "______________\n",
      "epoch 1634 train loss 0.07798098027706146\n",
      "val loss 0.1445448100566864\n",
      "______________\n",
      "epoch 1635 train loss 0.09546681493520737\n",
      "val loss 0.14439266920089722\n",
      "______________\n",
      "epoch 1636 train loss 0.08534792810678482\n",
      "val loss 0.14426790177822113\n",
      "______________\n",
      "epoch 1637 train loss 0.09036219120025635\n",
      "val loss 0.14416556060314178\n",
      "______________\n",
      "epoch 1638 train loss 0.13243332505226135\n",
      "val loss 0.14395448565483093\n",
      "______________\n",
      "epoch 1639 train loss 0.0863199383020401\n",
      "val loss 0.1437443345785141\n",
      "______________\n",
      "epoch 1640 train loss 0.10606903582811356\n",
      "val loss 0.1435430943965912\n",
      "______________\n",
      "epoch 1641 train loss 0.08961979299783707\n",
      "val loss 0.143370121717453\n",
      "______________\n",
      "epoch 1642 train loss 0.08328868448734283\n",
      "val loss 0.14317521452903748\n",
      "______________\n",
      "epoch 1643 train loss 0.09532800316810608\n",
      "val loss 0.1429925560951233\n",
      "______________\n",
      "epoch 1644 train loss 0.10291652381420135\n",
      "val loss 0.1427709013223648\n",
      "______________\n",
      "epoch 1645 train loss 0.06830936670303345\n",
      "val loss 0.14259853959083557\n",
      "______________\n",
      "epoch 1646 train loss 0.08703146874904633\n",
      "val loss 0.14243584871292114\n",
      "______________\n",
      "epoch 1647 train loss 0.0948110967874527\n",
      "val loss 0.14229774475097656\n",
      "______________\n",
      "epoch 1648 train loss 0.07121212780475616\n",
      "val loss 0.1421491801738739\n",
      "______________\n",
      "epoch 1649 train loss 0.08003063499927521\n",
      "val loss 0.14201408624649048\n",
      "______________\n",
      "epoch 1650 train loss 0.10833340883255005\n",
      "val loss 0.14185376465320587\n",
      "______________\n",
      "epoch 1651 train loss 0.07920737564563751\n",
      "val loss 0.1417669653892517\n",
      "______________\n",
      "epoch 1652 train loss 0.06694193184375763\n",
      "val loss 0.14170047640800476\n",
      "______________\n",
      "epoch 1653 train loss 0.09494588524103165\n",
      "val loss 0.1415926218032837\n",
      "______________\n",
      "epoch 1654 train loss 0.07363918423652649\n",
      "val loss 0.14147838950157166\n",
      "______________\n",
      "epoch 1655 train loss 0.07505236566066742\n",
      "val loss 0.1413576900959015\n",
      "______________\n",
      "epoch 1656 train loss 0.09462663531303406\n",
      "val loss 0.14128130674362183\n",
      "______________\n",
      "epoch 1657 train loss 0.08248862624168396\n",
      "val loss 0.14119458198547363\n",
      "______________\n",
      "epoch 1658 train loss 0.07854893803596497\n",
      "val loss 0.1411224603652954\n",
      "______________\n",
      "epoch 1659 train loss 0.10013896226882935\n",
      "val loss 0.14102640748023987\n",
      "______________\n",
      "epoch 1660 train loss 0.08024024963378906\n",
      "val loss 0.1409621387720108\n",
      "______________\n",
      "epoch 1661 train loss 0.12868019938468933\n",
      "val loss 0.1408727914094925\n",
      "______________\n",
      "epoch 1662 train loss 0.0961986780166626\n",
      "val loss 0.14080780744552612\n",
      "______________\n",
      "epoch 1663 train loss 0.08222555369138718\n",
      "val loss 0.14073696732521057\n",
      "______________\n",
      "epoch 1664 train loss 0.10316880792379379\n",
      "val loss 0.14064499735832214\n",
      "______________\n",
      "epoch 1665 train loss 0.07726649940013885\n",
      "val loss 0.14053462445735931\n",
      "______________\n",
      "epoch 1666 train loss 0.08878414332866669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 0.14041246473789215\n",
      "______________\n",
      "epoch 1667 train loss 0.09299381822347641\n",
      "val loss 0.14025422930717468\n",
      "______________\n",
      "epoch 1668 train loss 0.09801840782165527\n",
      "val loss 0.1400904506444931\n",
      "______________\n",
      "epoch 1669 train loss 0.09073350578546524\n",
      "val loss 0.13991405069828033\n",
      "______________\n",
      "epoch 1670 train loss 0.1049758791923523\n",
      "val loss 0.13972938060760498\n",
      "______________\n",
      "epoch 1671 train loss 0.06852348148822784\n",
      "val loss 0.13959258794784546\n",
      "______________\n",
      "epoch 1672 train loss 0.10389634221792221\n",
      "val loss 0.13943824172019958\n",
      "______________\n",
      "epoch 1673 train loss 0.08356232196092606\n",
      "val loss 0.1392812430858612\n",
      "______________\n",
      "epoch 1674 train loss 0.0748407244682312\n",
      "val loss 0.13915766775608063\n",
      "______________\n",
      "epoch 1675 train loss 0.09282203018665314\n",
      "val loss 0.13901659846305847\n",
      "______________\n",
      "epoch 1676 train loss 0.09867441654205322\n",
      "val loss 0.1388765275478363\n",
      "______________\n",
      "epoch 1677 train loss 0.07723255455493927\n",
      "val loss 0.13873769342899323\n",
      "______________\n",
      "epoch 1678 train loss 0.1088576689362526\n",
      "val loss 0.1386210024356842\n",
      "______________\n",
      "epoch 1679 train loss 0.12124577909708023\n",
      "val loss 0.13846275210380554\n",
      "______________\n",
      "epoch 1680 train loss 0.11816050857305527\n",
      "val loss 0.13826662302017212\n",
      "______________\n",
      "epoch 1681 train loss 0.09819148480892181\n",
      "val loss 0.13808874785900116\n",
      "______________\n",
      "epoch 1682 train loss 0.09061156213283539\n",
      "val loss 0.13793423771858215\n",
      "______________\n",
      "epoch 1683 train loss 0.11323507130146027\n",
      "val loss 0.13780298829078674\n",
      "______________\n",
      "epoch 1684 train loss 0.11822515726089478\n",
      "val loss 0.1375904679298401\n",
      "______________\n",
      "epoch 1685 train loss 0.07442277669906616\n",
      "val loss 0.13743636012077332\n",
      "______________\n",
      "epoch 1686 train loss 0.1072455644607544\n",
      "val loss 0.13726601004600525\n",
      "______________\n",
      "epoch 1687 train loss 0.10262025892734528\n",
      "val loss 0.13707120716571808\n",
      "______________\n",
      "epoch 1688 train loss 0.08420811593532562\n",
      "val loss 0.13691048324108124\n",
      "______________\n",
      "epoch 1689 train loss 0.09087497740983963\n",
      "val loss 0.13679885864257812\n",
      "______________\n",
      "epoch 1690 train loss 0.11024023592472076\n",
      "val loss 0.13670408725738525\n",
      "______________\n",
      "epoch 1691 train loss 0.09678445756435394\n",
      "val loss 0.1366434395313263\n",
      "______________\n",
      "epoch 1692 train loss 0.09075982123613358\n",
      "val loss 0.13659198582172394\n",
      "______________\n",
      "epoch 1693 train loss 0.09064485877752304\n",
      "val loss 0.13656941056251526\n",
      "______________\n",
      "epoch 1694 train loss 0.15169017016887665\n",
      "val loss 0.13647083938121796\n",
      "______________\n",
      "epoch 1695 train loss 0.07733376324176788\n",
      "val loss 0.13634981215000153\n",
      "______________\n",
      "epoch 1696 train loss 0.09299099445343018\n",
      "val loss 0.13621601462364197\n",
      "______________\n",
      "epoch 1697 train loss 0.11601463705301285\n",
      "val loss 0.13607242703437805\n",
      "______________\n",
      "epoch 1698 train loss 0.130540132522583\n",
      "val loss 0.135877788066864\n",
      "______________\n",
      "epoch 1699 train loss 0.07978454232215881\n",
      "val loss 0.13571274280548096\n",
      "______________\n",
      "epoch 1700 train loss 0.07635720819234848\n",
      "val loss 0.1355786770582199\n",
      "______________\n",
      "epoch 1701 train loss 0.0687456950545311\n",
      "val loss 0.1354590356349945\n",
      "______________\n",
      "epoch 1702 train loss 0.145318865776062\n",
      "val loss 0.13533154129981995\n",
      "______________\n",
      "epoch 1703 train loss 0.09814085066318512\n",
      "val loss 0.1351957619190216\n",
      "______________\n",
      "epoch 1704 train loss 0.08286818861961365\n",
      "val loss 0.13507263362407684\n",
      "______________\n",
      "epoch 1705 train loss 0.07037564367055893\n",
      "val loss 0.13495853543281555\n",
      "______________\n",
      "epoch 1706 train loss 0.09107406437397003\n",
      "val loss 0.13485737144947052\n",
      "______________\n",
      "epoch 1707 train loss 0.0701751559972763\n",
      "val loss 0.13478052616119385\n",
      "______________\n",
      "epoch 1708 train loss 0.09104852378368378\n",
      "val loss 0.13474291563034058\n",
      "______________\n",
      "epoch 1709 train loss 0.1412857174873352\n",
      "val loss 0.13468965888023376\n",
      "______________\n",
      "epoch 1710 train loss 0.08124648779630661\n",
      "val loss 0.13463559746742249\n",
      "______________\n",
      "epoch 1711 train loss 0.08125828206539154\n",
      "val loss 0.1346137374639511\n",
      "______________\n",
      "epoch 1712 train loss 0.082145556807518\n",
      "val loss 0.13456761837005615\n",
      "______________\n",
      "epoch 1713 train loss 0.09482033550739288\n",
      "val loss 0.1344866305589676\n",
      "______________\n",
      "epoch 1714 train loss 0.08545650541782379\n",
      "val loss 0.13441558182239532\n",
      "______________\n",
      "epoch 1715 train loss 0.07721783220767975\n",
      "val loss 0.13435028493404388\n",
      "______________\n",
      "epoch 1716 train loss 0.09770023822784424\n",
      "val loss 0.13426527380943298\n",
      "______________\n",
      "epoch 1717 train loss 0.09719522297382355\n",
      "val loss 0.13419905304908752\n",
      "______________\n",
      "epoch 1718 train loss 0.10655643045902252\n",
      "val loss 0.13408538699150085\n",
      "______________\n",
      "epoch 1719 train loss 0.06588467955589294\n",
      "val loss 0.13398995995521545\n",
      "______________\n",
      "epoch 1720 train loss 0.08246020972728729\n",
      "val loss 0.13392621278762817\n",
      "______________\n",
      "epoch 1721 train loss 0.08675512671470642\n",
      "val loss 0.1338333785533905\n",
      "______________\n",
      "epoch 1722 train loss 0.08388440310955048\n",
      "val loss 0.1337219476699829\n",
      "______________\n",
      "epoch 1723 train loss 0.07348735630512238\n",
      "val loss 0.13360197842121124\n",
      "______________\n",
      "epoch 1724 train loss 0.07879569381475449\n",
      "val loss 0.13349783420562744\n",
      "______________\n",
      "epoch 1725 train loss 0.09822104126214981\n",
      "val loss 0.1333961933851242\n",
      "______________\n",
      "epoch 1726 train loss 0.08503325283527374\n",
      "val loss 0.13329392671585083\n",
      "______________\n",
      "epoch 1727 train loss 0.0693521797657013\n",
      "val loss 0.133229598402977\n",
      "______________\n",
      "epoch 1728 train loss 0.09633804857730865\n",
      "val loss 0.13317658007144928\n",
      "______________\n",
      "epoch 1729 train loss 0.09051699936389923\n",
      "val loss 0.1331372708082199\n",
      "______________\n",
      "epoch 1730 train loss 0.11132821440696716\n",
      "val loss 0.13308610022068024\n",
      "______________\n",
      "epoch 1731 train loss 0.10113690793514252\n",
      "val loss 0.13301220536231995\n",
      "______________\n",
      "epoch 1732 train loss 0.09552258998155594\n",
      "val loss 0.13293148577213287\n",
      "______________\n",
      "epoch 1733 train loss 0.08629133552312851\n",
      "val loss 0.13284476101398468\n",
      "______________\n",
      "epoch 1734 train loss 0.08214681595563889\n",
      "val loss 0.1327393352985382\n",
      "______________\n",
      "epoch 1735 train loss 0.08964797854423523\n",
      "val loss 0.1326310932636261\n",
      "______________\n",
      "epoch 1736 train loss 0.07761172205209732\n",
      "val loss 0.13253998756408691\n",
      "______________\n",
      "epoch 1737 train loss 0.080591581761837\n",
      "val loss 0.1324501633644104\n",
      "______________\n",
      "epoch 1738 train loss 0.08778628706932068\n",
      "val loss 0.13235482573509216\n",
      "______________\n",
      "epoch 1739 train loss 0.07853718101978302\n",
      "val loss 0.13224399089813232\n",
      "______________\n",
      "epoch 1740 train loss 0.10070952028036118\n",
      "val loss 0.13212427496910095\n",
      "______________\n",
      "epoch 1741 train loss 0.10376452654600143\n",
      "val loss 0.1320551484823227\n",
      "______________\n",
      "epoch 1742 train loss 0.0819752961397171\n",
      "val loss 0.1319969892501831\n",
      "______________\n",
      "epoch 1743 train loss 0.08185352385044098\n",
      "val loss 0.13194499909877777\n",
      "______________\n",
      "epoch 1744 train loss 0.1063908040523529\n",
      "val loss 0.13188116252422333\n",
      "______________\n",
      "epoch 1745 train loss 0.10493601858615875\n",
      "val loss 0.13185591995716095\n",
      "______________\n",
      "epoch 1746 train loss 0.09967103600502014\n",
      "val loss 0.13184277713298798\n",
      "______________\n",
      "epoch 1747 train loss 0.0979052186012268\n",
      "val loss 0.1318308413028717\n",
      "______________\n",
      "epoch 1748 train loss 0.09618568420410156\n",
      "val loss 0.13181130588054657\n",
      "______________\n",
      "epoch 1749 train loss 0.09362947940826416\n",
      "val loss 0.13179048895835876\n",
      "______________\n",
      "epoch 1750 train loss 0.10376428067684174\n",
      "val loss 0.13173532485961914\n",
      "______________\n",
      "epoch 1751 train loss 0.07758694887161255\n",
      "val loss 0.1316591203212738\n",
      "______________\n",
      "epoch 1752 train loss 0.09135979413986206\n",
      "val loss 0.13156822323799133\n",
      "______________\n",
      "epoch 1753 train loss 0.1107100248336792\n",
      "val loss 0.13144846260547638\n",
      "______________\n",
      "epoch 1754 train loss 0.07380973547697067\n",
      "val loss 0.13135603070259094\n",
      "______________\n",
      "epoch 1755 train loss 0.08866025507450104\n",
      "val loss 0.13126862049102783\n",
      "______________\n",
      "epoch 1756 train loss 0.08560098707675934\n",
      "val loss 0.1311848759651184\n",
      "______________\n",
      "epoch 1757 train loss 0.0998508483171463\n",
      "val loss 0.13109242916107178\n",
      "______________\n",
      "epoch 1758 train loss 0.09741252660751343\n",
      "val loss 0.13095703721046448\n",
      "______________\n",
      "epoch 1759 train loss 0.09490653872489929\n",
      "val loss 0.13085250556468964\n",
      "______________\n",
      "epoch 1760 train loss 0.059624746441841125\n",
      "val loss 0.13075387477874756\n",
      "______________\n",
      "epoch 1761 train loss 0.09745784848928452\n",
      "val loss 0.1305687129497528\n",
      "______________\n",
      "epoch 1762 train loss 0.07440226525068283\n",
      "val loss 0.13040010631084442\n",
      "______________\n",
      "epoch 1763 train loss 0.08087578415870667\n",
      "val loss 0.1302519142627716\n",
      "______________\n",
      "epoch 1764 train loss 0.06992347538471222\n",
      "val loss 0.1301354467868805\n",
      "______________\n",
      "epoch 1765 train loss 0.10542742908000946\n",
      "val loss 0.1299976408481598\n",
      "______________\n",
      "epoch 1766 train loss 0.07065132260322571\n",
      "val loss 0.12987056374549866\n",
      "______________\n",
      "epoch 1767 train loss 0.11131858825683594\n",
      "val loss 0.12972459197044373\n",
      "______________\n",
      "epoch 1768 train loss 0.09062805771827698\n",
      "val loss 0.12958481907844543\n",
      "______________\n",
      "epoch 1769 train loss 0.11234962940216064\n",
      "val loss 0.1294444501399994\n",
      "______________\n",
      "epoch 1770 train loss 0.10134414583444595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 0.12928219139575958\n",
      "______________\n",
      "epoch 1771 train loss 0.1063118577003479\n",
      "val loss 0.12907841801643372\n",
      "______________\n",
      "epoch 1772 train loss 0.1094861775636673\n",
      "val loss 0.12887518107891083\n",
      "______________\n",
      "epoch 1773 train loss 0.07343532145023346\n",
      "val loss 0.12872351706027985\n",
      "______________\n",
      "epoch 1774 train loss 0.12401885539293289\n",
      "val loss 0.12856237590312958\n",
      "______________\n",
      "epoch 1775 train loss 0.0783233791589737\n",
      "val loss 0.128409281373024\n",
      "______________\n",
      "epoch 1776 train loss 0.09133182466030121\n",
      "val loss 0.1282447874546051\n",
      "______________\n",
      "epoch 1777 train loss 0.07010054588317871\n",
      "val loss 0.12808549404144287\n",
      "______________\n",
      "epoch 1778 train loss 0.08254209160804749\n",
      "val loss 0.12791350483894348\n",
      "______________\n",
      "epoch 1779 train loss 0.09671226143836975\n",
      "val loss 0.12772700190544128\n",
      "______________\n",
      "epoch 1780 train loss 0.07311300933361053\n",
      "val loss 0.12755607068538666\n",
      "______________\n",
      "epoch 1781 train loss 0.10699000954627991\n",
      "val loss 0.12736383080482483\n",
      "______________\n",
      "epoch 1782 train loss 0.0762159526348114\n",
      "val loss 0.1271667629480362\n",
      "______________\n",
      "epoch 1783 train loss 0.09553015977144241\n",
      "val loss 0.12697112560272217\n",
      "______________\n",
      "epoch 1784 train loss 0.09664247930049896\n",
      "val loss 0.12674130499362946\n",
      "______________\n",
      "epoch 1785 train loss 0.08395282924175262\n",
      "val loss 0.1265467256307602\n",
      "______________\n",
      "epoch 1786 train loss 0.09537723660469055\n",
      "val loss 0.12634146213531494\n",
      "______________\n",
      "epoch 1787 train loss 0.07903972268104553\n",
      "val loss 0.12617290019989014\n",
      "______________\n",
      "epoch 1788 train loss 0.0879238098859787\n",
      "val loss 0.12600085139274597\n",
      "______________\n",
      "epoch 1789 train loss 0.0942273736000061\n",
      "val loss 0.12582464516162872\n",
      "______________\n",
      "epoch 1790 train loss 0.09013617783784866\n",
      "val loss 0.12567350268363953\n",
      "______________\n",
      "epoch 1791 train loss 0.09839016199111938\n",
      "val loss 0.12550541758537292\n",
      "______________\n",
      "epoch 1792 train loss 0.056142229586839676\n",
      "val loss 0.1253897249698639\n",
      "______________\n",
      "epoch 1793 train loss 0.062355220317840576\n",
      "val loss 0.12531399726867676\n",
      "______________\n",
      "epoch 1794 train loss 0.08076994121074677\n",
      "val loss 0.12525752186775208\n",
      "______________\n",
      "epoch 1795 train loss 0.09684161841869354\n",
      "val loss 0.12519364058971405\n",
      "______________\n",
      "epoch 1796 train loss 0.06810285896062851\n",
      "val loss 0.1251080334186554\n",
      "______________\n",
      "epoch 1797 train loss 0.10140164196491241\n",
      "val loss 0.12498433142900467\n",
      "______________\n",
      "epoch 1798 train loss 0.09112909436225891\n",
      "val loss 0.124876469373703\n",
      "______________\n",
      "epoch 1799 train loss 0.08523663133382797\n",
      "val loss 0.12479179352521896\n",
      "______________\n",
      "epoch 1800 train loss 0.046200044453144073\n",
      "val loss 0.1247478500008583\n",
      "______________\n",
      "epoch 1801 train loss 0.08110104501247406\n",
      "val loss 0.12473054230213165\n",
      "______________\n",
      "epoch 1802 train loss 0.09214311838150024\n",
      "val loss 0.12468456476926804\n",
      "______________\n",
      "epoch 1803 train loss 0.0827145129442215\n",
      "val loss 0.1246441975235939\n",
      "______________\n",
      "epoch 1804 train loss 0.09469886124134064\n",
      "val loss 0.12460077553987503\n",
      "______________\n",
      "epoch 1805 train loss 0.10320731997489929\n",
      "val loss 0.12454372644424438\n",
      "______________\n",
      "epoch 1806 train loss 0.07143555581569672\n",
      "val loss 0.12452332675457001\n",
      "______________\n",
      "epoch 1807 train loss 0.06944209337234497\n",
      "val loss 0.12452324479818344\n",
      "______________\n",
      "epoch 1808 train loss 0.11277034878730774\n",
      "val loss 0.12451886385679245\n",
      "______________\n",
      "epoch 1809 train loss 0.0646962895989418\n",
      "val loss 0.12452200055122375\n",
      "______________\n",
      "epoch 1810 train loss 0.0948224812746048\n",
      "val loss 0.12452812492847443\n",
      "______________\n",
      "epoch 1811 train loss 0.08965486288070679\n",
      "val loss 0.12451141327619553\n",
      "______________\n",
      "epoch 1812 train loss 0.0783059149980545\n",
      "val loss 0.124555803835392\n",
      "______________\n",
      "epoch 1813 train loss 0.06961575150489807\n",
      "val loss 0.12459326535463333\n",
      "______________\n",
      "epoch 1814 train loss 0.07767530530691147\n",
      "val loss 0.12461180984973907\n",
      "______________\n",
      "epoch 1815 train loss 0.0894908607006073\n",
      "val loss 0.12460730969905853\n",
      "______________\n",
      "epoch 1816 train loss 0.07221665978431702\n",
      "val loss 0.12462806701660156\n",
      "______________\n",
      "epoch 1817 train loss 0.0835055559873581\n",
      "val loss 0.12463399767875671\n",
      "______________\n",
      "epoch 1818 train loss 0.11245648562908173\n",
      "val loss 0.12462678551673889\n",
      "______________\n",
      "epoch 1819 train loss 0.0869792252779007\n",
      "val loss 0.12465187907218933\n",
      "______________\n",
      "epoch 1820 train loss 0.07909107208251953\n",
      "val loss 0.12463291734457016\n",
      "______________\n",
      "epoch 1821 train loss 0.09937085956335068\n",
      "val loss 0.12456779927015305\n",
      "______________\n",
      "epoch 1822 train loss 0.07960236072540283\n",
      "val loss 0.12449989467859268\n",
      "______________\n",
      "epoch 1823 train loss 0.10624277591705322\n",
      "val loss 0.12443913519382477\n",
      "______________\n",
      "epoch 1824 train loss 0.09350385516881943\n",
      "val loss 0.12433794140815735\n",
      "______________\n",
      "epoch 1825 train loss 0.07074730843305588\n",
      "val loss 0.12425550818443298\n",
      "______________\n",
      "epoch 1826 train loss 0.08066242188215256\n",
      "val loss 0.12418894469738007\n",
      "______________\n",
      "epoch 1827 train loss 0.07020093500614166\n",
      "val loss 0.12410628795623779\n",
      "______________\n",
      "epoch 1828 train loss 0.08009852468967438\n",
      "val loss 0.12404289841651917\n",
      "______________\n",
      "epoch 1829 train loss 0.10194149613380432\n",
      "val loss 0.12396679818630219\n",
      "______________\n",
      "epoch 1830 train loss 0.08858117461204529\n",
      "val loss 0.12387238442897797\n",
      "______________\n",
      "epoch 1831 train loss 0.09565427899360657\n",
      "val loss 0.1237892210483551\n",
      "______________\n",
      "epoch 1832 train loss 0.08546880632638931\n",
      "val loss 0.12372466921806335\n",
      "______________\n",
      "epoch 1833 train loss 0.09511412680149078\n",
      "val loss 0.12363652884960175\n",
      "______________\n",
      "epoch 1834 train loss 0.08759583532810211\n",
      "val loss 0.12357114255428314\n",
      "______________\n",
      "epoch 1835 train loss 0.06914840638637543\n",
      "val loss 0.12350612878799438\n",
      "______________\n",
      "epoch 1836 train loss 0.09044872224330902\n",
      "val loss 0.12345573306083679\n",
      "______________\n",
      "epoch 1837 train loss 0.07772397994995117\n",
      "val loss 0.12342707812786102\n",
      "______________\n",
      "epoch 1838 train loss 0.08161839097738266\n",
      "val loss 0.12342321872711182\n",
      "______________\n",
      "epoch 1839 train loss 0.08901440352201462\n",
      "val loss 0.12338628619909286\n",
      "______________\n",
      "epoch 1840 train loss 0.06817086040973663\n",
      "val loss 0.12337779998779297\n",
      "______________\n",
      "epoch 1841 train loss 0.058475930243730545\n",
      "val loss 0.12339227646589279\n",
      "______________\n",
      "epoch 1842 train loss 0.08068118989467621\n",
      "val loss 0.12340078502893448\n",
      "______________\n",
      "epoch 1843 train loss 0.09231869876384735\n",
      "val loss 0.12338054180145264\n",
      "______________\n",
      "epoch 1844 train loss 0.06338504701852798\n",
      "val loss 0.12336832284927368\n",
      "______________\n",
      "epoch 1845 train loss 0.08235417306423187\n",
      "val loss 0.12333428114652634\n",
      "______________\n",
      "epoch 1846 train loss 0.09011328220367432\n",
      "val loss 0.12324883043766022\n",
      "______________\n",
      "epoch 1847 train loss 0.07683828473091125\n",
      "val loss 0.12316744029521942\n",
      "______________\n",
      "epoch 1848 train loss 0.10057753324508667\n",
      "val loss 0.1230676919221878\n",
      "______________\n",
      "epoch 1849 train loss 0.07976958155632019\n",
      "val loss 0.12295673787593842\n",
      "______________\n",
      "epoch 1850 train loss 0.07828576117753983\n",
      "val loss 0.12283746898174286\n",
      "______________\n",
      "epoch 1851 train loss 0.09419327229261398\n",
      "val loss 0.12270429730415344\n",
      "______________\n",
      "epoch 1852 train loss 0.07872790098190308\n",
      "val loss 0.12259930372238159\n",
      "______________\n",
      "epoch 1853 train loss 0.09340370446443558\n",
      "val loss 0.12242430448532104\n",
      "______________\n",
      "epoch 1854 train loss 0.08369889110326767\n",
      "val loss 0.1222742572426796\n",
      "______________\n",
      "epoch 1855 train loss 0.08344496786594391\n",
      "val loss 0.12213427573442459\n",
      "______________\n",
      "epoch 1856 train loss 0.09136447310447693\n",
      "val loss 0.12197037041187286\n",
      "______________\n",
      "epoch 1857 train loss 0.08299952745437622\n",
      "val loss 0.12181602418422699\n",
      "______________\n",
      "epoch 1858 train loss 0.07181359082460403\n",
      "val loss 0.12166748940944672\n",
      "______________\n",
      "epoch 1859 train loss 0.08426663279533386\n",
      "val loss 0.1215243935585022\n",
      "______________\n",
      "epoch 1860 train loss 0.08266746252775192\n",
      "val loss 0.1213984489440918\n",
      "______________\n",
      "epoch 1861 train loss 0.08177115023136139\n",
      "val loss 0.121294304728508\n",
      "______________\n",
      "epoch 1862 train loss 0.08579131960868835\n",
      "val loss 0.12119509279727936\n",
      "______________\n",
      "epoch 1863 train loss 0.06616874039173126\n",
      "val loss 0.12110356986522675\n",
      "______________\n",
      "epoch 1864 train loss 0.08043141663074493\n",
      "val loss 0.12102599442005157\n",
      "______________\n",
      "epoch 1865 train loss 0.1043989434838295\n",
      "val loss 0.12093435227870941\n",
      "______________\n",
      "epoch 1866 train loss 0.06864585727453232\n",
      "val loss 0.12082983553409576\n",
      "______________\n",
      "epoch 1867 train loss 0.10773630440235138\n",
      "val loss 0.12072594463825226\n",
      "______________\n",
      "epoch 1868 train loss 0.08929140865802765\n",
      "val loss 0.12060859799385071\n",
      "______________\n",
      "epoch 1869 train loss 0.07146460562944412\n",
      "val loss 0.12048223614692688\n",
      "______________\n",
      "epoch 1870 train loss 0.06174251437187195\n",
      "val loss 0.12039946019649506\n",
      "______________\n",
      "epoch 1871 train loss 0.062308646738529205\n",
      "val loss 0.12035688757896423\n",
      "______________\n",
      "epoch 1872 train loss 0.07032446563243866\n",
      "val loss 0.12028829008340836\n",
      "______________\n",
      "epoch 1873 train loss 0.07800102233886719\n",
      "val loss 0.12022298574447632\n",
      "______________\n",
      "epoch 1874 train loss 0.06981120258569717\n",
      "val loss 0.1201440691947937\n",
      "______________\n",
      "epoch 1875 train loss 0.08594223856925964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 0.12008346617221832\n",
      "______________\n",
      "epoch 1876 train loss 0.07395332306623459\n",
      "val loss 0.12003910541534424\n",
      "______________\n",
      "epoch 1877 train loss 0.09155160188674927\n",
      "val loss 0.11995736509561539\n",
      "______________\n",
      "epoch 1878 train loss 0.07852783054113388\n",
      "val loss 0.11987482756376266\n",
      "______________\n",
      "epoch 1879 train loss 0.08789939433336258\n",
      "val loss 0.11977994441986084\n",
      "______________\n",
      "epoch 1880 train loss 0.06467851251363754\n",
      "val loss 0.11968136578798294\n",
      "______________\n",
      "epoch 1881 train loss 0.05937909334897995\n",
      "val loss 0.11960476636886597\n",
      "______________\n",
      "epoch 1882 train loss 0.0994437113404274\n",
      "val loss 0.11951343715190887\n",
      "______________\n",
      "epoch 1883 train loss 0.08659110963344574\n",
      "val loss 0.1194242388010025\n",
      "______________\n",
      "epoch 1884 train loss 0.08818425238132477\n",
      "val loss 0.11930045485496521\n",
      "______________\n",
      "epoch 1885 train loss 0.08864790201187134\n",
      "val loss 0.11918330192565918\n",
      "______________\n",
      "epoch 1886 train loss 0.08167676627635956\n",
      "val loss 0.11905103921890259\n",
      "______________\n",
      "epoch 1887 train loss 0.08563876897096634\n",
      "val loss 0.11892744898796082\n",
      "______________\n",
      "epoch 1888 train loss 0.10590098798274994\n",
      "val loss 0.11877478659152985\n",
      "______________\n",
      "epoch 1889 train loss 0.07698293030261993\n",
      "val loss 0.1186455488204956\n",
      "______________\n",
      "epoch 1890 train loss 0.07933086901903152\n",
      "val loss 0.11853586137294769\n",
      "______________\n",
      "epoch 1891 train loss 0.08134353905916214\n",
      "val loss 0.11840926855802536\n",
      "______________\n",
      "epoch 1892 train loss 0.10762376338243484\n",
      "val loss 0.11826719343662262\n",
      "______________\n",
      "epoch 1893 train loss 0.0937466248869896\n",
      "val loss 0.11810029298067093\n",
      "______________\n",
      "epoch 1894 train loss 0.0710206851363182\n",
      "val loss 0.1179405227303505\n",
      "______________\n",
      "epoch 1895 train loss 0.06548399478197098\n",
      "val loss 0.11781413108110428\n",
      "______________\n",
      "epoch 1896 train loss 0.08874087035655975\n",
      "val loss 0.11767157912254333\n",
      "______________\n",
      "epoch 1897 train loss 0.08179937303066254\n",
      "val loss 0.11755646765232086\n",
      "______________\n",
      "epoch 1898 train loss 0.07642955332994461\n",
      "val loss 0.11743335425853729\n",
      "______________\n",
      "epoch 1899 train loss 0.05521661788225174\n",
      "val loss 0.11732250452041626\n",
      "______________\n",
      "epoch 1900 train loss 0.08029106259346008\n",
      "val loss 0.11722678691148758\n",
      "______________\n",
      "epoch 1901 train loss 0.08129622042179108\n",
      "val loss 0.11712121963500977\n",
      "______________\n",
      "epoch 1902 train loss 0.09953391551971436\n",
      "val loss 0.11701519787311554\n",
      "______________\n",
      "epoch 1903 train loss 0.06856141239404678\n",
      "val loss 0.11694677174091339\n",
      "______________\n",
      "epoch 1904 train loss 0.07983168959617615\n",
      "val loss 0.11685572564601898\n",
      "______________\n",
      "epoch 1905 train loss 0.09321095049381256\n",
      "val loss 0.11672802269458771\n",
      "______________\n",
      "epoch 1906 train loss 0.07238403707742691\n",
      "val loss 0.11660824716091156\n",
      "______________\n",
      "epoch 1907 train loss 0.07692790031433105\n",
      "val loss 0.11646535992622375\n",
      "______________\n",
      "epoch 1908 train loss 0.09693817794322968\n",
      "val loss 0.11632983386516571\n",
      "______________\n",
      "epoch 1909 train loss 0.08921810239553452\n",
      "val loss 0.11615471541881561\n",
      "______________\n",
      "epoch 1910 train loss 0.06731036305427551\n",
      "val loss 0.11598439514636993\n",
      "______________\n",
      "epoch 1911 train loss 0.07606246322393417\n",
      "val loss 0.11584842205047607\n",
      "______________\n",
      "epoch 1912 train loss 0.07115568220615387\n",
      "val loss 0.11574620008468628\n",
      "______________\n",
      "epoch 1913 train loss 0.07460707426071167\n",
      "val loss 0.11565640568733215\n",
      "______________\n",
      "epoch 1914 train loss 0.11885738372802734\n",
      "val loss 0.11554215848445892\n",
      "______________\n",
      "epoch 1915 train loss 0.07842719554901123\n",
      "val loss 0.1154373288154602\n",
      "______________\n",
      "epoch 1916 train loss 0.09333427250385284\n",
      "val loss 0.11531572043895721\n",
      "______________\n",
      "epoch 1917 train loss 0.09318631887435913\n",
      "val loss 0.11517706513404846\n",
      "______________\n",
      "epoch 1918 train loss 0.08091920614242554\n",
      "val loss 0.11504155397415161\n",
      "______________\n",
      "epoch 1919 train loss 0.10696140676736832\n",
      "val loss 0.11490140110254288\n",
      "______________\n",
      "epoch 1920 train loss 0.06853566318750381\n",
      "val loss 0.11480593681335449\n",
      "______________\n",
      "epoch 1921 train loss 0.11095573008060455\n",
      "val loss 0.11471626162528992\n",
      "______________\n",
      "epoch 1922 train loss 0.08447875082492828\n",
      "val loss 0.11463020741939545\n",
      "______________\n",
      "epoch 1923 train loss 0.08330212533473969\n",
      "val loss 0.11452404409646988\n",
      "______________\n",
      "epoch 1924 train loss 0.10432577133178711\n",
      "val loss 0.11441320180892944\n",
      "______________\n",
      "epoch 1925 train loss 0.11857124418020248\n",
      "val loss 0.11428069323301315\n",
      "______________\n",
      "epoch 1926 train loss 0.08292548358440399\n",
      "val loss 0.11415570229291916\n",
      "______________\n",
      "epoch 1927 train loss 0.08642447739839554\n",
      "val loss 0.11400627344846725\n",
      "______________\n",
      "epoch 1928 train loss 0.056345585733652115\n",
      "val loss 0.11389059573411942\n",
      "______________\n",
      "epoch 1929 train loss 0.09517834335565567\n",
      "val loss 0.11378985643386841\n",
      "______________\n",
      "epoch 1930 train loss 0.09866877645254135\n",
      "val loss 0.11370407044887543\n",
      "______________\n",
      "epoch 1931 train loss 0.09094023704528809\n",
      "val loss 0.11360578238964081\n",
      "______________\n",
      "epoch 1932 train loss 0.07281114906072617\n",
      "val loss 0.11348535120487213\n",
      "______________\n",
      "epoch 1933 train loss 0.0743948221206665\n",
      "val loss 0.11339844018220901\n",
      "______________\n",
      "epoch 1934 train loss 0.05883411690592766\n",
      "val loss 0.1133212298154831\n",
      "______________\n",
      "epoch 1935 train loss 0.07543016970157623\n",
      "val loss 0.11323624104261398\n",
      "______________\n",
      "epoch 1936 train loss 0.06462486833333969\n",
      "val loss 0.11316052824258804\n",
      "______________\n",
      "epoch 1937 train loss 0.11010396480560303\n",
      "val loss 0.11305892467498779\n",
      "______________\n",
      "epoch 1938 train loss 0.08635053783655167\n",
      "val loss 0.11298588663339615\n",
      "______________\n",
      "epoch 1939 train loss 0.09526243805885315\n",
      "val loss 0.11289586126804352\n",
      "______________\n",
      "epoch 1940 train loss 0.0830531194806099\n",
      "val loss 0.11281569302082062\n",
      "______________\n",
      "epoch 1941 train loss 0.07997341454029083\n",
      "val loss 0.1127060279250145\n",
      "______________\n",
      "epoch 1942 train loss 0.09318874776363373\n",
      "val loss 0.11261379718780518\n",
      "______________\n",
      "epoch 1943 train loss 0.06875177472829819\n",
      "val loss 0.11252781748771667\n",
      "______________\n",
      "epoch 1944 train loss 0.06363366544246674\n",
      "val loss 0.11247914284467697\n",
      "______________\n",
      "epoch 1945 train loss 0.10593190789222717\n",
      "val loss 0.11238697171211243\n",
      "______________\n",
      "epoch 1946 train loss 0.09946204721927643\n",
      "val loss 0.11225489526987076\n",
      "______________\n",
      "epoch 1947 train loss 0.07245835661888123\n",
      "val loss 0.11215531826019287\n",
      "______________\n",
      "epoch 1948 train loss 0.07743861526250839\n",
      "val loss 0.11208678036928177\n",
      "______________\n",
      "epoch 1949 train loss 0.09582266956567764\n",
      "val loss 0.11198665201663971\n",
      "______________\n",
      "epoch 1950 train loss 0.09332988411188126\n",
      "val loss 0.1119103729724884\n",
      "______________\n",
      "epoch 1951 train loss 0.06798475980758667\n",
      "val loss 0.11185495555400848\n",
      "______________\n",
      "epoch 1952 train loss 0.10941824316978455\n",
      "val loss 0.11175069957971573\n",
      "______________\n",
      "epoch 1953 train loss 0.07564454525709152\n",
      "val loss 0.11164996027946472\n",
      "______________\n",
      "epoch 1954 train loss 0.09147942066192627\n",
      "val loss 0.11156781017780304\n",
      "______________\n",
      "epoch 1955 train loss 0.08256968855857849\n",
      "val loss 0.11149220913648605\n",
      "______________\n",
      "epoch 1956 train loss 0.10808700323104858\n",
      "val loss 0.11143821477890015\n",
      "______________\n",
      "epoch 1957 train loss 0.07857749611139297\n",
      "val loss 0.11137224733829498\n",
      "______________\n",
      "epoch 1958 train loss 0.10734956711530685\n",
      "val loss 0.11130465567111969\n",
      "______________\n",
      "epoch 1959 train loss 0.07668070495128632\n",
      "val loss 0.11128401756286621\n",
      "______________\n",
      "epoch 1960 train loss 0.101629339158535\n",
      "val loss 0.11123804748058319\n",
      "______________\n",
      "epoch 1961 train loss 0.07874918729066849\n",
      "val loss 0.11124172806739807\n",
      "______________\n",
      "epoch 1962 train loss 0.06650931388139725\n",
      "val loss 0.1112341359257698\n",
      "______________\n",
      "epoch 1963 train loss 0.10474002361297607\n",
      "val loss 0.1112138032913208\n",
      "______________\n",
      "epoch 1964 train loss 0.06632421910762787\n",
      "val loss 0.11116954684257507\n",
      "______________\n",
      "epoch 1965 train loss 0.09203191846609116\n",
      "val loss 0.11110170185565948\n",
      "______________\n",
      "epoch 1966 train loss 0.08448716998100281\n",
      "val loss 0.11105941236019135\n",
      "______________\n",
      "epoch 1967 train loss 0.06706519424915314\n",
      "val loss 0.11100608110427856\n",
      "______________\n",
      "epoch 1968 train loss 0.09060602635145187\n",
      "val loss 0.11095339059829712\n",
      "______________\n",
      "epoch 1969 train loss 0.06073503941297531\n",
      "val loss 0.11088310182094574\n",
      "______________\n",
      "epoch 1970 train loss 0.07374930381774902\n",
      "val loss 0.11081727594137192\n",
      "______________\n",
      "epoch 1971 train loss 0.08214175701141357\n",
      "val loss 0.11074396222829819\n",
      "______________\n",
      "epoch 1972 train loss 0.07904395461082458\n",
      "val loss 0.11066710948944092\n",
      "______________\n",
      "epoch 1973 train loss 0.08161381632089615\n",
      "val loss 0.11057846993207932\n",
      "______________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1974 train loss 0.06210070848464966\n",
      "val loss 0.11049351096153259\n",
      "______________\n",
      "epoch 1975 train loss 0.07640516757965088\n",
      "val loss 0.11039852350950241\n",
      "______________\n",
      "epoch 1976 train loss 0.07738958299160004\n",
      "val loss 0.11029817163944244\n",
      "______________\n",
      "epoch 1977 train loss 0.07857005298137665\n",
      "val loss 0.11020424962043762\n",
      "______________\n",
      "epoch 1978 train loss 0.10593549907207489\n",
      "val loss 0.11006031185388565\n",
      "______________\n",
      "epoch 1979 train loss 0.06290172785520554\n",
      "val loss 0.109925776720047\n",
      "______________\n",
      "epoch 1980 train loss 0.0633811503648758\n",
      "val loss 0.10979132354259491\n",
      "______________\n",
      "epoch 1981 train loss 0.049256086349487305\n",
      "val loss 0.10969626158475876\n",
      "______________\n",
      "epoch 1982 train loss 0.06682717800140381\n",
      "val loss 0.1096091940999031\n",
      "______________\n",
      "epoch 1983 train loss 0.08670559525489807\n",
      "val loss 0.10951701551675797\n",
      "______________\n",
      "epoch 1984 train loss 0.0818590596318245\n",
      "val loss 0.10942773520946503\n",
      "______________\n",
      "epoch 1985 train loss 0.07078488171100616\n",
      "val loss 0.1093522310256958\n",
      "______________\n",
      "epoch 1986 train loss 0.06815499812364578\n",
      "val loss 0.10926505923271179\n",
      "______________\n",
      "epoch 1987 train loss 0.07151105999946594\n",
      "val loss 0.10919463634490967\n",
      "______________\n",
      "epoch 1988 train loss 0.08979502320289612\n",
      "val loss 0.10915395617485046\n",
      "______________\n",
      "epoch 1989 train loss 0.06814240664243698\n",
      "val loss 0.10911858081817627\n",
      "______________\n",
      "epoch 1990 train loss 0.07985842227935791\n",
      "val loss 0.10911358892917633\n",
      "______________\n",
      "epoch 1991 train loss 0.09750044345855713\n",
      "val loss 0.10909833014011383\n",
      "______________\n",
      "epoch 1992 train loss 0.08304980397224426\n",
      "val loss 0.10907265543937683\n",
      "______________\n",
      "epoch 1993 train loss 0.07699081301689148\n",
      "val loss 0.10905981063842773\n",
      "______________\n",
      "epoch 1994 train loss 0.08564167469739914\n",
      "val loss 0.10905350744724274\n",
      "______________\n",
      "epoch 1995 train loss 0.08092383295297623\n",
      "val loss 0.10905437171459198\n",
      "______________\n",
      "epoch 1996 train loss 0.06691810488700867\n",
      "val loss 0.10905937850475311\n",
      "______________\n",
      "epoch 1997 train loss 0.08302859961986542\n",
      "val loss 0.10907670855522156\n",
      "______________\n",
      "epoch 1998 train loss 0.07736034691333771\n",
      "val loss 0.10908693075180054\n",
      "______________\n",
      "epoch 1999 train loss 0.0661686509847641\n",
      "val loss 0.10911309719085693\n",
      "______________\n",
      "epoch 2000 train loss 0.0841064527630806\n",
      "val loss 0.10910490155220032\n",
      "______________\n",
      "epoch 2001 train loss 0.07440541684627533\n",
      "val loss 0.1090976670384407\n",
      "______________\n",
      "epoch 2002 train loss 0.0690714418888092\n",
      "val loss 0.1090952679514885\n",
      "______________\n",
      "epoch 2003 train loss 0.07323647290468216\n",
      "val loss 0.10913252830505371\n",
      "______________\n",
      "epoch 2004 train loss 0.07652044296264648\n",
      "val loss 0.10914976894855499\n",
      "______________\n",
      "epoch 2005 train loss 0.05610106885433197\n",
      "val loss 0.10922616720199585\n",
      "______________\n",
      "best loss 0.10905350744724274 {'Overall Survival (4 Years)': {'accuracy': -1, 'mse': 0.004035286, 'auc': 1.0}, 'FT': {'accuracy': -1, 'mse': 0.002339552, 'auc': 1.0}, 'Aspiration rate Post-therapy': {'accuracy': -1, 'mse': 0.016396627, 'auc': 0.9989837398373984}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EndpointSimulator(\n",
       "  (input_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=82, out_features=500, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (batchnorm): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (softmax): LogSoftmax(dim=1)\n",
       "  (outcome_layer): Linear(in_features=500, out_features=3, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = train_state(state=3,epochs=10000)\n",
    "model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d543de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pd_state1': <function __main__.breakup_state_models.<locals>.<lambda>(x)>,\n",
       " 'nd_state1': <function __main__.breakup_state_models.<locals>.<lambda>(x)>,\n",
       " 'chemo_state1': <function __main__.breakup_state_models.<locals>.<lambda>(x)>,\n",
       " 'DLT_Hematological_state1': <function __main__.breakup_state_models.<locals>.<lambda>(x)>,\n",
       " 'DLT_Gastrointestinal_state1': <function __main__.breakup_state_models.<locals>.<lambda>(x)>,\n",
       " 'DLT_Nephrological_state1': <function __main__.breakup_state_models.<locals>.<lambda>(x)>,\n",
       " 'DLT_Other_state1': <function __main__.breakup_state_models.<locals>.<lambda>(x)>,\n",
       " 'DLT_Infection (Pneumonia)_state1': <function __main__.breakup_state_models.<locals>.<lambda>(x)>,\n",
       " 'DLT_Dermatological_state1': <function __main__.breakup_state_models.<locals>.<lambda>(x)>,\n",
       " 'DLT_Neurological_state1': <function __main__.breakup_state_models.<locals>.<lambda>(x)>,\n",
       " 'DLT_Vascular_state1': <function __main__.breakup_state_models.<locals>.<lambda>(x)>,\n",
       " 'pd_state2': <function __main__.breakup_state_models.<locals>.<lambda>(x)>,\n",
       " 'nd_state2': <function __main__.breakup_state_models.<locals>.<lambda>(x)>,\n",
       " 'chemo_state2': <function __main__.breakup_state_models.<locals>.<lambda>(x)>,\n",
       " 'DLT_Hematological_state2': <function __main__.breakup_state_models.<locals>.<lambda>(x)>,\n",
       " 'DLT_Gastrointestinal_state2': <function __main__.breakup_state_models.<locals>.<lambda>(x)>,\n",
       " 'DLT_Nephrological_state2': <function __main__.breakup_state_models.<locals>.<lambda>(x)>,\n",
       " 'DLT_Other_state2': <function __main__.breakup_state_models.<locals>.<lambda>(x)>,\n",
       " 'DLT_Infection (Pneumonia)_state2': <function __main__.breakup_state_models.<locals>.<lambda>(x)>,\n",
       " 'DLT_Dermatological_state2': <function __main__.breakup_state_models.<locals>.<lambda>(x)>,\n",
       " 'DLT_Neurological_state2': <function __main__.breakup_state_models.<locals>.<lambda>(x)>,\n",
       " 'DLT_Vascular_state2': <function __main__.breakup_state_models.<locals>.<lambda>(x)>,\n",
       " 'Overall Survival (4 Years)_state3': <function __main__.breakup_outcome_models.<locals>.<lambda>(x)>,\n",
       " 'FT_state3': <function __main__.breakup_outcome_models.<locals>.<lambda>(x)>,\n",
       " 'Aspiration rate Post-therapy_state3': <function __main__.breakup_outcome_models.<locals>.<lambda>(x)>}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def breakup_state_models(state_model):\n",
    "    #for state 1 and 2\n",
    "    models = {}\n",
    "    models['pd'] = lambda x: state_model(x)[0]\n",
    "    models['nd'] = lambda x: state_model(x)[1]\n",
    "    models['chemo'] = lambda x: state_model(x)[2]\n",
    "    for i,dlt in enumerate(Const.dlt1):\n",
    "        models[dlt] = lambda x: state_model(x)[3][:,i]\n",
    "    return models\n",
    "\n",
    "def breakup_outcome_models(omodel):\n",
    "    models = {}\n",
    "    for i,name in enumerate(Const.outcomes):\n",
    "        models[name] = lambda x: omodel(x)[:,i].reshape(-1,1)\n",
    "    return models\n",
    "\n",
    "def get_all_models(m1,m2,m3):\n",
    "    state1_models = breakup_state_models(m1)\n",
    "    state2_models = breakup_state_models(m2)\n",
    "    state3_models = breakup_outcome_models(m3)\n",
    "    all_models = {}\n",
    "    for i,sm in enumerate([state1_models,state2_models,state3_models]):\n",
    "        for ii,m in sm.items():\n",
    "            all_models[ii +  '_state' + str(i+1)] = m\n",
    "    return all_models\n",
    "\n",
    "all_models = get_all_models(model,model2,model3)\n",
    "all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6481203c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2197865/1521532340.py:4: DtypeWarning: Columns (55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_file)\n",
      "/tmp/ipykernel_2197865/1521532340.py:19: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  self.means = self.processed_df.mean(axis=0)\n",
      "/tmp/ipykernel_2197865/1521532340.py:20: FutureWarning: The default value of numeric_only in DataFrame.std is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  self.stds = self.processed_df.std(axis=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([536, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "key ='FT_state3'\n",
    "ig = IntegratedGradients(all_models[key])\n",
    "dataset = DTDataset()\n",
    "xtestdf = dataset.get_input_state(step=int(key[-1]))\n",
    "xtest = df_to_torch(xtestdf)\n",
    "all_models[key](xtest).shape\n",
    "# attributions = ig.attribute(xtest,torch.zeros(xtest.shape),target=1)\n",
    "# test = pd.DataFrame(attributions,columns = xtestdf.columns, index=xtestdf.index)\n",
    "# test.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3adc4ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Must pass 2-d input. shape=(18, 536, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#         impacts[decision] = entry\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame([r[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results],columns \u001b[38;5;241m=\u001b[39m [r[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results],index\u001b[38;5;241m=\u001b[39mids)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mcheck_impact_of_decisions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_models\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 20\u001b[0m, in \u001b[0;36mcheck_impact_of_decisions\u001b[0;34m(model_dict, data)\u001b[0m\n\u001b[1;32m     18\u001b[0m             results\u001b[38;5;241m.\u001b[39mappend((decision \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m outcome\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_decision_change\u001b[39m\u001b[38;5;124m'\u001b[39m,decision_change))\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#         impacts[decision] = entry\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:761\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    753\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    754\u001b[0m             arrays,\n\u001b[1;32m    755\u001b[0m             columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    758\u001b[0m             typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    759\u001b[0m         )\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 761\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    766\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    770\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[1;32m    771\u001b[0m         {},\n\u001b[1;32m    772\u001b[0m         index,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    775\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    776\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/internals/construction.py:329\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    324\u001b[0m         values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;66;03m# by definition an array here\u001b[39;00m\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;66;03m# the dtypes will be coerced to a single dtype\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_prep_ndarraylike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy_on_sanitize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dtype_equal(values\u001b[38;5;241m.\u001b[39mdtype, dtype):\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;66;03m# GH#40110 see similar check inside sanitize_array\u001b[39;00m\n\u001b[1;32m    333\u001b[0m     rcf \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m (is_integer_dtype(dtype) \u001b[38;5;129;01mand\u001b[39;00m values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/internals/construction.py:583\u001b[0m, in \u001b[0;36m_prep_ndarraylike\u001b[0;34m(values, copy)\u001b[0m\n\u001b[1;32m    581\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mreshape((values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m values\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 583\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust pass 2-d input. shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalues\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "\u001b[0;31mValueError\u001b[0m: Must pass 2-d input. shape=(18, 536, 1)"
     ]
    }
   ],
   "source": [
    "def check_impact_of_decisions(model_dict,data):\n",
    "    results = []\n",
    "    ids = []\n",
    "    for decision in Const.decisions:\n",
    "        subset0 = dataset.get_input_state(step=3,fixed={decision: 0})\n",
    "        subset1 = dataset.get_input_state(step=3,fixed={decision: 1})\n",
    "        ids = subset0.index.values\n",
    "        x0 = df_to_torch(subset0)\n",
    "        x1 = df_to_torch(subset1)\n",
    "        entry = {}\n",
    "        for outcome in Const.outcomes:\n",
    "            model = model_dict[outcome+\"_state3\"]\n",
    "            y0 = model(x0).detach().cpu().numpy()\n",
    "            y1 = model(x1).detach().cpu().numpy()\n",
    "            change = y1 - y0\n",
    "            decision_change = np.abs((y0 > .5).astype(int) - (y1 > .5).astype(int))\n",
    "            results.append((decision + ': ' + outcome+'_change',change ))\n",
    "            results.append((decision + ': ' + outcome+'_decision_change',decision_change))\n",
    "#         impacts[decision] = entry\n",
    "    return pd.DataFrame([r[1] for r in results],columns = [r[0] for r in results],index=ids)\n",
    "\n",
    "check_impact_of_decisions(all_models,dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321249c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
