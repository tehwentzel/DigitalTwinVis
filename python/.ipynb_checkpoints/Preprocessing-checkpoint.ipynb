{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "050c878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7418d82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score,accuracy_score,precision_recall_fscore_support\n",
    "from Constants import *\n",
    "from Preprocessing import *\n",
    "from Models import *\n",
    "import copy\n",
    "from Utils import *\n",
    "from DeepSurvivalModels import *\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c427599a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(536, 66)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = DTDataset(use_smote=False)\n",
    "data.processed_df.T\n",
    "data.get_input_state(1).shape\n",
    "# data.processed_df#.shape, len(data.processed_df.index.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2937b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Utils import *\n",
    "model1,model2,model3,smodel3 = load_transition_models()\n",
    "model3.input_mean.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b29d259b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([147, 65])\n",
      "tshape torch.Size([147])\n",
      "torch.Size([147]) torch.Size([147])\n",
      "tshape torch.Size([147])\n",
      "torch.Size([147]) torch.Size([147])\n",
      "tshape torch.Size([147])\n",
      "torch.Size([147]) torch.Size([147])\n",
      "tshape torch.Size([147])\n",
      "torch.Size([147]) torch.Size([147])\n",
      "tshape torch.Size([147])\n",
      "torch.Size([147]) torch.Size([147])\n",
      "tshape torch.Size([147])\n",
      "torch.Size([147]) torch.Size([147])\n",
      "tshape torch.Size([147])\n",
      "torch.Size([147]) torch.Size([147])\n",
      "tshape torch.Size([147])\n",
      "torch.Size([147]) torch.Size([147])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 1., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 1., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 1.],\n",
       "         [0., 1., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 1., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 1., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]]),\n",
       " {'pd1': tensor([[0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [9.6381e-05, 9.9990e-01, 5.1420e-06],\n",
       "          [9.8699e-03, 9.8993e-01, 2.0230e-04],\n",
       "          [6.5057e-01, 3.4781e-01, 1.6198e-03],\n",
       "          [3.9192e-01, 6.0705e-01, 1.0271e-03],\n",
       "          [6.1163e-01, 3.8788e-01, 4.8592e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [6.6711e-01, 3.3198e-01, 9.1157e-04],\n",
       "          [8.6025e-01, 1.3890e-01, 8.5036e-04],\n",
       "          [5.7270e-01, 4.2698e-01, 3.2081e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [7.2470e-01, 2.7445e-01, 8.4905e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [6.4060e-01, 3.5843e-01, 9.7151e-04],\n",
       "          [2.5823e-04, 9.9974e-01, 5.2572e-07],\n",
       "          [4.0966e-01, 5.8928e-01, 1.0626e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [9.8548e-01, 1.4429e-02, 9.3756e-05],\n",
       "          [8.6312e-02, 9.1289e-01, 7.9357e-04],\n",
       "          [8.3449e-01, 1.6469e-01, 8.1684e-04],\n",
       "          [4.9389e-01, 5.0371e-01, 2.3967e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [9.9604e-01, 3.9557e-03, 4.4192e-37],\n",
       "          [4.5732e-01, 5.4150e-01, 1.1805e-03],\n",
       "          [9.9900e-01, 1.0040e-03, 2.5115e-37],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [7.6772e-01, 2.3147e-01, 8.1079e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [9.7805e-01, 2.1582e-02, 3.7029e-04],\n",
       "          [9.3576e-01, 6.3779e-02, 4.5901e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [9.8574e-01, 1.4128e-02, 1.3298e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [8.1370e-01, 1.8545e-01, 8.5226e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [2.5270e-01, 7.4646e-01, 8.3839e-04],\n",
       "          [8.5867e-01, 1.4087e-01, 4.6163e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [1.7858e-02, 9.8208e-01, 6.2036e-05],\n",
       "          [9.6156e-01, 3.8244e-02, 1.9091e-04],\n",
       "          [9.7036e-01, 2.9546e-02, 9.4685e-05],\n",
       "          [9.7038e-01, 2.8861e-02, 7.5609e-04],\n",
       "          [2.6030e-01, 7.3864e-01, 1.0583e-03],\n",
       "          [6.6934e-01, 3.3044e-01, 2.1729e-04],\n",
       "          [6.5694e-01, 3.4085e-01, 2.2122e-03],\n",
       "          [1.9156e-01, 8.0677e-01, 1.6743e-03],\n",
       "          [3.7956e-01, 6.1900e-01, 1.4394e-03],\n",
       "          [3.4690e-01, 6.5204e-01, 1.0623e-03],\n",
       "          [2.3121e-02, 9.7636e-01, 5.1619e-04],\n",
       "          [6.7397e-01, 3.2531e-01, 7.2591e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [2.4916e-01, 7.5084e-01, 2.3842e-06],\n",
       "          [6.5411e-01, 3.4435e-01, 1.5436e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [9.2108e-01, 7.7938e-02, 9.8619e-04],\n",
       "          [8.7471e-01, 1.2415e-01, 1.1421e-03],\n",
       "          [9.8218e-01, 1.7660e-02, 1.6112e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [9.5431e-01, 4.4923e-02, 7.6340e-04],\n",
       "          [1.1814e-01, 8.8113e-01, 7.3243e-04],\n",
       "          [9.8534e-01, 1.4340e-02, 3.2243e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [1.8134e-02, 9.8153e-01, 3.3207e-04],\n",
       "          [8.6854e-01, 1.3052e-01, 9.4395e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [9.8762e-01, 1.2261e-02, 1.1947e-04],\n",
       "          [4.9794e-01, 5.0172e-01, 3.4103e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [6.0433e-05, 9.9994e-01, 2.8649e-06],\n",
       "          [1.4653e-02, 9.8499e-01, 3.5603e-04],\n",
       "          [1.9159e-02, 9.8073e-01, 1.1399e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [4.3417e-01, 5.6513e-01, 7.0545e-04],\n",
       "          [4.2824e-01, 5.7120e-01, 5.5647e-04],\n",
       "          [1.9239e-01, 8.0611e-01, 1.4974e-03],\n",
       "          [9.9354e-01, 6.3930e-03, 6.5945e-05],\n",
       "          [7.5111e-01, 2.4871e-01, 1.8342e-04],\n",
       "          [8.9516e-01, 1.0440e-01, 4.3892e-04],\n",
       "          [4.4829e-01, 5.5076e-01, 9.5206e-04],\n",
       "          [3.5155e-01, 6.4816e-01, 2.9218e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [8.7834e-01, 1.2120e-01, 4.6421e-04],\n",
       "          [3.9265e-01, 6.0649e-01, 8.5168e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [4.3696e-02, 9.5630e-01, 7.4706e-06],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [8.0657e-01, 1.9199e-01, 1.4333e-03],\n",
       "          [1.7672e-02, 9.8228e-01, 4.9546e-05],\n",
       "          [9.7003e-01, 2.9546e-02, 4.1903e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [9.4771e-01, 5.1719e-02, 5.7365e-04],\n",
       "          [8.9857e-01, 1.0003e-01, 1.3930e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [9.2523e-01, 7.4064e-02, 7.0524e-04],\n",
       "          [9.3199e-01, 6.7529e-02, 4.7620e-04],\n",
       "          [9.0220e-01, 9.7438e-02, 3.6686e-04],\n",
       "          [7.7137e-01, 2.2771e-01, 9.2007e-04],\n",
       "          [8.9563e-01, 1.0404e-01, 3.2624e-04],\n",
       "          [8.3360e-01, 1.6560e-01, 7.9920e-04],\n",
       "          [4.5628e-02, 9.5365e-01, 7.2280e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [2.0358e-01, 7.9545e-01, 9.7110e-04],\n",
       "          [9.2720e-01, 7.2190e-02, 6.1252e-04],\n",
       "          [8.9644e-01, 1.0253e-01, 1.0293e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [4.0105e-01, 5.9760e-01, 1.3505e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [8.4530e-01, 1.5393e-01, 7.7052e-04],\n",
       "          [1.5297e-01, 8.4579e-01, 1.2414e-03],\n",
       "          [7.4091e-01, 2.5718e-01, 1.9120e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [7.8975e-01, 2.0916e-01, 1.0954e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [3.4747e-03, 9.9648e-01, 4.8346e-05],\n",
       "          [6.2249e-02, 9.3765e-01, 1.0242e-04],\n",
       "          [8.8075e-01, 1.1921e-01, 4.5083e-05],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [2.9694e-01, 6.9994e-01, 3.1159e-03],\n",
       "          [8.7779e-01, 1.2094e-01, 1.2706e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [8.9227e-01, 1.0671e-01, 1.0124e-03],\n",
       "          [6.7375e-01, 3.2389e-01, 2.3594e-03],\n",
       "          [9.8608e-01, 1.3798e-02, 1.2055e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [8.8355e-01, 1.1571e-01, 7.4305e-04],\n",
       "          [1.2383e-01, 8.7544e-01, 7.2576e-04],\n",
       "          [7.6282e-02, 9.2280e-01, 9.1796e-04],\n",
       "          [9.0926e-01, 9.0736e-02, 5.3784e-06],\n",
       "          [1.2106e-01, 8.7872e-01, 2.1850e-04],\n",
       "          [1.8126e-01, 8.1797e-01, 7.7373e-04],\n",
       "          [7.2233e-01, 2.7628e-01, 1.3842e-03],\n",
       "          [9.3981e-01, 5.9730e-02, 4.5547e-04]], grad_fn=<CopySlices>),\n",
       "  'nd1': tensor([[0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [1.1737e-06, 1.0000e+00, 1.1731e-06],\n",
       "          [7.4461e-06, 9.9999e-01, 7.4257e-06],\n",
       "          [1.8164e-05, 9.9996e-01, 1.8061e-05],\n",
       "          [4.1083e-06, 9.9999e-01, 4.0977e-06],\n",
       "          [2.7409e-06, 9.9999e-01, 2.7347e-06],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [4.2139e-06, 9.9999e-01, 4.2014e-06],\n",
       "          [7.6117e-06, 9.9998e-01, 7.5849e-06],\n",
       "          [6.1540e-07, 1.0000e+00, 6.1460e-07],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [3.7227e-06, 9.9999e-01, 3.7143e-06],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [4.3668e-06, 9.9999e-01, 4.3509e-06],\n",
       "          [4.2043e-09, 1.0000e+00, 4.2041e-09],\n",
       "          [6.5886e-06, 9.9999e-01, 6.5688e-06],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [1.3340e-06, 1.0000e+00, 1.3318e-06],\n",
       "          [1.3998e-05, 9.9997e-01, 1.3945e-05],\n",
       "          [5.2159e-06, 9.9999e-01, 5.1957e-06],\n",
       "          [4.0004e-05, 9.9992e-01, 3.9739e-05],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 1.0000e+00, 0.0000e+00],\n",
       "          [5.0543e-06, 9.9999e-01, 5.0353e-06],\n",
       "          [0.0000e+00, 1.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [5.6763e-06, 9.9999e-01, 5.6557e-06],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [9.6670e-06, 9.9998e-01, 9.6206e-06],\n",
       "          [5.4448e-06, 9.9999e-01, 5.4226e-06],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [2.2928e-06, 1.0000e+00, 2.2866e-06],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [9.2643e-06, 9.9998e-01, 9.2207e-06],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [3.7063e-06, 9.9999e-01, 3.6981e-06],\n",
       "          [2.5369e-06, 9.9999e-01, 2.5312e-06],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [2.7866e-07, 1.0000e+00, 2.7851e-07],\n",
       "          [1.4708e-06, 1.0000e+00, 1.4671e-06],\n",
       "          [4.3580e-07, 1.0000e+00, 4.3527e-07],\n",
       "          [3.3504e-05, 9.9993e-01, 3.3231e-05],\n",
       "          [6.1468e-06, 9.9999e-01, 6.1228e-06],\n",
       "          [4.0592e-07, 1.0000e+00, 4.0553e-07],\n",
       "          [3.0131e-05, 9.9994e-01, 2.9927e-05],\n",
       "          [2.2892e-05, 9.9995e-01, 2.2774e-05],\n",
       "          [9.7779e-06, 9.9998e-01, 9.7373e-06],\n",
       "          [4.5132e-06, 9.9999e-01, 4.5015e-06],\n",
       "          [1.7655e-05, 9.9996e-01, 1.7575e-05],\n",
       "          [3.6288e-06, 9.9999e-01, 3.6196e-06],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [1.2645e-10, 1.0000e+00, 1.2645e-10],\n",
       "          [1.7551e-05, 9.9996e-01, 1.7474e-05],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [2.1005e-05, 9.9996e-01, 2.0861e-05],\n",
       "          [1.8872e-05, 9.9996e-01, 1.8758e-05],\n",
       "          [2.0609e-06, 1.0000e+00, 2.0548e-06],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [1.9348e-05, 9.9996e-01, 1.9218e-05],\n",
       "          [1.1935e-05, 9.9998e-01, 1.1887e-05],\n",
       "          [1.2611e-05, 9.9997e-01, 1.2540e-05],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [1.3359e-05, 9.9997e-01, 1.3314e-05],\n",
       "          [1.0935e-05, 9.9998e-01, 1.0878e-05],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [1.5357e-06, 1.0000e+00, 1.5322e-06],\n",
       "          [1.0872e-06, 1.0000e+00, 1.0860e-06],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [3.2723e-07, 1.0000e+00, 3.2713e-07],\n",
       "          [1.4847e-05, 9.9997e-01, 1.4790e-05],\n",
       "          [1.3783e-06, 1.0000e+00, 1.3768e-06],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [2.0885e-06, 1.0000e+00, 2.0851e-06],\n",
       "          [2.0297e-06, 1.0000e+00, 2.0250e-06],\n",
       "          [2.7667e-05, 9.9994e-01, 2.7522e-05],\n",
       "          [1.5863e-06, 1.0000e+00, 1.5833e-06],\n",
       "          [4.4815e-07, 1.0000e+00, 4.4775e-07],\n",
       "          [2.6058e-06, 9.9999e-01, 2.6004e-06],\n",
       "          [6.3250e-06, 9.9999e-01, 6.3054e-06],\n",
       "          [4.6497e-07, 1.0000e+00, 4.6463e-07],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [2.6301e-06, 9.9999e-01, 2.6248e-06],\n",
       "          [4.8723e-06, 9.9999e-01, 4.8562e-06],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [4.4070e-09, 1.0000e+00, 4.4066e-09],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [2.0308e-05, 9.9996e-01, 2.0160e-05],\n",
       "          [3.4797e-07, 1.0000e+00, 3.4767e-07],\n",
       "          [7.4411e-06, 9.9999e-01, 7.4075e-06],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [1.1083e-05, 9.9998e-01, 1.1038e-05],\n",
       "          [3.3815e-05, 9.9993e-01, 3.3527e-05],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [8.2251e-06, 9.9998e-01, 8.1917e-06],\n",
       "          [5.3322e-06, 9.9999e-01, 5.3134e-06],\n",
       "          [2.4963e-06, 9.9999e-01, 2.4905e-06],\n",
       "          [5.5723e-06, 9.9999e-01, 5.5522e-06],\n",
       "          [1.5418e-06, 1.0000e+00, 1.5392e-06],\n",
       "          [5.5733e-06, 9.9999e-01, 5.5521e-06],\n",
       "          [2.0770e-05, 9.9996e-01, 2.0680e-05],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [9.6701e-06, 9.9998e-01, 9.6379e-06],\n",
       "          [9.6174e-06, 9.9998e-01, 9.5730e-06],\n",
       "          [1.4317e-05, 9.9997e-01, 1.4231e-05],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [8.4146e-06, 9.9998e-01, 8.3743e-06],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [9.5693e-06, 9.9998e-01, 9.5197e-06],\n",
       "          [1.7483e-05, 9.9997e-01, 1.7399e-05],\n",
       "          [2.8044e-05, 9.9994e-01, 2.7859e-05],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [1.1066e-05, 9.9998e-01, 1.1005e-05],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [1.6452e-06, 1.0000e+00, 1.6432e-06],\n",
       "          [2.7751e-07, 1.0000e+00, 2.7733e-07],\n",
       "          [5.3175e-08, 1.0000e+00, 5.3153e-08],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [6.8373e-05, 9.9986e-01, 6.7820e-05],\n",
       "          [1.8280e-05, 9.9996e-01, 1.8156e-05],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [1.8502e-05, 9.9996e-01, 1.8404e-05],\n",
       "          [3.6722e-05, 9.9993e-01, 3.6452e-05],\n",
       "          [2.2917e-06, 1.0000e+00, 2.2851e-06],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [8.6206e-06, 9.9998e-01, 8.5848e-06],\n",
       "          [8.3455e-06, 9.9998e-01, 8.3168e-06],\n",
       "          [1.6930e-05, 9.9997e-01, 1.6848e-05],\n",
       "          [8.4832e-10, 1.0000e+00, 8.4825e-10],\n",
       "          [7.4932e-07, 1.0000e+00, 7.4860e-07],\n",
       "          [8.5444e-06, 9.9998e-01, 8.5138e-06],\n",
       "          [1.6523e-05, 9.9997e-01, 1.6435e-05],\n",
       "          [5.8270e-06, 9.9999e-01, 5.8058e-06]], grad_fn=<CopySlices>),\n",
       "  'nd2': tensor([[9.1804e-01, 7.6701e-02, 5.2591e-03],\n",
       "          [8.8313e-01, 1.1675e-01, 1.1448e-04],\n",
       "          [7.2798e-01, 2.6649e-01, 5.5385e-03],\n",
       "          [6.6329e-01, 3.1996e-01, 1.6741e-02],\n",
       "          [6.6523e-01, 3.0363e-01, 3.1135e-02],\n",
       "          [5.0026e-01, 4.7821e-01, 2.1525e-02],\n",
       "          [5.8240e-01, 4.0963e-01, 7.9708e-03],\n",
       "          [3.9195e-01, 5.9221e-01, 1.5849e-02],\n",
       "          [5.9884e-01, 3.8598e-01, 1.5175e-02],\n",
       "          [4.7006e-01, 5.0687e-01, 2.3068e-02],\n",
       "          [4.5795e-01, 5.1363e-01, 2.8415e-02],\n",
       "          [7.1185e-01, 2.8274e-01, 5.4047e-03],\n",
       "          [4.4096e-01, 5.3821e-01, 2.0830e-02],\n",
       "          [4.0018e-01, 5.8262e-01, 1.7193e-02],\n",
       "          [5.9190e-01, 3.9129e-01, 1.6808e-02],\n",
       "          [5.5458e-01, 4.2013e-01, 2.5290e-02],\n",
       "          [6.0719e-01, 3.9164e-01, 1.1752e-03],\n",
       "          [6.3095e-01, 3.4974e-01, 1.9312e-02],\n",
       "          [3.4359e-01, 6.3865e-01, 1.7761e-02],\n",
       "          [5.0819e-01, 4.8319e-01, 8.6258e-03],\n",
       "          [6.9349e-01, 2.8433e-01, 2.2179e-02],\n",
       "          [5.9810e-01, 3.7350e-01, 2.8401e-02],\n",
       "          [5.9303e-01, 3.7605e-01, 3.0915e-02],\n",
       "          [5.9482e-01, 4.0043e-01, 4.7468e-03],\n",
       "          [4.4505e-01, 5.5461e-01, 3.3968e-04],\n",
       "          [6.1691e-01, 3.6517e-01, 1.7924e-02],\n",
       "          [5.0244e-01, 4.9725e-01, 3.0673e-04],\n",
       "          [6.8280e-01, 3.1451e-01, 2.6878e-03],\n",
       "          [4.2747e-01, 5.5687e-01, 1.5667e-02],\n",
       "          [4.4237e-01, 5.4115e-01, 1.6484e-02],\n",
       "          [4.6012e-01, 5.2081e-01, 1.9075e-02],\n",
       "          [6.1480e-01, 3.6606e-01, 1.9145e-02],\n",
       "          [4.0752e-01, 5.7314e-01, 1.9339e-02],\n",
       "          [6.2280e-01, 3.6197e-01, 1.5224e-02],\n",
       "          [5.1399e-01, 4.7580e-01, 1.0211e-02],\n",
       "          [8.8142e-01, 1.1284e-01, 5.7388e-03],\n",
       "          [6.0463e-01, 3.7558e-01, 1.9793e-02],\n",
       "          [5.8761e-01, 3.9687e-01, 1.5521e-02],\n",
       "          [4.8009e-01, 5.1034e-01, 9.5712e-03],\n",
       "          [4.1016e-01, 5.7075e-01, 1.9090e-02],\n",
       "          [5.8274e-01, 4.0224e-01, 1.5022e-02],\n",
       "          [7.4049e-01, 2.5921e-01, 2.9905e-04],\n",
       "          [6.5590e-01, 3.3373e-01, 1.0371e-02],\n",
       "          [8.0161e-01, 1.9585e-01, 2.5374e-03],\n",
       "          [5.4405e-01, 4.4341e-01, 1.2544e-02],\n",
       "          [5.5000e-01, 4.4486e-01, 5.1417e-03],\n",
       "          [7.3746e-01, 2.4788e-01, 1.4662e-02],\n",
       "          [4.1435e-01, 5.7768e-01, 7.9693e-03],\n",
       "          [5.6405e-01, 4.1164e-01, 2.4317e-02],\n",
       "          [4.7644e-01, 5.0401e-01, 1.9550e-02],\n",
       "          [6.8363e-01, 3.1132e-01, 5.0552e-03],\n",
       "          [6.2934e-01, 3.3856e-01, 3.2103e-02],\n",
       "          [8.1537e-01, 1.6874e-01, 1.5888e-02],\n",
       "          [4.9338e-01, 4.7748e-01, 2.9143e-02],\n",
       "          [4.4580e-01, 5.3306e-01, 2.1140e-02],\n",
       "          [5.3006e-01, 4.5205e-01, 1.7888e-02],\n",
       "          [6.6933e-01, 3.1257e-01, 1.8102e-02],\n",
       "          [4.5461e-01, 5.2603e-01, 1.9360e-02],\n",
       "          [9.0832e-01, 9.1421e-02, 2.6059e-04],\n",
       "          [5.4758e-01, 4.2629e-01, 2.6128e-02],\n",
       "          [4.1765e-01, 5.6396e-01, 1.8385e-02],\n",
       "          [5.3686e-01, 4.4430e-01, 1.8845e-02],\n",
       "          [5.9819e-01, 3.7765e-01, 2.4164e-02],\n",
       "          [4.4944e-01, 5.3618e-01, 1.4384e-02],\n",
       "          [3.1532e-01, 6.8444e-01, 2.4330e-04],\n",
       "          [4.1652e-01, 5.7002e-01, 1.3460e-02],\n",
       "          [5.1772e-01, 4.6201e-01, 2.0277e-02],\n",
       "          [5.2297e-01, 4.5780e-01, 1.9234e-02],\n",
       "          [5.7357e-01, 4.0507e-01, 2.1363e-02],\n",
       "          [6.3380e-01, 3.4871e-01, 1.7486e-02],\n",
       "          [4.4981e-01, 5.3187e-01, 1.8321e-02],\n",
       "          [6.8486e-01, 2.9306e-01, 2.2083e-02],\n",
       "          [4.4731e-01, 5.3268e-01, 2.0003e-02],\n",
       "          [5.0182e-01, 4.8044e-01, 1.7741e-02],\n",
       "          [5.5522e-01, 4.3344e-01, 1.1334e-02],\n",
       "          [4.3748e-01, 5.5303e-01, 9.4909e-03],\n",
       "          [5.9415e-01, 3.9714e-01, 8.7121e-03],\n",
       "          [5.1616e-01, 4.6192e-01, 2.1918e-02],\n",
       "          [5.1558e-01, 4.7464e-01, 9.7769e-03],\n",
       "          [5.5550e-01, 4.3216e-01, 1.2347e-02],\n",
       "          [4.5447e-01, 5.2401e-01, 2.1523e-02],\n",
       "          [7.2217e-01, 2.5908e-01, 1.8748e-02],\n",
       "          [6.1211e-01, 3.6868e-01, 1.9206e-02],\n",
       "          [4.9921e-01, 4.9033e-01, 1.0464e-02],\n",
       "          [8.8942e-01, 1.0780e-01, 2.7817e-03],\n",
       "          [4.2385e-01, 5.5758e-01, 1.8570e-02],\n",
       "          [5.2430e-01, 4.5582e-01, 1.9884e-02],\n",
       "          [3.0193e-01, 6.8913e-01, 8.9449e-03],\n",
       "          [4.2913e-01, 5.5536e-01, 1.5508e-02],\n",
       "          [8.7667e-01, 1.1651e-01, 6.8164e-03],\n",
       "          [5.0285e-01, 4.8102e-01, 1.6126e-02],\n",
       "          [4.2380e-01, 5.5748e-01, 1.8719e-02],\n",
       "          [5.0499e-01, 4.7694e-01, 1.8062e-02],\n",
       "          [3.9120e-01, 5.9380e-01, 1.5001e-02],\n",
       "          [5.9405e-01, 4.0352e-01, 2.4314e-03],\n",
       "          [6.7078e-01, 3.2175e-01, 7.4677e-03],\n",
       "          [6.0217e-01, 3.8020e-01, 1.7629e-02],\n",
       "          [8.7534e-01, 1.2278e-01, 1.8741e-03],\n",
       "          [4.1079e-01, 5.6990e-01, 1.9312e-02],\n",
       "          [6.0685e-01, 3.8420e-01, 8.9439e-03],\n",
       "          [5.3443e-01, 4.5399e-01, 1.1577e-02],\n",
       "          [5.4171e-01, 4.4014e-01, 1.8156e-02],\n",
       "          [6.3542e-01, 3.3630e-01, 2.8278e-02],\n",
       "          [6.9865e-01, 2.9943e-01, 1.9173e-03],\n",
       "          [5.0128e-01, 4.7376e-01, 2.4966e-02],\n",
       "          [6.4202e-01, 3.3979e-01, 1.8188e-02],\n",
       "          [5.4769e-01, 4.3893e-01, 1.3378e-02],\n",
       "          [6.0624e-01, 3.7726e-01, 1.6501e-02],\n",
       "          [4.2626e-01, 5.5802e-01, 1.5719e-02],\n",
       "          [5.5044e-01, 4.2792e-01, 2.1637e-02],\n",
       "          [5.0979e-01, 4.7641e-01, 1.3796e-02],\n",
       "          [4.9176e-01, 4.8894e-01, 1.9299e-02],\n",
       "          [4.3168e-01, 5.4933e-01, 1.8986e-02],\n",
       "          [7.5085e-01, 2.3831e-01, 1.0844e-02],\n",
       "          [5.5270e-01, 4.2321e-01, 2.4086e-02],\n",
       "          [5.7587e-01, 4.0414e-01, 1.9992e-02],\n",
       "          [3.5982e-01, 6.2463e-01, 1.5556e-02],\n",
       "          [4.8641e-01, 4.8528e-01, 2.8317e-02],\n",
       "          [5.8974e-01, 3.9925e-01, 1.1010e-02],\n",
       "          [4.6357e-01, 5.1773e-01, 1.8701e-02],\n",
       "          [7.2057e-01, 2.6665e-01, 1.2778e-02],\n",
       "          [6.9753e-01, 2.8442e-01, 1.8050e-02],\n",
       "          [5.9345e-01, 3.7571e-01, 3.0839e-02],\n",
       "          [5.1716e-01, 4.6164e-01, 2.1200e-02],\n",
       "          [6.5529e-01, 3.2278e-01, 2.1932e-02],\n",
       "          [4.0622e-01, 5.7465e-01, 1.9130e-02],\n",
       "          [5.9179e-01, 3.9680e-01, 1.1412e-02],\n",
       "          [5.2155e-01, 4.7239e-01, 6.0649e-03],\n",
       "          [6.1856e-01, 3.7530e-01, 6.1385e-03],\n",
       "          [3.8569e-01, 5.9923e-01, 1.5078e-02],\n",
       "          [5.1994e-01, 4.6903e-01, 1.1034e-02],\n",
       "          [4.7615e-01, 5.0989e-01, 1.3960e-02],\n",
       "          [6.2560e-01, 3.5685e-01, 1.7545e-02],\n",
       "          [5.6941e-01, 4.1192e-01, 1.8677e-02],\n",
       "          [4.2452e-01, 5.5537e-01, 2.0103e-02],\n",
       "          [6.7914e-01, 2.9800e-01, 2.2858e-02],\n",
       "          [6.1763e-01, 3.5958e-01, 2.2786e-02],\n",
       "          [5.6915e-01, 4.1592e-01, 1.4925e-02],\n",
       "          [5.3158e-01, 4.4656e-01, 2.1868e-02],\n",
       "          [7.1432e-01, 2.6172e-01, 2.3957e-02],\n",
       "          [3.8791e-01, 5.9531e-01, 1.6777e-02],\n",
       "          [4.7360e-01, 5.1003e-01, 1.6363e-02],\n",
       "          [7.1784e-01, 2.8207e-01, 9.5182e-05],\n",
       "          [6.3070e-01, 3.6362e-01, 5.6818e-03],\n",
       "          [5.2038e-01, 4.5782e-01, 2.1805e-02],\n",
       "          [6.8807e-01, 2.9770e-01, 1.4228e-02],\n",
       "          [6.3037e-01, 3.5432e-01, 1.5308e-02]], grad_fn=<CopySlices>),\n",
       "  'pd2': tensor([[9.9975e-01, 1.2415e-04, 1.2373e-04],\n",
       "          [1.0000e+00, 5.3349e-09, 5.3346e-09],\n",
       "          [9.9990e-01, 4.9991e-05, 4.9880e-05],\n",
       "          [9.9900e-01, 5.0025e-04, 4.9720e-04],\n",
       "          [9.9534e-01, 2.3412e-03, 2.3213e-03],\n",
       "          [9.9864e-01, 6.8087e-04, 6.7895e-04],\n",
       "          [9.9985e-01, 7.6025e-05, 7.5941e-05],\n",
       "          [9.9919e-01, 4.0489e-04, 4.0294e-04],\n",
       "          [9.9940e-01, 3.0177e-04, 3.0058e-04],\n",
       "          [9.9855e-01, 7.2773e-04, 7.2667e-04],\n",
       "          [9.9721e-01, 1.3952e-03, 1.3918e-03],\n",
       "          [9.9992e-01, 3.8162e-05, 3.8091e-05],\n",
       "          [9.9858e-01, 7.1054e-04, 7.0738e-04],\n",
       "          [9.9922e-01, 3.8846e-04, 3.8766e-04],\n",
       "          [9.9893e-01, 5.3765e-04, 5.3423e-04],\n",
       "          [9.9787e-01, 1.0650e-03, 1.0619e-03],\n",
       "          [1.0000e+00, 1.2221e-06, 1.2218e-06],\n",
       "          [9.9846e-01, 7.6949e-04, 7.6643e-04],\n",
       "          [9.9913e-01, 4.3550e-04, 4.3347e-04],\n",
       "          [9.9983e-01, 8.2644e-05, 8.2594e-05],\n",
       "          [9.9768e-01, 1.1659e-03, 1.1517e-03],\n",
       "          [9.9750e-01, 1.2517e-03, 1.2442e-03],\n",
       "          [9.9618e-01, 1.9145e-03, 1.9006e-03],\n",
       "          [9.9994e-01, 2.8527e-05, 2.8443e-05],\n",
       "          [1.0000e+00, 4.7284e-08, 4.7282e-08],\n",
       "          [9.9898e-01, 5.0919e-04, 5.0584e-04],\n",
       "          [1.0000e+00, 4.0304e-08, 4.0302e-08],\n",
       "          [9.9998e-01, 8.2173e-06, 8.2031e-06],\n",
       "          [9.9932e-01, 3.4055e-04, 3.4002e-04],\n",
       "          [9.9928e-01, 3.6055e-04, 3.5851e-04],\n",
       "          [9.9901e-01, 4.9371e-04, 4.9233e-04],\n",
       "          [9.9859e-01, 7.0799e-04, 7.0301e-04],\n",
       "          [9.9887e-01, 5.6552e-04, 5.6279e-04],\n",
       "          [9.9914e-01, 4.3303e-04, 4.3168e-04],\n",
       "          [9.9974e-01, 1.2990e-04, 1.2948e-04],\n",
       "          [9.9982e-01, 8.8602e-05, 8.8352e-05],\n",
       "          [9.9833e-01, 8.4077e-04, 8.3376e-04],\n",
       "          [9.9917e-01, 4.1381e-04, 4.1170e-04],\n",
       "          [9.9978e-01, 1.1009e-04, 1.0958e-04],\n",
       "          [9.9900e-01, 4.9911e-04, 4.9830e-04],\n",
       "          [9.9934e-01, 3.2959e-04, 3.2837e-04],\n",
       "          [1.0000e+00, 5.0735e-08, 5.0728e-08],\n",
       "          [9.9961e-01, 1.9748e-04, 1.9611e-04],\n",
       "          [9.9998e-01, 1.0568e-05, 1.0556e-05],\n",
       "          [9.9961e-01, 1.9770e-04, 1.9687e-04],\n",
       "          [9.9994e-01, 3.1945e-05, 3.1911e-05],\n",
       "          [9.9914e-01, 4.2948e-04, 4.2676e-04],\n",
       "          [9.9987e-01, 6.6965e-05, 6.6935e-05],\n",
       "          [9.9802e-01, 9.9055e-04, 9.8563e-04],\n",
       "          [9.9885e-01, 5.7559e-04, 5.7384e-04],\n",
       "          [9.9997e-01, 1.5776e-05, 1.5749e-05],\n",
       "          [9.9550e-01, 2.2584e-03, 2.2460e-03],\n",
       "          [9.9864e-01, 6.8187e-04, 6.7673e-04],\n",
       "          [9.9714e-01, 1.4318e-03, 1.4258e-03],\n",
       "          [9.9871e-01, 6.4718e-04, 6.4559e-04],\n",
       "          [9.9887e-01, 5.6825e-04, 5.6355e-04],\n",
       "          [9.9866e-01, 6.7382e-04, 6.6978e-04],\n",
       "          [9.9899e-01, 5.0560e-04, 5.0319e-04],\n",
       "          [1.0000e+00, 7.5069e-08, 7.5055e-08],\n",
       "          [9.9793e-01, 1.0345e-03, 1.0309e-03],\n",
       "          [9.9900e-01, 5.0193e-04, 4.9926e-04],\n",
       "          [9.9915e-01, 4.2785e-04, 4.2656e-04],\n",
       "          [9.9797e-01, 1.0198e-03, 1.0103e-03],\n",
       "          [9.9951e-01, 2.4419e-04, 2.4383e-04],\n",
       "          [1.0000e+00, 3.5907e-08, 3.5905e-08],\n",
       "          [9.9953e-01, 2.3622e-04, 2.3521e-04],\n",
       "          [9.9871e-01, 6.4837e-04, 6.4480e-04],\n",
       "          [9.9853e-01, 7.4058e-04, 7.3403e-04],\n",
       "          [9.9863e-01, 6.8589e-04, 6.8326e-04],\n",
       "          [9.9884e-01, 5.8238e-04, 5.7750e-04],\n",
       "          [9.9876e-01, 6.2400e-04, 6.1882e-04],\n",
       "          [9.9797e-01, 1.0190e-03, 1.0139e-03],\n",
       "          [9.9875e-01, 6.2442e-04, 6.2076e-04],\n",
       "          [9.9900e-01, 4.9846e-04, 4.9735e-04],\n",
       "          [9.9966e-01, 1.6763e-04, 1.6750e-04],\n",
       "          [9.9979e-01, 1.0403e-04, 1.0381e-04],\n",
       "          [9.9982e-01, 8.8136e-05, 8.7973e-05],\n",
       "          [9.9806e-01, 9.7363e-04, 9.6308e-04],\n",
       "          [9.9978e-01, 1.1014e-04, 1.0979e-04],\n",
       "          [9.9959e-01, 2.0669e-04, 2.0586e-04],\n",
       "          [9.9832e-01, 8.4040e-04, 8.3806e-04],\n",
       "          [9.9872e-01, 6.4018e-04, 6.3703e-04],\n",
       "          [9.9851e-01, 7.4701e-04, 7.3950e-04],\n",
       "          [9.9975e-01, 1.2316e-04, 1.2311e-04],\n",
       "          [9.9997e-01, 1.6625e-05, 1.6606e-05],\n",
       "          [9.9893e-01, 5.3700e-04, 5.3590e-04],\n",
       "          [9.9838e-01, 8.1077e-04, 8.0441e-04],\n",
       "          [9.9978e-01, 1.1125e-04, 1.1117e-04],\n",
       "          [9.9937e-01, 3.1562e-04, 3.1391e-04],\n",
       "          [9.9964e-01, 1.7968e-04, 1.7904e-04],\n",
       "          [9.9932e-01, 3.3834e-04, 3.3674e-04],\n",
       "          [9.9895e-01, 5.2656e-04, 5.2554e-04],\n",
       "          [9.9867e-01, 6.6689e-04, 6.6175e-04],\n",
       "          [9.9943e-01, 2.8635e-04, 2.8509e-04],\n",
       "          [9.9999e-01, 4.7889e-06, 4.7848e-06],\n",
       "          [9.9982e-01, 9.2257e-05, 9.1809e-05],\n",
       "          [9.9892e-01, 5.4157e-04, 5.3674e-04],\n",
       "          [9.9999e-01, 6.5146e-06, 6.5077e-06],\n",
       "          [9.9899e-01, 5.0405e-04, 5.0278e-04],\n",
       "          [9.9976e-01, 1.2190e-04, 1.2135e-04],\n",
       "          [9.9967e-01, 1.6588e-04, 1.6531e-04],\n",
       "          [9.9900e-01, 5.0110e-04, 4.9933e-04],\n",
       "          [9.9661e-01, 1.7060e-03, 1.6877e-03],\n",
       "          [9.9999e-01, 4.2357e-06, 4.2312e-06],\n",
       "          [9.9818e-01, 9.1303e-04, 9.0905e-04],\n",
       "          [9.9895e-01, 5.2454e-04, 5.2292e-04],\n",
       "          [9.9950e-01, 2.5023e-04, 2.4959e-04],\n",
       "          [9.9929e-01, 3.5265e-04, 3.5262e-04],\n",
       "          [9.9930e-01, 3.4888e-04, 3.4855e-04],\n",
       "          [9.9849e-01, 7.5639e-04, 7.5337e-04],\n",
       "          [9.9938e-01, 3.0931e-04, 3.0754e-04],\n",
       "          [9.9884e-01, 5.8027e-04, 5.7542e-04],\n",
       "          [9.9892e-01, 5.3939e-04, 5.3697e-04],\n",
       "          [9.9946e-01, 2.7067e-04, 2.7018e-04],\n",
       "          [9.9812e-01, 9.4255e-04, 9.3920e-04],\n",
       "          [9.9882e-01, 5.9336e-04, 5.9030e-04],\n",
       "          [9.9932e-01, 3.4226e-04, 3.4051e-04],\n",
       "          [9.9713e-01, 1.4359e-03, 1.4309e-03],\n",
       "          [9.9965e-01, 1.7502e-04, 1.7409e-04],\n",
       "          [9.9878e-01, 6.1348e-04, 6.0725e-04],\n",
       "          [9.9927e-01, 3.6366e-04, 3.6234e-04],\n",
       "          [9.9881e-01, 5.9608e-04, 5.9458e-04],\n",
       "          [9.9667e-01, 1.6712e-03, 1.6625e-03],\n",
       "          [9.9843e-01, 7.8901e-04, 7.8340e-04],\n",
       "          [9.9795e-01, 1.0292e-03, 1.0213e-03],\n",
       "          [9.9900e-01, 4.9950e-04, 4.9623e-04],\n",
       "          [9.9964e-01, 1.8027e-04, 1.7944e-04],\n",
       "          [9.9991e-01, 4.4613e-05, 4.4585e-05],\n",
       "          [9.9991e-01, 4.5553e-05, 4.5475e-05],\n",
       "          [9.9939e-01, 3.0503e-04, 3.0401e-04],\n",
       "          [9.9968e-01, 1.5872e-04, 1.5831e-04],\n",
       "          [9.9946e-01, 2.6932e-04, 2.6814e-04],\n",
       "          [9.9891e-01, 5.4687e-04, 5.4219e-04],\n",
       "          [9.9897e-01, 5.1593e-04, 5.1308e-04],\n",
       "          [9.9876e-01, 6.2309e-04, 6.1785e-04],\n",
       "          [9.9774e-01, 1.1330e-03, 1.1233e-03],\n",
       "          [9.9807e-01, 9.6836e-04, 9.6207e-04],\n",
       "          [9.9925e-01, 3.7432e-04, 3.7263e-04],\n",
       "          [9.9834e-01, 8.3529e-04, 8.2794e-04],\n",
       "          [9.9717e-01, 1.4228e-03, 1.4120e-03],\n",
       "          [9.9897e-01, 5.1630e-04, 5.1532e-04],\n",
       "          [9.9909e-01, 4.5817e-04, 4.5617e-04],\n",
       "          [1.0000e+00, 3.4208e-09, 3.4207e-09],\n",
       "          [9.9991e-01, 4.7099e-05, 4.6991e-05],\n",
       "          [9.9840e-01, 8.0275e-04, 7.9555e-04],\n",
       "          [9.9943e-01, 2.8508e-04, 2.8458e-04],\n",
       "          [9.9912e-01, 4.3828e-04, 4.3707e-04]], grad_fn=<CopySlices>),\n",
       "  'mod': tensor([[1.0000e+00, 1.1210e-44, 1.1210e-44, 1.1210e-44, 1.1210e-44, 1.1210e-44],\n",
       "          [1.0000e+00, 4.9739e-31, 4.9739e-31, 4.9739e-31, 4.9739e-31, 4.9739e-31],\n",
       "          [9.9963e-01, 7.4870e-05, 7.4753e-05, 7.4805e-05, 7.4743e-05, 7.4916e-05],\n",
       "          [9.9835e-01, 3.3111e-04, 3.3002e-04, 3.3075e-04, 3.2981e-04, 3.3195e-04],\n",
       "          [9.9625e-01, 7.4963e-04, 7.4653e-04, 7.4930e-04, 7.4625e-04, 7.5844e-04],\n",
       "          [9.9850e-01, 2.9899e-04, 2.9847e-04, 2.9924e-04, 2.9834e-04, 3.0102e-04],\n",
       "          [9.9921e-01, 1.5810e-04, 1.5782e-04, 1.5814e-04, 1.5777e-04, 1.5871e-04],\n",
       "          [1.0000e+00, 3.4453e-31, 3.4453e-31, 3.4453e-31, 3.4453e-31, 3.4453e-31],\n",
       "          [1.0000e+00, 5.4641e-31, 5.4641e-31, 5.4641e-31, 5.4641e-31, 5.4641e-31],\n",
       "          [9.9848e-01, 3.0466e-04, 3.0403e-04, 3.0478e-04, 3.0397e-04, 3.0665e-04],\n",
       "          [9.9776e-01, 4.4697e-04, 4.4566e-04, 4.4683e-04, 4.4551e-04, 4.5014e-04],\n",
       "          [9.9964e-01, 7.2794e-05, 7.2729e-05, 7.2807e-05, 7.2719e-05, 7.2964e-05],\n",
       "          [1.0000e+00, 3.3552e-31, 3.3552e-31, 3.3552e-31, 3.3552e-31, 3.3552e-31],\n",
       "          [9.9862e-01, 2.7632e-04, 2.7588e-04, 2.7648e-04, 2.7577e-04, 2.7803e-04],\n",
       "          [1.0000e+00, 1.0018e-30, 1.0018e-30, 1.0018e-30, 1.0018e-30, 1.0018e-30],\n",
       "          [9.9822e-01, 3.5650e-04, 3.5556e-04, 3.5629e-04, 3.5552e-04, 3.5844e-04],\n",
       "          [9.9999e-01, 1.7167e-06, 1.7165e-06, 1.7166e-06, 1.7165e-06, 1.7170e-06],\n",
       "          [9.9821e-01, 3.5805e-04, 3.5712e-04, 3.5810e-04, 3.5707e-04, 3.6104e-04],\n",
       "          [1.0000e+00, 5.0776e-31, 5.0776e-31, 5.0776e-31, 5.0776e-31, 5.0776e-31],\n",
       "          [9.9921e-01, 1.5812e-04, 1.5792e-04, 1.5808e-04, 1.5792e-04, 1.5919e-04],\n",
       "          [9.9750e-01, 5.0042e-04, 4.9834e-04, 4.9980e-04, 4.9803e-04, 5.0240e-04],\n",
       "          [9.9800e-01, 4.0023e-04, 3.9919e-04, 4.0020e-04, 3.9916e-04, 4.0277e-04],\n",
       "          [9.9433e-01, 1.1348e-03, 1.1278e-03, 1.1328e-03, 1.1266e-03, 1.1480e-03],\n",
       "          [1.0000e+00, 4.5941e-31, 4.5941e-31, 4.5941e-31, 4.5941e-31, 4.5941e-31],\n",
       "          [1.0000e+00, 7.0065e-45, 7.0065e-45, 7.0065e-45, 7.0065e-45, 7.0065e-45],\n",
       "          [9.9825e-01, 3.5074e-04, 3.4989e-04, 3.5064e-04, 3.4980e-04, 3.5284e-04],\n",
       "          [1.0000e+00, 9.8091e-45, 9.8091e-45, 9.8091e-45, 9.8091e-45, 9.8091e-45],\n",
       "          [1.0000e+00, 7.8252e-31, 7.8252e-31, 7.8252e-31, 7.8252e-31, 7.8252e-31],\n",
       "          [9.9807e-01, 3.8587e-04, 3.8489e-04, 3.8583e-04, 3.8485e-04, 3.8831e-04],\n",
       "          [1.0000e+00, 4.3648e-31, 4.3648e-31, 4.3648e-31, 4.3648e-31, 4.3648e-31],\n",
       "          [9.9724e-01, 5.5072e-04, 5.4941e-04, 5.5127e-04, 5.4932e-04, 5.5645e-04],\n",
       "          [9.9829e-01, 3.4107e-04, 3.4026e-04, 3.4093e-04, 3.4022e-04, 3.4377e-04],\n",
       "          [1.0000e+00, 4.8096e-31, 4.8096e-31, 4.8096e-31, 4.8096e-31, 4.8096e-31],\n",
       "          [9.9906e-01, 1.8795e-04, 1.8773e-04, 1.8803e-04, 1.8770e-04, 1.8913e-04],\n",
       "          [1.0000e+00, 5.5438e-31, 5.5438e-31, 5.5438e-31, 5.5438e-31, 5.5438e-31],\n",
       "          [1.0000e+00, 1.5414e-44, 1.5414e-44, 1.5414e-44, 1.5414e-44, 1.5414e-44],\n",
       "          [9.9791e-01, 4.1768e-04, 4.1640e-04, 4.1743e-04, 4.1626e-04, 4.2097e-04],\n",
       "          [1.0000e+00, 5.4069e-31, 5.4069e-31, 5.4069e-31, 5.4069e-31, 5.4069e-31],\n",
       "          [1.0000e+00, 5.9503e-31, 5.9503e-31, 5.9503e-31, 5.9503e-31, 5.9503e-31],\n",
       "          [9.9872e-01, 2.5614e-04, 2.5571e-04, 2.5629e-04, 2.5559e-04, 2.5757e-04],\n",
       "          [9.9899e-01, 2.0287e-04, 2.0250e-04, 2.0280e-04, 2.0247e-04, 2.0407e-04],\n",
       "          [1.0000e+00, 3.7784e-31, 3.7784e-31, 3.7784e-31, 3.7784e-31, 3.7784e-31],\n",
       "          [1.0000e+00, 6.0104e-31, 6.0104e-31, 6.0104e-31, 6.0104e-31, 6.0104e-31],\n",
       "          [1.0000e+00, 9.0380e-31, 9.0380e-31, 9.0380e-31, 9.0380e-31, 9.0380e-31],\n",
       "          [1.0000e+00, 3.0426e-31, 3.0426e-31, 3.0426e-31, 3.0426e-31, 3.0426e-31],\n",
       "          [9.9982e-01, 3.5066e-05, 3.5042e-05, 3.5059e-05, 3.5036e-05, 3.5112e-05],\n",
       "          [9.9921e-01, 1.5787e-04, 1.5765e-04, 1.5783e-04, 1.5765e-04, 1.5875e-04],\n",
       "          [9.9961e-01, 7.8159e-05, 7.8103e-05, 7.8167e-05, 7.8098e-05, 7.8509e-05],\n",
       "          [9.9398e-01, 1.2027e-03, 1.1968e-03, 1.2024e-03, 1.1970e-03, 1.2211e-03],\n",
       "          [9.9795e-01, 4.1012e-04, 4.0896e-04, 4.0986e-04, 4.0887e-04, 4.1276e-04],\n",
       "          [9.9967e-01, 6.5376e-05, 6.5326e-05, 6.5370e-05, 6.5324e-05, 6.5591e-05],\n",
       "          [9.9480e-01, 1.0391e-03, 1.0336e-03, 1.0377e-03, 1.0331e-03, 1.0520e-03],\n",
       "          [9.9395e-01, 1.2110e-03, 1.2049e-03, 1.2092e-03, 1.2045e-03, 1.2190e-03],\n",
       "          [9.9753e-01, 4.9414e-04, 4.9240e-04, 4.9389e-04, 4.9223e-04, 4.9757e-04],\n",
       "          [9.9847e-01, 3.0551e-04, 3.0495e-04, 3.0570e-04, 3.0480e-04, 3.0754e-04],\n",
       "          [9.9704e-01, 5.9320e-04, 5.9100e-04, 5.9328e-04, 5.9047e-04, 5.9633e-04],\n",
       "          [9.9885e-01, 2.3008e-04, 2.2962e-04, 2.3007e-04, 2.2957e-04, 2.3151e-04],\n",
       "          [1.0000e+00, 2.7355e-31, 2.7355e-31, 2.7355e-31, 2.7355e-31, 2.7355e-31],\n",
       "          [1.0000e+00, 3.1349e-07, 3.1348e-07, 3.1348e-07, 3.1348e-07, 3.1352e-07],\n",
       "          [9.9641e-01, 7.1689e-04, 7.1437e-04, 7.1704e-04, 7.1415e-04, 7.2522e-04],\n",
       "          [1.0000e+00, 4.4503e-31, 4.4503e-31, 4.4503e-31, 4.4503e-31, 4.4503e-31],\n",
       "          [9.9588e-01, 8.2355e-04, 8.1975e-04, 8.2268e-04, 8.1979e-04, 8.3256e-04],\n",
       "          [9.9605e-01, 7.8852e-04, 7.8551e-04, 7.8929e-04, 7.8520e-04, 7.9686e-04],\n",
       "          [9.9891e-01, 2.1857e-04, 2.1830e-04, 2.1866e-04, 2.1828e-04, 2.1974e-04],\n",
       "          [1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.0000e+00, 3.9574e-31, 3.9574e-31, 3.9574e-31, 3.9574e-31, 3.9574e-31],\n",
       "          [9.9563e-01, 8.7370e-04, 8.6996e-04, 8.7430e-04, 8.6977e-04, 8.8294e-04],\n",
       "          [9.9790e-01, 4.2022e-04, 4.1887e-04, 4.2009e-04, 4.1863e-04, 4.2293e-04],\n",
       "          [9.9658e-01, 6.8411e-04, 6.8205e-04, 6.8413e-04, 6.8203e-04, 6.9223e-04],\n",
       "          [1.0000e+00, 8.2132e-31, 8.2132e-31, 8.2132e-31, 8.2132e-31, 8.2132e-31],\n",
       "          [9.9766e-01, 4.6753e-04, 4.6564e-04, 4.6728e-04, 4.6546e-04, 4.6930e-04],\n",
       "          [9.9691e-01, 6.1816e-04, 6.1588e-04, 6.1726e-04, 6.1588e-04, 6.2357e-04],\n",
       "          [1.0000e+00, 4.1905e-31, 4.1905e-31, 4.1905e-31, 4.1905e-31, 4.1905e-31],\n",
       "          [9.9908e-01, 1.8447e-04, 1.8422e-04, 1.8445e-04, 1.8421e-04, 1.8537e-04],\n",
       "          [9.9953e-01, 9.3599e-05, 9.3493e-05, 9.3597e-05, 9.3493e-05, 9.4030e-05],\n",
       "          [1.0000e+00, 5.7348e-31, 5.7348e-31, 5.7348e-31, 5.7348e-31, 5.7348e-31],\n",
       "          [9.9984e-01, 3.1589e-05, 3.1567e-05, 3.1581e-05, 3.1564e-05, 3.1603e-05],\n",
       "          [9.9755e-01, 4.9066e-04, 4.8864e-04, 4.9048e-04, 4.8839e-04, 4.9319e-04],\n",
       "          [9.9939e-01, 1.2147e-04, 1.2128e-04, 1.2139e-04, 1.2126e-04, 1.2171e-04],\n",
       "          [1.0000e+00, 4.9275e-31, 4.9275e-31, 4.9275e-31, 4.9275e-31, 4.9275e-31],\n",
       "          [9.9914e-01, 1.7145e-04, 1.7118e-04, 1.7151e-04, 1.7114e-04, 1.7232e-04],\n",
       "          [9.9898e-01, 2.0374e-04, 2.0333e-04, 2.0363e-04, 2.0331e-04, 2.0459e-04],\n",
       "          [9.9629e-01, 7.4160e-04, 7.3875e-04, 7.4226e-04, 7.3811e-04, 7.4799e-04],\n",
       "          [9.9911e-01, 1.7851e-04, 1.7829e-04, 1.7849e-04, 1.7829e-04, 1.7988e-04],\n",
       "          [9.9963e-01, 7.3307e-05, 7.3260e-05, 7.3336e-05, 7.3254e-05, 7.3527e-05],\n",
       "          [9.9891e-01, 2.1702e-04, 2.1672e-04, 2.1720e-04, 2.1667e-04, 2.1836e-04],\n",
       "          [9.9791e-01, 4.1897e-04, 4.1775e-04, 4.1846e-04, 4.1755e-04, 4.2095e-04],\n",
       "          [9.9961e-01, 7.7414e-05, 7.7360e-05, 7.7419e-05, 7.7352e-05, 7.7655e-05],\n",
       "          [1.0000e+00, 4.0943e-31, 4.0943e-31, 4.0943e-31, 4.0943e-31, 4.0943e-31],\n",
       "          [1.0000e+00, 1.8217e-44, 1.8217e-44, 1.8217e-44, 1.8217e-44, 1.8217e-44],\n",
       "          [1.0000e+00, 7.8629e-31, 7.8629e-31, 7.8629e-31, 7.8629e-31, 7.8629e-31],\n",
       "          [9.9892e-01, 2.1669e-04, 2.1639e-04, 2.1688e-04, 2.1634e-04, 2.1803e-04],\n",
       "          [9.9858e-01, 2.8320e-04, 2.8251e-04, 2.8302e-04, 2.8246e-04, 2.8483e-04],\n",
       "          [1.0000e+00, 4.0644e-31, 4.0644e-31, 4.0644e-31, 4.0644e-31, 4.0644e-31],\n",
       "          [9.9999e-01, 2.0457e-06, 2.0455e-06, 2.0457e-06, 2.0455e-06, 2.0465e-06],\n",
       "          [1.0000e+00, 2.3011e-31, 2.3011e-31, 2.3011e-31, 2.3011e-31, 2.3011e-31],\n",
       "          [9.9614e-01, 7.7238e-04, 7.6970e-04, 7.7223e-04, 7.6931e-04, 7.7979e-04],\n",
       "          [9.9982e-01, 3.6652e-05, 3.6621e-05, 3.6650e-05, 3.6620e-05, 3.6707e-05],\n",
       "          [9.9750e-01, 4.9953e-04, 4.9816e-04, 4.9939e-04, 4.9811e-04, 5.0388e-04],\n",
       "          [1.0000e+00, 6.2391e-31, 6.2391e-31, 6.2391e-31, 6.2391e-31, 6.2391e-31],\n",
       "          [1.0000e+00, 5.4419e-31, 5.4419e-31, 5.4419e-31, 5.4419e-31, 5.4419e-31],\n",
       "          [9.9742e-01, 5.1456e-04, 5.1307e-04, 5.1461e-04, 5.1303e-04, 5.1987e-04],\n",
       "          [9.9441e-01, 1.1175e-03, 1.1118e-03, 1.1169e-03, 1.1115e-03, 1.1332e-03],\n",
       "          [1.0000e+00, 3.1469e-31, 3.1469e-31, 3.1469e-31, 3.1469e-31, 3.1469e-31],\n",
       "          [9.9760e-01, 4.7971e-04, 4.7846e-04, 4.7980e-04, 4.7826e-04, 4.8356e-04],\n",
       "          [9.9818e-01, 3.6455e-04, 3.6367e-04, 3.6437e-04, 3.6361e-04, 3.6750e-04],\n",
       "          [9.9893e-01, 2.1339e-04, 2.1304e-04, 2.1337e-04, 2.1304e-04, 2.1492e-04],\n",
       "          [9.9763e-01, 4.7304e-04, 4.7179e-04, 4.7264e-04, 4.7173e-04, 4.7625e-04],\n",
       "          [9.9926e-01, 1.4836e-04, 1.4823e-04, 1.4852e-04, 1.4820e-04, 1.4908e-04],\n",
       "          [9.9816e-01, 3.6701e-04, 3.6618e-04, 3.6701e-04, 3.6612e-04, 3.6906e-04],\n",
       "          [9.9714e-01, 5.7169e-04, 5.6949e-04, 5.7193e-04, 5.6890e-04, 5.7544e-04],\n",
       "          [1.0000e+00, 3.4670e-31, 3.4670e-31, 3.4670e-31, 3.4670e-31, 3.4670e-31],\n",
       "          [1.0000e+00, 3.0934e-31, 3.0934e-31, 3.0934e-31, 3.0934e-31, 3.0934e-31],\n",
       "          [9.9741e-01, 5.1920e-04, 5.1707e-04, 5.1818e-04, 5.1687e-04, 5.2140e-04],\n",
       "          [9.9741e-01, 5.1823e-04, 5.1651e-04, 5.1780e-04, 5.1655e-04, 5.2335e-04],\n",
       "          [9.9616e-01, 7.6688e-04, 7.6412e-04, 7.6708e-04, 7.6403e-04, 7.7369e-04],\n",
       "          [1.0000e+00, 4.8228e-31, 4.8228e-31, 4.8228e-31, 4.8228e-31, 4.8228e-31],\n",
       "          [9.9753e-01, 4.9306e-04, 4.9141e-04, 4.9295e-04, 4.9142e-04, 4.9664e-04],\n",
       "          [1.0000e+00, 5.5882e-31, 5.5882e-31, 5.5882e-31, 5.5882e-31, 5.5882e-31],\n",
       "          [1.0000e+00, 4.0965e-31, 4.0965e-31, 4.0965e-31, 4.0965e-31, 4.0965e-31],\n",
       "          [9.9743e-01, 5.1356e-04, 5.1203e-04, 5.1295e-04, 5.1189e-04, 5.1734e-04],\n",
       "          [9.9574e-01, 8.5273e-04, 8.4778e-04, 8.4962e-04, 8.4784e-04, 8.5720e-04],\n",
       "          [9.9475e-01, 1.0492e-03, 1.0436e-03, 1.0475e-03, 1.0434e-03, 1.0619e-03],\n",
       "          [1.0000e+00, 2.9354e-31, 2.9354e-31, 2.9354e-31, 2.9354e-31, 2.9354e-31],\n",
       "          [9.9728e-01, 5.4396e-04, 5.4230e-04, 5.4454e-04, 5.4220e-04, 5.4876e-04],\n",
       "          [1.0000e+00, 4.5139e-31, 4.5139e-31, 4.5139e-31, 4.5139e-31, 4.5139e-31],\n",
       "          [9.9946e-01, 1.0770e-04, 1.0754e-04, 1.0768e-04, 1.0752e-04, 1.0788e-04],\n",
       "          [9.9980e-01, 3.9419e-05, 3.9392e-05, 3.9414e-05, 3.9389e-05, 3.9495e-05],\n",
       "          [9.9994e-01, 1.2764e-05, 1.2761e-05, 1.2764e-05, 1.2761e-05, 1.2782e-05],\n",
       "          [1.0000e+00, 5.4159e-31, 5.4159e-31, 5.4159e-31, 5.4159e-31, 5.4159e-31],\n",
       "          [1.0000e+00, 4.7449e-31, 4.7449e-31, 4.7449e-31, 4.7449e-31, 4.7449e-31],\n",
       "          [1.0000e+00, 4.1974e-31, 4.1974e-31, 4.1974e-31, 4.1974e-31, 4.1974e-31],\n",
       "          [9.9221e-01, 1.5587e-03, 1.5472e-03, 1.5554e-03, 1.5455e-03, 1.5786e-03],\n",
       "          [9.9610e-01, 7.7989e-04, 7.7736e-04, 7.8038e-04, 7.7696e-04, 7.8803e-04],\n",
       "          [1.0000e+00, 3.7266e-31, 3.7266e-31, 3.7266e-31, 3.7266e-31, 3.7266e-31],\n",
       "          [9.9622e-01, 7.5709e-04, 7.5345e-04, 7.5591e-04, 7.5324e-04, 7.6383e-04],\n",
       "          [9.9396e-01, 1.2087e-03, 1.2016e-03, 1.2068e-03, 1.2008e-03, 1.2249e-03],\n",
       "          [9.9906e-01, 1.8695e-04, 1.8667e-04, 1.8695e-04, 1.8665e-04, 1.8812e-04],\n",
       "          [1.0000e+00, 4.9787e-31, 4.9787e-31, 4.9787e-31, 4.9787e-31, 4.9787e-31],\n",
       "          [9.9775e-01, 4.4974e-04, 4.4837e-04, 4.4949e-04, 4.4827e-04, 4.5405e-04],\n",
       "          [9.9791e-01, 4.1892e-04, 4.1723e-04, 4.1849e-04, 4.1724e-04, 4.2082e-04],\n",
       "          [9.9648e-01, 7.0497e-04, 7.0152e-04, 7.0399e-04, 7.0109e-04, 7.0901e-04],\n",
       "          [1.0000e+00, 8.6844e-07, 8.6838e-07, 8.6842e-07, 8.6838e-07, 8.6873e-07],\n",
       "          [9.9966e-01, 6.7996e-05, 6.7936e-05, 6.7994e-05, 6.7921e-05, 6.8176e-05],\n",
       "          [9.9825e-01, 3.5067e-04, 3.4985e-04, 3.5100e-04, 3.4972e-04, 3.5263e-04],\n",
       "          [9.9626e-01, 7.4783e-04, 7.4390e-04, 7.4569e-04, 7.4383e-04, 7.5379e-04],\n",
       "          [9.9818e-01, 3.6310e-04, 3.6235e-04, 3.6330e-04, 3.6223e-04, 3.6574e-04]],\n",
       "         grad_fn=<CopySlices>),\n",
       "  'cc': tensor([[2.7906e-02, 9.3318e-01, 2.0907e-02, 1.8011e-02],\n",
       "          [3.9053e-04, 9.9900e-01, 3.0671e-04, 3.0069e-04],\n",
       "          [1.8668e-02, 9.5471e-01, 1.3829e-02, 1.2796e-02],\n",
       "          [5.1569e-02, 8.7509e-01, 3.9134e-02, 3.4203e-02],\n",
       "          [8.8502e-02, 7.8358e-01, 6.9018e-02, 5.8898e-02],\n",
       "          [7.6560e-02, 8.2182e-01, 5.5905e-02, 4.5719e-02],\n",
       "          [2.6129e-02, 9.3819e-01, 1.8579e-02, 1.7104e-02],\n",
       "          [6.4023e-02, 8.5483e-01, 4.4678e-02, 3.6473e-02],\n",
       "          [4.9039e-02, 8.8402e-01, 3.6800e-02, 3.0138e-02],\n",
       "          [8.0568e-02, 8.1162e-01, 5.8805e-02, 4.9006e-02],\n",
       "          [9.8444e-02, 7.7246e-01, 7.0528e-02, 5.8565e-02],\n",
       "          [2.1805e-02, 9.4779e-01, 1.6021e-02, 1.4384e-02],\n",
       "          [7.5870e-02, 8.2304e-01, 5.5368e-02, 4.5723e-02],\n",
       "          [6.5810e-02, 8.5078e-01, 4.5179e-02, 3.8232e-02],\n",
       "          [5.0832e-02, 8.7590e-01, 3.8931e-02, 3.4341e-02],\n",
       "          [8.8161e-02, 7.9450e-01, 6.4087e-02, 5.3252e-02],\n",
       "          [2.8337e-03, 9.9277e-01, 2.2411e-03, 2.1526e-03],\n",
       "          [6.5553e-02, 8.4333e-01, 4.8948e-02, 4.2170e-02],\n",
       "          [6.6170e-02, 8.4532e-01, 4.8696e-02, 3.9816e-02],\n",
       "          [3.1462e-02, 9.2569e-01, 2.2455e-02, 2.0390e-02],\n",
       "          [6.8160e-02, 8.3402e-01, 5.2475e-02, 4.5345e-02],\n",
       "          [9.6897e-02, 7.7400e-01, 7.1292e-02, 5.7808e-02],\n",
       "          [8.6152e-02, 7.9191e-01, 6.5093e-02, 5.6839e-02],\n",
       "          [1.7372e-02, 9.5863e-01, 1.2537e-02, 1.1460e-02],\n",
       "          [1.3895e-03, 9.9666e-01, 9.9715e-04, 9.5788e-04],\n",
       "          [6.0908e-02, 8.5626e-01, 4.4643e-02, 3.8187e-02],\n",
       "          [1.2948e-03, 9.9687e-01, 9.3641e-04, 8.9986e-04],\n",
       "          [1.0894e-02, 9.7350e-01, 8.0865e-03, 7.5196e-03],\n",
       "          [5.9738e-02, 8.6193e-01, 4.1948e-02, 3.6387e-02],\n",
       "          [5.8326e-02, 8.6405e-01, 4.2878e-02, 3.4750e-02],\n",
       "          [6.7969e-02, 8.4194e-01, 4.8919e-02, 4.1173e-02],\n",
       "          [6.1796e-02, 8.5366e-01, 4.5012e-02, 3.9531e-02],\n",
       "          [7.3468e-02, 8.3199e-01, 5.1828e-02, 4.2714e-02],\n",
       "          [5.5121e-02, 8.7039e-01, 3.9542e-02, 3.4944e-02],\n",
       "          [3.5341e-02, 9.1748e-01, 2.5628e-02, 2.1549e-02],\n",
       "          [2.1778e-02, 9.4718e-01, 1.6415e-02, 1.4626e-02],\n",
       "          [6.4959e-02, 8.4517e-01, 4.8000e-02, 4.1868e-02],\n",
       "          [5.1787e-02, 8.7722e-01, 3.8154e-02, 3.2840e-02],\n",
       "          [2.9582e-02, 9.2998e-01, 2.1112e-02, 1.9325e-02],\n",
       "          [7.1789e-02, 8.3731e-01, 4.9615e-02, 4.1290e-02],\n",
       "          [4.9401e-02, 8.8376e-01, 3.5960e-02, 3.0882e-02],\n",
       "          [1.0019e-03, 9.9756e-01, 7.3351e-04, 7.0819e-04],\n",
       "          [3.4965e-02, 9.1476e-01, 2.6844e-02, 2.3434e-02],\n",
       "          [8.9823e-03, 9.7788e-01, 6.7916e-03, 6.3508e-03],\n",
       "          [4.9050e-02, 8.8692e-01, 3.5101e-02, 2.8925e-02],\n",
       "          [2.3101e-02, 9.4702e-01, 1.5597e-02, 1.4284e-02],\n",
       "          [5.5682e-02, 8.6729e-01, 4.1035e-02, 3.5988e-02],\n",
       "          [3.4303e-02, 9.2024e-01, 2.4080e-02, 2.1373e-02],\n",
       "          [7.6928e-02, 8.1755e-01, 5.6543e-02, 4.8975e-02],\n",
       "          [6.7164e-02, 8.4170e-01, 4.9401e-02, 4.1731e-02],\n",
       "          [1.4722e-02, 9.6357e-01, 1.1192e-02, 1.0514e-02],\n",
       "          [9.1070e-02, 7.8138e-01, 6.8580e-02, 5.8967e-02],\n",
       "          [4.9342e-02, 8.7885e-01, 3.8246e-02, 3.3558e-02],\n",
       "          [9.6039e-02, 7.7606e-01, 6.9993e-02, 5.7908e-02],\n",
       "          [7.7196e-02, 8.2282e-01, 5.4866e-02, 4.5118e-02],\n",
       "          [5.4480e-02, 8.7025e-01, 3.9769e-02, 3.5503e-02],\n",
       "          [5.8748e-02, 8.6133e-01, 4.3353e-02, 3.6573e-02],\n",
       "          [6.7586e-02, 8.4151e-01, 5.0062e-02, 4.0844e-02],\n",
       "          [8.8560e-04, 9.9786e-01, 6.3678e-04, 6.2143e-04],\n",
       "          [7.8321e-02, 8.1488e-01, 5.7970e-02, 4.8826e-02],\n",
       "          [7.1434e-02, 8.3788e-01, 4.9809e-02, 4.0872e-02],\n",
       "          [5.3634e-02, 8.7145e-01, 3.9623e-02, 3.5294e-02],\n",
       "          [6.7686e-02, 8.3716e-01, 5.0545e-02, 4.4606e-02],\n",
       "          [5.4186e-02, 8.7555e-01, 3.7720e-02, 3.2542e-02],\n",
       "          [1.0385e-03, 9.9745e-01, 7.7131e-04, 7.4452e-04],\n",
       "          [5.4700e-02, 8.7737e-01, 3.7109e-02, 3.0825e-02],\n",
       "          [6.5309e-02, 8.4557e-01, 4.7761e-02, 4.1360e-02],\n",
       "          [6.0745e-02, 8.5666e-01, 4.3654e-02, 3.8942e-02],\n",
       "          [6.9026e-02, 8.3669e-01, 5.0495e-02, 4.3791e-02],\n",
       "          [5.4348e-02, 8.6981e-01, 4.0313e-02, 3.5525e-02],\n",
       "          [5.7841e-02, 8.6368e-01, 4.1432e-02, 3.7045e-02],\n",
       "          [7.3793e-02, 8.2342e-01, 5.5332e-02, 4.7450e-02],\n",
       "          [7.4972e-02, 8.2755e-01, 5.3517e-02, 4.3961e-02],\n",
       "          [7.3424e-02, 8.3145e-01, 5.1571e-02, 4.3558e-02],\n",
       "          [3.6390e-02, 9.1377e-01, 2.6471e-02, 2.3366e-02],\n",
       "          [3.3356e-02, 9.1998e-01, 2.4795e-02, 2.1870e-02],\n",
       "          [2.6707e-02, 9.3524e-01, 1.9874e-02, 1.8179e-02],\n",
       "          [6.5527e-02, 8.4350e-01, 4.8073e-02, 4.2903e-02],\n",
       "          [2.7277e-02, 9.3597e-01, 1.9175e-02, 1.7580e-02],\n",
       "          [4.1755e-02, 9.0034e-01, 3.1068e-02, 2.6840e-02],\n",
       "          [8.4201e-02, 8.0844e-01, 5.8936e-02, 4.8425e-02],\n",
       "          [6.7133e-02, 8.4489e-01, 4.7683e-02, 4.0294e-02],\n",
       "          [5.9044e-02, 8.5799e-01, 4.3979e-02, 3.8986e-02],\n",
       "          [3.7787e-02, 9.1070e-01, 2.7105e-02, 2.4404e-02],\n",
       "          [1.3592e-02, 9.6743e-01, 9.8922e-03, 9.0857e-03],\n",
       "          [7.4926e-02, 8.3039e-01, 5.1660e-02, 4.3026e-02],\n",
       "          [6.4025e-02, 8.4684e-01, 4.7402e-02, 4.1728e-02],\n",
       "          [4.1833e-02, 9.0324e-01, 2.9218e-02, 2.5713e-02],\n",
       "          [5.7722e-02, 8.6690e-01, 4.1681e-02, 3.3700e-02],\n",
       "          [3.2912e-02, 9.2093e-01, 2.4497e-02, 2.1662e-02],\n",
       "          [5.6788e-02, 8.6685e-01, 4.1170e-02, 3.5190e-02],\n",
       "          [7.4988e-02, 8.3033e-01, 5.1639e-02, 4.3042e-02],\n",
       "          [5.3459e-02, 8.7244e-01, 3.8918e-02, 3.5185e-02],\n",
       "          [5.6702e-02, 8.6994e-01, 4.0392e-02, 3.2967e-02],\n",
       "          [9.5219e-03, 9.7701e-01, 7.0170e-03, 6.4466e-03],\n",
       "          [2.4542e-02, 9.4116e-01, 1.8102e-02, 1.6191e-02],\n",
       "          [5.4578e-02, 8.7049e-01, 3.9827e-02, 3.5101e-02],\n",
       "          [7.3105e-03, 9.8215e-01, 5.4542e-03, 5.0807e-03],\n",
       "          [7.1869e-02, 8.3514e-01, 5.0310e-02, 4.2682e-02],\n",
       "          [2.4887e-02, 9.3964e-01, 1.8521e-02, 1.6948e-02],\n",
       "          [3.7564e-02, 9.0883e-01, 2.8700e-02, 2.4906e-02],\n",
       "          [5.9677e-02, 8.5926e-01, 4.3583e-02, 3.7477e-02],\n",
       "          [8.5017e-02, 7.9858e-01, 6.2343e-02, 5.4057e-02],\n",
       "          [7.5251e-03, 9.8147e-01, 5.7044e-03, 5.3020e-03],\n",
       "          [8.6076e-02, 8.0163e-01, 6.1231e-02, 5.1065e-02],\n",
       "          [6.0677e-02, 8.5709e-01, 4.4026e-02, 3.8209e-02],\n",
       "          [4.7635e-02, 8.8776e-01, 3.4627e-02, 2.9973e-02],\n",
       "          [5.4529e-02, 8.7308e-01, 3.8999e-02, 3.3394e-02],\n",
       "          [6.2652e-02, 8.5798e-01, 4.2944e-02, 3.6429e-02],\n",
       "          [7.7476e-02, 8.1930e-01, 5.6399e-02, 4.6829e-02],\n",
       "          [4.3016e-02, 8.9889e-01, 3.0667e-02, 2.7431e-02],\n",
       "          [6.7854e-02, 8.3952e-01, 5.1408e-02, 4.1217e-02],\n",
       "          [7.2633e-02, 8.3338e-01, 5.1618e-02, 4.2368e-02],\n",
       "          [4.1320e-02, 9.0124e-01, 3.0371e-02, 2.7073e-02],\n",
       "          [7.6417e-02, 8.1956e-01, 5.5697e-02, 4.8331e-02],\n",
       "          [6.3822e-02, 8.4612e-01, 4.8170e-02, 4.1888e-02],\n",
       "          [5.8006e-02, 8.6654e-01, 4.1530e-02, 3.3927e-02],\n",
       "          [9.5444e-02, 7.7518e-01, 7.0529e-02, 5.8842e-02],\n",
       "          [3.2059e-02, 9.2162e-01, 2.4247e-02, 2.2077e-02],\n",
       "          [5.4808e-02, 8.6843e-01, 4.0795e-02, 3.5967e-02],\n",
       "          [4.3317e-02, 8.9602e-01, 3.1992e-02, 2.8675e-02],\n",
       "          [5.1513e-02, 8.7532e-01, 3.8777e-02, 3.4395e-02],\n",
       "          [8.6800e-02, 7.9264e-01, 6.5042e-02, 5.5522e-02],\n",
       "          [7.6124e-02, 8.2204e-01, 5.6111e-02, 4.5728e-02],\n",
       "          [7.4178e-02, 8.2406e-01, 5.4549e-02, 4.7218e-02],\n",
       "          [6.8670e-02, 8.3969e-01, 5.0752e-02, 4.0885e-02],\n",
       "          [3.1669e-02, 9.2312e-01, 2.3544e-02, 2.1664e-02],\n",
       "          [2.8017e-02, 9.3627e-01, 1.8807e-02, 1.6903e-02],\n",
       "          [1.9676e-02, 9.5363e-01, 1.3999e-02, 1.2697e-02],\n",
       "          [6.1877e-02, 8.5661e-01, 4.4228e-02, 3.7280e-02],\n",
       "          [4.0759e-02, 9.0410e-01, 2.9676e-02, 2.5465e-02],\n",
       "          [5.5898e-02, 8.7335e-01, 3.8754e-02, 3.1993e-02],\n",
       "          [4.9003e-02, 8.8117e-01, 3.7009e-02, 3.2822e-02],\n",
       "          [5.5474e-02, 8.6906e-01, 4.0092e-02, 3.5372e-02],\n",
       "          [6.6369e-02, 8.4331e-01, 4.9250e-02, 4.1068e-02],\n",
       "          [7.0800e-02, 8.2682e-01, 5.4755e-02, 4.7621e-02],\n",
       "          [6.7677e-02, 8.3581e-01, 5.1286e-02, 4.5225e-02],\n",
       "          [5.0196e-02, 8.8208e-01, 3.6093e-02, 3.1630e-02],\n",
       "          [6.5043e-02, 8.4455e-01, 4.8152e-02, 4.2251e-02],\n",
       "          [8.0128e-02, 8.0884e-01, 5.9792e-02, 5.1245e-02],\n",
       "          [6.0899e-02, 8.5815e-01, 4.2743e-02, 3.8209e-02],\n",
       "          [5.8222e-02, 8.6385e-01, 4.1432e-02, 3.6495e-02],\n",
       "          [3.0933e-04, 9.9918e-01, 2.5542e-04, 2.5061e-04],\n",
       "          [2.3257e-02, 9.4607e-01, 1.5923e-02, 1.4749e-02],\n",
       "          [6.0066e-02, 8.5645e-01, 4.4297e-02, 3.9184e-02],\n",
       "          [3.9549e-02, 9.0376e-01, 2.9815e-02, 2.6873e-02],\n",
       "          [5.7312e-02, 8.6704e-01, 4.0698e-02, 3.4954e-02]],\n",
       "         grad_fn=<CopySlices>),\n",
       "  'dlt1': tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.2711e-01, 2.0447e-01, 3.4400e-02, 5.6352e-02, 6.2099e-02],\n",
       "          [1.8648e-01, 2.2952e-01, 5.6910e-02, 1.0978e-01, 1.2920e-01],\n",
       "          [2.2752e-01, 2.6047e-01, 9.1195e-02, 1.3310e-01, 1.3057e-01],\n",
       "          [2.5006e-01, 2.4925e-01, 6.1621e-02, 1.1578e-01, 1.1842e-01],\n",
       "          [1.5185e-01, 1.9833e-01, 6.4473e-02, 9.2067e-02, 1.0218e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.2498e-01, 2.2047e-01, 7.4441e-02, 1.1320e-01, 1.1186e-01],\n",
       "          [2.2334e-01, 2.0959e-01, 8.5195e-02, 1.2285e-01, 1.3409e-01],\n",
       "          [1.7878e-01, 1.9093e-01, 5.2828e-02, 7.4961e-02, 1.0682e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.3620e-01, 2.3736e-01, 6.6012e-02, 1.2646e-01, 1.1555e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.3233e-01, 2.3012e-01, 7.9107e-02, 1.1523e-01, 1.2411e-01],\n",
       "          [6.8616e-02, 1.3333e-01, 9.3326e-03, 2.9744e-02, 2.7808e-02],\n",
       "          [2.2542e-01, 2.4127e-01, 7.5700e-02, 1.1093e-01, 1.1203e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.1033e-01, 1.7834e-01, 5.4516e-02, 1.0072e-01, 9.9965e-02],\n",
       "          [2.0118e-01, 2.3563e-01, 7.5078e-02, 1.2543e-01, 1.2798e-01],\n",
       "          [2.4709e-01, 2.4570e-01, 8.5679e-02, 1.1163e-01, 1.3746e-01],\n",
       "          [2.4395e-01, 2.8694e-01, 1.0268e-01, 1.4699e-01, 1.5320e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [4.6380e-08, 4.1791e-08, 1.1604e-14, 2.1068e-11, 4.4249e-11],\n",
       "          [2.4660e-01, 2.3121e-01, 7.6456e-02, 1.0744e-01, 1.2775e-01],\n",
       "          [4.1327e-08, 3.7742e-08, 1.3939e-14, 2.0189e-11, 4.7995e-11],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.1854e-01, 2.1985e-01, 6.5127e-02, 1.1624e-01, 1.3660e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.5088e-01, 2.3201e-01, 9.1409e-02, 1.2798e-01, 1.4694e-01],\n",
       "          [2.3509e-01, 2.2524e-01, 7.2946e-02, 1.2156e-01, 1.1615e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.8875e-01, 2.0881e-01, 6.0657e-02, 9.8053e-02, 1.2735e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.1977e-01, 2.3018e-01, 8.2990e-02, 1.2534e-01, 1.2414e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.3341e-01, 2.6018e-01, 6.0773e-02, 1.2429e-01, 1.0485e-01],\n",
       "          [2.1639e-01, 2.3110e-01, 6.2422e-02, 1.1351e-01, 9.6318e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.6172e-01, 1.7595e-01, 3.1053e-02, 7.8412e-02, 6.4118e-02],\n",
       "          [2.0696e-01, 2.2625e-01, 4.9456e-02, 9.6834e-02, 1.1876e-01],\n",
       "          [2.1607e-01, 1.5956e-01, 4.6943e-02, 8.4705e-02, 9.5555e-02],\n",
       "          [2.3790e-01, 2.4388e-01, 1.0644e-01, 1.5233e-01, 1.6715e-01],\n",
       "          [2.2771e-01, 2.2586e-01, 6.7926e-02, 1.1791e-01, 1.2561e-01],\n",
       "          [1.5972e-01, 1.5995e-01, 3.6528e-02, 7.3925e-02, 6.5145e-02],\n",
       "          [2.3311e-01, 2.6252e-01, 9.5827e-02, 1.5602e-01, 1.3833e-01],\n",
       "          [2.4111e-01, 2.8591e-01, 8.9687e-02, 1.3892e-01, 1.4675e-01],\n",
       "          [2.3352e-01, 2.4142e-01, 8.5768e-02, 1.3301e-01, 1.2696e-01],\n",
       "          [2.4773e-01, 2.5792e-01, 6.2686e-02, 1.2703e-01, 1.1339e-01],\n",
       "          [2.0191e-01, 2.3086e-01, 7.6220e-02, 1.2174e-01, 1.2616e-01],\n",
       "          [2.0586e-01, 2.4023e-01, 6.7347e-02, 1.1727e-01, 9.7954e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [5.5669e-02, 7.8753e-02, 6.9836e-03, 2.0365e-02, 1.6426e-02],\n",
       "          [2.4069e-01, 2.6497e-01, 9.0626e-02, 1.3478e-01, 1.2869e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.2947e-01, 2.5491e-01, 9.6475e-02, 1.3750e-01, 1.4865e-01],\n",
       "          [2.4096e-01, 2.4035e-01, 1.1191e-01, 1.2679e-01, 1.5698e-01],\n",
       "          [2.3239e-01, 2.0352e-01, 6.6863e-02, 9.5215e-02, 1.3513e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.6433e-01, 2.4098e-01, 1.0942e-01, 1.4064e-01, 1.7539e-01],\n",
       "          [1.8127e-01, 2.3172e-01, 8.1281e-02, 1.1331e-01, 1.1126e-01],\n",
       "          [2.4332e-01, 2.3924e-01, 8.2144e-02, 1.3659e-01, 1.5039e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.8512e-01, 2.2129e-01, 8.5144e-02, 1.0022e-01, 1.1913e-01],\n",
       "          [2.4260e-01, 2.5744e-01, 8.2564e-02, 1.3596e-01, 1.3621e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.1806e-01, 1.9358e-01, 6.4468e-02, 1.0244e-01, 1.2990e-01],\n",
       "          [1.8494e-01, 1.9422e-01, 4.4739e-02, 7.2479e-02, 9.1666e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.0607e-01, 1.3539e-01, 2.1823e-02, 5.9326e-02, 7.5948e-02],\n",
       "          [1.8736e-01, 2.0798e-01, 8.3404e-02, 1.0669e-01, 1.2416e-01],\n",
       "          [1.2883e-01, 1.9099e-01, 5.3533e-02, 7.1741e-02, 7.7274e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.2724e-01, 2.2065e-01, 6.0013e-02, 1.0657e-01, 1.0264e-01],\n",
       "          [2.2654e-01, 2.1639e-01, 6.1085e-02, 7.9935e-02, 9.5752e-02],\n",
       "          [2.1613e-01, 2.1855e-01, 1.0217e-01, 1.2015e-01, 1.3903e-01],\n",
       "          [2.0885e-01, 1.8597e-01, 5.9712e-02, 1.0162e-01, 1.0812e-01],\n",
       "          [1.8794e-01, 2.0811e-01, 5.2614e-02, 5.1400e-02, 8.6233e-02],\n",
       "          [2.2244e-01, 2.1266e-01, 6.7559e-02, 1.0187e-01, 1.2155e-01],\n",
       "          [2.2280e-01, 2.3074e-01, 7.1275e-02, 1.1425e-01, 1.0886e-01],\n",
       "          [1.9190e-01, 1.5990e-01, 4.7962e-02, 8.4814e-02, 6.9504e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.1993e-01, 2.1260e-01, 6.7802e-02, 9.9758e-02, 1.2040e-01],\n",
       "          [1.9718e-01, 2.1257e-01, 6.6581e-02, 1.1306e-01, 1.0860e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.0295e-01, 1.2640e-01, 1.4034e-02, 3.1640e-02, 2.8757e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.0298e-01, 2.1997e-01, 9.9647e-02, 1.5666e-01, 1.3228e-01],\n",
       "          [1.0561e-01, 1.5001e-01, 3.3136e-02, 5.5828e-02, 6.6367e-02],\n",
       "          [2.4909e-01, 2.1600e-01, 8.6870e-02, 1.2995e-01, 1.4615e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.2894e-01, 2.2472e-01, 8.7157e-02, 1.3280e-01, 1.2927e-01],\n",
       "          [2.6482e-01, 2.5933e-01, 1.0876e-01, 1.4713e-01, 1.4693e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.5649e-01, 2.3304e-01, 7.7830e-02, 1.3624e-01, 1.4664e-01],\n",
       "          [2.4270e-01, 2.4772e-01, 6.4362e-02, 1.2862e-01, 1.2410e-01],\n",
       "          [2.2476e-01, 2.3077e-01, 6.3180e-02, 1.0302e-01, 9.9347e-02],\n",
       "          [2.4450e-01, 2.5179e-01, 6.5682e-02, 1.1477e-01, 1.1407e-01],\n",
       "          [2.1266e-01, 2.0048e-01, 5.7292e-02, 8.7745e-02, 1.1660e-01],\n",
       "          [2.2362e-01, 2.4155e-01, 7.7656e-02, 1.2251e-01, 1.3474e-01],\n",
       "          [1.9108e-01, 2.1834e-01, 9.8152e-02, 1.0593e-01, 1.4038e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.2323e-01, 2.4160e-01, 8.7591e-02, 1.0787e-01, 1.3784e-01],\n",
       "          [2.3124e-01, 2.4959e-01, 7.9393e-02, 1.2565e-01, 1.3745e-01],\n",
       "          [2.6484e-01, 2.5078e-01, 9.2634e-02, 1.3479e-01, 1.7708e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.4238e-01, 2.2499e-01, 8.4433e-02, 1.1978e-01, 1.3058e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.7073e-01, 2.0794e-01, 9.0865e-02, 1.1833e-01, 1.2125e-01],\n",
       "          [2.1355e-01, 2.7432e-01, 8.0967e-02, 1.3905e-01, 1.1235e-01],\n",
       "          [2.4576e-01, 2.7552e-01, 1.0074e-01, 1.5646e-01, 1.4020e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.3917e-01, 2.2137e-01, 1.0234e-01, 1.0962e-01, 1.3513e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.2374e-01, 1.5528e-01, 4.1531e-02, 8.3253e-02, 8.6494e-02],\n",
       "          [1.8187e-01, 1.8338e-01, 3.6237e-02, 7.0584e-02, 7.4416e-02],\n",
       "          [1.4130e-01, 1.5814e-01, 2.9206e-02, 4.2602e-02, 5.8514e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.6943e-01, 2.6687e-01, 1.1225e-01, 1.4902e-01, 1.5240e-01],\n",
       "          [2.5110e-01, 2.6018e-01, 1.0083e-01, 1.4637e-01, 1.4760e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.1571e-01, 2.4245e-01, 9.7193e-02, 1.3572e-01, 1.5531e-01],\n",
       "          [2.5908e-01, 2.8174e-01, 8.1965e-02, 1.5638e-01, 1.6168e-01],\n",
       "          [1.9738e-01, 2.0052e-01, 6.4129e-02, 9.9809e-02, 1.0785e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.2761e-01, 2.5439e-01, 7.9226e-02, 1.3089e-01, 1.2437e-01],\n",
       "          [2.1797e-01, 2.2776e-01, 6.9612e-02, 1.1195e-01, 1.3125e-01],\n",
       "          [2.3584e-01, 2.5752e-01, 6.6849e-02, 1.4291e-01, 1.4562e-01],\n",
       "          [6.7730e-02, 8.7220e-02, 1.4905e-02, 2.8084e-02, 3.0520e-02],\n",
       "          [1.7269e-01, 1.8868e-01, 5.5215e-02, 7.6342e-02, 7.7030e-02],\n",
       "          [2.0111e-01, 2.0522e-01, 9.0246e-02, 8.2363e-02, 1.3531e-01],\n",
       "          [2.0646e-01, 2.6935e-01, 8.3503e-02, 1.2994e-01, 1.2614e-01],\n",
       "          [2.3096e-01, 2.2930e-01, 8.7634e-02, 1.1309e-01, 1.3855e-01]],\n",
       "         grad_fn=<CopySlices>),\n",
       "  'dlt2': tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0953, 0.0739, 0.0882, 0.1022, 0.0962],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0494, 0.0291, 0.0551, 0.0581, 0.0499],\n",
       "          [0.0443, 0.0264, 0.0470, 0.0509, 0.0474],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0516, 0.0325, 0.0537, 0.0635, 0.0519],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0472, 0.0292, 0.0528, 0.0564, 0.0501],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0331, 0.0197, 0.0361, 0.0384, 0.0347],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0576, 0.0395, 0.0634, 0.0623, 0.0641],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0017, 0.0006, 0.0021, 0.0021, 0.0015],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0461, 0.0275, 0.0493, 0.0582, 0.0473],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0445, 0.0271, 0.0478, 0.0536, 0.0453],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0564, 0.0360, 0.0572, 0.0638, 0.0616],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0478, 0.0278, 0.0488, 0.0603, 0.0474],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0580, 0.0370, 0.0542, 0.0679, 0.0562],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0549, 0.0344, 0.0557, 0.0653, 0.0567],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0566, 0.0384, 0.0616, 0.0660, 0.0585],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0630, 0.0461, 0.0628, 0.0760, 0.0678],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], grad_fn=<CopySlices>)})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def temporal_loss(timestoevents,weights=None,maxtime=48):\n",
    "    #list of expected times to events, usualy in order of Const.temporal_outcomes\n",
    "    #basically longer = better, we count > maxtime (weeks) as no event\n",
    "    if weights is None: \n",
    "        weights = [1 for i in range(len(timestoevents))]\n",
    "        \n",
    "    scores = [(w*maxtime/t)*torch.lt(t,maxtime) for w,t in zip(weights,timestoevents)]\n",
    "    scores = torch.stack(scores).sum(axis=0)\n",
    "    return scores\n",
    "\n",
    "def outcome_loss(ypred,weights=None):\n",
    "    #default weights is bad\n",
    "    if weights is None: \n",
    "        print('using default outcome loss weights, which is probably wrong since bad stuff should be negative')\n",
    "        weights = [1 for i in range(ypred.shape[1])]\n",
    "    l = torch.mul(ypred[:,0],weights[0])\n",
    "    for i,weight in enumerate(weights[1:]):\n",
    "        #weights with negative values will invert the outcome so e.g. Regional control becomes no regional control\n",
    "        #so the penaly is correct\n",
    "        newloss = torch.mul(ypred[:,i],weight)\n",
    "        l = torch.add(l,newloss)\n",
    "    return l\n",
    "\n",
    "def calc_optimal_decisions(dataset,ids,m1,m2,m3,sm3,\n",
    "                           weights=[0,0.5,.5,0], #weight for OS, FT, AS, and LRC as binary probabilities\n",
    "                           tweights=[1,1,1,0], #weight for OS, LRC, FDM, and event (any + FT or AS at 6m) as time to event in weeks\n",
    "                           outcome_loss_func=None,\n",
    "                           get_transitions=True):\n",
    "    m1.eval()\n",
    "    m2.eval()\n",
    "    m3.eval()\n",
    "    sm3.eval()\n",
    "    device = m1.get_device()\n",
    "    data = dataset.processed_df.copy().loc[ids]\n",
    "    \n",
    "    def get_dlt(state):\n",
    "        if state == 2:\n",
    "            return data[Const.dlt2].copy()\n",
    "        d = data[Const.dlt1].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_pd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.primary_disease_states2].copy()\n",
    "        d = data[Const.primary_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_nd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.nodal_disease_states2].copy()\n",
    "        d = data[Const.nodal_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_cc(state):\n",
    "        res = data[Const.ccs].copy()\n",
    "        if state == 1:\n",
    "            res.values[:,:] = np.zeros(res.values.shape)\n",
    "        return res\n",
    "    \n",
    "    def get_mod(state):\n",
    "        res = data[Const.modifications].copy()\n",
    "        #this should have an ic condition but we don't use it anumore anywa\n",
    "        return res\n",
    "        \n",
    "    def formatdf(d):\n",
    "        d = df_to_torch(d).to(device)\n",
    "        return d\n",
    "    \n",
    "    \n",
    "    outcomedf = data[Const.outcomes]\n",
    "    baseline = dataset.get_state('baseline').loc[ids]\n",
    "    baseline_input = formatdf(baseline)\n",
    "\n",
    "        \n",
    "    if outcome_loss_func is None:\n",
    "        outcome_loss_func = outcome_loss\n",
    "    \n",
    "    cat = lambda x: torch.cat([xx.to(device) for xx in x],axis=1).to(device)\n",
    "    format_transition = lambda x: x.to(device)\n",
    "    def get_outcome(d1,d2,d3):\n",
    "        d1 = torch.full((len(ids),1),d1).type(torch.FloatTensor)\n",
    "        d2 = torch.full((len(ids),1),d2).type(torch.FloatTensor)\n",
    "        d3 = torch.full((len(ids),1),d3).type(torch.FloatTensor)\n",
    "        \n",
    "        tinput1 = cat([baseline_input,d1])\n",
    "        ytransition = m1(tinput1)\n",
    "        [ypd1,ynd1,ymod,ydlt1] = [format_transition(xx) for xx in ytransition['predictions']]\n",
    "        d1_thresh = torch.gt(d1,.5).view(-1,1).to(device)\n",
    "        ypd1[:,0:2] = ypd1[:,0:2]*d1_thresh\n",
    "        ynd1[:,0:2] = ynd1[:,0:2]*d1_thresh\n",
    "        \n",
    "        tinput2 = cat([baseline_input,ypd1,ynd1,ymod,ydlt1,d1,d2])\n",
    "        ytransition2 = m2(tinput2)\n",
    "        [ypd2,ynd2,ycc,ydlt2] = [format_transition(xx) for xx in ytransition2['predictions']]\n",
    "        \n",
    "        input3 = cat([baseline_input, ypd2, ynd2, ycc, ydlt2, d1, d2,d3])\n",
    "        outcome = m3(input3)['predictions']\n",
    "        temporal_outcomes = sm3.time_to_event(input3,n_samples=1)\n",
    "        \n",
    "        transitions = {\n",
    "            'pd1': ypd1,\n",
    "            'nd1': ynd1,\n",
    "            'nd2': ynd2,\n",
    "            'pd2': ypd2,\n",
    "            'mod': ymod,\n",
    "            'cc': ycc,\n",
    "            'dlt1': ydlt1,\n",
    "            'dlt2': ydlt2,\n",
    "        }\n",
    "        return outcome, temporal_outcomes, transitions\n",
    "\n",
    "    losses = []\n",
    "    loss_order = []\n",
    "    transitions = {}\n",
    "    for d1 in [0,1]:\n",
    "        for d2 in [0,1]:\n",
    "            for d3 in [0,1]:\n",
    "                outcomes, tte, transition_entry = get_outcome(d1,d2,d3)\n",
    "                loss = outcome_loss_func(outcomes,weights)\n",
    "                tloss = temporal_loss(tte,tweights)\n",
    "                loss += tloss\n",
    "                losses.append(loss)\n",
    "                loss_order.append([d1,d2,d3])\n",
    "                transitions[str(d1)+str(d2)+str(d3)] = transition_entry\n",
    "    losses = torch.stack(losses,axis=1)\n",
    "    optimal_decisions = [loss_order[i] for i in torch.argmin(losses,axis=1)]\n",
    "    result = torch.tensor(optimal_decisions).type(torch.FloatTensor)\n",
    "    if get_transitions:\n",
    "        opt_transitions = {k: torch.zeros(v.shape).type(torch.FloatTensor) for k,v in transitions['000'].items()}\n",
    "        for i,od in enumerate(optimal_decisions):\n",
    "            key = ''.join([str(o) for o in od])\n",
    "            entry = transitions[key]\n",
    "            for kk,vv in entry.items():\n",
    "                opt_transitions[kk][i,:] = vv[i,:]\n",
    "        return result, opt_transitions\n",
    "    return result\n",
    "\n",
    "test, testtest = get_tt_split()\n",
    "calc_optimal_decisions(DTDataset(),testtest,model1,model2,model3,smodel3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122ee514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([389, 65])\n",
      "tshape torch.Size([389])\n",
      "torch.Size([389]) torch.Size([389])\n",
      "tshape torch.Size([389])\n",
      "torch.Size([389]) torch.Size([389])\n",
      "tshape torch.Size([389])\n",
      "torch.Size([389]) torch.Size([389])\n",
      "tshape torch.Size([389])\n",
      "torch.Size([389]) torch.Size([389])\n",
      "tshape torch.Size([389])\n",
      "torch.Size([389]) torch.Size([389])\n",
      "tshape torch.Size([389])\n",
      "torch.Size([389]) torch.Size([389])\n",
      "tshape torch.Size([389])\n",
      "torch.Size([389]) torch.Size([389])\n",
      "tshape torch.Size([389])\n",
      "torch.Size([389]) torch.Size([389])\n",
      "torch.Size([147, 65])\n",
      "tshape torch.Size([147])\n",
      "torch.Size([147]) torch.Size([147])\n",
      "tshape torch.Size([147])\n",
      "torch.Size([147]) torch.Size([147])\n",
      "tshape torch.Size([147])\n",
      "torch.Size([147]) torch.Size([147])\n",
      "tshape torch.Size([147])\n",
      "torch.Size([147]) torch.Size([147])\n",
      "tshape torch.Size([147])\n",
      "torch.Size([147]) torch.Size([147])\n",
      "tshape torch.Size([147])\n",
      "torch.Size([147]) torch.Size([147])\n",
      "tshape torch.Size([147])\n",
      "torch.Size([147]) torch.Size([147])\n",
      "tshape torch.Size([147])\n",
      "torch.Size([147]) torch.Size([147])\n",
      "torch.Size([3, 536, 91])\n"
     ]
    }
   ],
   "source": [
    "def torch_apply_along_axis(function, x, axis: int = 0):\n",
    "    return torch.stack([\n",
    "        function(x_i) for x_i in torch.unbind(x, dim=axis)\n",
    "    ], dim=axis)\n",
    "\n",
    "def get_unique_sequence(array):\n",
    "    #converts a row of boolean values to a unique number e.g. [1,1,0] => 11, [0,0,1] => 100\n",
    "    uniqueify = lambda r: torch.sum(torch.stack([i*(10**ii) for ii,i in enumerate(r)]))\n",
    "    return torch_apply_along_axis(uniqueify,array)\n",
    "\n",
    "def train_decision_model_triplet(\n",
    "    tmodel1,\n",
    "    tmodel2,\n",
    "    tmodel3,\n",
    "    smodel3,\n",
    "    use_default_split=True,\n",
    "    use_bagging_split=False,\n",
    "    use_attention=True,\n",
    "    lr=.001,\n",
    "    epochs=10000,\n",
    "    patience=5,\n",
    "    weights=[0,.5,.5,0], #realtive weight of survival, feeding tube, aspiration, andl lrc\n",
    "    tweights=[1,1,1,0], #weight for OS, LRC, FDM, and event (any + FT or AS at 6m) as time to event in weeks\n",
    "    opt_weights=[1,1,1], #weights for policy model for optimal decisions\n",
    "    imitation_weights=[.5,1,1],#weights of imitation decisions, because ic overtrains too quickly\n",
    "    imitation_weight=1,\n",
    "    reward_weight=1,\n",
    "    imitation_triplet_weight=2,\n",
    "    reward_triplet_weight = 2,\n",
    "    shufflecol_chance = 0.1,\n",
    "    split=.7,\n",
    "    resample_training=False,\n",
    "    save_path='../data/models/',\n",
    "    file_suffix='',\n",
    "    verbose=True,\n",
    "    use_gpu=False,\n",
    "    **model_kwargs,\n",
    "):\n",
    "    tmodel1.eval()\n",
    "    tmodel2.eval()\n",
    "    tmodel3.eval()\n",
    "\n",
    "    train_ids, test_ids = get_tt_split(use_default_split=use_default_split,use_bagging_split=use_bagging_split,resample_training=resample_training)\n",
    "    true_ids = train_ids + test_ids #for saving memory without upsampling\n",
    "\n",
    "    dataset = DTDataset()\n",
    "    data = dataset.processed_df.copy()\n",
    "    \n",
    "    def get_dlt(state):\n",
    "        if state == 2:\n",
    "            return data[Const.dlt2].copy()\n",
    "        d = data[Const.dlt1].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_pd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.primary_disease_states2].copy()\n",
    "        d = data[Const.primary_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_nd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.nodal_disease_states2].copy()\n",
    "        d = data[Const.nodal_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_cc(state):\n",
    "        res = data[Const.ccs].copy()\n",
    "        if state == 1:\n",
    "            res.values[:,:] = np.zeros(res.values.shape)\n",
    "        return res\n",
    "    \n",
    "    def get_mod(state):\n",
    "        res = data[Const.modifications].copy()\n",
    "        #this should have an ic condition but we don't use it anumore anywa\n",
    "        return res\n",
    "        \n",
    "    outcomedf = data[Const.outcomes]\n",
    "    baseline = dataset.get_state('baseline')\n",
    "    \n",
    "    def formatdf(d,dids=train_ids):\n",
    "        d = df_to_torch(d.loc[dids]).to(model.get_device())\n",
    "        return d\n",
    "    \n",
    "    def makegrad(v):\n",
    "        if not v.requires_grad:\n",
    "            v.requires_grad=True\n",
    "        return v\n",
    "    \n",
    "    if use_attention:\n",
    "        model = DecisionAttentionModel(baseline.shape[1],**model_kwargs)\n",
    "    else:\n",
    "        model_kwargs = {k:v for k,v in model_kwargs.items() if 'attention' not in k and 'embed' not in k}\n",
    "        model = DecisionModel(baseline.shape[1],**model_kwargs)\n",
    "        \n",
    "    device = 'cpu'\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "        \n",
    "    model.set_device(device)\n",
    "\n",
    "    tmodel1.set_device(device)\n",
    "    tmodel2.set_device(device)\n",
    "    tmodel3.set_device(device)\n",
    "    hashcode = str(hash(','.join([str(i) for i in train_ids])))\n",
    "    \n",
    "    save_file = save_path + 'model_' + model.identifier +'_hash' + hashcode + file_suffix + '.tar'\n",
    "    model.fit_normalizer(df_to_torch(baseline.loc[train_ids]))\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "    \n",
    "    \n",
    "    optimal_train,transitions_train = calc_optimal_decisions(dataset,train_ids,tmodel1,tmodel2,tmodel3,smodel3,\n",
    "                                           weights=weights,tweights=tweights,\n",
    "                                          )\n",
    "    optimal_test,transitions_test = calc_optimal_decisions(dataset,test_ids,tmodel1,tmodel2,tmodel3,smodel3,\n",
    "                                          weights=weights,tweights=tweights,\n",
    "                                         )\n",
    "    optimal_train = optimal_train.to(model.get_device())\n",
    "    optimal_test = optimal_test.to(model.get_device())\n",
    "    mse = torch.nn.MSELoss()\n",
    "    bce = torch.nn.BCELoss()\n",
    "    \n",
    "    def compare_decisions(d1,d2,d3,ids):\n",
    "#         ypred = np.concatenate([dd.cpu().detach().numpy().reshape(-1,1) for dd in [d1,d2,d3]],axis=1)\n",
    "        ytrue = df_to_torch(outcomedf.loc[ids])\n",
    "        dloss = bce(d1.view(-1),ytrue[:,0])\n",
    "        dloss += bce(d2.view(-1),ytrue[:,1])\n",
    "        dloss += bce(d3,view(-1),ytrue[:,2])\n",
    "        return dloss\n",
    "        \n",
    "    def remove_decisions(df):\n",
    "        cols = [c for c in df.columns if c not in Const.decisions ]\n",
    "        ddf = df[cols]\n",
    "        return ddf\n",
    "    \n",
    "    makeinput = lambda step,dids: df_to_torch(remove_decisions(dataset.get_input_state(step=step,ids=dids)))\n",
    "    threshold = lambda x: torch.gt(x,torch.rand(x.shape[0])).type(torch.FloatTensor)\n",
    "\n",
    "    randchoice = lambda x: x[torch.randint(len(x),(1,))[0]]\n",
    "    tloss_func = torch.nn.TripletMarginLoss()\n",
    "    def get_tloss(row,step,yt,x,imitation=True):\n",
    "        if yt[:,step].std() < .001:\n",
    "            return torch.tensor([0]).to(device)\n",
    "        positive_idx= torch.nonzero(yt[:,step] == yt[row,step])\n",
    "        if len(positive_idx) <= 1:\n",
    "            print('no losses','n positive',len(positive_idx),'yt',yt[row,step],'row',row,'step',step,'imitation',imitation,end='\\r')\n",
    "            return torch.tensor([0]).to(device)\n",
    "        positive_idx = torch.stack([ii for ii in positive_idx if ii != row]).view(-1)\n",
    "        negative_idx = torch.tensor([ii for ii in range(x.shape[0]) if ii not in positive_idx and ii != row])\n",
    "        if len(positive_idx) < 1 or len(negative_idx) < 1:\n",
    "            print('no losses','n positive',len(positive_idx),'n negative',len(negative_idx),'yt',yt[row,step],'row',row,'step',step,'imitation',imitation,end='\\r')\n",
    "            return torch.tensor([0]).to(device)\n",
    "        positive = x[randchoice(positive_idx)]\n",
    "        negative = x[randchoice(negative_idx)]\n",
    "        anchor = x[row]\n",
    "        if use_attention:\n",
    "            [anchor_embedding,pos_embedding,neg_embedding] = [model.get_embedding(xx.view(1,-1),position=step,use_saved_memory=True) for xx in [anchor,positive,negative]]\n",
    "        else:    \n",
    "            [anchor_embedding,pos_embedding,neg_embedding] = [model.get_embedding(xx.view(1,-1),position=step,concatenate=False)[int(imitation)] for xx in [anchor,positive,negative]]\n",
    "        tloss = tloss_func(anchor_embedding,pos_embedding,neg_embedding)\n",
    "        return tloss\n",
    "    #save the inputs from the whole dataset for future reference\n",
    "    if use_attention:\n",
    "        full_data = []\n",
    "        for mstep in [0,1,2]:\n",
    "            full_data_step = [baseline, get_dlt(min(mstep,1)),\n",
    "                         get_dlt(mstep),get_pd(mstep),get_nd(mstep),get_cc(mstep),get_mod(mstep)]\n",
    "            full_data_step = torch.cat([formatdf(fd,true_ids) for fd in full_data_step],axis=1)\n",
    "            full_data.append(full_data_step)\n",
    "        full_data = torch.stack(full_data)\n",
    "        model.save_memory(full_data)\n",
    "        print(full_data.shape)\n",
    "        \n",
    "    def step(train=True):\n",
    "        if train:\n",
    "            model.train(True)\n",
    "            tmodel1.train(True)\n",
    "            tmodel2.train(True)\n",
    "            tmodel3.train(True)\n",
    "            optimizer.zero_grad()\n",
    "            ids = train_ids\n",
    "            y_opt = makegrad(optimal_train)\n",
    "            transition_dict = {k: torch.clone(v).detach() for k,v in transitions_train.items()}\n",
    "        else:\n",
    "            ids = test_ids\n",
    "            model.eval()\n",
    "            tmodel1.eval()\n",
    "            tmodel2.eval()\n",
    "            tmodel3.eval()\n",
    "            y_opt = makegrad(optimal_test)\n",
    "            print(y_opt.mean(axis=0))\n",
    "            transition_dict = {k: torch.clone(v).detach() for k,v in transitions_test.items()}\n",
    "        model.set_device(device)\n",
    "        ytrain = df_to_torch(outcomedf.loc[ids]).to(device)\n",
    "        #imitation losses and decision 1\n",
    "        xxtrained = [baseline, get_dlt(0),get_dlt(0),get_pd(0),get_nd(0),get_cc(0),get_mod(0)]\n",
    "        xxtrain = [formatdf(xx,ids) for xx in xxtrained]\n",
    "        xxtrain = torch.cat(xxtrain,axis=1).to(device)\n",
    "        o1 = model(xxtrain,position=0,use_saved_memory= (not train))\n",
    "        decision1_imitation = o1[:,3]\n",
    "        decision1_opt = o1[:,0]\n",
    "    \n",
    "        imitation_loss1 = bce(decision1_imitation,ytrain[:,0])\n",
    "        imitation_loss1 = torch.mul(imitation_loss1,imitation_weights[0])\n",
    "        opt_loss1 = bce(decision1_opt,y_opt[:,0])\n",
    "        opt_loss1 = torch.mul(opt_loss1,opt_weights[0])\n",
    "        \n",
    "        x1_imitation = [baseline, get_dlt(1),get_dlt(0),get_pd(1),get_nd(1),get_cc(1),get_mod(1)]\n",
    "        x1_imitation = [formatdf(xx1,ids) for xx1 in x1_imitation]\n",
    "        x1_imitation = torch.cat(x1_imitation,axis=1).to(device)\n",
    "        \n",
    "        o2 = model(x1_imitation,position=1,use_saved_memory= (not train))\n",
    "            \n",
    "        decision2_imitation = o2[:,4]\n",
    "            \n",
    "        imitation_loss2 =  bce(decision2_imitation,ytrain[:,1])\n",
    "        imitation_loss2 = torch.mul(imitation_loss2,imitation_weights[1])\n",
    "        \n",
    "        \n",
    "        x2_imitation = [baseline, get_dlt(1),get_dlt(2),get_pd(2),get_nd(2),get_cc(2),get_mod(2)]\n",
    "        \n",
    "        x2_imitation = [formatdf(xx2,ids) for xx2 in x2_imitation]\n",
    "        x2_imitation = torch.cat(x2_imitation,axis=1).to(device)\n",
    "        \n",
    "        \n",
    "        o3 = model(x2_imitation,position=2,use_saved_memory= (not train))\n",
    "        \n",
    "        decision3_imitation = o3[:,5]\n",
    "        \n",
    "        imitation_loss3 = bce(decision3_imitation,ytrain[:,2])\n",
    "        imitation_loss3 = torch.mul(imitation_loss3,imitation_weights[2])\n",
    "        \n",
    "        opt_input2 = [\n",
    "            formatdf(baseline,ids), \n",
    "            transition_dict['dlt1'],\n",
    "            formatdf(get_dlt(0),ids),\n",
    "            transition_dict['pd1'],\n",
    "            transition_dict['nd1'], \n",
    "            formatdf(get_cc(0),ids),\n",
    "            transition_dict['mod']\n",
    "                 ]\n",
    "        opt_input2 = [o.to(device) for o in opt_input2]\n",
    "\n",
    "        opt_input2 = torch.cat(opt_input2,axis=1).to(device)\n",
    "        decision2_opt = model(opt_input2,position=1,use_saved_memory= (not train))[:,1]\n",
    "        \n",
    "        opt_loss2 = bce(decision2_opt,y_opt[:,1])\n",
    "        opt_loss2 = torch.mul(opt_loss2,opt_weights[1])\n",
    "        \n",
    "        opt_input3 = [\n",
    "            formatdf(baseline,ids),\n",
    "            transition_dict['dlt1'],\n",
    "            transition_dict['dlt2'],\n",
    "            transition_dict['pd2'],\n",
    "            transition_dict['nd2'],\n",
    "            transition_dict['cc'],\n",
    "            transition_dict['mod'],\n",
    "        ]\n",
    "        opt_input3 = [o.to(device) for o in opt_input3]\n",
    "        opt_input3 = torch.cat(opt_input3,axis=1).to(device)\n",
    "        decision3_opt = model(opt_input3,position=2,use_saved_memory= (not train))[:,2]\n",
    "        \n",
    "        opt_loss3 = bce(decision3_opt,y_opt[:,2])\n",
    "        opt_loss3 = torch.mul(opt_loss3,opt_weights[2])\n",
    "        \n",
    "        iloss = torch.add(torch.add(imitation_loss1,imitation_loss2),imitation_loss3)\n",
    "        iloss = torch.mul(iloss,imitation_weight)\n",
    "        \n",
    "        reward_loss = torch.add(torch.add(opt_loss1,opt_loss2),opt_loss3)\n",
    "        reward_loss =torch.mul(reward_loss,reward_weight)\n",
    "        \n",
    "        loss = torch.add(iloss,reward_loss)\n",
    "        \n",
    "        imitation_tloss = torch.FloatTensor([0]).to(device)\n",
    "        opt_tloss = torch.FloatTensor([0]).to(device)\n",
    "        n_rows = xxtrain.shape[0]\n",
    "        if reward_triplet_weight + imitation_triplet_weight > 0.0001:\n",
    "            for i in range(n_rows):\n",
    "                \n",
    "                if imitation_triplet_weight > .0001:\n",
    "                    imitation_tloss += get_tloss(i,0,ytrain,xxtrain,True)\n",
    "                    imitation_tloss += get_tloss(i,1,ytrain,x1_imitation,True)\n",
    "                    imitation_tloss += get_tloss(i,2,ytrain,x2_imitation,True)\n",
    "                if reward_triplet_weight > .0001:\n",
    "                    opt_tloss += get_tloss(i,0,y_opt,xxtrain,False)\n",
    "                    opt_tloss += get_tloss(i,1,y_opt,opt_input2,False)\n",
    "                    opt_tloss += get_tloss(i,2,y_opt,opt_input3,False)\n",
    "            loss += torch.mul(imitation_tloss[0],imitation_triplet_weight/n_rows)\n",
    "            loss += torch.mul(opt_tloss[0],reward_triplet_weight/n_rows)\n",
    "        losses = [iloss,reward_loss,imitation_tloss*imitation_triplet_weight/n_rows,opt_tloss*reward_triplet_weight/n_rows]\n",
    "        \n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            return losses\n",
    "        else:\n",
    "            scores = []\n",
    "            distributions = [decision1_opt.mean().item(),decision2_opt.mean().item(),decision3_opt.mean().item()]\n",
    "            imitation = [decision1_imitation,decision2_imitation,decision3_imitation]\n",
    "            optimal = [decision1_opt,decision2_opt,decision3_opt]\n",
    "            for i,decision_im in enumerate(imitation):\n",
    "                deci = decision_im.cpu().detach().numpy()\n",
    "                deci0 = (deci > .5).astype(int)\n",
    "                iout = ytrain[:,i].cpu().detach().numpy()\n",
    "                acci = accuracy_score(iout,deci0)\n",
    "                try:\n",
    "                    auci = roc_auc_score(iout,deci)\n",
    "                except:\n",
    "                    auci = -1\n",
    "                \n",
    "                deco = optimal[i].cpu().detach().numpy()\n",
    "                deci0 = (deco > .5).astype(int)\n",
    "                oout = y_opt[:,i].cpu().detach().numpy()\n",
    "                acco = accuracy_score(oout,deci0)\n",
    "                try:\n",
    "                    auco = roc_auc_score(oout,deco)\n",
    "                except:\n",
    "                    auco=-1\n",
    "                scores.append({'decision': i,'optimal_auc': auco,'imitation_auc': auci,'optimal_acc': acco,'imitation_acc': acci})\n",
    "            return losses, scores, distributions\n",
    "        \n",
    "    best_val_loss = torch.tensor(1000000000.0)\n",
    "    steps_since_improvement = 0\n",
    "    best_val_score = {}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        losses = step(True)\n",
    "        val_losses,val_metrics,val_distributions = step(False)\n",
    "        vl = val_losses[0] + val_losses[1]\n",
    "        for vm in val_metrics:\n",
    "            vl += (-((vm['optimal_auc']*reward_weight) + (vm['imitation_auc']*imitation_weight)))/10\n",
    "        if verbose:\n",
    "            print('______epoch',str(epoch),'_____')\n",
    "            print('val reward',val_losses[1].item())\n",
    "            print('imitation reward', val_losses[0].item())\n",
    "            print('distance losses',val_losses[2].item(),val_losses[-1].item())\n",
    "            print('distributions',val_distributions)\n",
    "            print(val_metrics)\n",
    "        if vl < best_val_loss:\n",
    "            best_val_loss = vl\n",
    "            best_val_score = val_metrics\n",
    "            best_val_distributions = val_distributions\n",
    "            steps_since_improvement = 0\n",
    "            torch.save(model.state_dict(),save_file)\n",
    "        else:\n",
    "            steps_since_improvement += 1\n",
    "        if steps_since_improvement > patience:\n",
    "            break\n",
    "    print('++++++++++Final+++++++++++')\n",
    "    print('best',best_val_loss)\n",
    "    print(best_val_score)\n",
    "    model.load_state_dict(torch.load(save_file))\n",
    "    model.eval()\n",
    "    return model, best_val_score, best_val_loss, best_val_distributions\n",
    "\n",
    "from Models import *\n",
    "args = {\n",
    "    'hidden_layers': [500], \n",
    "    'opt_layer_size': 20, \n",
    "    'imitation_layer_size': 20, \n",
    "    'dropout': 0.25, \n",
    "    'input_dropout': 0.25, \n",
    "    'shufflecol_chance': 0.5\n",
    "}\n",
    "\n",
    "#1.8424\n",
    "decision_model, decision_score, decision_loss, _ = train_decision_model_triplet(\n",
    "    model1,model2,model3,smodel3,\n",
    "    lr=.01,\n",
    "    imitation_weight=1,\n",
    "    reward_weight=1,\n",
    "    patience=10,\n",
    "    imitation_triplet_weight=1,\n",
    "    reward_triplet_weight =1,\n",
    "    verbose=True,\n",
    "    use_attention=True,\n",
    "    **args)\n",
    "decision_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd8f70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils import *\n",
    "from Models import *\n",
    "[t1_alt, t2_alt, t3_alt] = load_sklearn_transition_models()\n",
    "decision_model_alt, decision_score_alt, decision_loss_alt, _ = train_decision_model_triplet(\n",
    "    t1_alt,t2_alt,t3_alt,\n",
    "    lr=.001,\n",
    "    imitation_weight=1,\n",
    "    reward_weight=1,\n",
    "    patience=10,\n",
    "    imitation_triplet_weight=1,\n",
    "    reward_triplet_weight =1,\n",
    "    verbose=True,\n",
    "    use_attention=True,\n",
    "    **args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7364bc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array([1,3,54,3.2,23,4])\n",
    "np.isnan(test.astype(np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a7faad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decision_model(\n",
    "    tmodel1,\n",
    "    tmodel2,\n",
    "    tmodel3,\n",
    "    use_default_split=True,\n",
    "    use_bagging_split=False,\n",
    "    lr=.0001,\n",
    "    epochs=10000,\n",
    "    patience=50,\n",
    "    weights=[-1,1,1,-1], #realtive weight of survival, feeding tube, aspiration, andl lrc\n",
    "    imitation_weights=[.5,1,1],#weights of imitation decisions, because ic overtrains too quickly\n",
    "    imitation_weight=0.1,\n",
    "    shufflecol_chance = 0.1,\n",
    "    reward_weight=1,\n",
    "    imitation_triplet_weight=0,\n",
    "    reward_triplet_weight = 0,\n",
    "    split=.7,\n",
    "    resample_training=False,\n",
    "    save_path='../data/models/',\n",
    "    file_suffix='',\n",
    "    use_gpu=True,\n",
    "    use_attention=True,\n",
    "    verbose=True,\n",
    "    threshold_decisions=True,#convert decisiosn to binary in simulation, usually breaks it\n",
    "    use_smote=False,\n",
    "    validate_with_memory=True,\n",
    "    **model_kwargs):\n",
    "    \n",
    "    tmodel1.eval()\n",
    "    tmodel2.eval()\n",
    "    tmodel3.eval()\n",
    "\n",
    "    train_ids, test_ids = get_tt_split(use_default_split=use_default_split,use_bagging_split=use_bagging_split,resample_training=resample_training)\n",
    "    true_ids = train_ids + test_ids #for saving memory without upsampling\n",
    "    if use_smote:\n",
    "        dataset = DTDataset(use_smote=True,smote_ids = train_ids)\n",
    "        train_ids = [i for i in dataset.processed_df.index.values if i not in test_ids]\n",
    "    else:\n",
    "        dataset = DTDataset()\n",
    "    data = dataset.processed_df.copy()\n",
    "    \n",
    "    def get_dlt(state):\n",
    "        if state == 2:\n",
    "            return data[Const.dlt2].copy()\n",
    "        d = data[Const.dlt1].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_pd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.primary_disease_states2].copy()\n",
    "        d = data[Const.primary_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_nd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.nodal_disease_states2].copy()\n",
    "        d = data[Const.nodal_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_cc(state):\n",
    "        res = data[Const.ccs].copy()\n",
    "        if state == 1:\n",
    "            res.values[:,:] = np.zeros(res.values.shape)\n",
    "        return res\n",
    "    \n",
    "    def get_mod(state):\n",
    "        res = data[Const.modifications].copy()\n",
    "        #this should have an ic condition but we don't use it anumore anywa\n",
    "        return res\n",
    "        \n",
    "    outcomedf = data[Const.outcomes]\n",
    "    baseline = dataset.get_state('baseline')\n",
    "    \n",
    "    def formatdf(d,dids=train_ids):\n",
    "        d = df_to_torch(d.loc[dids]).to(model.get_device())\n",
    "        return d\n",
    "    \n",
    "    def makegrad(v):\n",
    "        if not v.requires_grad:\n",
    "            v.requires_grad=True\n",
    "        return v\n",
    "    \n",
    "    if use_attention:\n",
    "        model = DecisionAttentionModel(baseline.shape[1],**model_kwargs)\n",
    "    else:\n",
    "        model_kwargs = {k:v for k,v in model_kwargs.items() if 'attention' not in k and 'embed' not in k}\n",
    "        model = DecisionModel(baseline.shape[1],**model_kwargs)\n",
    "\n",
    "    device = 'cpu'\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "        \n",
    "    model.set_device(device)\n",
    "\n",
    "    tmodel1.set_device(device)\n",
    "    tmodel2.set_device(device)\n",
    "    tmodel3.set_device(device)\n",
    "    \n",
    "    hashcode = str(hash(','.join([str(i) for i in train_ids])))\n",
    "    \n",
    "    save_file = save_path + 'model_' + model.identifier +'_hash' + hashcode + file_suffix + '.tar'\n",
    "    model.fit_normalizer(df_to_torch(baseline.loc[train_ids]).to(model.get_device()))\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "\n",
    "    def outcome_loss(ypred):\n",
    "        l = torch.mul(ypred[:,0],weights[0])\n",
    "        for i,weight in enumerate(weights[1:]):\n",
    "            #weights with negative values will invert the outcome so e.g. Regional control becomes no regional control\n",
    "            #so the penaly is correct\n",
    "            newloss = torch.mul(ypred[:,i],weight)\n",
    "            l = torch.add(l,newloss)\n",
    "        return l\n",
    "    \n",
    "    mse = torch.nn.MSELoss()\n",
    "    bce = torch.nn.BCELoss()\n",
    "    device = model.get_device()\n",
    "    def compare_decisions(d1,d2,d3,ids):\n",
    "#         ypred = np.concatenate([dd.cpu().detach().numpy().reshape(-1,1) for dd in [d1,d2,d3]],axis=1)\n",
    "        ytrue = df_to_torch(outcomedf.loc[ids])\n",
    "        dloss = bce(d1.view(-1),ytrue[:,0])\n",
    "        dloss += bce(d2.view(-1),ytrue[:,1])\n",
    "        dloss += bce(d3,view(-1),ytrue[:,2])\n",
    "        return dloss\n",
    "        \n",
    "    def remove_decisions(df):\n",
    "        cols = [c for c in df.columns if c not in Const.decisions ]\n",
    "        ddf = df[cols]\n",
    "        return ddf\n",
    "    \n",
    "    makeinput = lambda step,dids: df_to_torch(remove_decisions(dataset.get_input_state(step=step,ids=dids))).to(device)\n",
    "    thresh = lambda x: torch.sigmoid(100000000*(x - .5))\n",
    "\n",
    "    optimal_train,transitions_train = calc_optimal_decisions(dataset,train_ids,tmodel1,tmodel2,tmodel3,\n",
    "                                                weights=weights,\n",
    "                                                outcome_loss_func = outcome_loss,\n",
    "                                          )\n",
    "    optimal_test,transitions_test = calc_optimal_decisions(dataset,test_ids,tmodel1,tmodel2,tmodel3,\n",
    "                                                           weights=weights,\n",
    "                                                           outcome_loss_func = outcome_loss,\n",
    "                                         )\n",
    "    optimal_train = optimal_train.to(model.get_device())\n",
    "    optimal_test = optimal_test.to(model.get_device())\n",
    "    \n",
    "    #save the inputs from the whole dataset for future reference\n",
    "    if use_attention:\n",
    "        full_data = []\n",
    "        for mstep in [0,1,2]:\n",
    "            full_data_step = [baseline, get_dlt(min(mstep,1)),\n",
    "                         get_dlt(mstep),get_pd(mstep),get_nd(mstep),get_cc(mstep),get_mod(mstep)]\n",
    "            full_data_step = torch.cat([formatdf(fd,true_ids) for fd in full_data_step],axis=1)\n",
    "            full_data.append(full_data_step)\n",
    "        full_data = torch.stack(full_data)\n",
    "        model.save_memory(full_data.to(device))\n",
    "        print(full_data.shape)\n",
    "        \n",
    "    randchoice = lambda x: x[torch.randint(len(x),(1,))[0]]\n",
    "    tloss_func = torch.nn.TripletMarginLoss()\n",
    "    def get_tloss(row,step,yt,x,imitation=True):\n",
    "        if yt[:,step].std() < .001:\n",
    "            return torch.tensor([0]).to(device)\n",
    "        positive_idx= torch.nonzero(yt[:,step] == yt[row,step])\n",
    "        positive_idx = torch.stack([ii for ii in positive_idx if ii != row]).view(-1)\n",
    "        negative_idx = torch.tensor([ii for ii in range(x.shape[0]) if ii not in positive_idx and ii != row])\n",
    "        if len(positive_idx) < 1 or len(negative_idx) < 1:\n",
    "            print('no losses','n positive',len(positive_idx),'n negative',len(negative_idx),'yt',yt[row,step],'row',row,'step',step,'imitation',imitation,end='\\r')\n",
    "            return torch.tensor([0]).to(device)\n",
    "        positive = x[randchoice(positive_idx)]\n",
    "        negative = x[randchoice(negative_idx)]\n",
    "        anchor = x[row]\n",
    "        if use_attention:\n",
    "            [anchor_embedding,pos_embedding,neg_embedding] = [model.get_embedding(xx.view(1,-1),position=step,use_saved_memory=True) for xx in [anchor,positive,negative]]\n",
    "        else:    \n",
    "            [anchor_embedding,pos_embedding,neg_embedding] = [model.get_embedding(xx.view(1,-1),position=step,concatenate=False)[int(imitation)] for xx in [anchor,positive,negative]]\n",
    "        tloss = tloss_func(anchor_embedding,pos_embedding,neg_embedding)\n",
    "        return tloss\n",
    "    \n",
    "    def step(train=True):\n",
    "        if train:\n",
    "            model.train(True)\n",
    "            tmodel1.train(True)\n",
    "            tmodel2.train(True)\n",
    "            tmodel3.train(True)\n",
    "            optimizer.zero_grad()\n",
    "            ids = train_ids\n",
    "            y_opt = makegrad(optimal_train)\n",
    "        else:\n",
    "            ids = test_ids\n",
    "            model.eval()\n",
    "            tmodel1.eval()\n",
    "            tmodel2.eval()\n",
    "            tmodel3.eval()\n",
    "            y_opt = makegrad(optimal_test)\n",
    "            \n",
    "            \n",
    "        ytrain = df_to_torch(outcomedf.loc[ids]).to(device)\n",
    "        #imitation losses and decision 1\n",
    "        xxtrained = [baseline, get_dlt(0),get_dlt(0),get_pd(0),get_nd(0),get_cc(0),get_mod(0)]\n",
    "        xxtrain = torch.cat([formatdf(xx,ids) for xx in xxtrained],axis=1).to(device)\n",
    "        \n",
    "        use_memory = (not train) and validate_with_memory\n",
    "\n",
    "        o1 = model(xxtrain,position=0,use_saved_memory = use_memory)\n",
    "\n",
    "        decision1_imitation = o1[:,3]\n",
    "        \n",
    "        decision1_opt = o1[:,0]\n",
    "        if threshold_decisions:\n",
    "            decision1_opt = thresh(decision1_opt)\n",
    "\n",
    "        imitation_loss1 = bce(decision1_imitation,ytrain[:,0])\n",
    "        imitation_loss1 = torch.mul(imitation_loss1,imitation_weights[0])\n",
    "        \n",
    "        x1_imitation = [baseline, get_dlt(1),get_dlt(0),get_pd(1),get_nd(1),get_cc(1),get_mod(1)]\n",
    "        x1_imitation = [formatdf(xx1,ids) for xx1 in x1_imitation]\n",
    "        x1_imitation = torch.cat(x1_imitation,axis=1).to(device)\n",
    "        decision2_imitation = model(x1_imitation,position=1,use_saved_memory = use_memory)[:,4]\n",
    "\n",
    "        imitation_loss2 =  bce(decision2_imitation,ytrain[:,1])\n",
    "        imitation_loss2 = torch.mul(imitation_loss2,imitation_weights[1])\n",
    "        \n",
    "        x2_imitation = [baseline, get_dlt(1),get_dlt(2),get_pd(2),get_nd(2),get_cc(2),get_mod(2)]\n",
    "        \n",
    "        x2_imitation = [formatdf(xx2,ids) for xx2 in x2_imitation]\n",
    "        x2_imitation = torch.cat(x2_imitation,axis=1)\n",
    "        decision3_imitation = model(x2_imitation,position=2,use_saved_memory = use_memory)[:,5]\n",
    "\n",
    "        imitation_loss3 = bce(decision3_imitation,ytrain[:,2])\n",
    "        imitation_loss3 = torch.mul(imitation_loss3,imitation_weights[2])\n",
    "        \n",
    "        #reward decisions\n",
    "        xx1 = makeinput(1,ids)\n",
    "        xx2 = makeinput(2,ids)\n",
    "        xx3 = makeinput(3,ids)\n",
    "\n",
    "        xx1 = makegrad(xx1)\n",
    "        xx2 = makegrad(xx2)\n",
    "        xx3 = makegrad(xx3)\n",
    "        baseline_train_base = formatdf(baseline,ids)\n",
    "            \n",
    "        baseline_train = torch.clone(baseline_train_base)\n",
    "\n",
    "        \n",
    "        xi1 = torch.cat([xx1,decision1_opt.view(-1,1)],axis=1)\n",
    "        print(train,tmodel1.training,tmodel1.dropout.training)\n",
    "        [ypd1, ynd1, ymod, ydlt1] = tmodel1(xi1)['predictions']\n",
    "        print(train,tmodel1.training,tmodel1.dropout.training)\n",
    "        d1_thresh = torch.gt(decision1_opt.view(-1,1),.5).to(ypd1.device)\n",
    "        d1_scale = torch.cat([d1_thresh,d1_thresh,torch.ones(d1_thresh.view(-1,1).shape).to(ypd1.device)],dim=1)\n",
    "        ypd1= torch.mul(ypd1,d1_scale)\n",
    "        ynd1= torch.mul(ynd1,d1_scale)\n",
    "        \n",
    "        x1 = [baseline_train,ydlt1,formatdf(get_dlt(0),ids),ypd1,ynd1,formatdf(get_cc(1),ids),ymod]\n",
    "        x1= torch.cat([xx1.to(model.get_device()) for xx1 in x1],axis=1)\n",
    "        \n",
    "        decision2_opt = model(x1,position=1,use_saved_memory = use_memory)[:,1] \n",
    "        if threshold_decisions:\n",
    "            decision2_opt = thresh(decision2_opt)\n",
    "            \n",
    "        xi2 = torch.cat([xx2,decision1_opt.view(-1,1),decision2_opt.view(-1,1)],axis=1)\n",
    "        [ypd2,ynd2,ycc,ydlt2] = tmodel2(xi2)['predictions']\n",
    "\n",
    "        x2 = [baseline_train,ydlt1,ydlt2,ypd2,ynd2,ycc,ymod]\n",
    "        x2 = torch.cat([xx2.to(model.get_device()) for xx2 in x2],axis=1)\n",
    "        decision3_opt = model(x2,position=2,use_saved_memory = use_memory)[:,2]\n",
    "        \n",
    "        if threshold_decisions:\n",
    "            decision3_opt = thresh(decision3_opt)\n",
    "            \n",
    "        xi3 = torch.cat([xx3,decision1_opt.view(-1,1),decision2_opt.view(-1,1),decision3_opt.view(-1,1)],axis=1)\n",
    "        \n",
    "        outcomes = tmodel3(xi3)['predictions']\n",
    "\n",
    "        if not train and verbose:\n",
    "            print(torch.mean(outcomes,dim=0))\n",
    "            \n",
    "        reward_loss = torch.mean(outcome_loss(outcomes))\n",
    "        loss = torch.add(imitation_loss1,imitation_loss2)\n",
    "        loss = torch.add(loss,imitation_loss3)\n",
    "        loss = torch.mul(loss,imitation_weight/3)\n",
    "        loss = torch.add(loss,torch.mul(reward_loss,reward_weight))\n",
    "        \n",
    "        imitation_tloss = torch.FloatTensor([0]).to(device)\n",
    "        opt_tloss = torch.FloatTensor([0]).to(device)\n",
    "        n_rows = x1.shape[0]\n",
    "        if reward_triplet_weight + imitation_triplet_weight > 0.0001:\n",
    "            for i in range(n_rows):\n",
    "                #skip if we're using an attention model idk\n",
    "                if not use_attention and imitation_triplet_weight > .0001:\n",
    "                    imitation_tloss += get_tloss(i,0,ytrain,xxtrain,True)\n",
    "                    imitation_tloss += get_tloss(i,1,ytrain,x1_imitation,True)\n",
    "                    imitation_tloss += get_tloss(i,2,ytrain,x2_imitation,True)\n",
    "                if reward_triplet_weight > .0001:\n",
    "                    opt_tloss += get_tloss(i,0,y_opt,xxtrain,False)\n",
    "                    opt_tloss += get_tloss(i,1,y_opt,x1,False)\n",
    "                    opt_tloss += get_tloss(i,2,y_opt,x2,False)\n",
    "            loss += torch.mul(imitation_tloss[0],imitation_triplet_weight/n_rows)\n",
    "            loss += torch.mul(opt_tloss[0],reward_triplet_weight/n_rows)\n",
    "        \n",
    "        losses = [imitation_loss1+imitation_loss2+imitation_loss3,reward_loss,imitation_tloss,opt_tloss]\n",
    "\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            return losses\n",
    "        else:\n",
    "            scores = []\n",
    "            distributions = [decision1_opt.mean().item(),decision2_opt.mean().item(),decision3_opt.mean().item()]\n",
    "            imitation = [decision1_imitation,decision2_imitation,decision3_imitation]\n",
    "            optimal = [decision1_opt,decision2_opt,decision3_opt]\n",
    "            for i,decision_im in enumerate(imitation):\n",
    "                deci = decision_im.cpu().detach().numpy()\n",
    "                deci0 = (deci > .5).astype(int)\n",
    "                iout = ytrain[:,i].cpu().detach().numpy()\n",
    "                acci = accuracy_score(iout,deci0)\n",
    "                try:\n",
    "                    auci = roc_auc_score(iout,deci)\n",
    "                except:\n",
    "                    auci = -1\n",
    "                \n",
    "                deco = optimal[i].cpu().detach().numpy()\n",
    "                deci0 = (deco > .5).astype(int)\n",
    "                oout = y_opt[:,i].cpu().detach().numpy()\n",
    "                acco = accuracy_score(oout,deci0)\n",
    "                try:\n",
    "                    auco = roc_auc_score(oout,deco)\n",
    "                except:\n",
    "                    auco=-1\n",
    "                scores.append({'decision': i,'optimal_auc': auco,'imitation_auc': auci,'optimal_acc': acco,'imitation_acc': acci})\n",
    "            return losses, scores, distributions\n",
    "        \n",
    "    best_val_loss = torch.tensor(1000000000.0)\n",
    "    steps_since_improvement = 0\n",
    "    best_val_score = {}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        losses = step(True)\n",
    "        val_losses,val_metrics,val_distributions = step(False)\n",
    "        vl = val_losses[0] + val_losses[1]\n",
    "        for vm in val_metrics:\n",
    "            vl += (-((vm['optimal_auc']*reward_weight) + (vm['imitation_auc']*imitation_weight)))/10\n",
    "        if verbose:\n",
    "            print('______epoch',str(epoch),'_____')\n",
    "            print('val reward',val_losses[1].item())\n",
    "            print('imitation reward', val_losses[0].item())\n",
    "            if len(val_losses) > 2:\n",
    "                print('distance losses',val_losses[2].item(),val_losses[-1].item())\n",
    "            print('distributions',val_distributions)\n",
    "            print(val_metrics)\n",
    "        if vl < best_val_loss:\n",
    "            best_val_loss = vl\n",
    "            best_val_score = val_metrics\n",
    "            best_val_distributions = val_distributions\n",
    "            steps_since_improvement = 0\n",
    "            torch.save(model.state_dict(),save_file)\n",
    "        else:\n",
    "            steps_since_improvement += 1\n",
    "        if steps_since_improvement > patience:\n",
    "            break\n",
    "    print('++++++++++Final+++++++++++')\n",
    "    print('best',best_val_loss)\n",
    "    print(best_val_score)\n",
    "    model.load_state_dict(torch.load(save_file))\n",
    "    model.eval()\n",
    "    return model, best_val_score, best_val_loss, best_val_distributions\n",
    "\n",
    "from Models import *\n",
    "# args = {\n",
    "#     'hidden_layers': [50,50], \n",
    "#     'attention_heads': [2,2],\n",
    "#     'embed_size': 120, \n",
    "#     'dropout': 0.5, \n",
    "#     'input_dropout': 0.2, \n",
    "#     'shufflecol_chance':  0.2,\n",
    "# }\n",
    "args = {\n",
    "    'hidden_layers': [500], \n",
    "    'opt_layer_size': 20, \n",
    "    'imitation_layer_size': 20, \n",
    "    'dropout': 0.25, \n",
    "    'input_dropout': 0.25, \n",
    "    'shufflecol_chance': 0.5\n",
    "}\n",
    "# from Models import *\n",
    "# decision_model, _, _, _ = train_decision_model(\n",
    "#     tmodel1[0],tmodel2[0],tmodel3_smote[0],\n",
    "#     lr=.001,\n",
    "#     use_attention=True,\n",
    "#     imitation_weight=1,\n",
    "#     imitation_triplet_weight=0,\n",
    "#     reward_triplet_weight=0,\n",
    "#     reward_weight=2,\n",
    "#     validate_with_memory=True,\n",
    "#     use_smote=False,\n",
    "#     **args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf95052",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sigmoid(1000000000000*(torch.tensor([.4]) - .5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062ec356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch_triplet_model(m1,m2,m3,weights=[-1,1,1,-1]):\n",
    "    hidden_layers = [[50],[100],[200],[500],[1000]]\n",
    "    extra_layers = [20,50,100,200]\n",
    "    best_loss = 100000000000\n",
    "    best_metrics = {}\n",
    "    best_args = {}\n",
    "    best_model = None\n",
    "    best_record = {}\n",
    "    k = 0\n",
    "    records = []\n",
    "    for hl in hidden_layers:\n",
    "        for el in extra_layers:\n",
    "            args = {'hidden_layers': hl, 'opt_layer_size': el, 'imitation_layer_size': el}\n",
    "            for dropout in [.1,.25,.5,.75]:\n",
    "                args['dropout'] = dropout\n",
    "                for input_dropout in [0.1,.25,.5]:\n",
    "                    args['input_dropout'] = input_dropout\n",
    "                    for shufflecol_chance in [.1,.5]:\n",
    "                        try:\n",
    "                            args['shufflecol_chance'] = shufflecol_chance\n",
    "                            model,m_metrics,m_loss,m_distribution = train_decision_model_triplet(m1,m2,m3,\n",
    "                                                                        lr=.01,\n",
    "                                                                        weights=weights,\n",
    "                                                                        verbose=False,\n",
    "                                                                        **args)\n",
    "                            entry = {\n",
    "                                'metrics': m_metrics,\n",
    "                                'loss': m_loss,\n",
    "                                'args': args,\n",
    "                                'distribution': m_distribution,\n",
    "                            }\n",
    "                            records.append(entry)\n",
    "                            print('done',k,m_loss)\n",
    "                            print('curr best',best_loss)\n",
    "                            k+=1\n",
    "                            if m_loss < best_loss:\n",
    "                                best_loss = m_loss\n",
    "                                best_metrics  = m_metrics\n",
    "                                best_model = model\n",
    "                                best_args = args\n",
    "                                best_record = entry\n",
    "                                print('_++++++++++New Best++++____')\n",
    "                                print(best_loss)\n",
    "                                print(best_metrics)\n",
    "                                print(best_args)\n",
    "                                print('___________')\n",
    "                                print('++++++++')\n",
    "                                print()\n",
    "                        except Exception as e:\n",
    "                            print('error',e,args)\n",
    "    print('_________')\n",
    "    print('+++++++++++')\n",
    "    print('best stuff',best_loss)\n",
    "    print(best_metrics)\n",
    "    print(best_args)\n",
    "    return best_model, records, best_record\n",
    "\n",
    "# decision_model_best,records, bestr = gridsearch_triplet_model(emodel1,emodel2,emodel3,weights=[-1,1,1,-1])\n",
    "# records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebdf76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91af46a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model1,'../data/models/final_transition1_model_' + emodel1.identifier + '.pt')\n",
    "# torch.save(model2,'../data/models/final_transition2_model_' + emodel2.identifier + '.pt')\n",
    "# torch.save(model3,'../data/models/final_outcome_model_' + emodel3.identifier + '.pt')\n",
    "# print('../data/models/final_transition1_model_' + emodel1.identifier + '.pt')\n",
    "# print('../data/models/final_transition2_model_' + emodel2.identifier + '.pt')\n",
    "# print('../data/models/final_outcome_model_' + emodel3.identifier + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30998ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_model.set_device('cpu')\n",
    "tmodel1[0].set_device('cpu')\n",
    "tmodel2[0].set_device('cpu')\n",
    "tmodel3[0].set_device('cpu')\n",
    "torch.save(decision_model,'../resources/decision_model.pt')\n",
    "torch.save(tmodel1[0],'../resources/transition1_model.pt')\n",
    "torch.save(tmodel2[0],'../resources/transition2_model.pt')\n",
    "torch.save(tmodel3_smote[0],'../resources/outcome_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59220fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_model_alt.set_device('cpu')\n",
    "torch.save(decision_model_alt,'../resources/decision_model_alt.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321249c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_predictions(dm):\n",
    "    res  = []\n",
    "    for state in [0,1,2]:\n",
    "        mem = dm.memory[state]\n",
    "        mem = torch.median(mem,dim=0)[0].type(torch.FloatTensor)\n",
    "        val = dm(mem.reshape(1,-1),position=state)\n",
    "        res.append(val.cpu().detach().numpy())\n",
    "    return np.stack(res)\n",
    "get_default_predictions(decision_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c01c02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
