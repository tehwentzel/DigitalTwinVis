{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050c878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7418d82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score,accuracy_score,precision_recall_fscore_support\n",
    "from Constants import *\n",
    "from Preprocessing import *\n",
    "from Models import *\n",
    "import copy\n",
    "from Utils import *\n",
    "from DeepSurvivalModels import *\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c427599a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(536, 60)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = DTDataset(use_smote=False)\n",
    "data.processed_df.T\n",
    "data.get_input_state(1).shape\n",
    "# data.processed_df#.shape, len(data.processed_df.index.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "055e18c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hpv</th>\n",
       "      <th>age</th>\n",
       "      <th>packs_per_year</th>\n",
       "      <th>gender</th>\n",
       "      <th>smoking_status</th>\n",
       "      <th>Aspiration rate Pre-therapy</th>\n",
       "      <th>OS (Calculated)</th>\n",
       "      <th>Locoregional control (Time)</th>\n",
       "      <th>FDM (months)</th>\n",
       "      <th>time_to_event</th>\n",
       "      <th>...</th>\n",
       "      <th>4_ipsi</th>\n",
       "      <th>4_contra</th>\n",
       "      <th>5A_ipsi</th>\n",
       "      <th>5A_contra</th>\n",
       "      <th>5B_ipsi</th>\n",
       "      <th>5B_contra</th>\n",
       "      <th>6_ipsi</th>\n",
       "      <th>6_contra</th>\n",
       "      <th>RPLN_ipsi</th>\n",
       "      <th>RPLN_contra</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>55.969444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.033333</td>\n",
       "      <td>4.700000</td>\n",
       "      <td>6.033333</td>\n",
       "      <td>4.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>20.950000</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>69.930556</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.466667</td>\n",
       "      <td>7.466667</td>\n",
       "      <td>7.466667</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>72.319444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.800000</td>\n",
       "      <td>7.800000</td>\n",
       "      <td>7.800000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>59.730556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.066667</td>\n",
       "      <td>8.066667</td>\n",
       "      <td>8.066667</td>\n",
       "      <td>8.066667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10201</th>\n",
       "      <td>1</td>\n",
       "      <td>49.566667</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>143.200000</td>\n",
       "      <td>143.200000</td>\n",
       "      <td>143.200000</td>\n",
       "      <td>143.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10202</th>\n",
       "      <td>0</td>\n",
       "      <td>48.705556</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>144.366667</td>\n",
       "      <td>144.366667</td>\n",
       "      <td>144.366667</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10203</th>\n",
       "      <td>1</td>\n",
       "      <td>77.116667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>148.366667</td>\n",
       "      <td>148.366667</td>\n",
       "      <td>136.033333</td>\n",
       "      <td>136.033333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10204</th>\n",
       "      <td>0</td>\n",
       "      <td>45.950000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>152.600000</td>\n",
       "      <td>152.600000</td>\n",
       "      <td>152.600000</td>\n",
       "      <td>152.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10205</th>\n",
       "      <td>1</td>\n",
       "      <td>49.733333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>155.533333</td>\n",
       "      <td>155.533333</td>\n",
       "      <td>155.533333</td>\n",
       "      <td>155.533333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>536 rows × 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       hpv        age  packs_per_year  gender  smoking_status  \\\n",
       "id                                                              \n",
       "3        1  55.969444             0.0       1             0.0   \n",
       "5        0  20.950000            38.0       1             1.0   \n",
       "6        1  69.930556            35.0       0             1.0   \n",
       "7        1  72.319444             0.0       1             1.0   \n",
       "8        1  59.730556             0.0       1             0.0   \n",
       "...    ...        ...             ...     ...             ...   \n",
       "10201    1  49.566667            30.0       1             1.0   \n",
       "10202    0  48.705556            30.0       1             1.0   \n",
       "10203    1  77.116667             0.0       1             0.0   \n",
       "10204    0  45.950000             5.0       1             0.5   \n",
       "10205    1  49.733333             0.0       1             0.0   \n",
       "\n",
       "       Aspiration rate Pre-therapy  OS (Calculated)  \\\n",
       "id                                                    \n",
       "3                                0         6.033333   \n",
       "5                                0         7.333333   \n",
       "6                                1         7.466667   \n",
       "7                                0         7.800000   \n",
       "8                                0         8.066667   \n",
       "...                            ...              ...   \n",
       "10201                            0       143.200000   \n",
       "10202                            0       144.366667   \n",
       "10203                            0       148.366667   \n",
       "10204                            0       152.600000   \n",
       "10205                            0       155.533333   \n",
       "\n",
       "       Locoregional control (Time)  FDM (months)  time_to_event  ...  4_ipsi  \\\n",
       "id                                                               ...           \n",
       "3                         4.700000      6.033333       4.700000  ...     0.0   \n",
       "5                         7.333333      7.333333       6.000000  ...     0.0   \n",
       "6                         7.466667      7.466667       6.000000  ...     0.0   \n",
       "7                         7.800000      7.800000       6.000000  ...     0.0   \n",
       "8                         8.066667      8.066667       8.066667  ...     0.0   \n",
       "...                            ...           ...            ...  ...     ...   \n",
       "10201                   143.200000    143.200000     143.200000  ...     0.0   \n",
       "10202                   144.366667    144.366667       6.000000  ...     0.0   \n",
       "10203                   148.366667    136.033333     136.033333  ...     0.0   \n",
       "10204                   152.600000    152.600000     152.600000  ...     0.0   \n",
       "10205                   155.533333    155.533333     155.533333  ...     0.0   \n",
       "\n",
       "       4_contra  5A_ipsi  5A_contra  5B_ipsi  5B_contra  6_ipsi  6_contra  \\\n",
       "id                                                                          \n",
       "3           0.0      0.0        0.0      0.0        0.0     0.0       0.0   \n",
       "5           0.0      0.0        0.0      0.0        0.0     0.0       0.0   \n",
       "6           0.0      0.0        0.0      0.0        0.0     0.0       0.0   \n",
       "7           0.0      0.0        0.0      0.0        0.0     0.0       0.0   \n",
       "8           0.0      0.0        0.0      0.0        0.0     0.0       0.0   \n",
       "...         ...      ...        ...      ...        ...     ...       ...   \n",
       "10201       0.0      0.0        0.0      0.0        0.0     0.0       0.0   \n",
       "10202       0.0      0.0        0.0      0.0        0.0     0.0       0.0   \n",
       "10203       0.0      0.0        0.0      0.0        0.0     0.0       0.0   \n",
       "10204       0.0      0.0        0.0      0.0        0.0     0.0       0.0   \n",
       "10205       0.0      0.0        0.0      0.0        0.0     0.0       0.0   \n",
       "\n",
       "       RPLN_ipsi  RPLN_contra  \n",
       "id                             \n",
       "3            0.0          0.0  \n",
       "5            0.0          0.0  \n",
       "6            0.0          0.0  \n",
       "7            0.0          0.0  \n",
       "8            0.0          0.0  \n",
       "...          ...          ...  \n",
       "10201        0.0          0.0  \n",
       "10202        0.0          0.0  \n",
       "10203        0.0          0.0  \n",
       "10204        0.0          0.0  \n",
       "10205        0.0          0.0  \n",
       "\n",
       "[536 rows x 108 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f2937b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DSM(\n",
       "  (act): Tanh()\n",
       "  (shape): ParameterList(\n",
       "      (0): Parameter containing: [torch.float32 of size 6]\n",
       "      (1): Parameter containing: [torch.float32 of size 6]\n",
       "      (2): Parameter containing: [torch.float32 of size 6]\n",
       "      (3): Parameter containing: [torch.float32 of size 6]\n",
       "  )\n",
       "  (scale): ParameterList(\n",
       "      (0): Parameter containing: [torch.float32 of size 6]\n",
       "      (1): Parameter containing: [torch.float32 of size 6]\n",
       "      (2): Parameter containing: [torch.float32 of size 6]\n",
       "      (3): Parameter containing: [torch.float32 of size 6]\n",
       "  )\n",
       "  (gate): ModuleList(\n",
       "    (0-3): 4 x Sequential(\n",
       "      (0): Linear(in_features=103, out_features=6, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (scaleg): ModuleList(\n",
       "    (0-3): 4 x Sequential(\n",
       "      (0): Linear(in_features=103, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (shapeg): ModuleList(\n",
       "    (0-3): 4 x Sequential(\n",
       "      (0): Linear(in_features=103, out_features=6, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (embedding): Sequential(\n",
       "    (0): Linear(in_features=77, out_features=100, bias=False)\n",
       "    (1): Tanh()\n",
       "  )\n",
       "  (input_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (embedding_dropout): Dropout(p=0.5, inplace=False)\n",
       "  (squish): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Utils import *\n",
    "model1,model2,model3,smodel3 = load_transition_models()\n",
    "smodel3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b29d259b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([146.,  16.,   4.]) 147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0., 0.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]]),\n",
       " {'pd1': tensor([[6.7318e-01, 3.2682e-01, 5.8590e-22],\n",
       "          [9.3988e-01, 5.9272e-02, 8.4541e-04],\n",
       "          [1.2919e-02, 9.8485e-01, 2.2295e-03],\n",
       "          [1.1029e-01, 8.7445e-01, 1.5256e-02],\n",
       "          [8.5421e-01, 1.3026e-01, 1.5534e-02],\n",
       "          [4.2654e-01, 5.4939e-01, 2.4075e-02],\n",
       "          [6.4758e-01, 3.4076e-01, 1.1656e-02],\n",
       "          [6.5080e-01, 3.2673e-01, 2.2471e-02],\n",
       "          [5.9006e-01, 3.8479e-01, 2.5148e-02],\n",
       "          [4.9648e-01, 4.7491e-01, 2.8615e-02],\n",
       "          [8.1444e-01, 1.6375e-01, 2.1807e-02],\n",
       "          [7.3776e-01, 2.5254e-01, 9.6972e-03],\n",
       "          [6.5491e-01, 3.1795e-01, 2.7136e-02],\n",
       "          [5.1371e-01, 4.6883e-01, 1.7463e-02],\n",
       "          [5.5453e-01, 4.2060e-01, 2.4868e-02],\n",
       "          [7.4511e-01, 2.2689e-01, 2.8007e-02],\n",
       "          [2.7420e-02, 9.7177e-01, 8.0629e-04],\n",
       "          [7.2435e-01, 2.5914e-01, 1.6511e-02],\n",
       "          [9.0802e-01, 7.7134e-02, 1.4848e-02],\n",
       "          [9.3864e-01, 5.2869e-02, 8.4873e-03],\n",
       "          [3.4033e-01, 6.4235e-01, 1.7317e-02],\n",
       "          [7.4185e-01, 2.3001e-01, 2.8133e-02],\n",
       "          [7.3786e-01, 2.3873e-01, 2.3411e-02],\n",
       "          [7.2180e-02, 9.2585e-01, 1.9749e-03],\n",
       "          [9.9884e-01, 1.1580e-03, 7.4312e-22],\n",
       "          [5.6175e-01, 4.1420e-01, 2.4043e-02],\n",
       "          [9.9892e-01, 1.0830e-03, 8.3653e-22],\n",
       "          [8.3967e-01, 1.5297e-01, 7.3588e-03],\n",
       "          [5.4546e-01, 4.3240e-01, 2.2138e-02],\n",
       "          [3.7540e-01, 6.0095e-01, 2.3653e-02],\n",
       "          [8.7546e-01, 1.0834e-01, 1.6203e-02],\n",
       "          [8.8863e-01, 9.6602e-02, 1.4767e-02],\n",
       "          [8.8950e-01, 9.2636e-02, 1.7864e-02],\n",
       "          [9.0147e-01, 8.7188e-02, 1.1346e-02],\n",
       "          [7.5416e-01, 2.2808e-01, 1.7760e-02],\n",
       "          [1.2671e-01, 8.7329e-01, 4.6933e-22],\n",
       "          [7.6680e-01, 2.1596e-01, 1.7237e-02],\n",
       "          [8.7252e-01, 1.1431e-01, 1.3174e-02],\n",
       "          [6.4599e-01, 3.3760e-01, 1.6414e-02],\n",
       "          [3.3265e-01, 6.4877e-01, 1.8583e-02],\n",
       "          [7.5120e-01, 2.2983e-01, 1.8966e-02],\n",
       "          [1.8793e-01, 8.1052e-01, 1.5558e-03],\n",
       "          [2.0152e-01, 7.8518e-01, 1.3296e-02],\n",
       "          [8.6762e-02, 9.0837e-01, 4.8714e-03],\n",
       "          [6.3669e-01, 3.3809e-01, 2.5224e-02],\n",
       "          [1.4123e-01, 8.5219e-01, 6.5796e-03],\n",
       "          [5.8503e-01, 3.9455e-01, 2.0418e-02],\n",
       "          [8.8491e-01, 1.0425e-01, 1.0839e-02],\n",
       "          [9.3565e-01, 5.3778e-02, 1.0567e-02],\n",
       "          [5.8665e-01, 3.8460e-01, 2.8757e-02],\n",
       "          [7.8799e-01, 2.0369e-01, 8.3182e-03],\n",
       "          [8.0899e-01, 1.6896e-01, 2.2048e-02],\n",
       "          [3.6580e-01, 6.1280e-01, 2.1395e-02],\n",
       "          [6.1920e-01, 3.5086e-01, 2.9947e-02],\n",
       "          [3.6341e-01, 6.1276e-01, 2.3829e-02],\n",
       "          [1.2609e-01, 8.5833e-01, 1.5585e-02],\n",
       "          [6.0102e-01, 3.7835e-01, 2.0635e-02],\n",
       "          [4.8684e-01, 4.7818e-01, 3.4984e-02],\n",
       "          [5.3761e-01, 4.6074e-01, 1.6541e-03],\n",
       "          [7.6098e-01, 2.2221e-01, 1.6812e-02],\n",
       "          [8.5875e-01, 1.2090e-01, 2.0357e-02],\n",
       "          [8.2852e-01, 1.5034e-01, 2.1133e-02],\n",
       "          [8.2701e-01, 1.5614e-01, 1.6851e-02],\n",
       "          [9.2152e-01, 6.7731e-02, 1.0747e-02],\n",
       "          [9.9925e-01, 7.4842e-04, 6.4384e-22],\n",
       "          [6.8083e-01, 2.9835e-01, 2.0823e-02],\n",
       "          [7.8192e-01, 1.9444e-01, 2.3635e-02],\n",
       "          [4.3046e-01, 5.4833e-01, 2.1206e-02],\n",
       "          [8.9565e-01, 9.2032e-02, 1.2323e-02],\n",
       "          [8.5030e-01, 1.3350e-01, 1.6194e-02],\n",
       "          [1.3801e-01, 8.4911e-01, 1.2882e-02],\n",
       "          [8.3674e-01, 1.4401e-01, 1.9246e-02],\n",
       "          [8.7217e-01, 1.0867e-01, 1.9162e-02],\n",
       "          [8.3516e-01, 1.4346e-01, 2.1376e-02],\n",
       "          [5.2675e-01, 4.6423e-01, 9.0153e-03],\n",
       "          [4.5767e-01, 5.2786e-01, 1.4474e-02],\n",
       "          [8.6534e-03, 9.8927e-01, 2.0789e-03],\n",
       "          [1.1483e-01, 8.7161e-01, 1.3556e-02],\n",
       "          [1.9710e-01, 7.8690e-01, 1.5996e-02],\n",
       "          [4.8118e-01, 4.9578e-01, 2.3046e-02],\n",
       "          [4.5660e-01, 5.1849e-01, 2.4912e-02],\n",
       "          [6.4916e-01, 3.2271e-01, 2.8134e-02],\n",
       "          [5.2528e-01, 4.5513e-01, 1.9588e-02],\n",
       "          [9.4646e-01, 4.5911e-02, 7.6294e-03],\n",
       "          [8.7629e-01, 1.1869e-01, 5.0177e-03],\n",
       "          [7.3631e-01, 2.4368e-01, 2.0012e-02],\n",
       "          [6.7464e-01, 3.0488e-01, 2.0483e-02],\n",
       "          [5.0547e-01, 4.8171e-01, 1.2825e-02],\n",
       "          [7.7858e-01, 2.0146e-01, 1.9964e-02],\n",
       "          [5.4644e-01, 4.5356e-01, 6.2059e-22],\n",
       "          [7.7127e-01, 2.0630e-01, 2.2434e-02],\n",
       "          [7.4584e-01, 2.3483e-01, 1.9324e-02],\n",
       "          [5.7910e-01, 3.9601e-01, 2.4897e-02],\n",
       "          [7.0343e-01, 2.7499e-01, 2.1572e-02],\n",
       "          [1.2798e-01, 8.6133e-01, 1.0694e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [8.0616e-01, 1.7812e-01, 1.5719e-02],\n",
       "          [6.1121e-02, 9.3578e-01, 3.0944e-03],\n",
       "          [8.9878e-01, 8.4604e-02, 1.6611e-02],\n",
       "          [6.2310e-02, 9.3001e-01, 7.6761e-03],\n",
       "          [3.7231e-01, 6.1035e-01, 1.7339e-02],\n",
       "          [8.2550e-01, 1.6094e-01, 1.3560e-02],\n",
       "          [8.9300e-01, 9.2479e-02, 1.4517e-02],\n",
       "          [7.6269e-02, 9.1899e-01, 4.7406e-03],\n",
       "          [6.6102e-01, 3.1312e-01, 2.5855e-02],\n",
       "          [7.4111e-01, 2.3656e-01, 2.2337e-02],\n",
       "          [8.0159e-01, 1.8180e-01, 1.6612e-02],\n",
       "          [6.3892e-01, 3.3980e-01, 2.1284e-02],\n",
       "          [7.0327e-01, 2.8034e-01, 1.6391e-02],\n",
       "          [4.6433e-01, 5.1134e-01, 2.4333e-02],\n",
       "          [1.8338e-01, 8.0198e-01, 1.4641e-02],\n",
       "          [7.9384e-01, 1.8231e-01, 2.3855e-02],\n",
       "          [5.8849e-01, 3.8207e-01, 2.9445e-02],\n",
       "          [4.4252e-01, 5.3518e-01, 2.2294e-02],\n",
       "          [8.2429e-01, 1.5796e-01, 1.7753e-02],\n",
       "          [7.1067e-01, 2.6505e-01, 2.4281e-02],\n",
       "          [7.5360e-01, 2.2591e-01, 2.0485e-02],\n",
       "          [6.2074e-01, 3.5239e-01, 2.6873e-02],\n",
       "          [8.8286e-01, 1.0856e-01, 8.5853e-03],\n",
       "          [1.5911e-01, 8.2600e-01, 1.4889e-02],\n",
       "          [8.5441e-01, 1.3072e-01, 1.4865e-02],\n",
       "          [5.1071e-01, 4.6539e-01, 2.3900e-02],\n",
       "          [8.4349e-01, 1.3878e-01, 1.7735e-02],\n",
       "          [6.1096e-01, 3.5766e-01, 3.1387e-02],\n",
       "          [8.5571e-01, 1.2781e-01, 1.6481e-02],\n",
       "          [8.9367e-01, 8.8965e-02, 1.7368e-02],\n",
       "          [2.0745e-01, 7.8258e-01, 9.9721e-03],\n",
       "          [3.6775e-01, 6.1908e-01, 1.3177e-02],\n",
       "          [6.5036e-01, 3.4194e-01, 7.7028e-03],\n",
       "          [8.9733e-01, 8.8371e-02, 1.4304e-02],\n",
       "          [6.4056e-01, 3.4410e-01, 1.5335e-02],\n",
       "          [2.9608e-01, 6.8419e-01, 1.9729e-02],\n",
       "          [6.1413e-01, 3.6509e-01, 2.0773e-02],\n",
       "          [8.1471e-01, 1.6739e-01, 1.7905e-02],\n",
       "          [8.2344e-01, 1.5428e-01, 2.2278e-02],\n",
       "          [8.2378e-01, 1.5746e-01, 1.8765e-02],\n",
       "          [4.3167e-01, 5.4478e-01, 2.3552e-02],\n",
       "          [8.2811e-01, 1.5528e-01, 1.6611e-02],\n",
       "          [7.2032e-01, 2.6015e-01, 1.9532e-02],\n",
       "          [7.9557e-01, 1.8590e-01, 1.8536e-02],\n",
       "          [2.8500e-01, 6.9271e-01, 2.2288e-02],\n",
       "          [1.7972e-01, 7.9583e-01, 2.4447e-02],\n",
       "          [9.7909e-01, 2.0167e-02, 7.4233e-04],\n",
       "          [7.2675e-01, 2.6516e-01, 8.0871e-03],\n",
       "          [3.9492e-01, 5.8505e-01, 2.0028e-02],\n",
       "          [5.7882e-01, 3.9772e-01, 2.3465e-02],\n",
       "          [6.9767e-01, 2.8013e-01, 2.2202e-02]], grad_fn=<CopySlices>),\n",
       "  'nd1': tensor([[7.7810e-35, 1.0000e+00, 7.7810e-35],\n",
       "          [1.1151e-04, 9.9978e-01, 1.0903e-04],\n",
       "          [2.4233e-03, 9.9525e-01, 2.3273e-03],\n",
       "          [8.0152e-03, 9.8420e-01, 7.7842e-03],\n",
       "          [7.8510e-03, 9.8457e-01, 7.5795e-03],\n",
       "          [6.1962e-03, 9.8758e-01, 6.2248e-03],\n",
       "          [2.5433e-03, 9.9502e-01, 2.4389e-03],\n",
       "          [6.2721e-03, 9.8745e-01, 6.2771e-03],\n",
       "          [6.5673e-03, 9.8702e-01, 6.4152e-03],\n",
       "          [1.0117e-02, 9.7983e-01, 1.0054e-02],\n",
       "          [8.6676e-03, 9.8280e-01, 8.5285e-03],\n",
       "          [1.2819e-03, 9.9749e-01, 1.2310e-03],\n",
       "          [9.1765e-03, 9.8188e-01, 8.9391e-03],\n",
       "          [4.4518e-03, 9.9104e-01, 4.5110e-03],\n",
       "          [9.6735e-03, 9.8074e-01, 9.5829e-03],\n",
       "          [9.0634e-03, 9.8208e-01, 8.8567e-03],\n",
       "          [2.0306e-04, 9.9960e-01, 2.0055e-04],\n",
       "          [6.7235e-03, 9.8661e-01, 6.6624e-03],\n",
       "          [9.0256e-03, 9.8213e-01, 8.8481e-03],\n",
       "          [4.9388e-03, 9.9029e-01, 4.7715e-03],\n",
       "          [5.4027e-03, 9.8925e-01, 5.3500e-03],\n",
       "          [9.0739e-03, 9.8206e-01, 8.8639e-03],\n",
       "          [9.9618e-03, 9.8019e-01, 9.8472e-03],\n",
       "          [4.8694e-04, 9.9903e-01, 4.8151e-04],\n",
       "          [1.1892e-33, 1.0000e+00, 1.1892e-33],\n",
       "          [6.2107e-03, 9.8771e-01, 6.0759e-03],\n",
       "          [1.4805e-33, 1.0000e+00, 1.4805e-33],\n",
       "          [1.5732e-03, 9.9689e-01, 1.5321e-03],\n",
       "          [6.6023e-03, 9.8697e-01, 6.4320e-03],\n",
       "          [6.6379e-03, 9.8664e-01, 6.7239e-03],\n",
       "          [8.1852e-03, 9.8362e-01, 8.1974e-03],\n",
       "          [1.0244e-02, 9.7982e-01, 9.9368e-03],\n",
       "          [1.0343e-02, 9.7956e-01, 1.0102e-02],\n",
       "          [5.0646e-03, 9.8994e-01, 4.9981e-03],\n",
       "          [5.4966e-03, 9.8909e-01, 5.4174e-03],\n",
       "          [1.2064e-34, 1.0000e+00, 1.2064e-34],\n",
       "          [7.0699e-03, 9.8619e-01, 6.7401e-03],\n",
       "          [6.5413e-03, 9.8697e-01, 6.4859e-03],\n",
       "          [6.0258e-03, 9.8817e-01, 5.8081e-03],\n",
       "          [4.9087e-03, 9.9011e-01, 4.9817e-03],\n",
       "          [9.1338e-03, 9.8162e-01, 9.2436e-03],\n",
       "          [1.0830e-04, 9.9979e-01, 1.0626e-04],\n",
       "          [4.9122e-03, 9.9033e-01, 4.7555e-03],\n",
       "          [1.2123e-03, 9.9765e-01, 1.1416e-03],\n",
       "          [7.4349e-03, 9.8513e-01, 7.4333e-03],\n",
       "          [1.6311e-03, 9.9673e-01, 1.6343e-03],\n",
       "          [5.9511e-03, 9.8819e-01, 5.8563e-03],\n",
       "          [4.2879e-03, 9.9154e-01, 4.1703e-03],\n",
       "          [9.5426e-03, 9.8105e-01, 9.4115e-03],\n",
       "          [8.1513e-03, 9.8375e-01, 8.0952e-03],\n",
       "          [1.9914e-03, 9.9609e-01, 1.9218e-03],\n",
       "          [1.2042e-02, 9.7607e-01, 1.1884e-02],\n",
       "          [5.1764e-03, 9.8985e-01, 4.9740e-03],\n",
       "          [1.0493e-02, 9.7911e-01, 1.0399e-02],\n",
       "          [6.7667e-03, 9.8638e-01, 6.8560e-03],\n",
       "          [1.0717e-02, 9.7905e-01, 1.0234e-02],\n",
       "          [7.5030e-03, 9.8516e-01, 7.3395e-03],\n",
       "          [1.3420e-02, 9.7341e-01, 1.3172e-02],\n",
       "          [8.7103e-05, 9.9983e-01, 8.3449e-05],\n",
       "          [7.2224e-03, 9.8550e-01, 7.2826e-03],\n",
       "          [1.1027e-02, 9.7812e-01, 1.0849e-02],\n",
       "          [1.2985e-02, 9.7424e-01, 1.2776e-02],\n",
       "          [8.1485e-03, 9.8378e-01, 8.0724e-03],\n",
       "          [4.7459e-03, 9.9059e-01, 4.6612e-03],\n",
       "          [1.6154e-33, 1.0000e+00, 1.6154e-33],\n",
       "          [6.7155e-03, 9.8650e-01, 6.7891e-03],\n",
       "          [9.4175e-03, 9.8136e-01, 9.2259e-03],\n",
       "          [7.3345e-03, 9.8579e-01, 6.8741e-03],\n",
       "          [7.4850e-03, 9.8503e-01, 7.4865e-03],\n",
       "          [8.1420e-03, 9.8388e-01, 7.9738e-03],\n",
       "          [6.3003e-03, 9.8777e-01, 5.9282e-03],\n",
       "          [8.2408e-03, 9.8371e-01, 8.0518e-03],\n",
       "          [1.0199e-02, 9.7988e-01, 9.9162e-03],\n",
       "          [7.2160e-03, 9.8576e-01, 7.0250e-03],\n",
       "          [1.7469e-03, 9.9656e-01, 1.6974e-03],\n",
       "          [3.3275e-03, 9.9338e-01, 3.2907e-03],\n",
       "          [2.3040e-03, 9.9550e-01, 2.1936e-03],\n",
       "          [8.9800e-03, 9.8257e-01, 8.4548e-03],\n",
       "          [5.6299e-03, 9.8905e-01, 5.3151e-03],\n",
       "          [6.0891e-03, 9.8804e-01, 5.8731e-03],\n",
       "          [6.6541e-03, 9.8667e-01, 6.6736e-03],\n",
       "          [8.9738e-03, 9.8248e-01, 8.5498e-03],\n",
       "          [7.3889e-03, 9.8550e-01, 7.1072e-03],\n",
       "          [4.5110e-03, 9.9114e-01, 4.3525e-03],\n",
       "          [9.1529e-04, 9.9819e-01, 8.9216e-04],\n",
       "          [5.7621e-03, 9.8851e-01, 5.7303e-03],\n",
       "          [5.5180e-03, 9.8919e-01, 5.2887e-03],\n",
       "          [2.6195e-03, 9.9481e-01, 2.5681e-03],\n",
       "          [7.0497e-03, 9.8592e-01, 7.0341e-03],\n",
       "          [8.1320e-35, 1.0000e+00, 8.1320e-35],\n",
       "          [6.6450e-03, 9.8691e-01, 6.4486e-03],\n",
       "          [5.5789e-03, 9.8888e-01, 5.5447e-03],\n",
       "          [1.0434e-02, 9.7966e-01, 9.9068e-03],\n",
       "          [6.2711e-03, 9.8744e-01, 6.2864e-03],\n",
       "          [5.5078e-03, 9.8908e-01, 5.4169e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [8.5472e-03, 9.8309e-01, 8.3581e-03],\n",
       "          [1.2219e-03, 9.9759e-01, 1.1845e-03],\n",
       "          [9.9881e-03, 9.8023e-01, 9.7783e-03],\n",
       "          [4.2633e-03, 9.9164e-01, 4.0994e-03],\n",
       "          [5.8488e-03, 9.8834e-01, 5.8148e-03],\n",
       "          [7.6075e-03, 9.8459e-01, 7.8018e-03],\n",
       "          [1.0215e-02, 9.7987e-01, 9.9129e-03],\n",
       "          [1.7928e-03, 9.9648e-01, 1.7296e-03],\n",
       "          [8.9397e-03, 9.8215e-01, 8.9082e-03],\n",
       "          [8.9380e-03, 9.8218e-01, 8.8850e-03],\n",
       "          [8.0024e-03, 9.8400e-01, 8.0000e-03],\n",
       "          [5.9382e-03, 9.8827e-01, 5.7942e-03],\n",
       "          [3.8827e-03, 9.9224e-01, 3.8765e-03],\n",
       "          [6.0350e-03, 9.8809e-01, 5.8711e-03],\n",
       "          [8.1676e-03, 9.8390e-01, 7.9289e-03],\n",
       "          [1.1325e-02, 9.7758e-01, 1.1096e-02],\n",
       "          [1.0342e-02, 9.7947e-01, 1.0183e-02],\n",
       "          [4.0810e-03, 9.9210e-01, 3.8212e-03],\n",
       "          [8.6672e-03, 9.8289e-01, 8.4445e-03],\n",
       "          [5.9933e-03, 9.8815e-01, 5.8525e-03],\n",
       "          [7.3505e-03, 9.8526e-01, 7.3935e-03],\n",
       "          [8.8549e-03, 9.8257e-01, 8.5743e-03],\n",
       "          [3.3633e-03, 9.9340e-01, 3.2405e-03],\n",
       "          [8.0402e-03, 9.8438e-01, 7.5796e-03],\n",
       "          [6.4935e-03, 9.8741e-01, 6.0975e-03],\n",
       "          [7.9013e-03, 9.8466e-01, 7.4410e-03],\n",
       "          [8.9829e-03, 9.8218e-01, 8.8406e-03],\n",
       "          [1.1465e-02, 9.7729e-01, 1.1243e-02],\n",
       "          [9.5480e-03, 9.8112e-01, 9.3355e-03],\n",
       "          [1.0254e-02, 9.7972e-01, 1.0026e-02],\n",
       "          [4.5128e-03, 9.9113e-01, 4.3602e-03],\n",
       "          [3.0165e-03, 9.9397e-01, 3.0103e-03],\n",
       "          [1.4749e-03, 9.9712e-01, 1.4052e-03],\n",
       "          [7.5990e-03, 9.8499e-01, 7.4146e-03],\n",
       "          [4.7294e-03, 9.9051e-01, 4.7613e-03],\n",
       "          [5.6985e-03, 9.8851e-01, 5.7910e-03],\n",
       "          [7.6930e-03, 9.8492e-01, 7.3898e-03],\n",
       "          [1.0115e-02, 9.7979e-01, 1.0095e-02],\n",
       "          [1.0698e-02, 9.7891e-01, 1.0387e-02],\n",
       "          [7.1357e-03, 9.8584e-01, 7.0245e-03],\n",
       "          [9.1140e-03, 9.8193e-01, 8.9607e-03],\n",
       "          [8.0471e-03, 9.8432e-01, 7.6321e-03],\n",
       "          [8.2560e-03, 9.8362e-01, 8.1259e-03],\n",
       "          [8.4120e-03, 9.8344e-01, 8.1474e-03],\n",
       "          [6.8797e-03, 9.8653e-01, 6.5865e-03],\n",
       "          [1.1536e-02, 9.7729e-01, 1.1176e-02],\n",
       "          [3.0667e-04, 9.9940e-01, 2.9681e-04],\n",
       "          [2.3037e-03, 9.9535e-01, 2.3444e-03],\n",
       "          [5.9526e-03, 9.8841e-01, 5.6376e-03],\n",
       "          [8.0183e-03, 9.8438e-01, 7.5991e-03],\n",
       "          [7.1334e-03, 9.8591e-01, 6.9561e-03]], grad_fn=<CopySlices>),\n",
       "  'nd2': tensor([[9.6082e-01, 3.7498e-02, 1.6823e-03],\n",
       "          [8.3123e-01, 1.6787e-01, 9.0080e-04],\n",
       "          [8.0620e-01, 1.8875e-01, 5.0515e-03],\n",
       "          [7.3338e-01, 2.4952e-01, 1.7103e-02],\n",
       "          [8.3621e-01, 1.5184e-01, 1.1958e-02],\n",
       "          [5.9347e-01, 3.8894e-01, 1.7588e-02],\n",
       "          [7.2218e-01, 2.7303e-01, 4.7921e-03],\n",
       "          [5.0859e-01, 4.7772e-01, 1.3687e-02],\n",
       "          [6.9574e-01, 2.9503e-01, 9.2296e-03],\n",
       "          [4.8718e-01, 4.9751e-01, 1.5313e-02],\n",
       "          [5.5105e-01, 4.3318e-01, 1.5769e-02],\n",
       "          [8.9959e-01, 9.8874e-02, 1.5395e-03],\n",
       "          [5.4940e-01, 4.3358e-01, 1.7019e-02],\n",
       "          [4.9420e-01, 4.9111e-01, 1.4694e-02],\n",
       "          [7.2514e-01, 2.5084e-01, 2.4021e-02],\n",
       "          [6.9813e-01, 2.8746e-01, 1.4405e-02],\n",
       "          [8.0544e-01, 1.9372e-01, 8.4150e-04],\n",
       "          [7.5028e-01, 2.3375e-01, 1.5972e-02],\n",
       "          [5.1906e-01, 4.6809e-01, 1.2844e-02],\n",
       "          [6.5525e-01, 3.3239e-01, 1.2366e-02],\n",
       "          [8.3888e-01, 1.4840e-01, 1.2720e-02],\n",
       "          [7.0087e-01, 2.8470e-01, 1.4423e-02],\n",
       "          [7.0477e-01, 2.6977e-01, 2.5460e-02],\n",
       "          [8.2632e-01, 1.7194e-01, 1.7365e-03],\n",
       "          [7.1342e-01, 2.8651e-01, 6.6240e-05],\n",
       "          [8.0161e-01, 1.8634e-01, 1.2053e-02],\n",
       "          [8.0540e-01, 1.9453e-01, 6.8354e-05],\n",
       "          [7.3971e-01, 2.5325e-01, 7.0333e-03],\n",
       "          [6.2623e-01, 3.5281e-01, 2.0967e-02],\n",
       "          [5.4683e-01, 4.3467e-01, 1.8502e-02],\n",
       "          [5.1166e-01, 4.7524e-01, 1.3098e-02],\n",
       "          [7.5413e-01, 2.2699e-01, 1.8873e-02],\n",
       "          [5.8204e-01, 4.0294e-01, 1.5025e-02],\n",
       "          [7.6834e-01, 2.1923e-01, 1.2427e-02],\n",
       "          [7.3278e-01, 2.6098e-01, 6.2424e-03],\n",
       "          [9.3108e-01, 6.6678e-02, 2.2467e-03],\n",
       "          [6.8844e-01, 2.9193e-01, 1.9635e-02],\n",
       "          [8.1596e-01, 1.7547e-01, 8.5638e-03],\n",
       "          [5.1967e-01, 4.6268e-01, 1.7651e-02],\n",
       "          [5.2528e-01, 4.6006e-01, 1.4660e-02],\n",
       "          [7.1969e-01, 2.6468e-01, 1.5631e-02],\n",
       "          [8.9045e-01, 1.0906e-01, 4.9443e-04],\n",
       "          [7.7371e-01, 2.1251e-01, 1.3779e-02],\n",
       "          [9.2052e-01, 7.8140e-02, 1.3422e-03],\n",
       "          [5.5846e-01, 4.2745e-01, 1.4095e-02],\n",
       "          [7.4284e-01, 2.5187e-01, 5.2938e-03],\n",
       "          [8.6913e-01, 1.1827e-01, 1.2598e-02],\n",
       "          [6.1442e-01, 3.7614e-01, 9.4463e-03],\n",
       "          [7.3835e-01, 2.4473e-01, 1.6921e-02],\n",
       "          [6.3114e-01, 3.4973e-01, 1.9130e-02],\n",
       "          [8.0710e-01, 1.8792e-01, 4.9811e-03],\n",
       "          [7.4846e-01, 2.3197e-01, 1.9573e-02],\n",
       "          [8.4946e-01, 1.3464e-01, 1.5906e-02],\n",
       "          [6.1318e-01, 3.7063e-01, 1.6189e-02],\n",
       "          [5.5635e-01, 4.2480e-01, 1.8845e-02],\n",
       "          [6.1317e-01, 3.6291e-01, 2.3929e-02],\n",
       "          [7.4435e-01, 2.4266e-01, 1.2988e-02],\n",
       "          [5.1610e-01, 4.6518e-01, 1.8716e-02],\n",
       "          [8.8592e-01, 1.1352e-01, 5.6299e-04],\n",
       "          [6.4285e-01, 3.4038e-01, 1.6768e-02],\n",
       "          [5.6967e-01, 4.1460e-01, 1.5733e-02],\n",
       "          [6.6256e-01, 3.1891e-01, 1.8531e-02],\n",
       "          [5.6525e-01, 4.1114e-01, 2.3610e-02],\n",
       "          [5.3955e-01, 4.5164e-01, 8.8044e-03],\n",
       "          [6.7667e-01, 3.2325e-01, 7.7525e-05],\n",
       "          [5.2937e-01, 4.5818e-01, 1.2452e-02],\n",
       "          [5.2679e-01, 4.5058e-01, 2.2625e-02],\n",
       "          [5.7012e-01, 4.0729e-01, 2.2582e-02],\n",
       "          [7.5793e-01, 2.2430e-01, 1.7771e-02],\n",
       "          [6.1865e-01, 3.6014e-01, 2.1212e-02],\n",
       "          [5.1320e-01, 4.6827e-01, 1.8533e-02],\n",
       "          [8.3701e-01, 1.4927e-01, 1.3723e-02],\n",
       "          [6.2808e-01, 3.5680e-01, 1.5121e-02],\n",
       "          [6.6633e-01, 3.1953e-01, 1.4149e-02],\n",
       "          [7.8113e-01, 2.1203e-01, 6.8353e-03],\n",
       "          [5.9645e-01, 3.9452e-01, 9.0275e-03],\n",
       "          [7.4497e-01, 2.4896e-01, 6.0677e-03],\n",
       "          [5.8367e-01, 3.9121e-01, 2.5129e-02],\n",
       "          [6.1932e-01, 3.7426e-01, 6.4147e-03],\n",
       "          [5.8020e-01, 4.0557e-01, 1.4239e-02],\n",
       "          [5.3049e-01, 4.5229e-01, 1.7219e-02],\n",
       "          [7.3285e-01, 2.5319e-01, 1.3965e-02],\n",
       "          [7.6096e-01, 2.2622e-01, 1.2819e-02],\n",
       "          [6.3382e-01, 3.5444e-01, 1.1738e-02],\n",
       "          [8.6291e-01, 1.3343e-01, 3.6663e-03],\n",
       "          [4.8975e-01, 4.9783e-01, 1.2427e-02],\n",
       "          [7.0168e-01, 2.8282e-01, 1.5501e-02],\n",
       "          [4.2621e-01, 5.6489e-01, 8.8994e-03],\n",
       "          [5.5785e-01, 4.2968e-01, 1.2466e-02],\n",
       "          [9.5147e-01, 4.5370e-02, 3.1574e-03],\n",
       "          [4.9765e-01, 4.8615e-01, 1.6204e-02],\n",
       "          [4.7217e-01, 5.1575e-01, 1.2087e-02],\n",
       "          [6.4207e-01, 3.3156e-01, 2.6372e-02],\n",
       "          [4.9129e-01, 4.9698e-01, 1.1734e-02],\n",
       "          [6.4245e-01, 3.4521e-01, 1.2334e-02],\n",
       "          [7.7589e-01, 2.1773e-01, 6.3785e-03],\n",
       "          [6.7768e-01, 3.0479e-01, 1.7530e-02],\n",
       "          [8.5656e-01, 1.4206e-01, 1.3785e-03],\n",
       "          [5.5058e-01, 4.3521e-01, 1.4207e-02],\n",
       "          [7.9556e-01, 1.9762e-01, 6.8117e-03],\n",
       "          [6.3405e-01, 3.5299e-01, 1.2952e-02],\n",
       "          [6.6843e-01, 3.1624e-01, 1.5325e-02],\n",
       "          [7.4032e-01, 2.4073e-01, 1.8947e-02],\n",
       "          [7.6350e-01, 2.3509e-01, 1.4076e-03],\n",
       "          [6.4411e-01, 3.3567e-01, 2.0213e-02],\n",
       "          [7.7056e-01, 2.0890e-01, 2.0536e-02],\n",
       "          [6.7697e-01, 3.0575e-01, 1.7279e-02],\n",
       "          [7.4013e-01, 2.4685e-01, 1.3027e-02],\n",
       "          [4.4048e-01, 5.4964e-01, 9.8748e-03],\n",
       "          [6.2508e-01, 3.5696e-01, 1.7962e-02],\n",
       "          [6.9906e-01, 2.8720e-01, 1.3733e-02],\n",
       "          [6.4261e-01, 3.3976e-01, 1.7637e-02],\n",
       "          [5.0905e-01, 4.7296e-01, 1.7993e-02],\n",
       "          [9.1907e-01, 7.6595e-02, 4.3339e-03],\n",
       "          [7.0953e-01, 2.7176e-01, 1.8716e-02],\n",
       "          [6.4524e-01, 3.3476e-01, 2.0007e-02],\n",
       "          [5.0502e-01, 4.8241e-01, 1.2563e-02],\n",
       "          [5.8489e-01, 3.9785e-01, 1.7263e-02],\n",
       "          [6.5429e-01, 3.3220e-01, 1.3512e-02],\n",
       "          [5.4379e-01, 4.3562e-01, 2.0592e-02],\n",
       "          [7.9851e-01, 1.8894e-01, 1.2547e-02],\n",
       "          [8.5005e-01, 1.3821e-01, 1.1739e-02],\n",
       "          [7.2996e-01, 2.5391e-01, 1.6127e-02],\n",
       "          [6.4871e-01, 3.3188e-01, 1.9416e-02],\n",
       "          [7.0012e-01, 2.7870e-01, 2.1179e-02],\n",
       "          [5.6839e-01, 4.1686e-01, 1.4752e-02],\n",
       "          [5.5968e-01, 4.2395e-01, 1.6370e-02],\n",
       "          [6.3364e-01, 3.5903e-01, 7.3268e-03],\n",
       "          [7.0453e-01, 2.9070e-01, 4.7677e-03],\n",
       "          [5.6918e-01, 4.1594e-01, 1.4883e-02],\n",
       "          [7.2852e-01, 2.6381e-01, 7.6757e-03],\n",
       "          [5.5315e-01, 4.3017e-01, 1.6678e-02],\n",
       "          [7.4660e-01, 2.3849e-01, 1.4916e-02],\n",
       "          [6.7887e-01, 2.9999e-01, 2.1138e-02],\n",
       "          [5.5075e-01, 4.3275e-01, 1.6503e-02],\n",
       "          [8.2137e-01, 1.6490e-01, 1.3733e-02],\n",
       "          [7.0611e-01, 2.6281e-01, 3.1077e-02],\n",
       "          [6.2302e-01, 3.5704e-01, 1.9937e-02],\n",
       "          [4.7415e-01, 5.0380e-01, 2.2044e-02],\n",
       "          [8.3913e-01, 1.4833e-01, 1.2539e-02],\n",
       "          [5.1821e-01, 4.6181e-01, 1.9985e-02],\n",
       "          [6.1709e-01, 3.5801e-01, 2.4902e-02],\n",
       "          [8.4539e-01, 1.5459e-01, 1.9328e-05],\n",
       "          [7.3912e-01, 2.5295e-01, 7.9328e-03],\n",
       "          [5.6439e-01, 4.1612e-01, 1.9486e-02],\n",
       "          [7.9629e-01, 1.9091e-01, 1.2798e-02],\n",
       "          [6.3518e-01, 3.4291e-01, 2.1909e-02]], grad_fn=<CopySlices>),\n",
       "  'pd2': tensor([[9.9991e-01, 4.3664e-05, 4.3686e-05],\n",
       "          [1.0000e+00, 2.1052e-06, 2.1054e-06],\n",
       "          [9.9980e-01, 1.0037e-04, 1.0042e-04],\n",
       "          [9.9750e-01, 1.2493e-03, 1.2512e-03],\n",
       "          [9.9834e-01, 8.3000e-04, 8.3130e-04],\n",
       "          [9.9832e-01, 8.3647e-04, 8.3888e-04],\n",
       "          [9.9988e-01, 5.9715e-05, 5.9735e-05],\n",
       "          [9.9911e-01, 4.4687e-04, 4.4772e-04],\n",
       "          [9.9949e-01, 2.5630e-04, 2.5670e-04],\n",
       "          [9.9857e-01, 7.1454e-04, 7.1574e-04],\n",
       "          [9.9875e-01, 6.2326e-04, 6.2481e-04],\n",
       "          [9.9998e-01, 1.0450e-05, 1.0455e-05],\n",
       "          [9.9831e-01, 8.4294e-04, 8.4496e-04],\n",
       "          [9.9899e-01, 5.0510e-04, 5.0631e-04],\n",
       "          [9.9544e-01, 2.2783e-03, 2.2836e-03],\n",
       "          [9.9868e-01, 6.5937e-04, 6.6102e-04],\n",
       "          [1.0000e+00, 2.2384e-06, 2.2388e-06],\n",
       "          [9.9808e-01, 9.5680e-04, 9.5896e-04],\n",
       "          [9.9923e-01, 3.8505e-04, 3.8578e-04],\n",
       "          [9.9916e-01, 4.1981e-04, 4.2007e-04],\n",
       "          [9.9822e-01, 8.9148e-04, 8.9210e-04],\n",
       "          [9.9867e-01, 6.6582e-04, 6.6748e-04],\n",
       "          [9.9538e-01, 2.3021e-03, 2.3129e-03],\n",
       "          [9.9997e-01, 1.5208e-05, 1.5209e-05],\n",
       "          [1.0000e+00, 6.4196e-09, 6.4197e-09],\n",
       "          [9.9876e-01, 6.2013e-04, 6.2131e-04],\n",
       "          [1.0000e+00, 8.7416e-09, 8.7417e-09],\n",
       "          [9.9974e-01, 1.2920e-04, 1.2915e-04],\n",
       "          [9.9757e-01, 1.2146e-03, 1.2191e-03],\n",
       "          [9.9813e-01, 9.3118e-04, 9.3428e-04],\n",
       "          [9.9923e-01, 3.8386e-04, 3.8458e-04],\n",
       "          [9.9645e-01, 1.7747e-03, 1.7753e-03],\n",
       "          [9.9888e-01, 5.5748e-04, 5.5860e-04],\n",
       "          [9.9891e-01, 5.4544e-04, 5.4696e-04],\n",
       "          [9.9980e-01, 9.8488e-05, 9.8587e-05],\n",
       "          [9.9991e-01, 4.6083e-05, 4.6098e-05],\n",
       "          [9.9706e-01, 1.4726e-03, 1.4714e-03],\n",
       "          [9.9937e-01, 3.1421e-04, 3.1450e-04],\n",
       "          [9.9830e-01, 8.5007e-04, 8.4927e-04],\n",
       "          [9.9887e-01, 5.6570e-04, 5.6745e-04],\n",
       "          [9.9819e-01, 9.0558e-04, 9.0612e-04],\n",
       "          [1.0000e+00, 9.3033e-07, 9.3043e-07],\n",
       "          [9.9849e-01, 7.5621e-04, 7.5595e-04],\n",
       "          [9.9997e-01, 1.4502e-05, 1.4508e-05],\n",
       "          [9.9894e-01, 5.2785e-04, 5.2877e-04],\n",
       "          [9.9985e-01, 7.5039e-05, 7.5048e-05],\n",
       "          [9.9776e-01, 1.1168e-03, 1.1191e-03],\n",
       "          [9.9957e-01, 2.1474e-04, 2.1486e-04],\n",
       "          [9.9806e-01, 9.6780e-04, 9.7073e-04],\n",
       "          [9.9785e-01, 1.0731e-03, 1.0760e-03],\n",
       "          [9.9988e-01, 5.8656e-05, 5.8720e-05],\n",
       "          [9.9665e-01, 1.6744e-03, 1.6770e-03],\n",
       "          [9.9672e-01, 1.6358e-03, 1.6411e-03],\n",
       "          [9.9838e-01, 8.0906e-04, 8.1030e-04],\n",
       "          [9.9804e-01, 9.8043e-04, 9.8370e-04],\n",
       "          [9.9559e-01, 2.2025e-03, 2.2030e-03],\n",
       "          [9.9866e-01, 6.6883e-04, 6.6921e-04],\n",
       "          [9.9799e-01, 1.0025e-03, 1.0048e-03],\n",
       "          [1.0000e+00, 1.2354e-06, 1.2354e-06],\n",
       "          [9.9830e-01, 8.4778e-04, 8.4965e-04],\n",
       "          [9.9874e-01, 6.2943e-04, 6.3056e-04],\n",
       "          [9.9821e-01, 8.9340e-04, 8.9593e-04],\n",
       "          [9.9631e-01, 1.8453e-03, 1.8475e-03],\n",
       "          [9.9966e-01, 1.7149e-04, 1.7178e-04],\n",
       "          [1.0000e+00, 9.0865e-09, 9.0866e-09],\n",
       "          [9.9928e-01, 3.6189e-04, 3.6228e-04],\n",
       "          [9.9732e-01, 1.3365e-03, 1.3403e-03],\n",
       "          [9.9636e-01, 1.8240e-03, 1.8195e-03],\n",
       "          [9.9783e-01, 1.0842e-03, 1.0876e-03],\n",
       "          [9.9713e-01, 1.4339e-03, 1.4338e-03],\n",
       "          [9.9771e-01, 1.1477e-03, 1.1465e-03],\n",
       "          [9.9796e-01, 1.0191e-03, 1.0215e-03],\n",
       "          [9.9882e-01, 5.8771e-04, 5.8885e-04],\n",
       "          [9.9901e-01, 4.9218e-04, 4.9361e-04],\n",
       "          [9.9973e-01, 1.3701e-04, 1.3709e-04],\n",
       "          [9.9962e-01, 1.8838e-04, 1.8857e-04],\n",
       "          [9.9974e-01, 1.2898e-04, 1.2908e-04],\n",
       "          [9.9521e-01, 2.3945e-03, 2.3948e-03],\n",
       "          [9.9981e-01, 9.2913e-05, 9.2993e-05],\n",
       "          [9.9896e-01, 5.2069e-04, 5.2210e-04],\n",
       "          [9.9827e-01, 8.6311e-04, 8.6526e-04],\n",
       "          [9.9850e-01, 7.4923e-04, 7.5049e-04],\n",
       "          [9.9877e-01, 6.1652e-04, 6.1703e-04],\n",
       "          [9.9928e-01, 3.6101e-04, 3.6132e-04],\n",
       "          [9.9988e-01, 6.1197e-05, 6.1253e-05],\n",
       "          [9.9931e-01, 3.4380e-04, 3.4447e-04],\n",
       "          [9.9867e-01, 6.6342e-04, 6.6492e-04],\n",
       "          [9.9967e-01, 1.6723e-04, 1.6729e-04],\n",
       "          [9.9930e-01, 3.4784e-04, 3.4834e-04],\n",
       "          [9.9966e-01, 1.7137e-04, 1.7156e-04],\n",
       "          [9.9887e-01, 5.6503e-04, 5.6653e-04],\n",
       "          [9.9936e-01, 3.2195e-04, 3.2258e-04],\n",
       "          [9.9421e-01, 2.9007e-03, 2.8933e-03],\n",
       "          [9.9931e-01, 3.4385e-04, 3.4445e-04],\n",
       "          [9.9908e-01, 4.5841e-04, 4.5829e-04],\n",
       "          [9.9974e-01, 1.3219e-04, 1.3227e-04],\n",
       "          [9.9800e-01, 9.9832e-04, 9.9728e-04],\n",
       "          [9.9999e-01, 7.0310e-06, 7.0319e-06],\n",
       "          [9.9903e-01, 4.8534e-04, 4.8631e-04],\n",
       "          [9.9955e-01, 2.2522e-04, 2.2526e-04],\n",
       "          [9.9897e-01, 5.1312e-04, 5.1332e-04],\n",
       "          [9.9871e-01, 6.4238e-04, 6.4317e-04],\n",
       "          [9.9657e-01, 1.7122e-03, 1.7130e-03],\n",
       "          [9.9999e-01, 5.7671e-06, 5.7675e-06],\n",
       "          [9.9767e-01, 1.1622e-03, 1.1648e-03],\n",
       "          [9.9626e-01, 1.8700e-03, 1.8739e-03],\n",
       "          [9.9810e-01, 9.4705e-04, 9.4933e-04],\n",
       "          [9.9897e-01, 5.1462e-04, 5.1632e-04],\n",
       "          [9.9955e-01, 2.2355e-04, 2.2402e-04],\n",
       "          [9.9824e-01, 8.8157e-04, 8.8275e-04],\n",
       "          [9.9868e-01, 6.6055e-04, 6.6074e-04],\n",
       "          [9.9825e-01, 8.7692e-04, 8.7799e-04],\n",
       "          [9.9807e-01, 9.6458e-04, 9.6665e-04],\n",
       "          [9.9976e-01, 1.2090e-04, 1.2110e-04],\n",
       "          [9.9736e-01, 1.3168e-03, 1.3204e-03],\n",
       "          [9.9808e-01, 9.6086e-04, 9.6404e-04],\n",
       "          [9.9928e-01, 3.6213e-04, 3.6261e-04],\n",
       "          [9.9820e-01, 8.9970e-04, 9.0170e-04],\n",
       "          [9.9891e-01, 5.4358e-04, 5.4354e-04],\n",
       "          [9.9722e-01, 1.3905e-03, 1.3895e-03],\n",
       "          [9.9865e-01, 6.7603e-04, 6.7677e-04],\n",
       "          [9.9829e-01, 8.5460e-04, 8.5700e-04],\n",
       "          [9.9800e-01, 9.9831e-04, 1.0003e-03],\n",
       "          [9.9749e-01, 1.2566e-03, 1.2581e-03],\n",
       "          [9.9621e-01, 1.8963e-03, 1.8956e-03],\n",
       "          [9.9894e-01, 5.3158e-04, 5.3266e-04],\n",
       "          [9.9805e-01, 9.7672e-04, 9.7559e-04],\n",
       "          [9.9979e-01, 1.0560e-04, 1.0566e-04],\n",
       "          [9.9987e-01, 6.3969e-05, 6.4009e-05],\n",
       "          [9.9882e-01, 5.8749e-04, 5.8861e-04],\n",
       "          [9.9966e-01, 1.7152e-04, 1.7156e-04],\n",
       "          [9.9844e-01, 7.8014e-04, 7.8254e-04],\n",
       "          [9.9821e-01, 8.9529e-04, 8.9678e-04],\n",
       "          [9.9642e-01, 1.7910e-03, 1.7899e-03],\n",
       "          [9.9815e-01, 9.2369e-04, 9.2536e-04],\n",
       "          [9.9852e-01, 7.3992e-04, 7.4177e-04],\n",
       "          [9.9201e-01, 3.9871e-03, 4.0047e-03],\n",
       "          [9.9737e-01, 1.3167e-03, 1.3166e-03],\n",
       "          [9.9686e-01, 1.5682e-03, 1.5700e-03],\n",
       "          [9.9811e-01, 9.4647e-04, 9.4755e-04],\n",
       "          [9.9779e-01, 1.1011e-03, 1.1057e-03],\n",
       "          [9.9572e-01, 2.1358e-03, 2.1439e-03],\n",
       "          [1.0000e+00, 1.3059e-09, 1.3059e-09],\n",
       "          [9.9965e-01, 1.7337e-04, 1.7337e-04],\n",
       "          [9.9749e-01, 1.2544e-03, 1.2522e-03],\n",
       "          [9.9869e-01, 6.5152e-04, 6.5387e-04],\n",
       "          [9.9668e-01, 1.6606e-03, 1.6621e-03]], grad_fn=<CopySlices>),\n",
       "  'mod': tensor([[1.0000e+00, 3.0105e-32, 3.0105e-32, 3.0105e-32, 3.0105e-32, 3.0105e-32],\n",
       "          [9.9883e-01, 2.3301e-04, 2.3272e-04, 2.3307e-04, 2.3129e-04, 2.3530e-04],\n",
       "          [9.8292e-01, 3.3853e-03, 3.4096e-03, 3.4441e-03, 3.3423e-03, 3.5028e-03],\n",
       "          [9.5226e-01, 9.4123e-03, 9.7142e-03, 9.6156e-03, 9.1410e-03, 9.8605e-03],\n",
       "          [9.3438e-01, 1.3171e-02, 1.3141e-02, 1.3281e-02, 1.2636e-02, 1.3391e-02],\n",
       "          [9.5198e-01, 9.5633e-03, 9.7530e-03, 9.6144e-03, 9.1966e-03, 9.8882e-03],\n",
       "          [9.7814e-01, 4.3420e-03, 4.3803e-03, 4.4019e-03, 4.2643e-03, 4.4719e-03],\n",
       "          [9.5757e-01, 8.4607e-03, 8.5252e-03, 8.5549e-03, 8.0948e-03, 8.7936e-03],\n",
       "          [9.3759e-01, 1.2409e-02, 1.2536e-02, 1.2639e-02, 1.1985e-02, 1.2842e-02],\n",
       "          [9.3365e-01, 1.3220e-02, 1.3280e-02, 1.3599e-02, 1.2589e-02, 1.3662e-02],\n",
       "          [9.4360e-01, 1.1293e-02, 1.1311e-02, 1.1474e-02, 1.0724e-02, 1.1595e-02],\n",
       "          [9.8612e-01, 2.8092e-03, 2.7751e-03, 2.7750e-03, 2.6969e-03, 2.8203e-03],\n",
       "          [9.3256e-01, 1.3534e-02, 1.3513e-02, 1.3739e-02, 1.2859e-02, 1.3796e-02],\n",
       "          [9.6867e-01, 6.2193e-03, 6.3363e-03, 6.2846e-03, 6.0409e-03, 6.4481e-03],\n",
       "          [9.3078e-01, 1.3737e-02, 1.3975e-02, 1.3942e-02, 1.3314e-02, 1.4251e-02],\n",
       "          [9.2149e-01, 1.5877e-02, 1.5570e-02, 1.6040e-02, 1.4936e-02, 1.6086e-02],\n",
       "          [9.9669e-01, 6.6021e-04, 6.6165e-04, 6.6426e-04, 6.5532e-04, 6.7279e-04],\n",
       "          [9.4883e-01, 1.0230e-02, 1.0258e-02, 1.0205e-02, 9.8913e-03, 1.0583e-02],\n",
       "          [9.3496e-01, 1.3062e-02, 1.3023e-02, 1.3136e-02, 1.2465e-02, 1.3350e-02],\n",
       "          [9.5435e-01, 9.1913e-03, 9.0992e-03, 9.1677e-03, 8.8019e-03, 9.3891e-03],\n",
       "          [9.6046e-01, 7.8499e-03, 7.9178e-03, 7.9521e-03, 7.6500e-03, 8.1684e-03],\n",
       "          [9.2151e-01, 1.5872e-02, 1.5564e-02, 1.6036e-02, 1.4933e-02, 1.6083e-02],\n",
       "          [9.2636e-01, 1.4586e-02, 1.4836e-02, 1.4797e-02, 1.4075e-02, 1.5347e-02],\n",
       "          [9.9693e-01, 6.1341e-04, 6.1462e-04, 6.1546e-04, 6.0429e-04, 6.2279e-04],\n",
       "          [1.0000e+00, 1.0583e-29, 1.0583e-29, 1.0583e-29, 1.0583e-29, 1.0583e-29],\n",
       "          [9.5327e-01, 9.4925e-03, 9.3250e-03, 9.4504e-03, 8.9584e-03, 9.5028e-03],\n",
       "          [1.0000e+00, 1.2178e-29, 1.2178e-29, 1.2178e-29, 1.2178e-29, 1.2178e-29],\n",
       "          [9.8314e-01, 3.3729e-03, 3.3657e-03, 3.3543e-03, 3.3063e-03, 3.4611e-03],\n",
       "          [9.5545e-01, 8.8764e-03, 9.0053e-03, 8.8816e-03, 8.5908e-03, 9.1968e-03],\n",
       "          [9.5078e-01, 9.7778e-03, 9.9894e-03, 9.8857e-03, 9.4082e-03, 1.0162e-02],\n",
       "          [9.3665e-01, 1.2710e-02, 1.2818e-02, 1.2639e-02, 1.2161e-02, 1.3023e-02],\n",
       "          [9.3128e-01, 1.3772e-02, 1.3676e-02, 1.3883e-02, 1.3310e-02, 1.4079e-02],\n",
       "          [9.2888e-01, 1.4308e-02, 1.4218e-02, 1.4394e-02, 1.3620e-02, 1.4583e-02],\n",
       "          [9.5709e-01, 8.6016e-03, 8.6514e-03, 8.5266e-03, 8.2735e-03, 8.8548e-03],\n",
       "          [9.5234e-01, 9.5362e-03, 9.5669e-03, 9.5649e-03, 9.1424e-03, 9.8521e-03],\n",
       "          [1.0000e+00, 4.0963e-32, 4.0963e-32, 4.0963e-32, 4.0963e-32, 4.0963e-32],\n",
       "          [9.5897e-01, 8.2441e-03, 8.1677e-03, 8.2977e-03, 7.9061e-03, 8.4178e-03],\n",
       "          [9.5370e-01, 9.2312e-03, 9.2294e-03, 9.3674e-03, 8.9685e-03, 9.5029e-03],\n",
       "          [9.6690e-01, 6.5606e-03, 6.5284e-03, 6.6430e-03, 6.4682e-03, 6.8961e-03],\n",
       "          [9.6287e-01, 7.3596e-03, 7.5206e-03, 7.4717e-03, 7.1227e-03, 7.6505e-03],\n",
       "          [9.3275e-01, 1.3461e-02, 1.3450e-02, 1.3464e-02, 1.2966e-02, 1.3905e-02],\n",
       "          [9.9873e-01, 2.5239e-04, 2.5315e-04, 2.5436e-04, 2.5086e-04, 2.5569e-04],\n",
       "          [9.6484e-01, 7.0022e-03, 7.0692e-03, 7.0582e-03, 6.7985e-03, 7.2270e-03],\n",
       "          [9.8923e-01, 2.1377e-03, 2.1504e-03, 2.1666e-03, 2.1232e-03, 2.1881e-03],\n",
       "          [9.4226e-01, 1.1538e-02, 1.1643e-02, 1.1634e-02, 1.1032e-02, 1.1897e-02],\n",
       "          [9.8685e-01, 2.6132e-03, 2.6598e-03, 2.6606e-03, 2.5588e-03, 2.6626e-03],\n",
       "          [9.5096e-01, 9.8527e-03, 9.8071e-03, 9.8775e-03, 9.4205e-03, 1.0079e-02],\n",
       "          [9.6576e-01, 6.8736e-03, 6.8415e-03, 6.9327e-03, 6.6065e-03, 6.9841e-03],\n",
       "          [9.3234e-01, 1.3554e-02, 1.3549e-02, 1.3576e-02, 1.3105e-02, 1.3878e-02],\n",
       "          [9.3680e-01, 1.2651e-02, 1.2826e-02, 1.2759e-02, 1.2064e-02, 1.2904e-02],\n",
       "          [9.8119e-01, 3.7761e-03, 3.7667e-03, 3.7555e-03, 3.6593e-03, 3.8491e-03],\n",
       "          [9.1176e-01, 1.7677e-02, 1.7623e-02, 1.7935e-02, 1.6915e-02, 1.8093e-02],\n",
       "          [9.4314e-01, 1.1264e-02, 1.1475e-02, 1.1487e-02, 1.0997e-02, 1.1638e-02],\n",
       "          [9.2676e-01, 1.4590e-02, 1.4612e-02, 1.5011e-02, 1.3956e-02, 1.5067e-02],\n",
       "          [9.5036e-01, 9.8616e-03, 1.0072e-02, 9.9704e-03, 9.4854e-03, 1.0249e-02],\n",
       "          [9.4953e-01, 9.9839e-03, 1.0056e-02, 1.0118e-02, 9.8170e-03, 1.0495e-02],\n",
       "          [9.4486e-01, 1.1051e-02, 1.1059e-02, 1.1151e-02, 1.0632e-02, 1.1251e-02],\n",
       "          [9.1415e-01, 1.7195e-02, 1.7066e-02, 1.7690e-02, 1.6244e-02, 1.7660e-02],\n",
       "          [9.9895e-01, 2.0911e-04, 2.0904e-04, 2.0984e-04, 2.0774e-04, 2.1084e-04],\n",
       "          [9.4425e-01, 1.1135e-02, 1.1195e-02, 1.1119e-02, 1.0738e-02, 1.1561e-02],\n",
       "          [9.2853e-01, 1.4322e-02, 1.4283e-02, 1.4509e-02, 1.3670e-02, 1.4687e-02],\n",
       "          [9.0021e-01, 1.9819e-02, 2.0159e-02, 2.0046e-02, 1.9254e-02, 2.0517e-02],\n",
       "          [9.4884e-01, 1.0124e-02, 1.0192e-02, 1.0267e-02, 9.8739e-03, 1.0705e-02],\n",
       "          [9.5110e-01, 9.8157e-03, 9.8586e-03, 9.7146e-03, 9.3979e-03, 1.0112e-02],\n",
       "          [1.0000e+00, 1.0828e-29, 1.0828e-29, 1.0828e-29, 1.0828e-29, 1.0828e-29],\n",
       "          [9.5350e-01, 9.2606e-03, 9.3463e-03, 9.3676e-03, 8.9274e-03, 9.6001e-03],\n",
       "          [9.2605e-01, 1.4622e-02, 1.5001e-02, 1.4913e-02, 1.4218e-02, 1.5191e-02],\n",
       "          [9.5732e-01, 8.5097e-03, 8.5049e-03, 8.6490e-03, 8.2225e-03, 8.7980e-03],\n",
       "          [9.4643e-01, 1.0732e-02, 1.0780e-02, 1.0685e-02, 1.0396e-02, 1.0983e-02],\n",
       "          [9.3094e-01, 1.3680e-02, 1.3840e-02, 1.3770e-02, 1.3447e-02, 1.4319e-02],\n",
       "          [9.6749e-01, 6.4676e-03, 6.4560e-03, 6.5699e-03, 6.3101e-03, 6.7026e-03],\n",
       "          [9.2445e-01, 1.5176e-02, 1.5112e-02, 1.5322e-02, 1.4490e-02, 1.5450e-02],\n",
       "          [9.3115e-01, 1.3866e-02, 1.3749e-02, 1.3934e-02, 1.3188e-02, 1.4113e-02],\n",
       "          [9.4121e-01, 1.1828e-02, 1.1716e-02, 1.1934e-02, 1.1170e-02, 1.2139e-02],\n",
       "          [9.8606e-01, 2.7708e-03, 2.7768e-03, 2.8067e-03, 2.7212e-03, 2.8627e-03],\n",
       "          [9.7111e-01, 5.7010e-03, 5.8347e-03, 5.8158e-03, 5.6395e-03, 5.8955e-03],\n",
       "          [9.8194e-01, 3.5799e-03, 3.6272e-03, 3.6478e-03, 3.5240e-03, 3.6843e-03],\n",
       "          [9.5086e-01, 9.7547e-03, 9.7402e-03, 9.8980e-03, 9.5644e-03, 1.0182e-02],\n",
       "          [9.5240e-01, 9.3729e-03, 9.6354e-03, 9.5542e-03, 9.2524e-03, 9.7888e-03],\n",
       "          [9.4237e-01, 1.1461e-02, 1.1658e-02, 1.1794e-02, 1.1038e-02, 1.1678e-02],\n",
       "          [9.5490e-01, 8.9763e-03, 9.0534e-03, 9.1412e-03, 8.5614e-03, 9.3665e-03],\n",
       "          [9.3291e-01, 1.3453e-02, 1.3413e-02, 1.3651e-02, 1.2814e-02, 1.3761e-02],\n",
       "          [9.5614e-01, 8.7543e-03, 8.7118e-03, 8.8522e-03, 8.5214e-03, 9.0233e-03],\n",
       "          [9.5679e-01, 8.6843e-03, 8.6240e-03, 8.6664e-03, 8.3362e-03, 8.8952e-03],\n",
       "          [9.8763e-01, 2.4798e-03, 2.4701e-03, 2.4565e-03, 2.4155e-03, 2.5479e-03],\n",
       "          [9.5722e-01, 8.5609e-03, 8.6125e-03, 8.6066e-03, 8.1604e-03, 8.8370e-03],\n",
       "          [9.4665e-01, 1.0625e-02, 1.0768e-02, 1.0634e-02, 1.0257e-02, 1.1070e-02],\n",
       "          [9.8022e-01, 3.9420e-03, 3.9760e-03, 3.9808e-03, 3.8216e-03, 4.0642e-03],\n",
       "          [9.4794e-01, 1.0440e-02, 1.0467e-02, 1.0438e-02, 9.9883e-03, 1.0731e-02],\n",
       "          [1.0000e+00, 4.0509e-32, 4.0509e-32, 4.0509e-32, 4.0509e-32, 4.0509e-32],\n",
       "          [9.2777e-01, 1.4314e-02, 1.4665e-02, 1.4613e-02, 1.3873e-02, 1.4769e-02],\n",
       "          [9.5778e-01, 8.4486e-03, 8.5038e-03, 8.4959e-03, 8.0565e-03, 8.7121e-03],\n",
       "          [9.3394e-01, 1.3133e-02, 1.3151e-02, 1.3395e-02, 1.2799e-02, 1.3586e-02],\n",
       "          [9.4711e-01, 1.0567e-02, 1.0660e-02, 1.0668e-02, 1.0117e-02, 1.0880e-02],\n",
       "          [9.6662e-01, 6.5943e-03, 6.6934e-03, 6.7320e-03, 6.5255e-03, 6.8308e-03],\n",
       "          [1.0000e+00, 1.8925e-19, 1.8925e-19, 1.8925e-19, 1.8925e-19, 1.8925e-19],\n",
       "          [9.5944e-01, 8.1055e-03, 8.1429e-03, 8.1705e-03, 7.8448e-03, 8.2991e-03],\n",
       "          [9.9231e-01, 1.5361e-03, 1.5348e-03, 1.5518e-03, 1.5067e-03, 1.5578e-03],\n",
       "          [9.3025e-01, 1.4020e-02, 1.3955e-02, 1.4102e-02, 1.3358e-02, 1.4310e-02],\n",
       "          [9.7277e-01, 5.3696e-03, 5.5001e-03, 5.4810e-03, 5.3091e-03, 5.5680e-03],\n",
       "          [9.5894e-01, 8.0939e-03, 8.2830e-03, 8.2897e-03, 8.0022e-03, 8.3913e-03],\n",
       "          [9.4952e-01, 1.0087e-02, 1.0119e-02, 1.0053e-02, 9.7757e-03, 1.0446e-02],\n",
       "          [9.3127e-01, 1.3764e-02, 1.3683e-02, 1.3880e-02, 1.3307e-02, 1.4095e-02],\n",
       "          [9.9015e-01, 1.9632e-03, 1.9681e-03, 1.9877e-03, 1.9373e-03, 1.9941e-03],\n",
       "          [9.4531e-01, 1.0920e-02, 1.1009e-02, 1.1012e-02, 1.0484e-02, 1.1270e-02],\n",
       "          [9.3300e-01, 1.3388e-02, 1.3526e-02, 1.3520e-02, 1.2863e-02, 1.3704e-02],\n",
       "          [9.3770e-01, 1.2463e-02, 1.2508e-02, 1.2396e-02, 1.2016e-02, 1.2917e-02],\n",
       "          [9.4953e-01, 1.0066e-02, 1.0185e-02, 1.0154e-02, 9.7525e-03, 1.0311e-02],\n",
       "          [9.6797e-01, 6.4002e-03, 6.4888e-03, 6.3529e-03, 6.1575e-03, 6.6355e-03],\n",
       "          [9.6022e-01, 7.9856e-03, 7.9899e-03, 8.0606e-03, 7.6233e-03, 8.1163e-03],\n",
       "          [9.5942e-01, 8.0564e-03, 8.0268e-03, 8.2260e-03, 7.9063e-03, 8.3642e-03],\n",
       "          [9.3583e-01, 1.2908e-02, 1.2728e-02, 1.3006e-02, 1.2268e-02, 1.3257e-02],\n",
       "          [9.2865e-01, 1.4268e-02, 1.4287e-02, 1.4593e-02, 1.3556e-02, 1.4647e-02],\n",
       "          [9.5534e-01, 8.8408e-03, 9.0509e-03, 9.0336e-03, 8.6130e-03, 9.1170e-03],\n",
       "          [9.3020e-01, 1.3969e-02, 1.4013e-02, 1.4073e-02, 1.3419e-02, 1.4328e-02],\n",
       "          [9.3919e-01, 1.2070e-02, 1.2369e-02, 1.2230e-02, 1.1714e-02, 1.2427e-02],\n",
       "          [9.4729e-01, 1.0535e-02, 1.0601e-02, 1.0594e-02, 1.0109e-02, 1.0868e-02],\n",
       "          [9.3673e-01, 1.2699e-02, 1.2663e-02, 1.2875e-02, 1.2079e-02, 1.2952e-02],\n",
       "          [9.7044e-01, 5.8814e-03, 5.8984e-03, 5.9413e-03, 5.7333e-03, 6.1056e-03],\n",
       "          [9.6124e-01, 7.7388e-03, 7.6803e-03, 7.8253e-03, 7.5432e-03, 7.9764e-03],\n",
       "          [9.4992e-01, 1.0041e-02, 1.0056e-02, 1.0150e-02, 9.6703e-03, 1.0159e-02],\n",
       "          [9.2856e-01, 1.4221e-02, 1.4360e-02, 1.4497e-02, 1.3697e-02, 1.4670e-02],\n",
       "          [9.2831e-01, 1.4301e-02, 1.4390e-02, 1.4530e-02, 1.3763e-02, 1.4703e-02],\n",
       "          [9.2524e-01, 1.4981e-02, 1.4900e-02, 1.5257e-02, 1.4243e-02, 1.5382e-02],\n",
       "          [9.3421e-01, 1.3140e-02, 1.3038e-02, 1.3276e-02, 1.2762e-02, 1.3570e-02],\n",
       "          [9.2909e-01, 1.4260e-02, 1.4181e-02, 1.4345e-02, 1.3580e-02, 1.4541e-02],\n",
       "          [9.7335e-01, 5.2764e-03, 5.3075e-03, 5.3882e-03, 5.1549e-03, 5.5205e-03],\n",
       "          [9.7388e-01, 5.2136e-03, 5.2618e-03, 5.2649e-03, 5.0651e-03, 5.3108e-03],\n",
       "          [9.9119e-01, 1.7574e-03, 1.7550e-03, 1.7741e-03, 1.7267e-03, 1.8015e-03],\n",
       "          [9.4463e-01, 1.1133e-02, 1.1075e-02, 1.1171e-02, 1.0639e-02, 1.1349e-02],\n",
       "          [9.6497e-01, 6.9503e-03, 7.0124e-03, 7.0131e-03, 6.8597e-03, 7.1934e-03],\n",
       "          [9.5946e-01, 8.0376e-03, 8.2073e-03, 8.1649e-03, 7.7629e-03, 8.3674e-03],\n",
       "          [9.4613e-01, 1.0758e-02, 1.0743e-02, 1.0894e-02, 1.0401e-02, 1.1073e-02],\n",
       "          [9.4480e-01, 1.0993e-02, 1.0989e-02, 1.1161e-02, 1.0627e-02, 1.1426e-02],\n",
       "          [9.2404e-01, 1.5290e-02, 1.5129e-02, 1.5385e-02, 1.4555e-02, 1.5597e-02],\n",
       "          [9.3892e-01, 1.2096e-02, 1.2342e-02, 1.2243e-02, 1.1737e-02, 1.2659e-02],\n",
       "          [9.3805e-01, 1.2278e-02, 1.2519e-02, 1.2435e-02, 1.1932e-02, 1.2784e-02],\n",
       "          [9.5129e-01, 9.7757e-03, 9.7059e-03, 9.8493e-03, 9.3622e-03, 1.0021e-02],\n",
       "          [9.3527e-01, 1.2793e-02, 1.2942e-02, 1.3036e-02, 1.2581e-02, 1.3376e-02],\n",
       "          [9.3416e-01, 1.3202e-02, 1.3172e-02, 1.3369e-02, 1.2661e-02, 1.3439e-02],\n",
       "          [9.4708e-01, 1.0515e-02, 1.0712e-02, 1.0686e-02, 1.0108e-02, 1.0899e-02],\n",
       "          [9.2978e-01, 1.3828e-02, 1.4365e-02, 1.4126e-02, 1.3362e-02, 1.4535e-02],\n",
       "          [9.9799e-01, 4.0173e-04, 3.9846e-04, 4.0203e-04, 3.9522e-04, 4.0880e-04],\n",
       "          [9.8235e-01, 3.5177e-03, 3.5173e-03, 3.5573e-03, 3.4584e-03, 3.6004e-03],\n",
       "          [9.5723e-01, 8.5428e-03, 8.5442e-03, 8.5112e-03, 8.3603e-03, 8.8160e-03],\n",
       "          [9.2001e-01, 1.5787e-02, 1.6277e-02, 1.6194e-02, 1.5432e-02, 1.6305e-02],\n",
       "          [9.4821e-01, 1.0416e-02, 1.0306e-02, 1.0539e-02, 9.8923e-03, 1.0639e-02]],\n",
       "         grad_fn=<CopySlices>),\n",
       "  'cc': tensor([[9.1766e-03, 9.7494e-01, 8.3038e-03, 7.5826e-03],\n",
       "          [2.0960e-03, 9.9398e-01, 1.9926e-03, 1.9335e-03],\n",
       "          [1.4837e-02, 9.5905e-01, 1.3554e-02, 1.2559e-02],\n",
       "          [4.6107e-02, 8.7984e-01, 3.9111e-02, 3.4941e-02],\n",
       "          [3.9627e-02, 8.9459e-01, 3.4918e-02, 3.0867e-02],\n",
       "          [4.2083e-02, 8.8937e-01, 3.6468e-02, 3.2080e-02],\n",
       "          [1.2563e-02, 9.6534e-01, 1.1344e-02, 1.0755e-02],\n",
       "          [3.5225e-02, 9.0686e-01, 3.0523e-02, 2.7389e-02],\n",
       "          [2.3994e-02, 9.3484e-01, 2.1747e-02, 1.9423e-02],\n",
       "          [3.8093e-02, 8.9890e-01, 3.3275e-02, 2.9733e-02],\n",
       "          [3.8450e-02, 8.9808e-01, 3.3607e-02, 2.9862e-02],\n",
       "          [5.7060e-03, 9.8389e-01, 5.3493e-03, 5.0529e-03],\n",
       "          [4.2913e-02, 8.8603e-01, 3.7704e-02, 3.3357e-02],\n",
       "          [3.3407e-02, 9.1192e-01, 2.8941e-02, 2.5735e-02],\n",
       "          [6.0423e-02, 8.3968e-01, 5.3350e-02, 4.6546e-02],\n",
       "          [4.0451e-02, 8.9148e-01, 3.6127e-02, 3.1945e-02],\n",
       "          [2.3378e-03, 9.9329e-01, 2.2194e-03, 2.1577e-03],\n",
       "          [4.4546e-02, 8.8232e-01, 3.8817e-02, 3.4316e-02],\n",
       "          [3.1938e-02, 9.1448e-01, 2.8247e-02, 2.5333e-02],\n",
       "          [3.1970e-02, 9.1436e-01, 2.8152e-02, 2.5516e-02],\n",
       "          [3.7594e-02, 8.9921e-01, 3.3474e-02, 2.9724e-02],\n",
       "          [4.0612e-02, 8.9106e-01, 3.6267e-02, 3.2057e-02],\n",
       "          [6.1712e-02, 8.3758e-01, 5.3637e-02, 4.7071e-02],\n",
       "          [5.6814e-03, 9.8419e-01, 5.1628e-03, 4.9647e-03],\n",
       "          [2.2081e-04, 9.9935e-01, 2.1471e-04, 2.1297e-04],\n",
       "          [3.5425e-02, 9.0355e-01, 3.2380e-02, 2.8641e-02],\n",
       "          [2.4118e-04, 9.9929e-01, 2.3466e-04, 2.3255e-04],\n",
       "          [1.6142e-02, 9.5549e-01, 1.4725e-02, 1.3640e-02],\n",
       "          [5.0921e-02, 8.6779e-01, 4.3236e-02, 3.8055e-02],\n",
       "          [4.2737e-02, 8.8799e-01, 3.6889e-02, 3.2385e-02],\n",
       "          [3.3816e-02, 9.1023e-01, 2.9456e-02, 2.6501e-02],\n",
       "          [5.6310e-02, 8.5176e-01, 4.9122e-02, 4.2804e-02],\n",
       "          [3.7778e-02, 8.9917e-01, 3.3410e-02, 2.9646e-02],\n",
       "          [3.4622e-02, 9.0838e-01, 2.9980e-02, 2.7016e-02],\n",
       "          [1.5823e-02, 9.5635e-01, 1.4580e-02, 1.3250e-02],\n",
       "          [8.9965e-03, 9.7523e-01, 8.1838e-03, 7.5881e-03],\n",
       "          [5.1466e-02, 8.6530e-01, 4.4096e-02, 3.9136e-02],\n",
       "          [2.3278e-02, 9.3526e-01, 2.2029e-02, 1.9429e-02],\n",
       "          [4.0662e-02, 8.9028e-01, 3.6332e-02, 3.2729e-02],\n",
       "          [3.2900e-02, 9.1333e-01, 2.8448e-02, 2.5321e-02],\n",
       "          [4.4216e-02, 8.8354e-01, 3.8224e-02, 3.4024e-02],\n",
       "          [1.4734e-03, 9.9579e-01, 1.3860e-03, 1.3499e-03],\n",
       "          [3.8082e-02, 8.9835e-01, 3.3425e-02, 3.0142e-02],\n",
       "          [5.8501e-03, 9.8336e-01, 5.5499e-03, 5.2363e-03],\n",
       "          [3.6455e-02, 9.0351e-01, 3.1735e-02, 2.8297e-02],\n",
       "          [1.3870e-02, 9.6233e-01, 1.2380e-02, 1.1425e-02],\n",
       "          [4.1894e-02, 8.8839e-01, 3.7065e-02, 3.2652e-02],\n",
       "          [2.4357e-02, 9.3388e-01, 2.1877e-02, 1.9882e-02],\n",
       "          [4.4218e-02, 8.8256e-01, 3.9053e-02, 3.4174e-02],\n",
       "          [4.4908e-02, 8.8100e-01, 3.9501e-02, 3.4594e-02],\n",
       "          [1.2557e-02, 9.6548e-01, 1.1322e-02, 1.0645e-02],\n",
       "          [5.3209e-02, 8.5926e-01, 4.6714e-02, 4.0822e-02],\n",
       "          [5.1691e-02, 8.5896e-01, 4.8107e-02, 4.1246e-02],\n",
       "          [3.9841e-02, 8.9426e-01, 3.5007e-02, 3.0892e-02],\n",
       "          [4.3792e-02, 8.8523e-01, 3.7823e-02, 3.3152e-02],\n",
       "          [5.3333e-02, 8.5869e-01, 4.6658e-02, 4.1323e-02],\n",
       "          [3.7231e-02, 9.0111e-01, 3.2428e-02, 2.9228e-02],\n",
       "          [4.5086e-02, 8.7929e-01, 4.0086e-02, 3.5541e-02],\n",
       "          [1.8562e-03, 9.9471e-01, 1.7404e-03, 1.6969e-03],\n",
       "          [4.2189e-02, 8.8875e-01, 3.6593e-02, 3.2465e-02],\n",
       "          [3.8758e-02, 8.9670e-01, 3.4256e-02, 3.0290e-02],\n",
       "          [4.3031e-02, 8.8391e-01, 3.8571e-02, 3.4488e-02],\n",
       "          [5.3167e-02, 8.5649e-01, 4.8315e-02, 4.2028e-02],\n",
       "          [2.2782e-02, 9.3859e-01, 2.0152e-02, 1.8471e-02],\n",
       "          [2.4005e-04, 9.9930e-01, 2.3277e-04, 2.3077e-04],\n",
       "          [3.0813e-02, 9.1811e-01, 2.6910e-02, 2.4164e-02],\n",
       "          [5.3662e-02, 8.5657e-01, 4.7878e-02, 4.1892e-02],\n",
       "          [5.4641e-02, 8.5560e-01, 4.7385e-02, 4.2373e-02],\n",
       "          [4.4161e-02, 8.8302e-01, 3.8904e-02, 3.3910e-02],\n",
       "          [5.3436e-02, 8.5680e-01, 4.8027e-02, 4.1741e-02],\n",
       "          [4.4093e-02, 8.8337e-01, 3.8114e-02, 3.4418e-02],\n",
       "          [4.4984e-02, 8.7949e-01, 4.0243e-02, 3.5279e-02],\n",
       "          [3.8924e-02, 8.9611e-01, 3.4469e-02, 3.0496e-02],\n",
       "          [3.7564e-02, 8.9965e-01, 3.3187e-02, 2.9595e-02],\n",
       "          [1.5717e-02, 9.5643e-01, 1.4524e-02, 1.3334e-02],\n",
       "          [2.0467e-02, 9.4379e-01, 1.8681e-02, 1.7057e-02],\n",
       "          [1.6302e-02, 9.5546e-01, 1.4545e-02, 1.3689e-02],\n",
       "          [6.1705e-02, 8.3838e-01, 5.2779e-02, 4.7139e-02],\n",
       "          [1.7097e-02, 9.5285e-01, 1.5453e-02, 1.4601e-02],\n",
       "          [3.4661e-02, 9.0513e-01, 3.1669e-02, 2.8543e-02],\n",
       "          [4.3598e-02, 8.8540e-01, 3.7657e-02, 3.3348e-02],\n",
       "          [3.7203e-02, 8.9992e-01, 3.3580e-02, 2.9298e-02],\n",
       "          [3.5408e-02, 9.0419e-01, 3.1778e-02, 2.8623e-02],\n",
       "          [2.9886e-02, 9.1965e-01, 2.6452e-02, 2.4008e-02],\n",
       "          [1.2171e-02, 9.6615e-01, 1.1252e-02, 1.0431e-02],\n",
       "          [3.2662e-02, 9.1356e-01, 2.8256e-02, 2.5524e-02],\n",
       "          [4.0080e-02, 8.9385e-01, 3.4733e-02, 3.1333e-02],\n",
       "          [2.2520e-02, 9.3986e-01, 1.9488e-02, 1.8128e-02],\n",
       "          [3.3014e-02, 9.1231e-01, 2.8803e-02, 2.5874e-02],\n",
       "          [1.7382e-02, 9.5304e-01, 1.5654e-02, 1.3924e-02],\n",
       "          [4.0056e-02, 8.9274e-01, 3.5434e-02, 3.1771e-02],\n",
       "          [3.1764e-02, 9.1586e-01, 2.7496e-02, 2.4880e-02],\n",
       "          [6.9935e-02, 8.1645e-01, 6.0428e-02, 5.3191e-02],\n",
       "          [2.9806e-02, 9.2077e-01, 2.5968e-02, 2.3457e-02],\n",
       "          [2.9775e-02, 9.1963e-01, 2.6620e-02, 2.3980e-02],\n",
       "          [1.3331e-02, 9.6270e-01, 1.2399e-02, 1.1574e-02],\n",
       "          [4.0280e-02, 8.9339e-01, 3.5191e-02, 3.1142e-02],\n",
       "          [3.4915e-03, 9.9000e-01, 3.3124e-03, 3.1921e-03],\n",
       "          [3.5476e-02, 9.0523e-01, 3.1354e-02, 2.7942e-02],\n",
       "          [2.1414e-02, 9.4157e-01, 1.9254e-02, 1.7760e-02],\n",
       "          [3.2471e-02, 9.1216e-01, 2.9140e-02, 2.6227e-02],\n",
       "          [3.7757e-02, 9.0002e-01, 3.2953e-02, 2.9269e-02],\n",
       "          [5.5556e-02, 8.5368e-01, 4.8487e-02, 4.2278e-02],\n",
       "          [3.3213e-03, 9.9051e-01, 3.1505e-03, 3.0218e-03],\n",
       "          [4.8363e-02, 8.7202e-01, 4.2614e-02, 3.6998e-02],\n",
       "          [5.1986e-02, 8.6293e-01, 4.5482e-02, 3.9599e-02],\n",
       "          [4.5472e-02, 8.8015e-01, 3.9504e-02, 3.4876e-02],\n",
       "          [3.3109e-02, 9.1033e-01, 3.0175e-02, 2.6382e-02],\n",
       "          [2.6721e-02, 9.2971e-01, 2.2679e-02, 2.0886e-02],\n",
       "          [4.3282e-02, 8.8476e-01, 3.8099e-02, 3.3858e-02],\n",
       "          [3.4766e-02, 9.0614e-01, 3.1001e-02, 2.8090e-02],\n",
       "          [4.5375e-02, 8.7927e-01, 4.0211e-02, 3.5146e-02],\n",
       "          [4.4475e-02, 8.8230e-01, 3.8845e-02, 3.4382e-02],\n",
       "          [1.6937e-02, 9.5294e-01, 1.5783e-02, 1.4335e-02],\n",
       "          [4.9461e-02, 8.6877e-01, 4.3593e-02, 3.8174e-02],\n",
       "          [4.8841e-02, 8.7005e-01, 4.3244e-02, 3.7863e-02],\n",
       "          [3.2004e-02, 9.1512e-01, 2.7839e-02, 2.5033e-02],\n",
       "          [4.4210e-02, 8.8268e-01, 3.8823e-02, 3.4289e-02],\n",
       "          [3.4522e-02, 9.0734e-01, 3.0405e-02, 2.7728e-02],\n",
       "          [5.1364e-02, 8.6416e-01, 4.4396e-02, 4.0083e-02],\n",
       "          [3.6399e-02, 9.0206e-01, 3.2719e-02, 2.8826e-02],\n",
       "          [3.9203e-02, 8.9320e-01, 3.6285e-02, 3.1307e-02],\n",
       "          [4.2569e-02, 8.8662e-01, 3.7627e-02, 3.3181e-02],\n",
       "          [4.9533e-02, 8.6878e-01, 4.3687e-02, 3.8000e-02],\n",
       "          [5.8716e-02, 8.4448e-01, 5.1741e-02, 4.5063e-02],\n",
       "          [3.6935e-02, 9.0139e-01, 3.2653e-02, 2.9019e-02],\n",
       "          [3.7969e-02, 8.9893e-01, 3.3082e-02, 3.0015e-02],\n",
       "          [1.8064e-02, 9.5116e-01, 1.6021e-02, 1.4755e-02],\n",
       "          [1.2867e-02, 9.6456e-01, 1.1642e-02, 1.0934e-02],\n",
       "          [3.7709e-02, 8.9928e-01, 3.3485e-02, 2.9526e-02],\n",
       "          [2.1013e-02, 9.4301e-01, 1.8839e-02, 1.7140e-02],\n",
       "          [3.8342e-02, 8.9920e-01, 3.3187e-02, 2.9270e-02],\n",
       "          [3.9559e-02, 8.9089e-01, 3.6908e-02, 3.2641e-02],\n",
       "          [5.4518e-02, 8.5605e-01, 4.7939e-02, 4.1491e-02],\n",
       "          [4.3124e-02, 8.8501e-01, 3.8104e-02, 3.3759e-02],\n",
       "          [4.0222e-02, 8.9270e-01, 3.5479e-02, 3.1601e-02],\n",
       "          [7.2541e-02, 8.0842e-01, 6.3760e-02, 5.5283e-02],\n",
       "          [5.0292e-02, 8.6782e-01, 4.3291e-02, 3.8596e-02],\n",
       "          [5.4269e-02, 8.5337e-01, 4.9052e-02, 4.3312e-02],\n",
       "          [4.0779e-02, 8.9170e-01, 3.5866e-02, 3.1657e-02],\n",
       "          [4.9025e-02, 8.7191e-01, 4.1601e-02, 3.7461e-02],\n",
       "          [6.0519e-02, 8.4347e-01, 5.1002e-02, 4.5006e-02],\n",
       "          [7.7884e-05, 9.9977e-01, 7.5373e-05, 7.4996e-05],\n",
       "          [2.0125e-02, 9.4550e-01, 1.8043e-02, 1.6336e-02],\n",
       "          [4.6023e-02, 8.7729e-01, 4.0313e-02, 3.6370e-02],\n",
       "          [3.5183e-02, 9.0293e-01, 3.2747e-02, 2.9145e-02],\n",
       "          [5.3987e-02, 8.5788e-01, 4.6820e-02, 4.1309e-02]],\n",
       "         grad_fn=<CopySlices>),\n",
       "  'dlt1': tensor([[3.5269e-11, 5.2388e-09, 6.7675e-12, 2.8862e-12, 1.8815e-08],\n",
       "          [4.6342e-02, 9.3984e-02, 1.3766e-01, 1.3566e-01, 2.1164e-01],\n",
       "          [7.8307e-02, 2.6356e-01, 1.1950e-01, 1.1773e-01, 2.7086e-01],\n",
       "          [1.9957e-01, 1.9636e-01, 2.0712e-01, 7.0206e-02, 2.0403e-01],\n",
       "          [1.3905e-01, 3.2097e-01, 2.4859e-01, 1.4321e-01, 2.2690e-01],\n",
       "          [1.7228e-01, 3.1670e-01, 1.7394e-01, 6.8475e-02, 1.9693e-01],\n",
       "          [1.1403e-01, 1.5607e-01, 1.0982e-01, 1.3172e-01, 1.4041e-01],\n",
       "          [2.0887e-01, 2.0460e-01, 1.6770e-01, 9.3031e-02, 1.4483e-01],\n",
       "          [1.5881e-01, 4.2158e-01, 2.2901e-01, 1.1805e-01, 2.1699e-01],\n",
       "          [2.1394e-01, 2.4886e-01, 1.5724e-01, 8.0031e-02, 1.9077e-01],\n",
       "          [2.4551e-01, 1.9536e-01, 1.6029e-01, 1.6862e-01, 1.8215e-01],\n",
       "          [2.9014e-01, 2.0008e-01, 1.5131e-01, 1.0769e-01, 1.6704e-01],\n",
       "          [2.3256e-01, 2.7178e-01, 1.4632e-01, 1.0471e-01, 2.6250e-01],\n",
       "          [1.5724e-01, 2.2487e-01, 1.5489e-01, 4.6371e-02, 1.1670e-01],\n",
       "          [1.2957e-01, 2.9049e-01, 2.3611e-01, 9.7022e-02, 2.0452e-01],\n",
       "          [2.7042e-01, 3.3915e-01, 2.2677e-01, 1.4830e-01, 2.5941e-01],\n",
       "          [4.8161e-02, 3.1381e-01, 1.0204e-01, 2.6908e-02, 7.7216e-02],\n",
       "          [1.2134e-01, 2.8364e-01, 1.6159e-01, 6.9543e-02, 1.8543e-01],\n",
       "          [2.4322e-01, 2.5958e-01, 1.6836e-01, 1.5936e-01, 2.0196e-01],\n",
       "          [1.3318e-01, 2.2676e-01, 2.4932e-01, 2.1409e-01, 2.3510e-01],\n",
       "          [1.9664e-01, 2.0868e-01, 2.2423e-01, 7.8969e-02, 1.6571e-01],\n",
       "          [2.7110e-01, 3.3781e-01, 2.2603e-01, 1.4826e-01, 2.6076e-01],\n",
       "          [1.5494e-01, 2.9113e-01, 2.0073e-01, 1.1979e-01, 2.1464e-01],\n",
       "          [9.6429e-02, 7.9347e-02, 8.4894e-02, 3.9995e-02, 3.3071e-01],\n",
       "          [4.4713e-11, 6.0357e-08, 8.9446e-11, 2.8969e-13, 1.6906e-08],\n",
       "          [2.2866e-01, 2.3223e-01, 1.5231e-01, 1.4317e-01, 3.4012e-01],\n",
       "          [6.9224e-11, 4.1624e-08, 9.6994e-11, 6.1577e-13, 2.3154e-08],\n",
       "          [1.2495e-01, 1.5017e-01, 7.4629e-02, 2.2743e-01, 2.8127e-01],\n",
       "          [2.3318e-01, 1.9495e-01, 1.5274e-01, 1.1278e-01, 2.0013e-01],\n",
       "          [1.6869e-01, 3.0564e-01, 1.8342e-01, 6.0691e-02, 1.6735e-01],\n",
       "          [2.0689e-01, 2.8765e-01, 2.0106e-01, 1.0639e-01, 1.8390e-01],\n",
       "          [1.4221e-01, 2.4402e-01, 1.8186e-01, 1.6118e-01, 2.4448e-01],\n",
       "          [2.6195e-01, 2.4871e-01, 1.7160e-01, 1.6848e-01, 2.2169e-01],\n",
       "          [2.4252e-01, 2.5041e-01, 2.0279e-01, 1.2429e-01, 1.3546e-01],\n",
       "          [1.7930e-01, 3.0336e-01, 1.8959e-01, 1.2438e-01, 1.8691e-01],\n",
       "          [4.8056e-11, 3.9704e-09, 6.5072e-12, 2.5954e-12, 1.5088e-08],\n",
       "          [1.4344e-01, 1.5812e-01, 1.0163e-01, 1.7002e-01, 2.4363e-01],\n",
       "          [2.0146e-01, 1.9341e-01, 1.3527e-01, 1.7771e-01, 2.0208e-01],\n",
       "          [1.7718e-01, 7.5688e-02, 1.1762e-01, 2.1778e-01, 1.7523e-01],\n",
       "          [1.4818e-01, 2.8508e-01, 1.6766e-01, 4.4555e-02, 1.3651e-01],\n",
       "          [1.1097e-01, 2.8560e-01, 2.3703e-01, 1.0573e-01, 1.5435e-01],\n",
       "          [1.0999e-01, 5.6059e-02, 3.2978e-01, 3.8843e-02, 1.6207e-01],\n",
       "          [1.1163e-01, 1.9082e-01, 2.2191e-01, 7.8218e-02, 1.7095e-01],\n",
       "          [1.5478e-01, 1.5732e-01, 1.0390e-01, 1.7429e-01, 2.2233e-01],\n",
       "          [1.9014e-01, 3.1252e-01, 1.7944e-01, 8.4649e-02, 2.0103e-01],\n",
       "          [8.8303e-02, 2.2081e-01, 1.8707e-01, 3.6881e-02, 1.0608e-01],\n",
       "          [1.6901e-01, 3.0020e-01, 2.4750e-01, 8.0891e-02, 2.0800e-01],\n",
       "          [1.8426e-01, 1.9572e-01, 1.6245e-01, 1.6631e-01, 2.5593e-01],\n",
       "          [1.9322e-01, 2.5331e-01, 1.7730e-01, 1.4840e-01, 1.8793e-01],\n",
       "          [2.0432e-01, 3.1178e-01, 1.6798e-01, 1.1019e-01, 2.4946e-01],\n",
       "          [1.0202e-01, 2.0006e-01, 1.8824e-01, 1.0375e-01, 2.4075e-01],\n",
       "          [1.5671e-01, 3.1029e-01, 2.5216e-01, 1.5133e-01, 2.2159e-01],\n",
       "          [1.6292e-01, 3.7064e-01, 2.3208e-01, 1.7311e-01, 2.6306e-01],\n",
       "          [2.1809e-01, 2.7134e-01, 1.7690e-01, 1.0058e-01, 2.1376e-01],\n",
       "          [1.7131e-01, 3.0258e-01, 1.8337e-01, 6.1207e-02, 1.7047e-01],\n",
       "          [1.8252e-01, 1.4787e-01, 1.1441e-01, 9.9257e-02, 2.1927e-01],\n",
       "          [1.0774e-01, 3.1731e-01, 2.4709e-01, 1.1067e-01, 1.8035e-01],\n",
       "          [2.4321e-01, 2.7629e-01, 1.8912e-01, 1.0972e-01, 2.4524e-01],\n",
       "          [1.7416e-02, 1.8637e-01, 3.1347e-01, 4.8614e-02, 1.5047e-01],\n",
       "          [1.1216e-01, 2.9951e-01, 1.9318e-01, 6.1528e-02, 1.3872e-01],\n",
       "          [2.5822e-01, 2.3497e-01, 1.6862e-01, 1.4436e-01, 1.9705e-01],\n",
       "          [1.9118e-01, 2.8754e-01, 2.0595e-01, 1.2424e-01, 2.2705e-01],\n",
       "          [1.7784e-01, 1.5591e-01, 9.5256e-02, 1.3189e-01, 2.1506e-01],\n",
       "          [3.1739e-01, 2.9522e-01, 1.7762e-01, 1.2557e-01, 1.5783e-01],\n",
       "          [5.3104e-11, 3.1608e-08, 7.1519e-11, 4.9026e-13, 1.8662e-08],\n",
       "          [2.0654e-01, 2.2806e-01, 1.7253e-01, 6.8404e-02, 1.3411e-01],\n",
       "          [2.9107e-01, 2.0842e-01, 1.4157e-01, 1.3009e-01, 2.6929e-01],\n",
       "          [1.4412e-01, 1.4705e-01, 1.0969e-01, 1.5436e-01, 2.2390e-01],\n",
       "          [1.7512e-01, 2.4216e-01, 1.6243e-01, 1.1056e-01, 1.7852e-01],\n",
       "          [1.8086e-01, 2.2759e-01, 1.0995e-01, 1.8005e-01, 2.4530e-01],\n",
       "          [1.7524e-01, 1.0871e-01, 8.2735e-02, 1.4758e-01, 2.1516e-01],\n",
       "          [1.9054e-01, 3.5910e-01, 2.7377e-01, 1.4745e-01, 2.2178e-01],\n",
       "          [2.6831e-01, 2.3316e-01, 1.6673e-01, 1.6882e-01, 2.3304e-01],\n",
       "          [3.3262e-01, 2.2994e-01, 2.0213e-01, 2.0246e-01, 2.1201e-01],\n",
       "          [8.9716e-02, 1.7074e-01, 1.2294e-01, 9.3808e-02, 1.9238e-01],\n",
       "          [1.3072e-01, 1.8492e-01, 1.1465e-01, 6.4821e-02, 1.2914e-01],\n",
       "          [2.1918e-01, 1.6822e-01, 2.1898e-01, 6.9136e-02, 1.9009e-01],\n",
       "          [1.9342e-01, 1.2679e-01, 9.8911e-02, 1.1053e-01, 2.4682e-01],\n",
       "          [1.4182e-01, 2.6826e-01, 2.1885e-01, 1.0826e-01, 2.0475e-01],\n",
       "          [2.4169e-01, 2.6366e-01, 1.2746e-01, 1.1525e-01, 3.8634e-01],\n",
       "          [1.8919e-01, 2.4691e-01, 1.7357e-01, 8.6112e-02, 1.7248e-01],\n",
       "          [1.8992e-01, 2.7223e-01, 1.7421e-01, 2.3408e-01, 3.0651e-01],\n",
       "          [1.6976e-01, 1.3986e-01, 1.1811e-01, 1.4562e-01, 2.5036e-01],\n",
       "          [1.2824e-01, 2.3934e-01, 2.5217e-01, 2.0500e-01, 2.2642e-01],\n",
       "          [1.4596e-01, 2.9021e-01, 1.2845e-01, 2.2861e-01, 1.5010e-01],\n",
       "          [2.0687e-01, 2.3125e-01, 1.6927e-01, 1.0896e-01, 1.5704e-01],\n",
       "          [1.5138e-01, 2.5755e-01, 2.0680e-01, 1.2691e-01, 2.1834e-01],\n",
       "          [1.5859e-01, 1.5403e-01, 1.4209e-01, 8.2903e-02, 1.5774e-01],\n",
       "          [2.1871e-01, 2.5310e-01, 1.7520e-01, 9.4749e-02, 1.7725e-01],\n",
       "          [5.1294e-11, 4.1246e-09, 6.8067e-12, 5.4996e-12, 1.3372e-08],\n",
       "          [3.5730e-01, 2.8424e-01, 1.6183e-01, 1.3512e-01, 2.9885e-01],\n",
       "          [2.0304e-01, 2.3647e-01, 1.6926e-01, 1.0753e-01, 1.5329e-01],\n",
       "          [1.4080e-01, 2.1470e-01, 1.7288e-01, 1.4273e-01, 2.2996e-01],\n",
       "          [1.7719e-01, 3.3200e-01, 1.8228e-01, 8.3066e-02, 1.7453e-01],\n",
       "          [1.6197e-01, 1.4294e-01, 1.3058e-01, 5.2381e-02, 1.3433e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.4306e-01, 1.4355e-01, 1.1201e-01, 1.1601e-01, 1.5435e-01],\n",
       "          [6.1656e-02, 1.3687e-01, 1.1163e-01, 7.1868e-02, 1.9823e-01],\n",
       "          [2.5388e-01, 2.5537e-01, 1.7120e-01, 1.6547e-01, 2.1297e-01],\n",
       "          [1.1072e-01, 2.1035e-01, 2.0958e-01, 8.7702e-02, 1.4365e-01],\n",
       "          [1.4119e-01, 1.9243e-01, 1.4031e-01, 5.1622e-02, 1.4431e-01],\n",
       "          [1.2553e-01, 2.2763e-01, 1.7993e-01, 6.3392e-02, 1.0854e-01],\n",
       "          [1.4035e-01, 2.4670e-01, 1.8538e-01, 1.6021e-01, 2.3976e-01],\n",
       "          [9.4361e-02, 1.0473e-01, 7.6673e-02, 9.3631e-02, 2.0933e-01],\n",
       "          [2.3606e-01, 1.9989e-01, 1.5783e-01, 1.0722e-01, 1.9445e-01],\n",
       "          [1.4470e-01, 3.2142e-01, 2.3801e-01, 1.0816e-01, 2.0802e-01],\n",
       "          [1.1766e-01, 3.1307e-01, 1.8774e-01, 7.2050e-02, 1.7265e-01],\n",
       "          [1.8981e-01, 3.1918e-01, 1.5846e-01, 1.3389e-01, 2.4217e-01],\n",
       "          [2.6992e-01, 2.4081e-01, 1.6120e-01, 6.2792e-02, 1.1583e-01],\n",
       "          [2.8761e-01, 2.0626e-01, 1.6221e-01, 1.1808e-01, 2.2942e-01],\n",
       "          [1.7389e-01, 1.1397e-01, 1.1322e-01, 1.4191e-01, 2.0064e-01],\n",
       "          [2.6429e-01, 1.8080e-01, 1.6506e-01, 1.5509e-01, 2.2106e-01],\n",
       "          [2.2579e-01, 2.6669e-01, 1.5438e-01, 9.3501e-02, 2.2663e-01],\n",
       "          [2.5923e-01, 2.9487e-01, 2.6044e-01, 2.6087e-01, 3.3124e-01],\n",
       "          [1.5280e-01, 3.0117e-01, 2.0699e-01, 1.1519e-01, 2.2424e-01],\n",
       "          [3.2646e-01, 2.4301e-01, 1.6964e-01, 1.4457e-01, 3.1167e-01],\n",
       "          [2.1246e-01, 2.5002e-01, 1.7871e-01, 8.2418e-02, 1.4825e-01],\n",
       "          [2.3842e-01, 2.5339e-01, 1.4143e-01, 1.0488e-01, 2.7893e-01],\n",
       "          [1.0404e-01, 1.4715e-01, 1.7205e-01, 1.2431e-01, 2.1284e-01],\n",
       "          [1.9756e-01, 1.0261e-01, 8.8334e-02, 1.6027e-01, 2.4684e-01],\n",
       "          [1.4501e-01, 2.5812e-01, 1.6653e-01, 3.2439e-01, 2.3041e-01],\n",
       "          [1.3738e-01, 4.1531e-01, 2.7639e-01, 1.2765e-01, 2.2705e-01],\n",
       "          [1.4527e-01, 3.2836e-01, 2.4308e-01, 1.2920e-01, 1.9853e-01],\n",
       "          [2.2785e-01, 2.4604e-01, 1.7876e-01, 1.2592e-01, 2.5890e-01],\n",
       "          [1.7221e-01, 1.9589e-01, 1.0580e-01, 1.8704e-01, 2.7936e-01],\n",
       "          [2.5882e-01, 2.5177e-01, 1.7190e-01, 1.6743e-01, 2.1816e-01],\n",
       "          [1.3286e-01, 1.1751e-01, 1.5936e-01, 6.1159e-02, 1.0935e-01],\n",
       "          [1.2747e-01, 2.8174e-01, 1.6951e-01, 5.1683e-02, 1.4043e-01],\n",
       "          [6.0013e-02, 1.4546e-01, 8.6192e-02, 1.4760e-01, 2.2576e-01],\n",
       "          [2.4720e-01, 2.0847e-01, 1.7379e-01, 1.7161e-01, 2.2629e-01],\n",
       "          [1.1528e-01, 2.3468e-01, 1.5622e-01, 6.0241e-02, 1.2778e-01],\n",
       "          [1.5727e-01, 2.7394e-01, 1.7409e-01, 4.9133e-02, 1.4673e-01],\n",
       "          [1.3881e-01, 2.0120e-01, 1.5163e-01, 1.3208e-01, 3.2688e-01],\n",
       "          [1.5896e-01, 1.7386e-01, 1.0650e-01, 1.3950e-01, 2.0780e-01],\n",
       "          [2.0597e-01, 2.4525e-01, 1.5253e-01, 1.5352e-01, 2.7288e-01],\n",
       "          [2.3668e-01, 2.3781e-01, 2.1964e-01, 1.4607e-01, 1.8690e-01],\n",
       "          [1.3935e-01, 2.5723e-01, 1.7761e-01, 7.0668e-02, 2.3521e-01],\n",
       "          [1.4543e-01, 1.7436e-01, 1.2041e-01, 1.8387e-01, 2.2722e-01],\n",
       "          [1.5555e-01, 2.0325e-01, 8.7838e-02, 1.1447e-01, 2.7729e-01],\n",
       "          [1.4325e-01, 3.1831e-01, 2.5917e-01, 1.2775e-01, 2.1015e-01],\n",
       "          [2.1959e-01, 2.0689e-01, 1.5138e-01, 9.6468e-02, 2.3336e-01],\n",
       "          [2.2298e-01, 2.3392e-01, 2.0886e-01, 8.7499e-02, 2.4909e-01],\n",
       "          [4.9732e-02, 5.6919e-02, 4.8578e-02, 6.8751e-02, 8.4043e-02],\n",
       "          [8.1404e-02, 1.6577e-01, 1.1221e-01, 6.5162e-02, 1.1010e-01],\n",
       "          [2.6879e-01, 1.5951e-01, 1.3660e-01, 1.3679e-01, 1.7616e-01],\n",
       "          [1.6584e-01, 3.9747e-01, 2.6234e-01, 1.3328e-01, 2.1985e-01],\n",
       "          [1.8954e-01, 2.0550e-01, 1.1748e-01, 2.0779e-01, 2.2682e-01]],\n",
       "         grad_fn=<CopySlices>),\n",
       "  'dlt2': tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0140, 0.0022, 0.0046, 0.0099, 0.0090],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0348, 0.0049, 0.0084, 0.0130, 0.0124],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0519, 0.0088, 0.0159, 0.0308, 0.0209],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0761, 0.0241, 0.0374, 0.0528, 0.0503],\n",
       "          [0.1133, 0.0358, 0.0484, 0.0617, 0.0614],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0434, 0.0047, 0.0150, 0.0157, 0.0192],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.1240, 0.0496, 0.0671, 0.0833, 0.0833],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.1259, 0.0496, 0.0755, 0.0897, 0.0878],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0528, 0.0179, 0.0247, 0.0416, 0.0375],\n",
       "          [0.0679, 0.0208, 0.0306, 0.0405, 0.0471],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0677, 0.0134, 0.0263, 0.0377, 0.0297],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0233, 0.0042, 0.0083, 0.0090, 0.0114],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0548, 0.0146, 0.0173, 0.0297, 0.0272],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.1073, 0.0363, 0.0538, 0.0692, 0.0721],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.1208, 0.0478, 0.0693, 0.0782, 0.0922],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.1029, 0.0377, 0.0531, 0.0750, 0.0771],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], grad_fn=<CopySlices>)})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def temporal_loss(timestoevents,weights=None,maxtime=48,threshold=True):\n",
    "    #list of expected times to events, usualy in order of Const.temporal_outcomes\n",
    "    #basically longer = better, we count > maxtime (weeks) as no event\n",
    "    if weights is None: \n",
    "        weights = [1 for i in range(len(timestoevents))]\n",
    "    scores =  [(w*maxtime/t)for w,t in zip(weights,timestoevents)]\n",
    "    if threshold:\n",
    "        scores = [s*torch.lt(t,maxtime) for s,t in zip(scores,timestoevents)]\n",
    "    scores = torch.stack(scores).sum(axis=0)\n",
    "    return scores\n",
    "\n",
    "def outcome_loss(ypred,weights=None):\n",
    "    #default weights is bad\n",
    "    if weights is None: \n",
    "        print('using default outcome loss weights, which is probably wrong since bad stuff should be negative')\n",
    "        weights = [1 for i in range(ypred.shape[1])]\n",
    "    l = torch.mul(ypred[:,0],weights[0])\n",
    "    for i,weight in enumerate(weights[1:]):\n",
    "        #weights with negative values will invert the outcome so e.g. Regional control becomes no regional control\n",
    "        #so the penaly is correct\n",
    "        newloss = torch.mul(ypred[:,i+1],weight)\n",
    "        l = torch.add(l,newloss)\n",
    "    return l\n",
    "\n",
    "def calc_optimal_decisions(dataset,ids,m1,m2,m3,sm3,\n",
    "                           weights=[0,0.5,.5,0], #weight for OS, FT, AS, and LRC as binary probabilities\n",
    "                           tweights=[1,1,1,1], #weight for OS, LRC, FDM, and event (any + FT or AS at 6m) as time to event in weeks\n",
    "                           outcome_loss_func=None,\n",
    "                           threshold_temporal_loss = False,\n",
    "                           maxtime=48,\n",
    "                           get_transitions=True):\n",
    "    m1.eval()\n",
    "    m2.eval()\n",
    "    m3.eval()\n",
    "    sm3.eval()\n",
    "    device = m1.get_device()\n",
    "    data = dataset.processed_df.copy().loc[ids]\n",
    "    \n",
    "    def get_dlt(state):\n",
    "        if state == 2:\n",
    "            return data[Const.dlt2].copy()\n",
    "        d = data[Const.dlt1].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_pd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.primary_disease_states2].copy()\n",
    "        d = data[Const.primary_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_nd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.nodal_disease_states2].copy()\n",
    "        d = data[Const.nodal_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_cc(state):\n",
    "        res = data[Const.ccs].copy()\n",
    "        if state == 1:\n",
    "            res.values[:,:] = np.zeros(res.values.shape)\n",
    "        return res\n",
    "    \n",
    "    def get_mod(state):\n",
    "        res = data[Const.modifications].copy()\n",
    "        #this should have an ic condition but we don't use it anumore anywa\n",
    "        return res\n",
    "        \n",
    "    def formatdf(d):\n",
    "        d = df_to_torch(d).to(device)\n",
    "        return d\n",
    "    \n",
    "    \n",
    "    outcomedf = data[Const.outcomes]\n",
    "    baseline = dataset.get_state('baseline').loc[ids]\n",
    "    baseline_input = formatdf(baseline)\n",
    "\n",
    "        \n",
    "    if outcome_loss_func is None:\n",
    "        outcome_loss_func = outcome_loss\n",
    "    \n",
    "    cat = lambda x: torch.cat([xx.to(device) for xx in x],axis=1).to(device)\n",
    "    format_transition = lambda x: x.to(device)\n",
    "    def get_outcome(d1,d2,d3):\n",
    "        d1 = torch.full((len(ids),1),d1).type(torch.FloatTensor)\n",
    "        d2 = torch.full((len(ids),1),d2).type(torch.FloatTensor)\n",
    "        d3 = torch.full((len(ids),1),d3).type(torch.FloatTensor)\n",
    "        \n",
    "        tinput1 = cat([baseline_input,d1])\n",
    "        ytransition = m1(tinput1)\n",
    "        [ypd1,ynd1,ymod,ydlt1] = [format_transition(xx) for xx in ytransition['predictions']]\n",
    "        d1_thresh = torch.gt(d1,.5).view(-1,1).to(device)\n",
    "        ypd1[:,0:2] = ypd1[:,0:2]*d1_thresh\n",
    "        ynd1[:,0:2] = ynd1[:,0:2]*d1_thresh\n",
    "        \n",
    "        tinput2 = cat([baseline_input,ypd1,ynd1,ymod,ydlt1,d1,d2])\n",
    "        ytransition2 = m2(tinput2)\n",
    "        [ypd2,ynd2,ycc,ydlt2] = [format_transition(xx) for xx in ytransition2['predictions']]\n",
    "        \n",
    "        input3 = cat([baseline_input, ypd2, ynd2, ycc, ydlt2, d1, d2,d3])\n",
    "        outcome = m3(input3)['predictions']\n",
    "        temporal_outcomes = sm3.time_to_event(input3,n_samples=1)\n",
    "        \n",
    "        transitions = {\n",
    "            'pd1': ypd1,\n",
    "            'nd1': ynd1,\n",
    "            'nd2': ynd2,\n",
    "            'pd2': ypd2,\n",
    "            'mod': ymod,\n",
    "            'cc': ycc,\n",
    "            'dlt1': ydlt1,\n",
    "            'dlt2': ydlt2,\n",
    "        }\n",
    "        return outcome, temporal_outcomes, transitions\n",
    "\n",
    "    losses = []\n",
    "    loss_order = []\n",
    "    transitions = {}\n",
    "    for d1 in [0,1]:\n",
    "        for d2 in [0,1]:\n",
    "            for d3 in [0,1]:\n",
    "                outcomes, tte, transition_entry = get_outcome(d1,d2,d3)\n",
    "                loss = outcome_loss_func(outcomes,weights)\n",
    "                tloss = temporal_loss(tte,tweights,maxtime=maxtime,threshold=threshold_temporal_loss)\n",
    "                loss += tloss\n",
    "                losses.append(loss)\n",
    "                loss_order.append([d1,d2,d3])\n",
    "                transitions[str(d1)+str(d2)+str(d3)] = transition_entry\n",
    "    losses = torch.stack(losses,axis=1)\n",
    "    optimal_decisions = [loss_order[i] for i in torch.argmin(losses,axis=1)]\n",
    "    result = torch.tensor(optimal_decisions).type(torch.FloatTensor)\n",
    "    print(result.sum(axis=0),result.shape[0])\n",
    "    if get_transitions:\n",
    "        opt_transitions = {k: torch.zeros(v.shape).type(torch.FloatTensor) for k,v in transitions['000'].items()}\n",
    "        for i,od in enumerate(optimal_decisions):\n",
    "            key = ''.join([str(o) for o in od])\n",
    "            entry = transitions[key]\n",
    "            for kk,vv in entry.items():\n",
    "                opt_transitions[kk][i,:] = vv[i,:]\n",
    "        return result, opt_transitions\n",
    "    return result\n",
    "\n",
    "test, testtest = get_tt_split()\n",
    "calc_optimal_decisions(DTDataset(),\n",
    "                       testtest,model1,model2,model3,smodel3,\n",
    "                       threshold_temporal_loss=False,\n",
    "                       maxtime=48,\n",
    "                       weights=[0,0,0,0],\n",
    "                       tweights=[2,0.1,0,0],\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "122ee514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([205.,  44.,   7.]) 389\n",
      "tensor([69., 18.,  6.]) 147\n",
      "torch.Size([3, 536, 85])\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 0 _____\n",
      "val reward 1.5337661504745483\n",
      "imitation reward 2.2648766040802\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.7510436177253723, 0.004403852391988039, 0.0016084234230220318]\n",
      "[{'decision': 0, 'optimal_auc': 0.9234485321441843, 'imitation_auc': 0.5333969465648856, 'optimal_acc': 0.5714285714285714, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.954349698535745, 'imitation_auc': 0.647554347826087, 'optimal_acc': 0.8775510204081632, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.8026004728132388, 'imitation_auc': 0.6993006993006993, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 1 _____\n",
      "val reward 1.6847261190414429\n",
      "imitation reward 1.781301498413086\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.10992904007434845, 0.011692939326167107, 0.0004267352051101625]\n",
      "[{'decision': 0, 'optimal_auc': 0.9026384243775548, 'imitation_auc': 0.48854961832061067, 'optimal_acc': 0.5306122448979592, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.9328165374677002, 'imitation_auc': 0.6445652173913043, 'optimal_acc': 0.8775510204081632, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.4940898345153665, 'imitation_auc': 0.7113795295613478, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 2 _____\n",
      "val reward 1.1242891550064087\n",
      "imitation reward 1.2755963802337646\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.6826441884040833, 0.034179966896772385, 0.0007347496575675905]\n",
      "[{'decision': 0, 'optimal_auc': 0.9321813452248235, 'imitation_auc': 0.44179389312977096, 'optimal_acc': 0.7619047619047619, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.9595176571920758, 'imitation_auc': 0.647554347826087, 'optimal_acc': 0.8775510204081632, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.4787234042553191, 'imitation_auc': 0.6795931341385888, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.6394557823129252}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 3 _____\n",
      "val reward 0.9645942449569702\n",
      "imitation reward 1.0888067483901978\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.6561893820762634, 0.1572721004486084, 0.0012432544026523829]\n",
      "[{'decision': 0, 'optimal_auc': 0.9305091044221478, 'imitation_auc': 0.5262404580152672, 'optimal_acc': 0.7687074829931972, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.9638242894056848, 'imitation_auc': 0.660054347826087, 'optimal_acc': 0.9319727891156463, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': 0.5780141843971631, 'imitation_auc': 0.7596948506039415, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 4 _____\n",
      "val reward 0.91025310754776\n",
      "imitation reward 1.1622142791748047\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.4913337230682373, 0.2522747218608856, 0.0006600032793357968]\n",
      "[{'decision': 0, 'optimal_auc': 0.927536231884058, 'imitation_auc': 0.571087786259542, 'optimal_acc': 0.8367346938775511, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.9646856158484065, 'imitation_auc': 0.6546195652173913, 'optimal_acc': 0.8231292517006803, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': 0.9716312056737589, 'imitation_auc': 0.7832167832167832, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 5 _____\n",
      "val reward 0.7629498243331909\n",
      "imitation reward 1.1960818767547607\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.45403942465782166, 0.13963112235069275, 0.00041806144872680306]\n",
      "[{'decision': 0, 'optimal_auc': 0.9316239316239316, 'imitation_auc': 0.5935114503816794, 'optimal_acc': 0.8571428571428571, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.9728682170542635, 'imitation_auc': 0.6548913043478262, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.7551020408163265}, {'decision': 2, 'optimal_auc': 0.9917257683215129, 'imitation_auc': 0.7889383343928797, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 6 _____\n",
      "val reward 0.7280138731002808\n",
      "imitation reward 1.1159520149230957\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.5580188632011414, 0.12425674498081207, 0.0007261065184138715]\n",
      "[{'decision': 0, 'optimal_auc': 0.9407283537718321, 'imitation_auc': 0.6211832061068702, 'optimal_acc': 0.8163265306122449, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.9771748492678726, 'imitation_auc': 0.660054347826087, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.7482993197278912}, {'decision': 2, 'optimal_auc': 0.9940898345153664, 'imitation_auc': 0.7879847425301971, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.8095238095238095}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 7 _____\n",
      "val reward 0.7209341526031494\n",
      "imitation reward 1.1795762777328491\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.5774122476577759, 0.16645672917366028, 0.001591524458490312]\n",
      "[{'decision': 0, 'optimal_auc': 0.9420289855072463, 'imitation_auc': 0.6369274809160305, 'optimal_acc': 0.8231292517006803, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.9806201550387597, 'imitation_auc': 0.6603260869565217, 'optimal_acc': 0.9183673469387755, 'imitation_acc': 0.7482993197278912}, {'decision': 2, 'optimal_auc': 0.9952718676122931, 'imitation_auc': 0.7813095994914177, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.7755102040816326}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 8 _____\n",
      "val reward 0.651024580001831\n",
      "imitation reward 1.1851986646652222\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.5483449101448059, 0.16513141989707947, 0.0027091731317341328]\n",
      "[{'decision': 0, 'optimal_auc': 0.9427722036417688, 'imitation_auc': 0.6407442748091603, 'optimal_acc': 0.8299319727891157, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.9810508182601205, 'imitation_auc': 0.6546195652173914, 'optimal_acc': 0.9115646258503401, 'imitation_acc': 0.7414965986394558}, {'decision': 2, 'optimal_auc': 0.9964539007092198, 'imitation_auc': 0.7930705657978385, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.8027210884353742}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 9 _____\n",
      "val reward 0.5636880397796631\n",
      "imitation reward 1.2030971050262451\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.49506062269210815, 0.14040586352348328, 0.004634405020624399]\n",
      "[{'decision': 0, 'optimal_auc': 0.9435154217762913, 'imitation_auc': 0.6374045801526718, 'optimal_acc': 0.8571428571428571, 'imitation_acc': 0.8843537414965986}, {'decision': 1, 'optimal_auc': 0.9827734711455641, 'imitation_auc': 0.647554347826087, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.7551020408163265}, {'decision': 2, 'optimal_auc': 0.9976359338061466, 'imitation_auc': 0.8003814367450731, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.7959183673469388}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 10 _____\n",
      "val reward 0.5538517832756042\n",
      "imitation reward 1.1581895351409912\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.4824775755405426, 0.17274615168571472, 0.010003807954490185]\n",
      "[{'decision': 0, 'optimal_auc': 0.94611668524712, 'imitation_auc': 0.6278625954198475, 'optimal_acc': 0.8503401360544217, 'imitation_acc': 0.8707482993197279}, {'decision': 1, 'optimal_auc': 0.9832041343669251, 'imitation_auc': 0.6567934782608695, 'optimal_acc': 0.9183673469387755, 'imitation_acc': 0.7551020408163265}, {'decision': 2, 'optimal_auc': 0.9988179669030732, 'imitation_auc': 0.8080101716465352, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 11 _____\n",
      "val reward 0.5145827531814575\n",
      "imitation reward 1.137531042098999\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.5155688524246216, 0.1512889415025711, 0.01925537921488285]\n",
      "[{'decision': 0, 'optimal_auc': 0.9483463396506875, 'imitation_auc': 0.6288167938931298, 'optimal_acc': 0.8503401360544217, 'imitation_acc': 0.8571428571428571}, {'decision': 1, 'optimal_auc': 0.9840654608096469, 'imitation_auc': 0.6605978260869565, 'optimal_acc': 0.9387755102040817, 'imitation_acc': 0.7414965986394558}, {'decision': 2, 'optimal_auc': 0.9976359338061466, 'imitation_auc': 0.8127781309599491, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.8231292517006803}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 12 _____\n",
      "val reward 0.49431464076042175\n",
      "imitation reward 1.1419498920440674\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.5348871946334839, 0.12724439799785614, 0.0265685822814703]\n",
      "[{'decision': 0, 'optimal_auc': 0.9516908212560387, 'imitation_auc': 0.623091603053435, 'optimal_acc': 0.8435374149659864, 'imitation_acc': 0.8435374149659864}, {'decision': 1, 'optimal_auc': 0.9827734711455641, 'imitation_auc': 0.6551630434782608, 'optimal_acc': 0.9523809523809523, 'imitation_acc': 0.7551020408163265}, {'decision': 2, 'optimal_auc': 0.9976359338061466, 'imitation_auc': 0.8070565797838525, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.8163265306122449}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 13 _____\n",
      "val reward 0.4872055649757385\n",
      "imitation reward 1.1538927555084229\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.5186882019042969, 0.16160738468170166, 0.029097622260451317]\n",
      "[{'decision': 0, 'optimal_auc': 0.9574507617985879, 'imitation_auc': 0.6288167938931297, 'optimal_acc': 0.8843537414965986, 'imitation_acc': 0.8571428571428571}, {'decision': 1, 'optimal_auc': 0.9801894918173988, 'imitation_auc': 0.6464673913043477, 'optimal_acc': 0.9387755102040817, 'imitation_acc': 0.7551020408163265}, {'decision': 2, 'optimal_auc': 0.9976359338061466, 'imitation_auc': 0.8057851239669422, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.8095238095238095}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 14 _____\n",
      "val reward 0.5011643171310425\n",
      "imitation reward 1.2092411518096924\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.4914214015007019, 0.1847756952047348, 0.025336677208542824]\n",
      "[{'decision': 0, 'optimal_auc': 0.9585655890003716, 'imitation_auc': 0.6326335877862596, 'optimal_acc': 0.8775510204081632, 'imitation_acc': 0.8639455782312925}, {'decision': 1, 'optimal_auc': 0.9801894918173988, 'imitation_auc': 0.6385869565217391, 'optimal_acc': 0.9183673469387755, 'imitation_acc': 0.7619047619047619}, {'decision': 2, 'optimal_auc': 0.9976359338061466, 'imitation_auc': 0.8022886204704387, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.8163265306122449}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 15 _____\n",
      "val reward 0.4305866062641144\n",
      "imitation reward 1.313634991645813\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.5000457763671875, 0.13021108508110046, 0.020905688405036926]\n",
      "[{'decision': 0, 'optimal_auc': 0.9589371980676329, 'imitation_auc': 0.6307251908396947, 'optimal_acc': 0.8843537414965986, 'imitation_acc': 0.8775510204081632}, {'decision': 1, 'optimal_auc': 0.9832041343669251, 'imitation_auc': 0.6195652173913043, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': 0.9964539007092198, 'imitation_auc': 0.79815638906548, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.8163265306122449}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 16 _____\n",
      "val reward 0.484078586101532\n",
      "imitation reward 1.2010010480880737\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.510075032711029, 0.17605118453502655, 0.02459491416811943]\n",
      "[{'decision': 0, 'optimal_auc': 0.9607952434039391, 'imitation_auc': 0.6292938931297709, 'optimal_acc': 0.8775510204081632, 'imitation_acc': 0.8571428571428571}, {'decision': 1, 'optimal_auc': 0.9806201550387597, 'imitation_auc': 0.626358695652174, 'optimal_acc': 0.9319727891156463, 'imitation_acc': 0.7687074829931972}, {'decision': 2, 'optimal_auc': 0.9964539007092198, 'imitation_auc': 0.7952956134774316, 'optimal_acc': 0.9795918367346939, 'imitation_acc': 0.8163265306122449}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 17 _____\n",
      "val reward 0.4376947581768036\n",
      "imitation reward 1.190829873085022\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.5141815543174744, 0.14158056676387787, 0.02395659126341343]\n",
      "[{'decision': 0, 'optimal_auc': 0.9609810479375697, 'imitation_auc': 0.6140267175572519, 'optimal_acc': 0.8707482993197279, 'imitation_acc': 0.8367346938775511}, {'decision': 1, 'optimal_auc': 0.9823428079242033, 'imitation_auc': 0.6247282608695652, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.7619047619047619}, {'decision': 2, 'optimal_auc': 0.9964539007092198, 'imitation_auc': 0.7975206611570247, 'optimal_acc': 0.9795918367346939, 'imitation_acc': 0.8163265306122449}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 18 _____\n",
      "val reward 0.4222581684589386\n",
      "imitation reward 1.2779427766799927\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.49046221375465393, 0.10846579074859619, 0.02304469794034958]\n",
      "[{'decision': 0, 'optimal_auc': 0.9606094388703084, 'imitation_auc': 0.6068702290076337, 'optimal_acc': 0.8843537414965986, 'imitation_acc': 0.8435374149659864}, {'decision': 1, 'optimal_auc': 0.9857881136950903, 'imitation_auc': 0.6252717391304348, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': 0.9964539007092198, 'imitation_auc': 0.7997457088366179, 'optimal_acc': 0.9795918367346939, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 19 _____\n",
      "val reward 0.4717499613761902\n",
      "imitation reward 1.3164355754852295\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.5057424306869507, 0.15870174765586853, 0.026207534596323967]\n",
      "[{'decision': 0, 'optimal_auc': 0.9611668524712003, 'imitation_auc': 0.6235687022900763, 'optimal_acc': 0.8775510204081632, 'imitation_acc': 0.8367346938775511}, {'decision': 1, 'optimal_auc': 0.9823428079242034, 'imitation_auc': 0.6293478260869566, 'optimal_acc': 0.9455782312925171, 'imitation_acc': 0.7687074829931972}, {'decision': 2, 'optimal_auc': 0.9964539007092198, 'imitation_auc': 0.8026064844246663, 'optimal_acc': 0.9795918367346939, 'imitation_acc': 0.8367346938775511}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 20 _____\n",
      "val reward 0.5445703268051147\n",
      "imitation reward 1.3181467056274414\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.5098860263824463, 0.18568944931030273, 0.02942318469285965]\n",
      "[{'decision': 0, 'optimal_auc': 0.9619100706057229, 'imitation_auc': 0.6331106870229007, 'optimal_acc': 0.8979591836734694, 'imitation_acc': 0.8367346938775511}, {'decision': 1, 'optimal_auc': 0.9801894918173988, 'imitation_auc': 0.6331521739130435, 'optimal_acc': 0.9251700680272109, 'imitation_acc': 0.7687074829931972}, {'decision': 2, 'optimal_auc': 0.9964539007092198, 'imitation_auc': 0.7994278448823903, 'optimal_acc': 0.9795918367346939, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 21 _____\n",
      "val reward 0.48049110174179077\n",
      "imitation reward 1.3492180109024048\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.5143001675605774, 0.14887647330760956, 0.031042780727148056]\n",
      "[{'decision': 0, 'optimal_auc': 0.9615384615384616, 'imitation_auc': 0.6359732824427481, 'optimal_acc': 0.891156462585034, 'imitation_acc': 0.8435374149659864}, {'decision': 1, 'optimal_auc': 0.9810508182601205, 'imitation_auc': 0.6418478260869566, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.7687074829931972}, {'decision': 2, 'optimal_auc': 0.9952718676122931, 'imitation_auc': 0.7860775588048315, 'optimal_acc': 0.9795918367346939, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 22 _____\n",
      "val reward 0.47507691383361816\n",
      "imitation reward 1.3724004030227661\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.5212827920913696, 0.11096969991922379, 0.032218966633081436]\n",
      "[{'decision': 0, 'optimal_auc': 0.9600520252694166, 'imitation_auc': 0.633587786259542, 'optimal_acc': 0.8843537414965986, 'imitation_acc': 0.8571428571428571}, {'decision': 1, 'optimal_auc': 0.9806201550387597, 'imitation_auc': 0.651358695652174, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.7619047619047619}, {'decision': 2, 'optimal_auc': 0.9940898345153664, 'imitation_auc': 0.7705022250476796, 'optimal_acc': 0.9795918367346939, 'imitation_acc': 0.8299319727891157}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 23 _____\n",
      "val reward 0.4692211449146271\n",
      "imitation reward 1.377138614654541\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.5033467411994934, 0.11550411581993103, 0.03296384587883949]\n",
      "[{'decision': 0, 'optimal_auc': 0.9589371980676329, 'imitation_auc': 0.6307251908396947, 'optimal_acc': 0.9115646258503401, 'imitation_acc': 0.8435374149659864}, {'decision': 1, 'optimal_auc': 0.9797588285960379, 'imitation_auc': 0.6510869565217391, 'optimal_acc': 0.9659863945578231, 'imitation_acc': 0.7619047619047619}, {'decision': 2, 'optimal_auc': 0.9940898345153664, 'imitation_auc': 0.7546090273363001, 'optimal_acc': 0.9795918367346939, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______epoch 24 _____\n",
      "val reward 0.4863758981227875\n",
      "imitation reward 1.4479124546051025\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.4986669719219208, 0.13891984522342682, 0.03281077370047569]\n",
      "[{'decision': 0, 'optimal_auc': 0.9581939799331104, 'imitation_auc': 0.625, 'optimal_acc': 0.9183673469387755, 'imitation_acc': 0.8503401360544217}, {'decision': 1, 'optimal_auc': 0.9788975021533161, 'imitation_auc': 0.6475543478260869, 'optimal_acc': 0.9659863945578231, 'imitation_acc': 0.7551020408163265}, {'decision': 2, 'optimal_auc': 0.9940898345153664, 'imitation_auc': 0.7453909726636999, 'optimal_acc': 0.9795918367346939, 'imitation_acc': 0.8163265306122449}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 25 _____\n",
      "val reward 0.5377295613288879\n",
      "imitation reward 1.5792698860168457\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.5153266787528992, 0.1599303036928177, 0.03194335848093033]\n",
      "[{'decision': 0, 'optimal_auc': 0.9570791527313267, 'imitation_auc': 0.6164122137404581, 'optimal_acc': 0.8843537414965986, 'imitation_acc': 0.8367346938775511}, {'decision': 1, 'optimal_auc': 0.9793281653746769, 'imitation_auc': 0.6448369565217391, 'optimal_acc': 0.9523809523809523, 'imitation_acc': 0.7482993197278912}, {'decision': 2, 'optimal_auc': 0.9940898345153664, 'imitation_auc': 0.7374443738080101, 'optimal_acc': 0.9795918367346939, 'imitation_acc': 0.8095238095238095}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 26 _____\n",
      "val reward 0.5351510643959045\n",
      "imitation reward 1.6859853267669678\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.514539361000061, 0.1513669639825821, 0.03068169392645359]\n",
      "[{'decision': 0, 'optimal_auc': 0.9541062801932366, 'imitation_auc': 0.6145038167938932, 'optimal_acc': 0.8639455782312925, 'imitation_acc': 0.8231292517006803}, {'decision': 1, 'optimal_auc': 0.9797588285960379, 'imitation_auc': 0.6315217391304347, 'optimal_acc': 0.9523809523809523, 'imitation_acc': 0.7482993197278912}, {'decision': 2, 'optimal_auc': 0.9952718676122931, 'imitation_auc': 0.7301335028607756, 'optimal_acc': 0.9795918367346939, 'imitation_acc': 0.8027210884353742}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 27 _____\n",
      "val reward 0.5161556601524353\n",
      "imitation reward 1.7876050472259521\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.4991632103919983, 0.10919716209173203, 0.029361816123127937]\n",
      "[{'decision': 0, 'optimal_auc': 0.9518766257896693, 'imitation_auc': 0.6063931297709924, 'optimal_acc': 0.891156462585034, 'imitation_acc': 0.8367346938775511}, {'decision': 1, 'optimal_auc': 0.979328165374677, 'imitation_auc': 0.621195652173913, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.7482993197278912}, {'decision': 2, 'optimal_auc': 0.9952718676122931, 'imitation_auc': 0.7190082644628099, 'optimal_acc': 0.9863945578231292, 'imitation_acc': 0.8027210884353742}]\n",
      "tensor([0.4694, 0.1224, 0.0408], grad_fn=<MeanBackward1>)\n",
      "______epoch 28 _____\n",
      "val reward 0.5359873175621033\n",
      "imitation reward 1.6603002548217773\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.51347815990448, 0.12277456372976303, 0.03219631686806679]\n",
      "[{'decision': 0, 'optimal_auc': 0.9507617985878856, 'imitation_auc': 0.6073473282442747, 'optimal_acc': 0.8571428571428571, 'imitation_acc': 0.8435374149659864}, {'decision': 1, 'optimal_auc': 0.9776055124892333, 'imitation_auc': 0.6187499999999999, 'optimal_acc': 0.9727891156462585, 'imitation_acc': 0.7482993197278912}, {'decision': 2, 'optimal_auc': 0.9952718676122931, 'imitation_auc': 0.7158296249205339, 'optimal_acc': 0.9931972789115646, 'imitation_acc': 0.8027210884353742}]\n",
      "++++++++++Final+++++++++++\n",
      "best tensor(1.1309, grad_fn=<AddBackward0>)\n",
      "[{'decision': 0, 'optimal_auc': 0.9609810479375697, 'imitation_auc': 0.6140267175572519, 'optimal_acc': 0.8707482993197279, 'imitation_acc': 0.8367346938775511}, {'decision': 1, 'optimal_auc': 0.9823428079242033, 'imitation_auc': 0.6247282608695652, 'optimal_acc': 0.9591836734693877, 'imitation_acc': 0.7619047619047619}, {'decision': 2, 'optimal_auc': 0.9964539007092198, 'imitation_auc': 0.7975206611570247, 'optimal_acc': 0.9795918367346939, 'imitation_acc': 0.8163265306122449}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionAttentionModel(\n",
       "  (input_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=100, out_features=1000, bias=True)\n",
       "  )\n",
       "  (batchnorm): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       "  (relu): Softplus(beta=1, threshold=20)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (softmax): Softmax(dim=1)\n",
       "  (final_opt_layer): Linear(in_features=1000, out_features=100, bias=True)\n",
       "  (final_imitation_layer): Linear(in_features=1000, out_features=100, bias=True)\n",
       "  (final_layer): Linear(in_features=1000, out_features=6, bias=True)\n",
       "  (resize_layer): Linear(in_features=89, out_features=100, bias=True)\n",
       "  (attentions): ModuleList(\n",
       "    (0): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (norms): ModuleList(\n",
       "    (0): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (activation): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_unique_sequence(array):\n",
    "    #converts a row of boolean values to a unique number e.g. [1,1,0] => 11, [0,0,1] => 100\n",
    "    uniqueify = lambda r: torch.sum(torch.stack([i*(10**ii) for ii,i in enumerate(r)]))\n",
    "    return torch_apply_along_axis(uniqueify,array)\n",
    "\n",
    "def train_decision_model_triplet(\n",
    "    tmodel1,\n",
    "    tmodel2,\n",
    "    tmodel3,\n",
    "    smodel3,\n",
    "    use_default_split=True,\n",
    "    use_bagging_split=False,\n",
    "    use_attention=True,\n",
    "    lr=.001,\n",
    "    epochs=10000,\n",
    "    patience=5,\n",
    "    weights=[0,.5,.5,0], #realtive weight of survival, feeding tube, aspiration, andl lrc\n",
    "    tweights=[1,1,1,0], #weight for OS, LRC, FDM, and event (any + FT or AS at 6m) as time to event in weeks\n",
    "    opt_weights=[1,1,1], #weights for policy model for optimal decisions\n",
    "    imitation_weights=[.5,1,1],#weights of imitation decisions, because ic overtrains too quickly\n",
    "    imitation_weight=1,\n",
    "    reward_weight=1,\n",
    "    imitation_triplet_weight=2,\n",
    "    reward_triplet_weight = 2,\n",
    "    shufflecol_chance = 0.2,\n",
    "    split=.7,\n",
    "    resample_training=False,\n",
    "    save_path='../data/models/',\n",
    "    file_suffix='',\n",
    "    verbose=True,\n",
    "    use_gpu=False,\n",
    "    **model_kwargs,\n",
    "):\n",
    "    tmodel1.eval()\n",
    "    tmodel2.eval()\n",
    "    tmodel3.eval()\n",
    "\n",
    "    train_ids, test_ids = get_tt_split(use_default_split=use_default_split,use_bagging_split=use_bagging_split,resample_training=resample_training)\n",
    "    true_ids = train_ids + test_ids #for saving memory without upsampling\n",
    "\n",
    "    dataset = DTDataset()\n",
    "    data = dataset.processed_df.copy()\n",
    "    \n",
    "    def get_dlt(state):\n",
    "        if state == 2:\n",
    "            return data[Const.dlt2].copy()\n",
    "        d = data[Const.dlt1].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_pd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.primary_disease_states2].copy()\n",
    "        d = data[Const.primary_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_nd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.nodal_disease_states2].copy()\n",
    "        d = data[Const.nodal_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_cc(state):\n",
    "        res = data[Const.ccs].copy()\n",
    "        if state == 1:\n",
    "            res.values[:,:] = np.zeros(res.values.shape)\n",
    "        return res\n",
    "    \n",
    "    def get_mod(state):\n",
    "        res = data[Const.modifications].copy()\n",
    "        #this should have an ic condition but we don't use it anumore anywa\n",
    "        return res\n",
    "        \n",
    "    outcomedf = data[Const.outcomes]\n",
    "    baseline = dataset.get_state('baseline')\n",
    "    \n",
    "    def formatdf(d,dids=train_ids):\n",
    "        d = df_to_torch(d.loc[dids]).to(model.get_device())\n",
    "        return d\n",
    "    \n",
    "    def makegrad(v):\n",
    "        if not v.requires_grad:\n",
    "            v.requires_grad=True\n",
    "        return v\n",
    "    \n",
    "    if use_attention:\n",
    "        model = DecisionAttentionModel(baseline.shape[1],**model_kwargs)\n",
    "    else:\n",
    "        model_kwargs = {k:v for k,v in model_kwargs.items() if 'attention' not in k and 'embed' not in k}\n",
    "        model = DecisionModel(baseline.shape[1],**model_kwargs)\n",
    "        \n",
    "    device = 'cpu'\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "        \n",
    "    model.set_device(device)\n",
    "\n",
    "    tmodel1.set_device(device)\n",
    "    tmodel2.set_device(device)\n",
    "    tmodel3.set_device(device)\n",
    "    smodel3.set_device(device)\n",
    "    hashcode = str(hash(','.join([str(i) for i in train_ids])))\n",
    "    \n",
    "    save_file = save_path + 'model_' + model.identifier +'_hash' + hashcode + file_suffix + '.tar'\n",
    "    model.fit_normalizer(df_to_torch(baseline.loc[train_ids]))\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "    \n",
    "    \n",
    "    optimal_train,transitions_train = calc_optimal_decisions(dataset,train_ids,tmodel1,tmodel2,tmodel3,smodel3,\n",
    "                                           weights=weights,tweights=tweights,\n",
    "                                          )\n",
    "    optimal_test,transitions_test = calc_optimal_decisions(dataset,test_ids,tmodel1,tmodel2,tmodel3,smodel3,\n",
    "                                          weights=weights,tweights=tweights,\n",
    "                                         )\n",
    "    optimal_train = optimal_train.to(model.get_device())\n",
    "    optimal_test = optimal_test.to(model.get_device())\n",
    "    mse = torch.nn.MSELoss()\n",
    "    bce = torch.nn.BCELoss()\n",
    "    \n",
    "    def compare_decisions(d1,d2,d3,ids):\n",
    "#         ypred = np.concatenate([dd.cpu().detach().numpy().reshape(-1,1) for dd in [d1,d2,d3]],axis=1)\n",
    "        ytrue = df_to_torch(outcomedf.loc[ids])\n",
    "        dloss = bce(d1.view(-1),ytrue[:,0])\n",
    "        dloss += bce(d2.view(-1),ytrue[:,1])\n",
    "        dloss += bce(d3,view(-1),ytrue[:,2])\n",
    "        return dloss\n",
    "        \n",
    "    def remove_decisions(df):\n",
    "        cols = [c for c in df.columns if c not in Const.decisions ]\n",
    "        ddf = df[cols]\n",
    "        return ddf\n",
    "    \n",
    "    makeinput = lambda step,dids: df_to_torch(remove_decisions(dataset.get_input_state(step=step,ids=dids)))\n",
    "    threshold = lambda x: torch.gt(x,torch.rand(x.shape[0])).type(torch.FloatTensor)\n",
    "\n",
    "    randchoice = lambda x: x[torch.randint(len(x),(1,))[0]]\n",
    "    tloss_func = torch.nn.TripletMarginLoss()\n",
    "    def get_tloss(row,step,yt,x,imitation=True):\n",
    "        if yt[:,step].std() < .001:\n",
    "            return torch.tensor([0]).to(device)\n",
    "        positive_idx= torch.nonzero(yt[:,step] == yt[row,step])\n",
    "        if len(positive_idx) <= 1:\n",
    "            print('no losses','n positive',len(positive_idx),'yt',yt[row,step],'row',row,'step',step,'imitation',imitation,end='\\r')\n",
    "            return torch.tensor([0]).to(device)\n",
    "        positive_idx = torch.stack([ii for ii in positive_idx if ii != row]).view(-1)\n",
    "        negative_idx = torch.tensor([ii for ii in range(x.shape[0]) if ii not in positive_idx and ii != row])\n",
    "        if len(positive_idx) < 1 or len(negative_idx) < 1:\n",
    "            print('no losses','n positive',len(positive_idx),'n negative',len(negative_idx),'yt',yt[row,step],'row',row,'step',step,'imitation',imitation,end='\\r')\n",
    "            return torch.tensor([0]).to(device)\n",
    "        positive = x[randchoice(positive_idx)]\n",
    "        negative = x[randchoice(negative_idx)]\n",
    "        anchor = x[row]\n",
    "        if use_attention:\n",
    "            [anchor_embedding,pos_embedding,neg_embedding] = [model.get_embedding(xx.view(1,-1),position=step,use_saved_memory=True) for xx in [anchor,positive,negative]]\n",
    "        else:    \n",
    "            [anchor_embedding,pos_embedding,neg_embedding] = [model.get_embedding(xx.view(1,-1),position=step,concatenate=False)[int(imitation)] for xx in [anchor,positive,negative]]\n",
    "        tloss = tloss_func(anchor_embedding,pos_embedding,neg_embedding)\n",
    "        return tloss\n",
    "    #save the inputs from the whole dataset for future reference\n",
    "    if use_attention:\n",
    "        full_data = []\n",
    "        for mstep in [0,1,2]:\n",
    "            full_data_step = [baseline, get_dlt(min(mstep,1)),\n",
    "                         get_dlt(mstep),get_pd(mstep),get_nd(mstep),get_cc(mstep),get_mod(mstep)]\n",
    "            full_data_step = torch.cat([formatdf(fd,true_ids) for fd in full_data_step],axis=1)\n",
    "            full_data.append(full_data_step)\n",
    "        full_data = torch.stack(full_data)\n",
    "        model.save_memory(full_data)\n",
    "        print(full_data.shape)\n",
    "        \n",
    "    def step(train=True):\n",
    "        if train:\n",
    "            model.train(True)\n",
    "            tmodel1.train(True)\n",
    "            tmodel2.train(True)\n",
    "            tmodel3.train(True)\n",
    "            optimizer.zero_grad()\n",
    "            ids = train_ids\n",
    "            y_opt = makegrad(optimal_train)\n",
    "            transition_dict = {k: torch.clone(v).detach() for k,v in transitions_train.items()}\n",
    "        else:\n",
    "            ids = test_ids\n",
    "            model.eval()\n",
    "            tmodel1.eval()\n",
    "            tmodel2.eval()\n",
    "            tmodel3.eval()\n",
    "            y_opt = makegrad(optimal_test)\n",
    "            print(y_opt.mean(axis=0))\n",
    "            transition_dict = {k: torch.clone(v).detach() for k,v in transitions_test.items()}\n",
    "        model.set_device(device)\n",
    "        ytrain = df_to_torch(outcomedf.loc[ids]).to(device)\n",
    "        #imitation losses and decision 1\n",
    "        xxtrained = [baseline, get_dlt(0),get_dlt(0),get_pd(0),get_nd(0),get_cc(0),get_mod(0)]\n",
    "        xxtrain = [formatdf(xx,ids) for xx in xxtrained]\n",
    "        xxtrain = torch.cat(xxtrain,axis=1).to(device)\n",
    "        o1 = model(xxtrain,position=0,use_saved_memory= (not train))\n",
    "        decision1_imitation = o1[:,3]\n",
    "        decision1_opt = o1[:,0]\n",
    "    \n",
    "        imitation_loss1 = bce(decision1_imitation,ytrain[:,0])\n",
    "        imitation_loss1 = torch.mul(imitation_loss1,imitation_weights[0])\n",
    "        opt_loss1 = bce(decision1_opt,y_opt[:,0])\n",
    "        opt_loss1 = torch.mul(opt_loss1,opt_weights[0])\n",
    "        \n",
    "        x1_imitation = [baseline, get_dlt(1),get_dlt(0),get_pd(1),get_nd(1),get_cc(1),get_mod(1)]\n",
    "        x1_imitation = [formatdf(xx1,ids) for xx1 in x1_imitation]\n",
    "        x1_imitation = torch.cat(x1_imitation,axis=1).to(device)\n",
    "        \n",
    "        o2 = model(x1_imitation,position=1,use_saved_memory= (not train))\n",
    "            \n",
    "        decision2_imitation = o2[:,4]\n",
    "            \n",
    "        imitation_loss2 =  bce(decision2_imitation,ytrain[:,1])\n",
    "        imitation_loss2 = torch.mul(imitation_loss2,imitation_weights[1])\n",
    "        \n",
    "        \n",
    "        x2_imitation = [baseline, get_dlt(1),get_dlt(2),get_pd(2),get_nd(2),get_cc(2),get_mod(2)]\n",
    "        \n",
    "        x2_imitation = [formatdf(xx2,ids) for xx2 in x2_imitation]\n",
    "        x2_imitation = torch.cat(x2_imitation,axis=1).to(device)\n",
    "        \n",
    "        \n",
    "        o3 = model(x2_imitation,position=2,use_saved_memory= (not train))\n",
    "        \n",
    "        decision3_imitation = o3[:,5]\n",
    "        \n",
    "        imitation_loss3 = bce(decision3_imitation,ytrain[:,2])\n",
    "        imitation_loss3 = torch.mul(imitation_loss3,imitation_weights[2])\n",
    "        \n",
    "        opt_input2 = [\n",
    "            formatdf(baseline,ids), \n",
    "            transition_dict['dlt1'],\n",
    "            formatdf(get_dlt(0),ids),\n",
    "            transition_dict['pd1'],\n",
    "            transition_dict['nd1'], \n",
    "            formatdf(get_cc(0),ids),\n",
    "            transition_dict['mod']\n",
    "                 ]\n",
    "        opt_input2 = [o.to(device) for o in opt_input2]\n",
    "\n",
    "        opt_input2 = torch.cat(opt_input2,axis=1).to(device)\n",
    "        decision2_opt = model(opt_input2,position=1,use_saved_memory= (not train))[:,1]\n",
    "        \n",
    "        opt_loss2 = bce(decision2_opt,y_opt[:,1])\n",
    "        opt_loss2 = torch.mul(opt_loss2,opt_weights[1])\n",
    "        \n",
    "        opt_input3 = [\n",
    "            formatdf(baseline,ids),\n",
    "            transition_dict['dlt1'],\n",
    "            transition_dict['dlt2'],\n",
    "            transition_dict['pd2'],\n",
    "            transition_dict['nd2'],\n",
    "            transition_dict['cc'],\n",
    "            transition_dict['mod'],\n",
    "        ]\n",
    "        opt_input3 = [o.to(device) for o in opt_input3]\n",
    "        opt_input3 = torch.cat(opt_input3,axis=1).to(device)\n",
    "        decision3_opt = model(opt_input3,position=2,use_saved_memory= (not train))[:,2]\n",
    "        \n",
    "        opt_loss3 = bce(decision3_opt,y_opt[:,2])\n",
    "        opt_loss3 = torch.mul(opt_loss3,opt_weights[2])\n",
    "        \n",
    "        iloss = torch.add(torch.add(imitation_loss1,imitation_loss2),imitation_loss3)\n",
    "        iloss = torch.mul(iloss,imitation_weight)\n",
    "        \n",
    "        reward_loss = torch.add(torch.add(opt_loss1,opt_loss2),opt_loss3)\n",
    "        reward_loss =torch.mul(reward_loss,reward_weight)\n",
    "        \n",
    "        loss = torch.add(iloss,reward_loss)\n",
    "        \n",
    "        imitation_tloss = torch.FloatTensor([0]).to(device)\n",
    "        opt_tloss = torch.FloatTensor([0]).to(device)\n",
    "        n_rows = xxtrain.shape[0]\n",
    "        if reward_triplet_weight + imitation_triplet_weight > 0.0001:\n",
    "            for i in range(n_rows):\n",
    "                \n",
    "                if imitation_triplet_weight > .0001:\n",
    "                    imitation_tloss += get_tloss(i,0,ytrain,xxtrain,True)\n",
    "                    imitation_tloss += get_tloss(i,1,ytrain,x1_imitation,True)\n",
    "                    imitation_tloss += get_tloss(i,2,ytrain,x2_imitation,True)\n",
    "                if reward_triplet_weight > .0001:\n",
    "                    opt_tloss += get_tloss(i,0,y_opt,xxtrain,False)\n",
    "                    opt_tloss += get_tloss(i,1,y_opt,opt_input2,False)\n",
    "                    opt_tloss += get_tloss(i,2,y_opt,opt_input3,False)\n",
    "            loss += torch.mul(imitation_tloss[0],imitation_triplet_weight/n_rows)\n",
    "            loss += torch.mul(opt_tloss[0],reward_triplet_weight/n_rows)\n",
    "        losses = [iloss,reward_loss,imitation_tloss*imitation_triplet_weight/n_rows,opt_tloss*reward_triplet_weight/n_rows]\n",
    "        \n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            return losses\n",
    "        else:\n",
    "            scores = []\n",
    "            distributions = [decision1_opt.mean().item(),decision2_opt.mean().item(),decision3_opt.mean().item()]\n",
    "            imitation = [decision1_imitation,decision2_imitation,decision3_imitation]\n",
    "            optimal = [decision1_opt,decision2_opt,decision3_opt]\n",
    "            for i,decision_im in enumerate(imitation):\n",
    "                deci = decision_im.cpu().detach().numpy()\n",
    "                deci0 = (deci > .5).astype(int)\n",
    "                iout = ytrain[:,i].cpu().detach().numpy()\n",
    "                acci = accuracy_score(iout,deci0)\n",
    "                try:\n",
    "                    auci = roc_auc_score(iout,deci)\n",
    "                except:\n",
    "                    auci = -1\n",
    "                \n",
    "                deco = optimal[i].cpu().detach().numpy()\n",
    "                deci0 = (deco > .5).astype(int)\n",
    "                oout = y_opt[:,i].cpu().detach().numpy()\n",
    "                acco = accuracy_score(oout,deci0)\n",
    "                try:\n",
    "                    auco = roc_auc_score(oout,deco)\n",
    "                except:\n",
    "                    auco=-1\n",
    "                scores.append({'decision': i,'optimal_auc': auco,'imitation_auc': auci,'optimal_acc': acco,'imitation_acc': acci})\n",
    "            return losses, scores, distributions\n",
    "        \n",
    "    best_val_loss = torch.tensor(1000000000.0)\n",
    "    steps_since_improvement = 0\n",
    "    best_val_score = {}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        losses = step(True)\n",
    "        val_losses,val_metrics,val_distributions = step(False)\n",
    "        vl = val_losses[0] + val_losses[1]\n",
    "        for vm in val_metrics:\n",
    "            vl += (-((vm['optimal_auc']*reward_weight) + (vm['imitation_auc']*imitation_weight)))/10\n",
    "        if verbose:\n",
    "            print('______epoch',str(epoch),'_____')\n",
    "            print('val reward',val_losses[1].item())\n",
    "            print('imitation reward', val_losses[0].item())\n",
    "            print('distance losses',val_losses[2].item(),val_losses[-1].item())\n",
    "            print('distributions',val_distributions)\n",
    "            print(val_metrics)\n",
    "        if vl < best_val_loss:\n",
    "            best_val_loss = vl\n",
    "            best_val_score = val_metrics\n",
    "            best_val_distributions = val_distributions\n",
    "            steps_since_improvement = 0\n",
    "            torch.save(model.state_dict(),save_file)\n",
    "        else:\n",
    "            steps_since_improvement += 1\n",
    "        if steps_since_improvement > patience:\n",
    "            break\n",
    "    print('++++++++++Final+++++++++++')\n",
    "    print('best',best_val_loss)\n",
    "    print(best_val_score)\n",
    "    model.load_state_dict(torch.load(save_file))\n",
    "    model.eval()\n",
    "    return model, best_val_score, best_val_loss, best_val_distributions\n",
    "\n",
    "from Models import *\n",
    "args = {\n",
    "    'hidden_layers': [1000], \n",
    "    'opt_layer_size': 20, \n",
    "    'imitation_layer_size': 20, \n",
    "    'dropout': 0.25, \n",
    "    'input_dropout': 0.1, \n",
    "    'shufflecol_chance': 0.5\n",
    "}\n",
    "\n",
    "#1.8424\n",
    "decision_model, decision_score, decision_loss, _ = train_decision_model_triplet(\n",
    "    model1,model2,model3,smodel3,\n",
    "    lr=.01,\n",
    "    imitation_weight=1,\n",
    "    reward_weight=1,\n",
    "    patience=10,\n",
    "    imitation_triplet_weight=0,\n",
    "    reward_triplet_weight =0,\n",
    "    verbose=True,\n",
    "    weights=[1,1,1,1], #realtive weight of survival, feeding tube, aspiration, andl lrc\n",
    "    tweights=[0,0,3,0], #weight for OS, LRC, FDM, and event (any + FT or AS at 6m) as time to event in weeks\n",
    "    use_attention=True,\n",
    "    **args)\n",
    "decision_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "73f37f8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decision</th>\n",
       "      <th>optimal_auc</th>\n",
       "      <th>imitation_auc</th>\n",
       "      <th>optimal_acc</th>\n",
       "      <th>imitation_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.960981</td>\n",
       "      <td>0.614027</td>\n",
       "      <td>0.870748</td>\n",
       "      <td>0.836735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.982343</td>\n",
       "      <td>0.624728</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>0.761905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.996454</td>\n",
       "      <td>0.797521</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.816327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   decision  optimal_auc  imitation_auc  optimal_acc  imitation_acc\n",
       "0         0     0.960981       0.614027     0.870748       0.836735\n",
       "1         1     0.982343       0.624728     0.959184       0.761905\n",
       "2         2     0.996454       0.797521     0.979592       0.816327"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_model.set_device('cpu')\n",
    "torch.save(decision_model,'../resources/decision_model.pt')\n",
    "pd.DataFrame(decision_score).to_csv('../results/policy_model_score.csv')\n",
    "pd.DataFrame(decision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc3c8bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decision</th>\n",
       "      <th>optimal_auc</th>\n",
       "      <th>imitation_auc</th>\n",
       "      <th>optimal_acc</th>\n",
       "      <th>imitation_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.958904</td>\n",
       "      <td>0.617366</td>\n",
       "      <td>0.993197</td>\n",
       "      <td>0.884354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.680163</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>0.768707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.800699</td>\n",
       "      <td>0.816910</td>\n",
       "      <td>0.965986</td>\n",
       "      <td>0.809524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   decision  optimal_auc  imitation_auc  optimal_acc  imitation_acc\n",
       "0         0     0.958904       0.617366     0.993197       0.884354\n",
       "1         1     0.865385       0.680163     0.959184       0.768707\n",
       "2         2     0.800699       0.816910     0.965986       0.809524"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(decision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b25922a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.8424\n",
    "decision_model2, decision_score2, decision_loss2, _ = train_decision_model_triplet(\n",
    "    model1,model2,model3,smodel3,\n",
    "    lr=.01,\n",
    "    imitation_weight=1,\n",
    "    reward_weight=1,\n",
    "    patience=10,\n",
    "    imitation_triplet_weight=0,\n",
    "    reward_triplet_weight =0,\n",
    "    verbose=True,\n",
    "    weights=[0,0,0,0], #realtive weight of survival, feeding tube, aspiration, andl lrc\n",
    "    tweights=[1,1,0,0], #weight for OS, LRC, FDM, and event (any + FT or AS at 6m) as time to event in weeks\n",
    "    use_attention=True,\n",
    "    **args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a7faad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decision_model(\n",
    "    tmodel1,\n",
    "    tmodel2,\n",
    "    tmodel3,\n",
    "    smodel3,\n",
    "    use_default_split=True,\n",
    "    use_bagging_split=False,\n",
    "    lr=.0001,\n",
    "    epochs=10000,\n",
    "    patience=50,\n",
    "    weights=[0,.5,.5,0], #realtive weight of survival, feeding tube, aspiration, andl lrc\n",
    "    tweights=[1,1,1,0]\n",
    "    imitation_weights=[.5,1,1],#weights of imitation decisions, because ic overtrains too quickly\n",
    "    imitation_weight=0.1,\n",
    "    shufflecol_chance = 0.1,\n",
    "    reward_weight=1,\n",
    "    imitation_triplet_weight=0,\n",
    "    reward_triplet_weight = 0,\n",
    "    split=.7,\n",
    "    resample_training=False,\n",
    "    save_path='../data/models/',\n",
    "    file_suffix='',\n",
    "    use_gpu=True,\n",
    "    use_attention=True,\n",
    "    verbose=True,\n",
    "    threshold_decisions=True,#convert decisiosn to binary in simulation, usually breaks it\n",
    "    use_smote=False,\n",
    "    validate_with_memory=True,\n",
    "    **model_kwargs):\n",
    "    #outdated method of doing stuff, haven't updated with new loss functions idk\n",
    "    tmodel1.eval()\n",
    "    tmodel2.eval()\n",
    "    tmodel3.eval()\n",
    "    smodel3.eval()\n",
    "    train_ids, test_ids = get_tt_split(use_default_split=use_default_split,use_bagging_split=use_bagging_split,resample_training=resample_training)\n",
    "    true_ids = train_ids + test_ids #for saving memory without upsampling\n",
    "    if use_smote:\n",
    "        dataset = DTDataset(use_smote=True,smote_ids = train_ids)\n",
    "        train_ids = [i for i in dataset.processed_df.index.values if i not in test_ids]\n",
    "    else:\n",
    "        dataset = DTDataset()\n",
    "    data = dataset.processed_df.copy()\n",
    "    \n",
    "    def get_dlt(state):\n",
    "        if state == 2:\n",
    "            return data[Const.dlt2].copy()\n",
    "        d = data[Const.dlt1].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_pd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.primary_disease_states2].copy()\n",
    "        d = data[Const.primary_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_nd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.nodal_disease_states2].copy()\n",
    "        d = data[Const.nodal_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_cc(state):\n",
    "        res = data[Const.ccs].copy()\n",
    "        if state == 1:\n",
    "            res.values[:,:] = np.zeros(res.values.shape)\n",
    "        return res\n",
    "    \n",
    "    def get_mod(state):\n",
    "        res = data[Const.modifications].copy()\n",
    "        #this should have an ic condition but we don't use it anumore anywa\n",
    "        return res\n",
    "        \n",
    "    outcomedf = data[Const.outcomes]\n",
    "    baseline = dataset.get_state('baseline')\n",
    "    \n",
    "    def formatdf(d,dids=train_ids):\n",
    "        d = df_to_torch(d.loc[dids]).to(model.get_device())\n",
    "        return d\n",
    "    \n",
    "    def makegrad(v):\n",
    "        if not v.requires_grad:\n",
    "            v.requires_grad=True\n",
    "        return v\n",
    "    \n",
    "    if use_attention:\n",
    "        model = DecisionAttentionModel(baseline.shape[1],**model_kwargs)\n",
    "    else:\n",
    "        model_kwargs = {k:v for k,v in model_kwargs.items() if 'attention' not in k and 'embed' not in k}\n",
    "        model = DecisionModel(baseline.shape[1],**model_kwargs)\n",
    "\n",
    "    device = 'cpu'\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "        \n",
    "    model.set_device(device)\n",
    "\n",
    "    tmodel1.set_device(device)\n",
    "    tmodel2.set_device(device)\n",
    "    tmodel3.set_device(device)\n",
    "    smodel3.set_device(device)\n",
    "    \n",
    "    hashcode = str(hash(','.join([str(i) for i in train_ids])))\n",
    "    \n",
    "    save_file = save_path + 'model_' + model.identifier +'_hash' + hashcode + file_suffix + '.tar'\n",
    "    model.fit_normalizer(df_to_torch(baseline.loc[train_ids]).to(model.get_device()))\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "    \n",
    "    mse = torch.nn.MSELoss()\n",
    "    bce = torch.nn.BCELoss()\n",
    "    device = model.get_device()\n",
    "    def compare_decisions(d1,d2,d3,ids):\n",
    "#         ypred = np.concatenate([dd.cpu().detach().numpy().reshape(-1,1) for dd in [d1,d2,d3]],axis=1)\n",
    "        ytrue = df_to_torch(outcomedf.loc[ids])\n",
    "        dloss = bce(d1.view(-1),ytrue[:,0])\n",
    "        dloss += bce(d2.view(-1),ytrue[:,1])\n",
    "        dloss += bce(d3,view(-1),ytrue[:,2])\n",
    "        return dloss\n",
    "        \n",
    "    def remove_decisions(df):\n",
    "        cols = [c for c in df.columns if c not in Const.decisions ]\n",
    "        ddf = df[cols]\n",
    "        return ddf\n",
    "    \n",
    "    makeinput = lambda step,dids: df_to_torch(remove_decisions(dataset.get_input_state(step=step,ids=dids))).to(device)\n",
    "    thresh = lambda x: torch.sigmoid(100000000*(x - .5))\n",
    "\n",
    "    optimal_train,transitions_train = calc_optimal_decisions(dataset,train_ids,tmodel1,tmodel2,tmodel3,smodel3,\n",
    "                                           weights=weights,tweights=tweights,\n",
    "                                          )\n",
    "    optimal_test,transitions_test = calc_optimal_decisions(dataset,test_ids,tmodel1,tmodel2,tmodel3,smodel3,\n",
    "                                          weights=weights,tweights=tweights,\n",
    "                                         )\n",
    "    optimal_train = optimal_train.to(model.get_device())\n",
    "    optimal_test = optimal_test.to(model.get_device())\n",
    "    \n",
    "    #save the inputs from the whole dataset for future reference\n",
    "    if use_attention:\n",
    "        full_data = []\n",
    "        for mstep in [0,1,2]:\n",
    "            full_data_step = [baseline, get_dlt(min(mstep,1)),\n",
    "                         get_dlt(mstep),get_pd(mstep),get_nd(mstep),get_cc(mstep),get_mod(mstep)]\n",
    "            full_data_step = torch.cat([formatdf(fd,true_ids) for fd in full_data_step],axis=1)\n",
    "            full_data.append(full_data_step)\n",
    "        full_data = torch.stack(full_data)\n",
    "        model.save_memory(full_data.to(device))\n",
    "        print(full_data.shape)\n",
    "        \n",
    "    randchoice = lambda x: x[torch.randint(len(x),(1,))[0]]\n",
    "    tloss_func = torch.nn.TripletMarginLoss()\n",
    "    def get_tloss(row,step,yt,x,imitation=True):\n",
    "        if yt[:,step].std() < .001:\n",
    "            return torch.tensor([0]).to(device)\n",
    "        positive_idx= torch.nonzero(yt[:,step] == yt[row,step])\n",
    "        positive_idx = torch.stack([ii for ii in positive_idx if ii != row]).view(-1)\n",
    "        negative_idx = torch.tensor([ii for ii in range(x.shape[0]) if ii not in positive_idx and ii != row])\n",
    "        if len(positive_idx) < 1 or len(negative_idx) < 1:\n",
    "            print('no losses','n positive',len(positive_idx),'n negative',len(negative_idx),'yt',yt[row,step],'row',row,'step',step,'imitation',imitation,end='\\r')\n",
    "            return torch.tensor([0]).to(device)\n",
    "        positive = x[randchoice(positive_idx)]\n",
    "        negative = x[randchoice(negative_idx)]\n",
    "        anchor = x[row]\n",
    "        if use_attention:\n",
    "            [anchor_embedding,pos_embedding,neg_embedding] = [model.get_embedding(xx.view(1,-1),position=step,use_saved_memory=True) for xx in [anchor,positive,negative]]\n",
    "        else:    \n",
    "            [anchor_embedding,pos_embedding,neg_embedding] = [model.get_embedding(xx.view(1,-1),position=step,concatenate=False)[int(imitation)] for xx in [anchor,positive,negative]]\n",
    "        tloss = tloss_func(anchor_embedding,pos_embedding,neg_embedding)\n",
    "        return tloss\n",
    "    \n",
    "    def step(train=True):\n",
    "        if train:\n",
    "            model.train(True)\n",
    "            tmodel1.train(True)\n",
    "            tmodel2.train(True)\n",
    "            tmodel3.train(True)\n",
    "            optimizer.zero_grad()\n",
    "            ids = train_ids\n",
    "            y_opt = makegrad(optimal_train)\n",
    "        else:\n",
    "            ids = test_ids\n",
    "            model.eval()\n",
    "            tmodel1.eval()\n",
    "            tmodel2.eval()\n",
    "            tmodel3.eval()\n",
    "            y_opt = makegrad(optimal_test)\n",
    "            \n",
    "            \n",
    "        ytrain = df_to_torch(outcomedf.loc[ids]).to(device)\n",
    "        #imitation losses and decision 1\n",
    "        xxtrained = [baseline, get_dlt(0),get_dlt(0),get_pd(0),get_nd(0),get_cc(0),get_mod(0)]\n",
    "        xxtrain = torch.cat([formatdf(xx,ids) for xx in xxtrained],axis=1).to(device)\n",
    "        \n",
    "        use_memory = (not train) and validate_with_memory\n",
    "\n",
    "        o1 = model(xxtrain,position=0,use_saved_memory = use_memory)\n",
    "\n",
    "        decision1_imitation = o1[:,3]\n",
    "        \n",
    "        decision1_opt = o1[:,0]\n",
    "        if threshold_decisions:\n",
    "            decision1_opt = thresh(decision1_opt)\n",
    "\n",
    "        imitation_loss1 = bce(decision1_imitation,ytrain[:,0])\n",
    "        imitation_loss1 = torch.mul(imitation_loss1,imitation_weights[0])\n",
    "        \n",
    "        x1_imitation = [baseline, get_dlt(1),get_dlt(0),get_pd(1),get_nd(1),get_cc(1),get_mod(1)]\n",
    "        x1_imitation = [formatdf(xx1,ids) for xx1 in x1_imitation]\n",
    "        x1_imitation = torch.cat(x1_imitation,axis=1).to(device)\n",
    "        decision2_imitation = model(x1_imitation,position=1,use_saved_memory = use_memory)[:,4]\n",
    "\n",
    "        imitation_loss2 =  bce(decision2_imitation,ytrain[:,1])\n",
    "        imitation_loss2 = torch.mul(imitation_loss2,imitation_weights[1])\n",
    "        \n",
    "        x2_imitation = [baseline, get_dlt(1),get_dlt(2),get_pd(2),get_nd(2),get_cc(2),get_mod(2)]\n",
    "        \n",
    "        x2_imitation = [formatdf(xx2,ids) for xx2 in x2_imitation]\n",
    "        x2_imitation = torch.cat(x2_imitation,axis=1)\n",
    "        decision3_imitation = model(x2_imitation,position=2,use_saved_memory = use_memory)[:,5]\n",
    "\n",
    "        imitation_loss3 = bce(decision3_imitation,ytrain[:,2])\n",
    "        imitation_loss3 = torch.mul(imitation_loss3,imitation_weights[2])\n",
    "        \n",
    "        #reward decisions\n",
    "        xx1 = makeinput(1,ids)\n",
    "        xx2 = makeinput(2,ids)\n",
    "        xx3 = makeinput(3,ids)\n",
    "\n",
    "        xx1 = makegrad(xx1)\n",
    "        xx2 = makegrad(xx2)\n",
    "        xx3 = makegrad(xx3)\n",
    "        baseline_train_base = formatdf(baseline,ids)\n",
    "            \n",
    "        baseline_train = torch.clone(baseline_train_base)\n",
    "\n",
    "        \n",
    "        xi1 = torch.cat([xx1,decision1_opt.view(-1,1)],axis=1)\n",
    "        print(train,tmodel1.training,tmodel1.dropout.training)\n",
    "        [ypd1, ynd1, ymod, ydlt1] = tmodel1(xi1)['predictions']\n",
    "        print(train,tmodel1.training,tmodel1.dropout.training)\n",
    "        d1_thresh = torch.gt(decision1_opt.view(-1,1),.5).to(ypd1.device)\n",
    "        d1_scale = torch.cat([d1_thresh,d1_thresh,torch.ones(d1_thresh.view(-1,1).shape).to(ypd1.device)],dim=1)\n",
    "        ypd1= torch.mul(ypd1,d1_scale)\n",
    "        ynd1= torch.mul(ynd1,d1_scale)\n",
    "        \n",
    "        x1 = [baseline_train,ydlt1,formatdf(get_dlt(0),ids),ypd1,ynd1,formatdf(get_cc(1),ids),ymod]\n",
    "        x1= torch.cat([xx1.to(model.get_device()) for xx1 in x1],axis=1)\n",
    "        \n",
    "        decision2_opt = model(x1,position=1,use_saved_memory = use_memory)[:,1] \n",
    "        if threshold_decisions:\n",
    "            decision2_opt = thresh(decision2_opt)\n",
    "            \n",
    "        xi2 = torch.cat([xx2,decision1_opt.view(-1,1),decision2_opt.view(-1,1)],axis=1)\n",
    "        [ypd2,ynd2,ycc,ydlt2] = tmodel2(xi2)['predictions']\n",
    "\n",
    "        x2 = [baseline_train,ydlt1,ydlt2,ypd2,ynd2,ycc,ymod]\n",
    "        x2 = torch.cat([xx2.to(model.get_device()) for xx2 in x2],axis=1)\n",
    "        decision3_opt = model(x2,position=2,use_saved_memory = use_memory)[:,2]\n",
    "        \n",
    "        if threshold_decisions:\n",
    "            decision3_opt = thresh(decision3_opt)\n",
    "            \n",
    "        xi3 = torch.cat([xx3,decision1_opt.view(-1,1),decision2_opt.view(-1,1),decision3_opt.view(-1,1)],axis=1)\n",
    "        \n",
    "        outcomes = tmodel3(xi3)['predictions']\n",
    "        survival = smodel3.time_to_event(xi3,n_samples=1)\n",
    "        if not train and verbose:\n",
    "            print(torch.mean(outcomes,dim=0))\n",
    "            \n",
    "        reward_loss = torch.mean(outcome_loss(outcomes,weights) + temporal_loss(survival,tweights))\n",
    "        loss = torch.add(imitation_loss1,imitation_loss2)\n",
    "        loss = torch.add(loss,imitation_loss3)\n",
    "        loss = torch.mul(loss,imitation_weight/3)\n",
    "        loss = torch.add(loss,torch.mul(reward_loss,reward_weight))\n",
    "        \n",
    "        imitation_tloss = torch.FloatTensor([0]).to(device)\n",
    "        opt_tloss = torch.FloatTensor([0]).to(device)\n",
    "        n_rows = x1.shape[0]\n",
    "        if reward_triplet_weight + imitation_triplet_weight > 0.0001:\n",
    "            for i in range(n_rows):\n",
    "                #skip if we're using an attention model idk\n",
    "                if not use_attention and imitation_triplet_weight > .0001:\n",
    "                    imitation_tloss += get_tloss(i,0,ytrain,xxtrain,True)\n",
    "                    imitation_tloss += get_tloss(i,1,ytrain,x1_imitation,True)\n",
    "                    imitation_tloss += get_tloss(i,2,ytrain,x2_imitation,True)\n",
    "                if reward_triplet_weight > .0001:\n",
    "                    opt_tloss += get_tloss(i,0,y_opt,xxtrain,False)\n",
    "                    opt_tloss += get_tloss(i,1,y_opt,x1,False)\n",
    "                    opt_tloss += get_tloss(i,2,y_opt,x2,False)\n",
    "            loss += torch.mul(imitation_tloss[0],imitation_triplet_weight/n_rows)\n",
    "            loss += torch.mul(opt_tloss[0],reward_triplet_weight/n_rows)\n",
    "        \n",
    "        losses = [imitation_loss1+imitation_loss2+imitation_loss3,reward_loss,imitation_tloss,opt_tloss]\n",
    "\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            return losses\n",
    "        else:\n",
    "            scores = []\n",
    "            distributions = [decision1_opt.mean().item(),decision2_opt.mean().item(),decision3_opt.mean().item()]\n",
    "            imitation = [decision1_imitation,decision2_imitation,decision3_imitation]\n",
    "            optimal = [decision1_opt,decision2_opt,decision3_opt]\n",
    "            for i,decision_im in enumerate(imitation):\n",
    "                deci = decision_im.cpu().detach().numpy()\n",
    "                deci0 = (deci > .5).astype(int)\n",
    "                iout = ytrain[:,i].cpu().detach().numpy()\n",
    "                acci = accuracy_score(iout,deci0)\n",
    "                try:\n",
    "                    auci = roc_auc_score(iout,deci)\n",
    "                except:\n",
    "                    auci = -1\n",
    "                \n",
    "                deco = optimal[i].cpu().detach().numpy()\n",
    "                deci0 = (deco > .5).astype(int)\n",
    "                oout = y_opt[:,i].cpu().detach().numpy()\n",
    "                acco = accuracy_score(oout,deci0)\n",
    "                try:\n",
    "                    auco = roc_auc_score(oout,deco)\n",
    "                except:\n",
    "                    auco=-1\n",
    "                scores.append({'decision': i,'optimal_auc': auco,'imitation_auc': auci,'optimal_acc': acco,'imitation_acc': acci})\n",
    "            return losses, scores, distributions\n",
    "        \n",
    "    best_val_loss = torch.tensor(1000000000.0)\n",
    "    steps_since_improvement = 0\n",
    "    best_val_score = {}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        losses = step(True)\n",
    "        val_losses,val_metrics,val_distributions = step(False)\n",
    "        vl = val_losses[0] + val_losses[1]\n",
    "        for vm in val_metrics:\n",
    "            vl += (-((vm['optimal_auc']*reward_weight) + (vm['imitation_auc']*imitation_weight)))/10\n",
    "        if verbose:\n",
    "            print('______epoch',str(epoch),'_____')\n",
    "            print('val reward',val_losses[1].item())\n",
    "            print('imitation reward', val_losses[0].item())\n",
    "            if len(val_losses) > 2:\n",
    "                print('distance losses',val_losses[2].item(),val_losses[-1].item())\n",
    "            print('distributions',val_distributions)\n",
    "            print(val_metrics)\n",
    "        if vl < best_val_loss:\n",
    "            best_val_loss = vl\n",
    "            best_val_score = val_metrics\n",
    "            best_val_distributions = val_distributions\n",
    "            steps_since_improvement = 0\n",
    "            torch.save(model.state_dict(),save_file)\n",
    "        else:\n",
    "            steps_since_improvement += 1\n",
    "        if steps_since_improvement > patience:\n",
    "            break\n",
    "    print('++++++++++Final+++++++++++')\n",
    "    print('best',best_val_loss)\n",
    "    print(best_val_score)\n",
    "    model.load_state_dict(torch.load(save_file))\n",
    "    model.eval()\n",
    "    return model, best_val_score, best_val_loss, best_val_distributions\n",
    "\n",
    "from Models import *\n",
    "# args = {\n",
    "#     'hidden_layers': [50,50], \n",
    "#     'attention_heads': [2,2],\n",
    "#     'embed_size': 120, \n",
    "#     'dropout': 0.5, \n",
    "#     'input_dropout': 0.2, \n",
    "#     'shufflecol_chance':  0.2,\n",
    "# }\n",
    "args = {\n",
    "    'hidden_layers': [500], \n",
    "    'opt_layer_size': 20, \n",
    "    'imitation_layer_size': 20, \n",
    "    'dropout': 0.25, \n",
    "    'input_dropout': 0.25, \n",
    "    'shufflecol_chance': 0.5\n",
    "}\n",
    "from Models import *\n",
    "decision_model, _, _, _ = train_decision_model(\n",
    "    model1,model2,model3,smodel3,\n",
    "    lr=.001,\n",
    "    use_attention=True,\n",
    "    imitation_weight=1,\n",
    "    imitation_triplet_weight=0,\n",
    "    reward_triplet_weight=0,\n",
    "    reward_weight=2,\n",
    "    validate_with_memory=True,\n",
    "    use_smote=False,\n",
    "    **args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
