{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "050c878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "069b6698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OS (Calculated)', 'Locoregional control (Time)', 'FDM (months)']"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Const.timeseries_outcomes[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7418d82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score,accuracy_score,precision_recall_fscore_support\n",
    "from Constants import *\n",
    "from Preprocessing import *\n",
    "from Models import *\n",
    "import copy\n",
    "from Utils import *\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be070e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>L1A</th>\n",
       "      <th>L1B</th>\n",
       "      <th>L2A</th>\n",
       "      <th>L2B</th>\n",
       "      <th>L3</th>\n",
       "      <th>L4</th>\n",
       "      <th>L5A</th>\n",
       "      <th>L5B</th>\n",
       "      <th>L6</th>\n",
       "      <th>...</th>\n",
       "      <th>R1A</th>\n",
       "      <th>R1B</th>\n",
       "      <th>R2A</th>\n",
       "      <th>R2B</th>\n",
       "      <th>R3</th>\n",
       "      <th>R4</th>\n",
       "      <th>R5A</th>\n",
       "      <th>R5B</th>\n",
       "      <th>R6</th>\n",
       "      <th>RRPLN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>10201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>10202</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>10203</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>10204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>10205</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>536 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  L1A  L1B  L2A  L2B   L3   L4  L5A  L5B   L6  ...  R1A  R1B  R2A  \\\n",
       "0        3  0.0  0.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1        5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  1.0   \n",
       "2        6  0.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  1.0   \n",
       "3        7  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "4        8  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  1.0   \n",
       "..     ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "531  10201  0.0  0.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "532  10202  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  1.0   \n",
       "533  10203  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  1.0   \n",
       "534  10204  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  1.0   \n",
       "535  10205  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "     R2B   R3   R4  R5A  R5B   R6  RRPLN  \n",
       "0    0.0  0.0  0.0  0.0  0.0  0.0    0.0  \n",
       "1    1.0  0.0  0.0  0.0  0.0  0.0    0.0  \n",
       "2    1.0  1.0  0.0  0.0  0.0  0.0    0.0  \n",
       "3    0.0  0.0  0.0  0.0  0.0  0.0    0.0  \n",
       "4    1.0  0.0  0.0  0.0  0.0  0.0    0.0  \n",
       "..   ...  ...  ...  ...  ...  ...    ...  \n",
       "531  0.0  0.0  0.0  0.0  0.0  0.0    0.0  \n",
       "532  1.0  0.0  0.0  0.0  0.0  0.0    0.0  \n",
       "533  1.0  1.0  0.0  0.0  0.0  0.0    0.0  \n",
       "534  1.0  1.0  0.0  0.0  0.0  0.0    0.0  \n",
       "535  0.0  1.0  0.0  0.0  0.0  0.0    0.0  \n",
       "\n",
       "[536 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('../data/digital_twin_ln_monograms.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c427599a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>id</th>\n",
       "      <th>3</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>...</th>\n",
       "      <th>10196</th>\n",
       "      <th>10197</th>\n",
       "      <th>10198</th>\n",
       "      <th>10199</th>\n",
       "      <th>10200</th>\n",
       "      <th>10201</th>\n",
       "      <th>10202</th>\n",
       "      <th>10203</th>\n",
       "      <th>10204</th>\n",
       "      <th>10205</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hpv</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>55.969444</td>\n",
       "      <td>20.95</td>\n",
       "      <td>69.930556</td>\n",
       "      <td>72.319444</td>\n",
       "      <td>59.730556</td>\n",
       "      <td>60.083333</td>\n",
       "      <td>67.708333</td>\n",
       "      <td>57.858333</td>\n",
       "      <td>51.758333</td>\n",
       "      <td>56.25</td>\n",
       "      <td>...</td>\n",
       "      <td>47.619444</td>\n",
       "      <td>50.163889</td>\n",
       "      <td>70.888889</td>\n",
       "      <td>67.825</td>\n",
       "      <td>56.336111</td>\n",
       "      <td>49.566667</td>\n",
       "      <td>48.705556</td>\n",
       "      <td>77.116667</td>\n",
       "      <td>45.95</td>\n",
       "      <td>49.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>packs_per_year</th>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoking_status</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aspiration rate Pre-therapy</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_dose</th>\n",
       "      <td>66.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>69.96</td>\n",
       "      <td>70.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>...</td>\n",
       "      <td>70.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>69.96</td>\n",
       "      <td>70.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>69.96</td>\n",
       "      <td>69.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dose_fraction</th>\n",
       "      <td>2.2</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.121212</td>\n",
       "      <td>2.121212</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.12</td>\n",
       "      <td>2.121212</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.121212</td>\n",
       "      <td>...</td>\n",
       "      <td>2.121212</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.121212</td>\n",
       "      <td>2.12</td>\n",
       "      <td>2.121212</td>\n",
       "      <td>1.714286</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>2.12</td>\n",
       "      <td>2.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bilateral</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>White/Caucasion</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hispanic/Latino</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>African American/Black</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Asian</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cc_none</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cc_platinum</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cc_cetuximab</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cc_others</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_dose_adjustment</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dose_modified</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dose_delayed</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dose_cancelled</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dose_delayed_&amp;_modified</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regiment_modification</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T-category_1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T-category_2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T-category_3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T-category_4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N-category_0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N-category_1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N-category_2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N-category_3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AJCC_1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AJCC_2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AJCC_3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AJCC_4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pathological Grade_0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pathological Grade_1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pathological Grade_2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pathological Grade_3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pathological Grade_4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subsite_BOT</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subsite_GPS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subsite_NOS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subsite_Soft palate</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subsite_Tonsil</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>treatment_CC</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>treatment_IC+CC</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>treatment_IC+Radiation alone</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>treatment_Radiation alone</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DLT_Dermatological</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DLT_Neurological</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DLT_Gastrointestinal</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DLT_Hematological</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DLT_Nephrological</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DLT_Vascular</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DLT_Infection (Pneumonia)</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DLT_Other</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DLT_Dermatological 2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DLT_Neurological 2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DLT_Gastrointestinal 2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DLT_Hematological 2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DLT_Nephrological 2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DLT_Vascular 2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DLT_Infection (Pneumonia) 2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DLT_Other 2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CR Primary</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PR Primary</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SD Primary</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CR Nodal</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PR Nodal</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SD Nodal</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CR Primary 2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PR Primary 2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SD Primary 2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CR Nodal 2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PR Nodal 2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SD Nodal 2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision 1 (Induction Chemo) Y/N</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision 2 (CC / RT alone)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision 3 Neck Dissection (Y/N)</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall Survival (4 Years)</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FT</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aspiration rate Post-therapy</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LRC</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1A_ipsi</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1A_contra</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1B_ipsi</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1B_contra</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2A_ipsi</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2A_contra</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2B_ipsi</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2B_contra</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_ipsi</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_contra</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_ipsi</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_contra</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5A_ipsi</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5A_contra</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5B_ipsi</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5B_contra</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6_ipsi</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6_contra</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RPLN_ipsi</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RPLN_contra</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104 rows × 536 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "id                                    3      5          6          7      \\\n",
       "hpv                                       1      0          1          1   \n",
       "age                               55.969444  20.95  69.930556  72.319444   \n",
       "packs_per_year                          0.0   38.0       35.0        0.0   \n",
       "gender                                    1      1          0          1   \n",
       "smoking_status                          0.0    1.0        1.0        1.0   \n",
       "Aspiration rate Pre-therapy               0      0          1          0   \n",
       "total_dose                             66.0   72.0       70.0       70.0   \n",
       "dose_fraction                           2.2    1.8   2.121212   2.121212   \n",
       "bilateral                             False  False       True      False   \n",
       "White/Caucasion                        True   True       True       True   \n",
       "Hispanic/Latino                       False  False      False      False   \n",
       "African American/Black                False  False      False      False   \n",
       "Asian                                 False  False      False      False   \n",
       "cc_none                                   0      0          0          1   \n",
       "cc_platinum                               0      1          1          0   \n",
       "cc_cetuximab                              1      0          0          0   \n",
       "cc_others                                 0      0          0          0   \n",
       "no_dose_adjustment                        1      1          1          1   \n",
       "dose_modified                             0      0          0          0   \n",
       "dose_delayed                              0      0          0          0   \n",
       "dose_cancelled                            0      0          0          0   \n",
       "dose_delayed_&_modified                   0      0          0          0   \n",
       "regiment_modification                     0      0          0          0   \n",
       "T-category_1                              0      0          0          1   \n",
       "T-category_2                              1      0          0          0   \n",
       "T-category_3                              0      0          0          0   \n",
       "T-category_4                              0      1          1          0   \n",
       "N-category_0                              0      0          0          0   \n",
       "N-category_1                              1      0          0          0   \n",
       "N-category_2                              0      1          1          1   \n",
       "N-category_3                              0      0          0          0   \n",
       "AJCC_1                                    1      0          0          0   \n",
       "AJCC_2                                    0      0          0          1   \n",
       "AJCC_3                                    0      0          1          0   \n",
       "AJCC_4                                    0      1          0          0   \n",
       "Pathological Grade_0                      1      0          0          1   \n",
       "Pathological Grade_1                      0      0          0          0   \n",
       "Pathological Grade_2                      0      1          1          0   \n",
       "Pathological Grade_3                      0      0          0          0   \n",
       "Pathological Grade_4                      0      0          0          0   \n",
       "subsite_BOT                               1      1          1          0   \n",
       "subsite_GPS                               0      0          0          0   \n",
       "subsite_NOS                               0      0          0          1   \n",
       "subsite_Soft palate                       0      0          0          0   \n",
       "subsite_Tonsil                            0      0          0          0   \n",
       "treatment_CC                              1      1          1          0   \n",
       "treatment_IC+CC                           0      0          0          0   \n",
       "treatment_IC+Radiation alone              0      0          0          0   \n",
       "treatment_Radiation alone                 0      0          0          1   \n",
       "DLT_Dermatological                      0.0    0.0        0.0        0.0   \n",
       "DLT_Neurological                        0.0    0.0        0.0        0.0   \n",
       "DLT_Gastrointestinal                    0.0    0.0        0.0        0.0   \n",
       "DLT_Hematological                       0.0    0.0        0.0        0.0   \n",
       "DLT_Nephrological                         0      0          0          0   \n",
       "DLT_Vascular                              0      0          0          0   \n",
       "DLT_Infection (Pneumonia)                 0      0          0          0   \n",
       "DLT_Other                               0.0    0.0        0.0        0.0   \n",
       "DLT_Dermatological 2                    0.0    0.0        0.0        0.0   \n",
       "DLT_Neurological 2                      0.0    0.0        0.0        0.0   \n",
       "DLT_Gastrointestinal 2                  0.0    0.0        0.0        0.0   \n",
       "DLT_Hematological 2                     0.0    0.0        0.0        0.0   \n",
       "DLT_Nephrological 2                       0      0          0          0   \n",
       "DLT_Vascular 2                            0      0          0          0   \n",
       "DLT_Infection (Pneumonia) 2               0      0          0          0   \n",
       "DLT_Other 2                             0.0    0.0        0.0        0.0   \n",
       "CR Primary                                0      0          0          0   \n",
       "PR Primary                                0      0          0          0   \n",
       "SD Primary                                0      0          0          0   \n",
       "CR Nodal                                  0      0          0          0   \n",
       "PR Nodal                                  0      0          0          0   \n",
       "SD Nodal                                  0      0          0          0   \n",
       "CR Primary 2                              0      1          0          1   \n",
       "PR Primary 2                              1      0          1          0   \n",
       "SD Primary 2                              0      0          0          0   \n",
       "CR Nodal 2                                0      1          1          1   \n",
       "PR Nodal 2                                1      0          0          0   \n",
       "SD Nodal 2                                0      0          0          0   \n",
       "Decision 1 (Induction Chemo) Y/N          0      0          0          0   \n",
       "Decision 2 (CC / RT alone)                1      1          1          0   \n",
       "Decision 3 Neck Dissection (Y/N)          0      0          0          0   \n",
       "Overall Survival (4 Years)                0      0          0          0   \n",
       "FT                                        0      1          1          1   \n",
       "Aspiration rate Post-therapy              0      0          1          0   \n",
       "LRC                                       0      1          1          1   \n",
       "1A_ipsi                                 0.0    0.0        0.0        0.0   \n",
       "1A_contra                               0.0    0.0        0.0        0.0   \n",
       "1B_ipsi                                 0.0    0.0        0.0        0.0   \n",
       "1B_contra                               0.0    0.0        0.0        1.0   \n",
       "2A_ipsi                                 0.0    1.0        1.0        0.0   \n",
       "2A_contra                               1.0    0.0        1.0        0.0   \n",
       "2B_ipsi                                 0.0    1.0        1.0        0.0   \n",
       "2B_contra                               1.0    0.0        1.0        0.0   \n",
       "3_ipsi                                  0.0    0.0        1.0        0.0   \n",
       "3_contra                                0.0    0.0        1.0        0.0   \n",
       "4_ipsi                                  0.0    0.0        0.0        0.0   \n",
       "4_contra                                0.0    0.0        0.0        0.0   \n",
       "5A_ipsi                                 0.0    0.0        0.0        0.0   \n",
       "5A_contra                               0.0    0.0        0.0        0.0   \n",
       "5B_ipsi                                 0.0    0.0        0.0        0.0   \n",
       "5B_contra                               0.0    0.0        0.0        0.0   \n",
       "6_ipsi                                  0.0    0.0        0.0        0.0   \n",
       "6_contra                                0.0    0.0        0.0        0.0   \n",
       "RPLN_ipsi                               0.0    0.0        0.0        0.0   \n",
       "RPLN_contra                             0.0    0.0        0.0        0.0   \n",
       "\n",
       "id                                    8          9          10         11     \\\n",
       "hpv                                       1          1         -1          1   \n",
       "age                               59.730556  60.083333  67.708333  57.858333   \n",
       "packs_per_year                          0.0        0.0       40.0       44.0   \n",
       "gender                                    1          1          1          1   \n",
       "smoking_status                          0.0        0.0        1.0        1.0   \n",
       "Aspiration rate Pre-therapy               0          0          0          0   \n",
       "total_dose                             66.0       66.0      69.96       70.0   \n",
       "dose_fraction                           2.2        2.2       2.12   2.121212   \n",
       "bilateral                             False      False      False      False   \n",
       "White/Caucasion                       False       True       True       True   \n",
       "Hispanic/Latino                       False      False      False      False   \n",
       "African American/Black                False      False      False      False   \n",
       "Asian                                 False      False      False      False   \n",
       "cc_none                                   1          0          0          0   \n",
       "cc_platinum                               0          0          0          0   \n",
       "cc_cetuximab                              0          1          1          1   \n",
       "cc_others                                 0          0          0          0   \n",
       "no_dose_adjustment                        1          1          1          1   \n",
       "dose_modified                             0          0          0          0   \n",
       "dose_delayed                              0          0          0          0   \n",
       "dose_cancelled                            0          0          0          0   \n",
       "dose_delayed_&_modified                   0          0          0          0   \n",
       "regiment_modification                     0          0          0          0   \n",
       "T-category_1                              1          1          0          0   \n",
       "T-category_2                              0          0          0          1   \n",
       "T-category_3                              0          0          1          0   \n",
       "T-category_4                              0          0          0          0   \n",
       "N-category_0                              0          0          0          0   \n",
       "N-category_1                              1          1          1          1   \n",
       "N-category_2                              0          0          0          0   \n",
       "N-category_3                              0          0          0          0   \n",
       "AJCC_1                                    1          1          0          1   \n",
       "AJCC_2                                    0          0          0          0   \n",
       "AJCC_3                                    0          0          1          0   \n",
       "AJCC_4                                    0          0          0          0   \n",
       "Pathological Grade_0                      0          0          0          0   \n",
       "Pathological Grade_1                      0          0          0          0   \n",
       "Pathological Grade_2                      0          0          1          1   \n",
       "Pathological Grade_3                      1          1          0          0   \n",
       "Pathological Grade_4                      0          0          0          0   \n",
       "subsite_BOT                               0          1          1          0   \n",
       "subsite_GPS                               0          0          0          0   \n",
       "subsite_NOS                               0          0          0          1   \n",
       "subsite_Soft palate                       0          0          0          0   \n",
       "subsite_Tonsil                            1          0          0          0   \n",
       "treatment_CC                              0          1          1          1   \n",
       "treatment_IC+CC                           0          0          0          0   \n",
       "treatment_IC+Radiation alone              0          0          0          0   \n",
       "treatment_Radiation alone                 1          0          0          0   \n",
       "DLT_Dermatological                      0.0        0.0        0.0        0.0   \n",
       "DLT_Neurological                        0.0        0.0        0.0        0.0   \n",
       "DLT_Gastrointestinal                    0.0        0.0        0.0        0.0   \n",
       "DLT_Hematological                       0.0        0.0        0.0        0.0   \n",
       "DLT_Nephrological                         0          0          0          0   \n",
       "DLT_Vascular                              0          0          0          0   \n",
       "DLT_Infection (Pneumonia)                 0          0          0          0   \n",
       "DLT_Other                               0.0        0.0        0.0        0.0   \n",
       "DLT_Dermatological 2                    0.0        0.0        0.0        0.0   \n",
       "DLT_Neurological 2                      0.0        0.0        0.0        0.0   \n",
       "DLT_Gastrointestinal 2                  0.0        0.0        0.0        0.0   \n",
       "DLT_Hematological 2                     0.0        0.0        0.0        0.0   \n",
       "DLT_Nephrological 2                       0          0          0          0   \n",
       "DLT_Vascular 2                            0          0          0          0   \n",
       "DLT_Infection (Pneumonia) 2               0          0          0          0   \n",
       "DLT_Other 2                             0.0        0.0        0.0        0.0   \n",
       "CR Primary                                0          0          0          0   \n",
       "PR Primary                                0          0          0          0   \n",
       "SD Primary                                0          0          0          0   \n",
       "CR Nodal                                  0          0          0          0   \n",
       "PR Nodal                                  0          0          0          0   \n",
       "SD Nodal                                  0          0          0          0   \n",
       "CR Primary 2                              1          0          0          0   \n",
       "PR Primary 2                              0          1          0          1   \n",
       "SD Primary 2                              0          0          0          0   \n",
       "CR Nodal 2                                1          0          1          0   \n",
       "PR Nodal 2                                0          1          0          1   \n",
       "SD Nodal 2                                0          0          0          0   \n",
       "Decision 1 (Induction Chemo) Y/N          0          0          0          0   \n",
       "Decision 2 (CC / RT alone)                0          1          1          1   \n",
       "Decision 3 Neck Dissection (Y/N)          0          0          1          0   \n",
       "Overall Survival (4 Years)                0          0          0          0   \n",
       "FT                                        0          0          1          0   \n",
       "Aspiration rate Post-therapy              0          0          0          0   \n",
       "LRC                                       1          1          0          0   \n",
       "1A_ipsi                                 0.0        0.0        0.0        0.0   \n",
       "1A_contra                               0.0        0.0        0.0        0.0   \n",
       "1B_ipsi                                 0.0        0.0        0.0        0.0   \n",
       "1B_contra                               0.0        0.0        0.0        0.0   \n",
       "2A_ipsi                                 1.0        1.0        1.0        1.0   \n",
       "2A_contra                               0.0        0.0        0.0        0.0   \n",
       "2B_ipsi                                 1.0        1.0        1.0        1.0   \n",
       "2B_contra                               0.0        0.0        0.0        0.0   \n",
       "3_ipsi                                  0.0        0.0        0.0        1.0   \n",
       "3_contra                                0.0        0.0        0.0        0.0   \n",
       "4_ipsi                                  0.0        0.0        0.0        0.0   \n",
       "4_contra                                0.0        0.0        0.0        0.0   \n",
       "5A_ipsi                                 0.0        0.0        0.0        0.0   \n",
       "5A_contra                               0.0        0.0        0.0        0.0   \n",
       "5B_ipsi                                 0.0        0.0        0.0        0.0   \n",
       "5B_contra                               0.0        0.0        0.0        0.0   \n",
       "6_ipsi                                  0.0        0.0        0.0        0.0   \n",
       "6_contra                                0.0        0.0        0.0        0.0   \n",
       "RPLN_ipsi                               0.0        0.0        0.0        0.0   \n",
       "RPLN_contra                             0.0        0.0        0.0        0.0   \n",
       "\n",
       "id                                    13        14     ...      10196  \\\n",
       "hpv                                       0         1  ...          0   \n",
       "age                               51.758333     56.25  ...  47.619444   \n",
       "packs_per_year                          0.0      40.0  ...        5.0   \n",
       "gender                                    1         1  ...          0   \n",
       "smoking_status                          0.0       1.0  ...        0.5   \n",
       "Aspiration rate Pre-therapy               0         0  ...          0   \n",
       "total_dose                             70.0      70.0  ...       70.0   \n",
       "dose_fraction                           2.0  2.121212  ...   2.121212   \n",
       "bilateral                              True     False  ...      False   \n",
       "White/Caucasion                        True      True  ...       True   \n",
       "Hispanic/Latino                       False     False  ...      False   \n",
       "African American/Black                False     False  ...      False   \n",
       "Asian                                 False     False  ...      False   \n",
       "cc_none                                   0         0  ...          0   \n",
       "cc_platinum                               1         0  ...          1   \n",
       "cc_cetuximab                              0         0  ...          0   \n",
       "cc_others                                 0         1  ...          0   \n",
       "no_dose_adjustment                        1         1  ...          1   \n",
       "dose_modified                             0         0  ...          0   \n",
       "dose_delayed                              0         0  ...          0   \n",
       "dose_cancelled                            0         0  ...          0   \n",
       "dose_delayed_&_modified                   0         0  ...          0   \n",
       "regiment_modification                     0         0  ...          0   \n",
       "T-category_1                              0         0  ...          0   \n",
       "T-category_2                              0         1  ...          1   \n",
       "T-category_3                              0         0  ...          0   \n",
       "T-category_4                              1         0  ...          0   \n",
       "N-category_0                              0         0  ...          0   \n",
       "N-category_1                              0         0  ...          0   \n",
       "N-category_2                              1         1  ...          1   \n",
       "N-category_3                              0         0  ...          0   \n",
       "AJCC_1                                    0         0  ...          0   \n",
       "AJCC_2                                    0         1  ...          0   \n",
       "AJCC_3                                    0         0  ...          0   \n",
       "AJCC_4                                    1         0  ...          1   \n",
       "Pathological Grade_0                      0         0  ...          1   \n",
       "Pathological Grade_1                      0         0  ...          0   \n",
       "Pathological Grade_2                      1         0  ...          0   \n",
       "Pathological Grade_3                      0         1  ...          0   \n",
       "Pathological Grade_4                      0         0  ...          0   \n",
       "subsite_BOT                               1         1  ...          0   \n",
       "subsite_GPS                               0         0  ...          0   \n",
       "subsite_NOS                               0         0  ...          0   \n",
       "subsite_Soft palate                       0         0  ...          0   \n",
       "subsite_Tonsil                            0         0  ...          1   \n",
       "treatment_CC                              1         1  ...          1   \n",
       "treatment_IC+CC                           0         0  ...          0   \n",
       "treatment_IC+Radiation alone              0         0  ...          0   \n",
       "treatment_Radiation alone                 0         0  ...          0   \n",
       "DLT_Dermatological                      0.0       0.0  ...        0.0   \n",
       "DLT_Neurological                        0.0       0.0  ...        0.0   \n",
       "DLT_Gastrointestinal                    0.0       0.0  ...        0.0   \n",
       "DLT_Hematological                       0.0       0.0  ...        0.0   \n",
       "DLT_Nephrological                         0         0  ...          0   \n",
       "DLT_Vascular                              0         0  ...          0   \n",
       "DLT_Infection (Pneumonia)                 0         0  ...          0   \n",
       "DLT_Other                               0.0       0.0  ...        0.0   \n",
       "DLT_Dermatological 2                    0.0       0.0  ...        0.0   \n",
       "DLT_Neurological 2                      0.0       0.0  ...        0.0   \n",
       "DLT_Gastrointestinal 2                  0.0       0.0  ...        0.0   \n",
       "DLT_Hematological 2                     0.0       0.0  ...        0.0   \n",
       "DLT_Nephrological 2                       0         0  ...          0   \n",
       "DLT_Vascular 2                            0         0  ...          0   \n",
       "DLT_Infection (Pneumonia) 2               0         0  ...          0   \n",
       "DLT_Other 2                             0.0       0.0  ...        0.0   \n",
       "CR Primary                                0         0  ...          0   \n",
       "PR Primary                                0         0  ...          0   \n",
       "SD Primary                                0         0  ...          0   \n",
       "CR Nodal                                  0         0  ...          0   \n",
       "PR Nodal                                  0         0  ...          0   \n",
       "SD Nodal                                  0         0  ...          0   \n",
       "CR Primary 2                              0         1  ...          1   \n",
       "PR Primary 2                              1         0  ...          0   \n",
       "SD Primary 2                              0         0  ...          0   \n",
       "CR Nodal 2                                0         0  ...          1   \n",
       "PR Nodal 2                                1         1  ...          0   \n",
       "SD Nodal 2                                0         0  ...          0   \n",
       "Decision 1 (Induction Chemo) Y/N          0         0  ...          0   \n",
       "Decision 2 (CC / RT alone)                1         1  ...          1   \n",
       "Decision 3 Neck Dissection (Y/N)          0         0  ...          0   \n",
       "Overall Survival (4 Years)                0         0  ...          1   \n",
       "FT                                        1         0  ...          0   \n",
       "Aspiration rate Post-therapy              0         0  ...          0   \n",
       "LRC                                       1         1  ...          1   \n",
       "1A_ipsi                                 0.0       0.0  ...        0.0   \n",
       "1A_contra                               0.0       0.0  ...        0.0   \n",
       "1B_ipsi                                 0.0       0.0  ...        0.0   \n",
       "1B_contra                               0.0       0.0  ...        0.0   \n",
       "2A_ipsi                                 1.0       1.0  ...        0.0   \n",
       "2A_contra                               1.0       0.0  ...        0.0   \n",
       "2B_ipsi                                 1.0       1.0  ...        0.0   \n",
       "2B_contra                               1.0       0.0  ...        0.0   \n",
       "3_ipsi                                  0.0       0.0  ...        0.0   \n",
       "3_contra                                0.0       1.0  ...        0.0   \n",
       "4_ipsi                                  0.0       0.0  ...        0.0   \n",
       "4_contra                                0.0       0.0  ...        0.0   \n",
       "5A_ipsi                                 0.0       0.0  ...        1.0   \n",
       "5A_contra                               0.0       0.0  ...        0.0   \n",
       "5B_ipsi                                 0.0       0.0  ...        0.0   \n",
       "5B_contra                               0.0       0.0  ...        0.0   \n",
       "6_ipsi                                  0.0       0.0  ...        0.0   \n",
       "6_contra                                0.0       0.0  ...        0.0   \n",
       "RPLN_ipsi                               0.0       0.0  ...        1.0   \n",
       "RPLN_contra                             0.0       0.0  ...        0.0   \n",
       "\n",
       "id                                    10197      10198     10199      10200  \\\n",
       "hpv                                       1         -1         0          1   \n",
       "age                               50.163889  70.888889    67.825  56.336111   \n",
       "packs_per_year                          0.0       50.0       0.0        0.0   \n",
       "gender                                    1          0         1          1   \n",
       "smoking_status                          0.0        0.5       0.0        0.0   \n",
       "Aspiration rate Pre-therapy               0          0         0          0   \n",
       "total_dose                             72.0       66.0      70.0      69.96   \n",
       "dose_fraction                           1.8        2.2  2.121212       2.12   \n",
       "bilateral                             False      False     False      False   \n",
       "White/Caucasion                        True       True      True       True   \n",
       "Hispanic/Latino                       False      False     False      False   \n",
       "African American/Black                False      False     False      False   \n",
       "Asian                                 False      False     False      False   \n",
       "cc_none                                   0          1         0          0   \n",
       "cc_platinum                               1          0         1          1   \n",
       "cc_cetuximab                              0          0         0          0   \n",
       "cc_others                                 0          0         0          0   \n",
       "no_dose_adjustment                        0          1         1          1   \n",
       "dose_modified                             0          0         0          0   \n",
       "dose_delayed                              0          0         0          0   \n",
       "dose_cancelled                            0          0         0          0   \n",
       "dose_delayed_&_modified                   1          0         0          0   \n",
       "regiment_modification                     0          0         0          0   \n",
       "T-category_1                              0          1         0          0   \n",
       "T-category_2                              0          0         1          0   \n",
       "T-category_3                              1          0         0          1   \n",
       "T-category_4                              0          0         0          0   \n",
       "N-category_0                              0          0         0          0   \n",
       "N-category_1                              0          0         0          1   \n",
       "N-category_2                              0          1         1          0   \n",
       "N-category_3                              1          0         0          0   \n",
       "AJCC_1                                    0          0         0          0   \n",
       "AJCC_2                                    0          0         0          1   \n",
       "AJCC_3                                    1          0         0          0   \n",
       "AJCC_4                                    0          1         1          0   \n",
       "Pathological Grade_0                      0          0         1          0   \n",
       "Pathological Grade_1                      0          1         0          0   \n",
       "Pathological Grade_2                      1          0         0          1   \n",
       "Pathological Grade_3                      0          0         0          0   \n",
       "Pathological Grade_4                      0          0         0          0   \n",
       "subsite_BOT                               1          0         1          0   \n",
       "subsite_GPS                               0          0         0          0   \n",
       "subsite_NOS                               0          0         0          1   \n",
       "subsite_Soft palate                       0          0         0          0   \n",
       "subsite_Tonsil                            0          1         0          0   \n",
       "treatment_CC                              0          0         1          1   \n",
       "treatment_IC+CC                           1          0         0          0   \n",
       "treatment_IC+Radiation alone              0          0         0          0   \n",
       "treatment_Radiation alone                 0          1         0          0   \n",
       "DLT_Dermatological                      1.0        0.0       0.0        0.0   \n",
       "DLT_Neurological                        0.0        0.0       0.0        0.0   \n",
       "DLT_Gastrointestinal                    0.0        0.0       0.0        0.0   \n",
       "DLT_Hematological                       0.0        0.0       0.0        0.0   \n",
       "DLT_Nephrological                         0          0         0          0   \n",
       "DLT_Vascular                              0          0         0          0   \n",
       "DLT_Infection (Pneumonia)                 0          0         0          0   \n",
       "DLT_Other                               0.0        0.0       0.0        0.0   \n",
       "DLT_Dermatological 2                    0.0        0.0       0.0        0.0   \n",
       "DLT_Neurological 2                      0.0        0.0       0.0        0.0   \n",
       "DLT_Gastrointestinal 2                  0.0        0.0       1.0        0.0   \n",
       "DLT_Hematological 2                     0.0        0.0       0.0        0.0   \n",
       "DLT_Nephrological 2                       0          0         0          0   \n",
       "DLT_Vascular 2                            0          0         0          0   \n",
       "DLT_Infection (Pneumonia) 2               0          0         0          0   \n",
       "DLT_Other 2                             0.0        0.0       0.0        0.0   \n",
       "CR Primary                                0          0         0          0   \n",
       "PR Primary                                1          0         0          0   \n",
       "SD Primary                                0          0         0          0   \n",
       "CR Nodal                                  0          0         0          0   \n",
       "PR Nodal                                  1          0         0          0   \n",
       "SD Nodal                                  0          0         0          0   \n",
       "CR Primary 2                              1          0         1          1   \n",
       "PR Primary 2                              0          1         0          0   \n",
       "SD Primary 2                              0          0         0          0   \n",
       "CR Nodal 2                                1          0         1          0   \n",
       "PR Nodal 2                                0          1         0          1   \n",
       "SD Nodal 2                                0          0         0          0   \n",
       "Decision 1 (Induction Chemo) Y/N          1          0         0          0   \n",
       "Decision 2 (CC / RT alone)                1          0         1          1   \n",
       "Decision 3 Neck Dissection (Y/N)          0          1         0          0   \n",
       "Overall Survival (4 Years)                1          1         1          1   \n",
       "FT                                        0          0         0          0   \n",
       "Aspiration rate Post-therapy              0          0         0          0   \n",
       "LRC                                       1          1         1          1   \n",
       "1A_ipsi                                 0.0        0.0       0.0        0.0   \n",
       "1A_contra                               0.0        0.0       0.0        0.0   \n",
       "1B_ipsi                                 0.0        0.0       0.0        0.0   \n",
       "1B_contra                               0.0        0.0       0.0        0.0   \n",
       "2A_ipsi                                 1.0        1.0       0.0        0.0   \n",
       "2A_contra                               0.0        0.0       1.0        0.0   \n",
       "2B_ipsi                                 1.0        1.0       0.0        0.0   \n",
       "2B_contra                               0.0        0.0       1.0        0.0   \n",
       "3_ipsi                                  0.0        0.0       0.0        1.0   \n",
       "3_contra                                0.0        0.0       1.0        0.0   \n",
       "4_ipsi                                  0.0        0.0       0.0        0.0   \n",
       "4_contra                                0.0        0.0       0.0        0.0   \n",
       "5A_ipsi                                 0.0        0.0       0.0        0.0   \n",
       "5A_contra                               0.0        0.0       0.0        0.0   \n",
       "5B_ipsi                                 0.0        0.0       0.0        0.0   \n",
       "5B_contra                               0.0        0.0       0.0        0.0   \n",
       "6_ipsi                                  0.0        0.0       0.0        0.0   \n",
       "6_contra                                0.0        0.0       0.0        0.0   \n",
       "RPLN_ipsi                               0.0        0.0       0.0        0.0   \n",
       "RPLN_contra                             0.0        0.0       0.0        0.0   \n",
       "\n",
       "id                                    10201      10202      10203  10204  \\\n",
       "hpv                                       1          0          1      0   \n",
       "age                               49.566667  48.705556  77.116667  45.95   \n",
       "packs_per_year                         30.0       30.0        0.0    5.0   \n",
       "gender                                    1          1          1      1   \n",
       "smoking_status                          1.0        1.0        0.0    0.5   \n",
       "Aspiration rate Pre-therapy               0          0          0      0   \n",
       "total_dose                             70.0       72.0       70.0  69.96   \n",
       "dose_fraction                      2.121212   1.714286   2.333333   2.12   \n",
       "bilateral                             False      False      False   True   \n",
       "White/Caucasion                        True      False       True   True   \n",
       "Hispanic/Latino                       False       True      False  False   \n",
       "African American/Black                False      False      False  False   \n",
       "Asian                                 False      False      False  False   \n",
       "cc_none                                   0          0          1      0   \n",
       "cc_platinum                               1          0          0      1   \n",
       "cc_cetuximab                              0          0          0      0   \n",
       "cc_others                                 0          1          0      0   \n",
       "no_dose_adjustment                        1          1          1      1   \n",
       "dose_modified                             0          0          0      0   \n",
       "dose_delayed                              0          0          0      0   \n",
       "dose_cancelled                            0          0          0      0   \n",
       "dose_delayed_&_modified                   0          0          0      0   \n",
       "regiment_modification                     0          0          0      0   \n",
       "T-category_1                              0          0          1      0   \n",
       "T-category_2                              0          0          0      0   \n",
       "T-category_3                              1          0          0      1   \n",
       "T-category_4                              0          1          0      0   \n",
       "N-category_0                              0          0          0      0   \n",
       "N-category_1                              1          0          1      0   \n",
       "N-category_2                              0          1          0      0   \n",
       "N-category_3                              0          0          0      1   \n",
       "AJCC_1                                    0          0          1      0   \n",
       "AJCC_2                                    1          0          0      0   \n",
       "AJCC_3                                    0          0          0      0   \n",
       "AJCC_4                                    0          1          0      1   \n",
       "Pathological Grade_0                      0          0          1      0   \n",
       "Pathological Grade_1                      0          0          0      0   \n",
       "Pathological Grade_2                      0          0          0      0   \n",
       "Pathological Grade_3                      1          1          0      1   \n",
       "Pathological Grade_4                      0          0          0      0   \n",
       "subsite_BOT                               1          0          0      0   \n",
       "subsite_GPS                               0          0          0      0   \n",
       "subsite_NOS                               0          1          0      0   \n",
       "subsite_Soft palate                       0          0          0      0   \n",
       "subsite_Tonsil                            0          0          1      1   \n",
       "treatment_CC                              1          1          0      1   \n",
       "treatment_IC+CC                           0          0          0      0   \n",
       "treatment_IC+Radiation alone              0          0          0      0   \n",
       "treatment_Radiation alone                 0          0          1      0   \n",
       "DLT_Dermatological                      0.0        0.0        0.0    0.0   \n",
       "DLT_Neurological                        0.0        0.0        0.0    0.0   \n",
       "DLT_Gastrointestinal                    0.0        0.0        0.0    0.0   \n",
       "DLT_Hematological                       0.0        0.0        0.0    0.0   \n",
       "DLT_Nephrological                         0          0          0      0   \n",
       "DLT_Vascular                              0          0          0      0   \n",
       "DLT_Infection (Pneumonia)                 0          0          0      0   \n",
       "DLT_Other                               0.0        0.0        0.0    0.0   \n",
       "DLT_Dermatological 2                    0.0        0.0        0.0    0.0   \n",
       "DLT_Neurological 2                      0.0        0.0        0.0    0.0   \n",
       "DLT_Gastrointestinal 2                  0.0        0.0        0.0    1.0   \n",
       "DLT_Hematological 2                     0.0        0.0        0.0    0.0   \n",
       "DLT_Nephrological 2                       0          0          0      0   \n",
       "DLT_Vascular 2                            0          0          0      0   \n",
       "DLT_Infection (Pneumonia) 2               0          0          0      0   \n",
       "DLT_Other 2                             0.0        0.0        0.0    0.0   \n",
       "CR Primary                                0          0          0      0   \n",
       "PR Primary                                0          0          0      0   \n",
       "SD Primary                                0          0          0      0   \n",
       "CR Nodal                                  0          0          0      0   \n",
       "PR Nodal                                  0          0          0      0   \n",
       "SD Nodal                                  0          0          0      0   \n",
       "CR Primary 2                              1          0          1      1   \n",
       "PR Primary 2                              0          1          0      0   \n",
       "SD Primary 2                              0          0          0      0   \n",
       "CR Nodal 2                                0          0          0      0   \n",
       "PR Nodal 2                                1          1          1      1   \n",
       "SD Nodal 2                                0          0          0      0   \n",
       "Decision 1 (Induction Chemo) Y/N          0          0          0      0   \n",
       "Decision 2 (CC / RT alone)                1          1          0      1   \n",
       "Decision 3 Neck Dissection (Y/N)          0          1          1      0   \n",
       "Overall Survival (4 Years)                1          1          1      1   \n",
       "FT                                        0          1          0      0   \n",
       "Aspiration rate Post-therapy              0          0          0      0   \n",
       "LRC                                       1          1          1      1   \n",
       "1A_ipsi                                 0.0        0.0        0.0    0.0   \n",
       "1A_contra                               0.0        0.0        0.0    0.0   \n",
       "1B_ipsi                                 0.0        0.0        0.0    0.0   \n",
       "1B_contra                               0.0        0.0        0.0    0.0   \n",
       "2A_ipsi                                 1.0        1.0        1.0    1.0   \n",
       "2A_contra                               0.0        0.0        0.0    0.0   \n",
       "2B_ipsi                                 1.0        1.0        1.0    1.0   \n",
       "2B_contra                               0.0        0.0        0.0    0.0   \n",
       "3_ipsi                                  0.0        0.0        1.0    1.0   \n",
       "3_contra                                0.0        0.0        0.0    0.0   \n",
       "4_ipsi                                  0.0        0.0        0.0    0.0   \n",
       "4_contra                                0.0        0.0        0.0    0.0   \n",
       "5A_ipsi                                 0.0        0.0        0.0    0.0   \n",
       "5A_contra                               0.0        0.0        0.0    0.0   \n",
       "5B_ipsi                                 0.0        0.0        0.0    0.0   \n",
       "5B_contra                               0.0        0.0        0.0    0.0   \n",
       "6_ipsi                                  0.0        0.0        0.0    0.0   \n",
       "6_contra                                0.0        0.0        0.0    0.0   \n",
       "RPLN_ipsi                               0.0        0.0        0.0    0.0   \n",
       "RPLN_contra                             0.0        0.0        0.0    0.0   \n",
       "\n",
       "id                                    10205  \n",
       "hpv                                       1  \n",
       "age                               49.733333  \n",
       "packs_per_year                          0.0  \n",
       "gender                                    1  \n",
       "smoking_status                          0.0  \n",
       "Aspiration rate Pre-therapy               0  \n",
       "total_dose                            69.96  \n",
       "dose_fraction                          2.12  \n",
       "bilateral                             False  \n",
       "White/Caucasion                        True  \n",
       "Hispanic/Latino                       False  \n",
       "African American/Black                False  \n",
       "Asian                                 False  \n",
       "cc_none                                   0  \n",
       "cc_platinum                               1  \n",
       "cc_cetuximab                              0  \n",
       "cc_others                                 0  \n",
       "no_dose_adjustment                        1  \n",
       "dose_modified                             0  \n",
       "dose_delayed                              0  \n",
       "dose_cancelled                            0  \n",
       "dose_delayed_&_modified                   0  \n",
       "regiment_modification                     0  \n",
       "T-category_1                              0  \n",
       "T-category_2                              0  \n",
       "T-category_3                              0  \n",
       "T-category_4                              1  \n",
       "N-category_0                              0  \n",
       "N-category_1                              1  \n",
       "N-category_2                              0  \n",
       "N-category_3                              0  \n",
       "AJCC_1                                    0  \n",
       "AJCC_2                                    0  \n",
       "AJCC_3                                    1  \n",
       "AJCC_4                                    0  \n",
       "Pathological Grade_0                      0  \n",
       "Pathological Grade_1                      0  \n",
       "Pathological Grade_2                      1  \n",
       "Pathological Grade_3                      0  \n",
       "Pathological Grade_4                      0  \n",
       "subsite_BOT                               1  \n",
       "subsite_GPS                               0  \n",
       "subsite_NOS                               0  \n",
       "subsite_Soft palate                       0  \n",
       "subsite_Tonsil                            0  \n",
       "treatment_CC                              1  \n",
       "treatment_IC+CC                           0  \n",
       "treatment_IC+Radiation alone              0  \n",
       "treatment_Radiation alone                 0  \n",
       "DLT_Dermatological                      0.0  \n",
       "DLT_Neurological                        0.0  \n",
       "DLT_Gastrointestinal                    0.0  \n",
       "DLT_Hematological                       0.0  \n",
       "DLT_Nephrological                         0  \n",
       "DLT_Vascular                              0  \n",
       "DLT_Infection (Pneumonia)                 0  \n",
       "DLT_Other                               0.0  \n",
       "DLT_Dermatological 2                    0.0  \n",
       "DLT_Neurological 2                      0.0  \n",
       "DLT_Gastrointestinal 2                  0.0  \n",
       "DLT_Hematological 2                     0.0  \n",
       "DLT_Nephrological 2                       0  \n",
       "DLT_Vascular 2                            0  \n",
       "DLT_Infection (Pneumonia) 2               0  \n",
       "DLT_Other 2                             0.0  \n",
       "CR Primary                                0  \n",
       "PR Primary                                0  \n",
       "SD Primary                                0  \n",
       "CR Nodal                                  0  \n",
       "PR Nodal                                  0  \n",
       "SD Nodal                                  0  \n",
       "CR Primary 2                              1  \n",
       "PR Primary 2                              0  \n",
       "SD Primary 2                              0  \n",
       "CR Nodal 2                                0  \n",
       "PR Nodal 2                                1  \n",
       "SD Nodal 2                                0  \n",
       "Decision 1 (Induction Chemo) Y/N          0  \n",
       "Decision 2 (CC / RT alone)                1  \n",
       "Decision 3 Neck Dissection (Y/N)          1  \n",
       "Overall Survival (4 Years)                1  \n",
       "FT                                        0  \n",
       "Aspiration rate Post-therapy              0  \n",
       "LRC                                       1  \n",
       "1A_ipsi                                 0.0  \n",
       "1A_contra                               0.0  \n",
       "1B_ipsi                                 0.0  \n",
       "1B_contra                               0.0  \n",
       "2A_ipsi                                 0.0  \n",
       "2A_contra                               0.0  \n",
       "2B_ipsi                                 0.0  \n",
       "2B_contra                               0.0  \n",
       "3_ipsi                                  1.0  \n",
       "3_contra                                0.0  \n",
       "4_ipsi                                  0.0  \n",
       "4_contra                                0.0  \n",
       "5A_ipsi                                 0.0  \n",
       "5A_contra                               0.0  \n",
       "5B_ipsi                                 0.0  \n",
       "5B_contra                               0.0  \n",
       "6_ipsi                                  0.0  \n",
       "6_contra                                0.0  \n",
       "RPLN_ipsi                               0.0  \n",
       "RPLN_contra                             0.0  \n",
       "\n",
       "[104 rows x 536 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = DTDataset(use_smote=False)\n",
    "data.processed_df.T\n",
    "# data.processed_df#.shape, len(data.processed_df.index.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c748333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[133,\n",
       " 47,\n",
       " 35,\n",
       " 10,\n",
       " 279,\n",
       " 5056,\n",
       " 5035,\n",
       " 224,\n",
       " 209,\n",
       " 10063,\n",
       " 2006,\n",
       " 5020,\n",
       " 271,\n",
       " 10014,\n",
       " 5080,\n",
       " 10097,\n",
       " 10125,\n",
       " 10106,\n",
       " 2032,\n",
       " 10169,\n",
       " 2024,\n",
       " 286,\n",
       " 2015,\n",
       " 2019,\n",
       " 10026,\n",
       " 5040,\n",
       " 236,\n",
       " 187,\n",
       " 10161,\n",
       " 211,\n",
       " 5103,\n",
       " 10178,\n",
       " 2026,\n",
       " 10137,\n",
       " 184,\n",
       " 199,\n",
       " 10040,\n",
       " 272,\n",
       " 68,\n",
       " 5105,\n",
       " 10177,\n",
       " 228,\n",
       " 44,\n",
       " 242,\n",
       " 9,\n",
       " 5101,\n",
       " 10104,\n",
       " 10165,\n",
       " 10007,\n",
       " 10133,\n",
       " 10145,\n",
       " 10016,\n",
       " 264,\n",
       " 5098,\n",
       " 10023,\n",
       " 10050,\n",
       " 5120,\n",
       " 227,\n",
       " 5118,\n",
       " 2005,\n",
       " 5053,\n",
       " 10135,\n",
       " 5007,\n",
       " 10092,\n",
       " 36,\n",
       " 2001,\n",
       " 5115,\n",
       " 10005,\n",
       " 10102,\n",
       " 189,\n",
       " 5036,\n",
       " 10088,\n",
       " 254,\n",
       " 10130,\n",
       " 10086,\n",
       " 25,\n",
       " 5001,\n",
       " 5065,\n",
       " 10084,\n",
       " 195,\n",
       " 5099,\n",
       " 3,\n",
       " 5093,\n",
       " 10094,\n",
       " 7,\n",
       " 5038,\n",
       " 10068,\n",
       " 5032,\n",
       " 202,\n",
       " 274,\n",
       " 45,\n",
       " 2017,\n",
       " 10176,\n",
       " 217,\n",
       " 10160,\n",
       " 5082,\n",
       " 10012,\n",
       " 10017,\n",
       " 10100,\n",
       " 2031,\n",
       " 77,\n",
       " 10066,\n",
       " 5078,\n",
       " 117,\n",
       " 10010,\n",
       " 10170,\n",
       " 10190,\n",
       " 10058,\n",
       " 5049,\n",
       " 5086,\n",
       " 5052,\n",
       " 268,\n",
       " 2029,\n",
       " 5084,\n",
       " 10105,\n",
       " 10013,\n",
       " 245,\n",
       " 5048,\n",
       " 2020,\n",
       " 215,\n",
       " 10046,\n",
       " 5117,\n",
       " 5033,\n",
       " 267,\n",
       " 5003,\n",
       " 168,\n",
       " 31,\n",
       " 10049,\n",
       " 10180,\n",
       " 190,\n",
       " 287,\n",
       " 284,\n",
       " 5054,\n",
       " 10101,\n",
       " 208,\n",
       " 5077,\n",
       " 10091,\n",
       " 10172,\n",
       " 288,\n",
       " 5109,\n",
       " 10126,\n",
       " 10153,\n",
       " 10123,\n",
       " 5107,\n",
       " 194,\n",
       " 10131,\n",
       " 10038]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tt_split()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61274712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5083,   116, 10154,    51, 10031,    15,    67,  5112, 10128,\n",
       "       10168,  2033,    18,  5067,  2000,  5081,  2025,   277,   221,\n",
       "         252,  2013,  5017, 10053,    50,  5069, 10006,    28, 10022,\n",
       "          75,    82, 10189,  5042,   262,    24,    42,    87, 10107,\n",
       "        2011,   280, 10163,  5070,   196, 10192,    71, 10167,  5060,\n",
       "        5076,  2004,   237,  5030,   216, 10188, 10146,   214, 10141,\n",
       "        5044,   277, 10149, 10111, 10008, 10111,  5066,  2000,  2009,\n",
       "       10201,  5018,    17,  5039,   247, 10021, 10006, 10041,    17,\n",
       "       10043,  2010,   253, 10117, 10188, 10060, 10159, 10173,     6,\n",
       "        2023, 10124, 10034,  5013,   103, 10129,  5018,    81, 10061,\n",
       "       10022,   125,  5011, 10090, 10083,   218,   178,   232,  5008,\n",
       "       10127,  5008, 10118,   237,  5000, 10182, 10090, 10089,  5028,\n",
       "         214,    96,   263, 10048,    51,  5055,    53,   220,   186,\n",
       "        5015, 10087,   153,  5009, 10111, 10114, 10055,   283,  5083,\n",
       "        5018,   196,    17, 10044,   103, 10029,  5066,    27, 10048,\n",
       "       10035, 10114,   243,    50,  2030,    80, 10182,  5087,   282,\n",
       "         218, 10035, 10192, 10098,   276,  5051, 10174,    15,   276,\n",
       "         275,    15,   121, 10194,   257, 10144,    18,  5013,   223,\n",
       "       10184, 10095,   235, 10006, 10151, 10148, 10116, 10127, 10093,\n",
       "       10098,    51,    26,  5068, 10141,   232,  5017,   188,    21,\n",
       "       10110,  5015,  2013, 10115,   232,   153,   210,  5031,   219,\n",
       "        5060, 10031, 10047,   216,  2028,  5089,  5009, 10141,   257,\n",
       "         237,  2007,   243, 10184,  5017, 10090,   285, 10159,  5026,\n",
       "          79,  5079, 10019,   220,    80,  5006,  5119,   247,  5013,\n",
       "       10127,  5096, 10113,  5043,  5022,  2009, 10164, 10173, 10111,\n",
       "       10175,  5113,  5041, 10194,    23,   116,  5031,    38,  2018,\n",
       "         121,  5071, 10185,  5006,   261, 10187, 10067, 10127,  5111,\n",
       "        2021,   262,    40,  5108,   282,   201,     8,     8,  5097,\n",
       "       10039,  5102,     8,    88,  2009,   109,    81,    71,  5102,\n",
       "       10089,  2025,  5097,  5081, 10203, 10035,   270,  5011, 10062,\n",
       "        5104,    80, 10121, 10052,  5095,   249, 10193,   285,    87,\n",
       "       10061, 10183,   237,   221, 10192, 10021, 10099, 10039, 10186,\n",
       "       10157,   255, 10194,  2012,   216, 10057,   222,  5002,   240,\n",
       "         256,  5106, 10116,  5062,  5085, 10037,   150,  5068,  5063,\n",
       "        5023,    78,  2027, 10019, 10003, 10185,    23,  2004,  5047,\n",
       "       10110, 10056,   103,    13,   253,  5055,  5016, 10008,   216,\n",
       "       10193,   200, 10187, 10098, 10074, 10167,  5034, 10095,   238,\n",
       "       10043,  5071, 10090, 10199,  5063,   229,   198,  2030,  2011,\n",
       "        5075, 10152,    55,  5005, 10195,  5045,  2018, 10117,  2025,\n",
       "          26,  2007,  5037,   243,   213, 10132,   125, 10204, 10158,\n",
       "         109, 10199,   150, 10132,   255,  2008,   282,   204, 10154,\n",
       "         210,  2012, 10099,  5039,  2022,   243, 10118, 10019, 10089,\n",
       "          99,    80,    57, 10196,   196,  5008,  2011,  2000,   289,\n",
       "        2018,   223])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tt_split(resample_training=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5ef50362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_loss(ytrue,ypred,weights=None):\n",
    "    #this is just the multiclass loss now\n",
    "    loss = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "    return loss(ypred,ytrue.argmax(axis=1))\n",
    "\n",
    "def state_loss(ytrue,ypred,subweights=None,weights=None):\n",
    "    if weights is None:\n",
    "        weights = [1,1,1,1]\n",
    "    if subweights is None:\n",
    "        subweights = [None,None,None]\n",
    "    pd_loss = torch.mul(mc_loss(ytrue[0],ypred[0],weights=subweights[0]),weights[0])\n",
    "    nd_loss = torch.mul(mc_loss(ytrue[1],ypred[1],weights=subweights[1]),weights[1])\n",
    "    mod_loss = torch.mul(mc_loss(ytrue[2],ypred[2],weights=subweights[2]),weights[2])\n",
    "    loss = torch.add(pd_loss,torch.add(nd_loss,mod_loss))\n",
    "    dlt_true = ytrue[3]\n",
    "    dlt_pred = ypred[3]\n",
    "    ndlt = dlt_true.shape[1]\n",
    "#     nloss = torch.nn.NLLLoss()\n",
    "    bce = torch.nn.BCELoss()\n",
    "    for i in range(ndlt):\n",
    "        dlt_loss = bce(dlt_pred[:,i].view(-1),dlt_true[:,i].view(-1))\n",
    "        dlt_loss = torch.mul(dlt_loss,weights[3]/ndlt)\n",
    "        loss = torch.add(loss,dlt_loss)\n",
    "    return loss\n",
    "\n",
    "def outcome_loss(ytrue,ypred,weights=None,**kwargs):\n",
    "    if weights is None:\n",
    "        weights = [1,1,1,1]\n",
    "    loss = 0\n",
    "    nloss = torch.nn.BCELoss()\n",
    "    for i in range(len(weights)):\n",
    "        iloss = nloss(ypred[:,i],ytrue[i])*weights[i]\n",
    "        loss += iloss\n",
    "    return loss\n",
    "\n",
    "def mc_metrics(yt,yp,numpy=False,is_dlt=False,is_squeezed=False):\n",
    "    if not numpy:\n",
    "        yt = yt.cpu().detach().numpy()\n",
    "        yp = yp.cpu().detach().numpy()\n",
    "    #dlt prediction (binary)\n",
    "    if is_dlt:\n",
    "        acc = accuracy_score(yt,yp>.5)\n",
    "        if yt.sum() > 1:\n",
    "            auc = roc_auc_score(yt,yp)\n",
    "        else:\n",
    "            auc=-1\n",
    "        error = np.mean((yt-yp)**2)\n",
    "        return {'accuracy': acc, 'mse': error, 'auc': auc}\n",
    "    #this is a catch for when I se the dlt prediction format (encoded integer ordinal, predict as a categorical and take the argmax)\n",
    "    elif yt.ndim > 1 or is_squeezed:\n",
    "        try:\n",
    "            bacc = balanced_accuracy_score(yt.argmax(axis=1),yp.argmax(axis=1))\n",
    "        except:\n",
    "            bacc = -1\n",
    "        try:\n",
    "            roc_micro = roc_auc_score(yt,yp,average='micro')\n",
    "        except:\n",
    "            roc_micro=-1\n",
    "        try:\n",
    "            roc_macro = roc_auc_score(yt,yp,average='macro')\n",
    "        except Exception as e:\n",
    "            try: \n",
    "                roc_macro = roc_auc_score(yt[:,0:2],yp[:,0:2],average='macro')\n",
    "            except:\n",
    "                roc_macro = -1\n",
    "        try:\n",
    "            roc_weighted = roc_auc_score(yt,yp,average='weighted')\n",
    "        except:\n",
    "            try:\n",
    "                roc_weighted = roc_auc_score(yt[:,0:2],yp[:,0:2],average='weighted')\n",
    "            except:\n",
    "                roc_weighted= -1\n",
    "        return {'accuracy': bacc, 'auc_micro': roc_micro,'auc_mean': roc_macro,'auc_weighted': roc_weighted}\n",
    "    #outcomes (binary)\n",
    "    else:\n",
    "        multiclass = yp.ndim > 1\n",
    "        if multiclass:\n",
    "            yp = yp.argmax(axis=1)\n",
    "        try:\n",
    "            if not multiclass:\n",
    "                bacc = accuracy_score(yt,(yp>.5).astype(int))\n",
    "            else:\n",
    "                bacc = accuracy_score(yt,yp)\n",
    "        except Exception as e:\n",
    "            print(e,yp,yt)\n",
    "            bacc = -1\n",
    "        try:\n",
    "            roc = roc_auc_score(yt,yp)\n",
    "        except:\n",
    "            roc = -1\n",
    "        try:\n",
    "            if not multiclass:\n",
    "                pr,re,fscore,supp = precision_recall_fscore_support(yt,(yp>.5).astype(int),average='binary')\n",
    "            else:\n",
    "                pr,re,fscore,supp = precision_recall_fscore_support(yt,yp,average='macro')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            [pr,re,fscore,supp] = [-1,-1,-1,-1]\n",
    "        error = np.mean((yt-yp)**2)\n",
    "        return {'accuracy': bacc, 'mse': error, 'auc': roc,'precision': pr,'recall':re,'f1':fscore}\n",
    "\n",
    "def state_metrics(ytrue,ypred,numpy=False):\n",
    "    pd_metrics = mc_metrics(ytrue[0],ypred[0],numpy=numpy)\n",
    "    nd_metrics = mc_metrics(ytrue[1],ypred[1],numpy=numpy)\n",
    "    mod_metrics = mc_metrics(ytrue[1],ypred[1],numpy=numpy)\n",
    "    \n",
    "    dlt_metrics = []\n",
    "    dlt_true = ytrue[3]\n",
    "    dlt_pred = ypred[3]\n",
    "    ndlt = dlt_true.shape[1]\n",
    "    for i in range(ndlt):\n",
    "        dm = mc_metrics(dlt_true[:,i],dlt_pred[:,i].view(-1),is_dlt=True)\n",
    "        dlt_metrics.append(dm)\n",
    "    dlt_acc =[d['accuracy'] for d in dlt_metrics]\n",
    "    dlt_error = [d['mse'] for d in dlt_metrics]\n",
    "    dlt_auc = [d['auc'] for d in dlt_metrics]\n",
    "    \n",
    "    acc_mean = np.mean([a for a in dlt_acc if a >= 0 and a < 1])\n",
    "    auc_mean = np.mean([a for a in dlt_auc if a >= 0])\n",
    "    results = {'pd': pd_metrics,'nd': nd_metrics,'mod': mod_metrics,\n",
    "               'dlts': {'accuracy': dlt_acc,'accuracy_mean': acc_mean,'auc': dlt_auc,'auc_mean': auc_mean}\n",
    "              }\n",
    "    return results\n",
    "\n",
    "def outcome_metrics(ytrue,ypred,numpy=False):\n",
    "    res = {}\n",
    "    for i, outcome in enumerate(Const.outcomes):\n",
    "        metrics = mc_metrics(ytrue[i],ypred[:,i])\n",
    "        res[outcome] = metrics\n",
    "    return res\n",
    "\n",
    "def transition_sample(state):\n",
    "    ids = get_dt_ids()\n",
    "    \n",
    "    train_ids, test_ids = get_tt_split()\n",
    "    \n",
    "    dataset = DTDataset()\n",
    "    \n",
    "    #only train on people with  IC for state 1 since other people can't have any outcomes otherwise\n",
    "    require = None\n",
    "    if state == 1:\n",
    "        require = Const.decisions[0] #we don't expect a state update if there is no treatment\n",
    "        valid_ids = dataset.get_input_state(require=require).index.values\n",
    "        train_ids = [t for t in train_ids if t in valid_ids]\n",
    "        test_ids = [t for t in test_ids if t in valid_ids]\n",
    "    xtrain = dataset.get_input_state(step=state,ids=train_ids,require=require)\n",
    "    xtest = dataset.get_input_state(step=state,ids=test_ids,require=require)\n",
    "    ytrain = dataset.get_intermediate_outcomes(step=state,ids=train_ids,require=require)\n",
    "    ytest = dataset.get_intermediate_outcomes(step=state,ids=test_ids,require=require)\n",
    "\n",
    "    xtrain = df_to_torch(xtrain)\n",
    "    xtest = df_to_torch(xtest)\n",
    "    ytrain = [df_to_torch(t) for t in ytrain]\n",
    "    ytest= [df_to_torch(t) for t in ytest]\n",
    "    return xtrain,xtest,ytrain,ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1adea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "91c4445c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train loss 2.4475817680358887\n",
      "val loss 2.441080093383789\n",
      "______________\n",
      "epoch 1 train loss 2.4501278400421143\n",
      "val loss 2.4391050338745117\n",
      "______________\n",
      "epoch 2 train loss 2.4410994052886963\n",
      "val loss 2.4372379779815674\n",
      "______________\n",
      "epoch 3 train loss 2.4327378273010254\n",
      "val loss 2.4353485107421875\n",
      "______________\n",
      "epoch 4 train loss 2.434887647628784\n",
      "val loss 2.4334068298339844\n",
      "______________\n",
      "epoch 5 train loss 2.4287657737731934\n",
      "val loss 2.431462049484253\n",
      "______________\n",
      "epoch 6 train loss 2.430398464202881\n",
      "val loss 2.4295308589935303\n",
      "______________\n",
      "epoch 7 train loss 2.4196600914001465\n",
      "val loss 2.427605390548706\n",
      "______________\n",
      "epoch 8 train loss 2.420475959777832\n",
      "val loss 2.4257102012634277\n",
      "______________\n",
      "epoch 9 train loss 2.417592763900757\n",
      "val loss 2.4238109588623047\n",
      "______________\n",
      "epoch 10 train loss 2.4214987754821777\n",
      "val loss 2.4219558238983154\n",
      "______________\n",
      "epoch 11 train loss 2.4091365337371826\n",
      "val loss 2.4200849533081055\n",
      "______________\n",
      "epoch 12 train loss 2.405454635620117\n",
      "val loss 2.418179750442505\n",
      "______________\n",
      "epoch 13 train loss 2.4006857872009277\n",
      "val loss 2.416257858276367\n",
      "______________\n",
      "epoch 14 train loss 2.4015560150146484\n",
      "val loss 2.4143073558807373\n",
      "______________\n",
      "epoch 15 train loss 2.3917043209075928\n",
      "val loss 2.412332057952881\n",
      "______________\n",
      "epoch 16 train loss 2.3953816890716553\n",
      "val loss 2.4103710651397705\n",
      "______________\n",
      "epoch 17 train loss 2.3920364379882812\n",
      "val loss 2.408413887023926\n",
      "______________\n",
      "epoch 18 train loss 2.38460111618042\n",
      "val loss 2.406459093093872\n",
      "______________\n",
      "epoch 19 train loss 2.3868050575256348\n",
      "val loss 2.404541015625\n",
      "______________\n",
      "epoch 20 train loss 2.3755626678466797\n",
      "val loss 2.402617931365967\n",
      "______________\n",
      "epoch 21 train loss 2.3778324127197266\n",
      "val loss 2.4006683826446533\n",
      "______________\n",
      "epoch 22 train loss 2.367630958557129\n",
      "val loss 2.3986988067626953\n",
      "______________\n",
      "epoch 23 train loss 2.364100217819214\n",
      "val loss 2.3966970443725586\n",
      "______________\n",
      "epoch 24 train loss 2.3658454418182373\n",
      "val loss 2.394676685333252\n",
      "______________\n",
      "epoch 25 train loss 2.352768898010254\n",
      "val loss 2.3926477432250977\n",
      "______________\n",
      "epoch 26 train loss 2.36421275138855\n",
      "val loss 2.3906548023223877\n",
      "______________\n",
      "epoch 27 train loss 2.3553547859191895\n",
      "val loss 2.3886616230010986\n",
      "______________\n",
      "epoch 28 train loss 2.3480758666992188\n",
      "val loss 2.386688470840454\n",
      "______________\n",
      "epoch 29 train loss 2.3429718017578125\n",
      "val loss 2.3847270011901855\n",
      "______________\n",
      "epoch 30 train loss 2.337144136428833\n",
      "val loss 2.382795810699463\n",
      "______________\n",
      "epoch 31 train loss 2.326167345046997\n",
      "val loss 2.3808822631835938\n",
      "______________\n",
      "epoch 32 train loss 2.3302674293518066\n",
      "val loss 2.378953695297241\n",
      "______________\n",
      "epoch 33 train loss 2.3233304023742676\n",
      "val loss 2.37701678276062\n",
      "______________\n",
      "epoch 34 train loss 2.3117270469665527\n",
      "val loss 2.3750641345977783\n",
      "______________\n",
      "epoch 35 train loss 2.3227932453155518\n",
      "val loss 2.3731489181518555\n",
      "______________\n",
      "epoch 36 train loss 2.313720703125\n",
      "val loss 2.3712668418884277\n",
      "______________\n",
      "epoch 37 train loss 2.3099710941314697\n",
      "val loss 2.3693878650665283\n",
      "______________\n",
      "epoch 38 train loss 2.2921700477600098\n",
      "val loss 2.3674681186676025\n",
      "______________\n",
      "epoch 39 train loss 2.3104166984558105\n",
      "val loss 2.365571975708008\n",
      "______________\n",
      "epoch 40 train loss 2.2715446949005127\n",
      "val loss 2.3636891841888428\n",
      "______________\n",
      "epoch 41 train loss 2.283015727996826\n",
      "val loss 2.3618717193603516\n",
      "______________\n",
      "epoch 42 train loss 2.2977542877197266\n",
      "val loss 2.3600330352783203\n",
      "______________\n",
      "epoch 43 train loss 2.287745714187622\n",
      "val loss 2.358271360397339\n",
      "______________\n",
      "epoch 44 train loss 2.279649496078491\n",
      "val loss 2.3565924167633057\n",
      "______________\n",
      "epoch 45 train loss 2.2577812671661377\n",
      "val loss 2.3549442291259766\n",
      "______________\n",
      "epoch 46 train loss 2.26985502243042\n",
      "val loss 2.353395700454712\n",
      "______________\n",
      "epoch 47 train loss 2.2637240886688232\n",
      "val loss 2.351886749267578\n",
      "______________\n",
      "epoch 48 train loss 2.2623302936553955\n",
      "val loss 2.3504135608673096\n",
      "______________\n",
      "epoch 49 train loss 2.274402618408203\n",
      "val loss 2.348944664001465\n",
      "______________\n",
      "epoch 50 train loss 2.252314567565918\n",
      "val loss 2.347491979598999\n",
      "______________\n",
      "epoch 51 train loss 2.23644757270813\n",
      "val loss 2.346059560775757\n",
      "______________\n",
      "epoch 52 train loss 2.241636037826538\n",
      "val loss 2.3447258472442627\n",
      "______________\n",
      "epoch 53 train loss 2.2391602993011475\n",
      "val loss 2.343479633331299\n",
      "______________\n",
      "epoch 54 train loss 2.2095117568969727\n",
      "val loss 2.3423070907592773\n",
      "______________\n",
      "epoch 55 train loss 2.2329511642456055\n",
      "val loss 2.341240644454956\n",
      "______________\n",
      "epoch 56 train loss 2.24782395362854\n",
      "val loss 2.3402099609375\n",
      "______________\n",
      "epoch 57 train loss 2.2122392654418945\n",
      "val loss 2.3392090797424316\n",
      "______________\n",
      "epoch 58 train loss 2.1984872817993164\n",
      "val loss 2.338219404220581\n",
      "______________\n",
      "epoch 59 train loss 2.189896583557129\n",
      "val loss 2.3372912406921387\n",
      "______________\n",
      "epoch 60 train loss 2.2045865058898926\n",
      "val loss 2.3364672660827637\n",
      "______________\n",
      "epoch 61 train loss 2.2078075408935547\n",
      "val loss 2.335764169692993\n",
      "______________\n",
      "epoch 62 train loss 2.1887781620025635\n",
      "val loss 2.335068702697754\n",
      "______________\n",
      "epoch 63 train loss 2.1934995651245117\n",
      "val loss 2.3343379497528076\n",
      "______________\n",
      "epoch 64 train loss 2.1644163131713867\n",
      "val loss 2.3337483406066895\n",
      "______________\n",
      "epoch 65 train loss 2.1792919635772705\n",
      "val loss 2.333272933959961\n",
      "______________\n",
      "epoch 66 train loss 2.162789821624756\n",
      "val loss 2.332895278930664\n",
      "______________\n",
      "epoch 67 train loss 2.1845035552978516\n",
      "val loss 2.332594633102417\n",
      "______________\n",
      "epoch 68 train loss 2.1767022609710693\n",
      "val loss 2.332484722137451\n",
      "______________\n",
      "epoch 69 train loss 2.1576309204101562\n",
      "val loss 2.3324661254882812\n",
      "______________\n",
      "epoch 70 train loss 2.1587271690368652\n",
      "val loss 2.3326005935668945\n",
      "______________\n",
      "epoch 71 train loss 2.134181261062622\n",
      "val loss 2.3328309059143066\n",
      "______________\n",
      "epoch 72 train loss 2.1460120677948\n",
      "val loss 2.333132266998291\n",
      "______________\n",
      "epoch 73 train loss 2.131608724594116\n",
      "val loss 2.3335390090942383\n",
      "______________\n",
      "epoch 74 train loss 2.141570806503296\n",
      "val loss 2.3340346813201904\n",
      "______________\n",
      "epoch 75 train loss 2.1141738891601562\n",
      "val loss 2.334526538848877\n",
      "______________\n",
      "epoch 76 train loss 2.112245559692383\n",
      "val loss 2.3351638317108154\n",
      "______________\n",
      "epoch 77 train loss 2.114830732345581\n",
      "val loss 2.335822820663452\n",
      "______________\n",
      "epoch 78 train loss 2.1142373085021973\n",
      "val loss 2.3365862369537354\n",
      "______________\n",
      "epoch 79 train loss 2.104710340499878\n",
      "val loss 2.33748197555542\n",
      "______________\n",
      "epoch 80 train loss 2.1096529960632324\n",
      "val loss 2.338320732116699\n",
      "______________\n",
      "best loss 2.3324661254882812 {'pd': {'accuracy': 0.36142857142857143, 'auc_micro': 0.6952534191472246, 'auc_mean': 0.5613027493601995, 'auc_weighted': 0.6205988357561384}, 'nd': {'accuracy': 0.2962962962962963, 'auc_micro': 0.21078037007240547, 'auc_mean': 0.44299718886983036, 'auc_weighted': 0.33524871355060026}, 'mod': {'accuracy': 0.2962962962962963, 'auc_micro': 0.21078037007240547, 'auc_mean': 0.44299718886983036, 'auc_weighted': 0.33524871355060026}, 'dlts': {'accuracy': [0.8035714285714286, 0.8928571428571429, 0.7857142857142857, 0.9464285714285714, 0.9107142857142857], 'accuracy_mean': 0.8678571428571429, 'auc': [0.5272727272727273, 0.3933333333333333, 0.4412878787878788, 0.25786163522012573, 0.7686274509803922], 'auc_mean': 0.4776766051188915}}\n",
      "{'predictions': [tensor([[0.5479, 0.1684, 0.2837],\n",
      "        [0.5317, 0.2674, 0.2009],\n",
      "        [0.4818, 0.2734, 0.2448],\n",
      "        [0.4302, 0.2230, 0.3469],\n",
      "        [0.4538, 0.2961, 0.2501],\n",
      "        [0.5652, 0.2113, 0.2235],\n",
      "        [0.4225, 0.3056, 0.2719],\n",
      "        [0.4088, 0.2759, 0.3153],\n",
      "        [0.4958, 0.2670, 0.2372],\n",
      "        [0.4594, 0.2312, 0.3094],\n",
      "        [0.4941, 0.2863, 0.2196],\n",
      "        [0.3814, 0.2501, 0.3685],\n",
      "        [0.6191, 0.1673, 0.2136],\n",
      "        [0.4529, 0.3572, 0.1899],\n",
      "        [0.3854, 0.2820, 0.3327],\n",
      "        [0.5061, 0.2589, 0.2350],\n",
      "        [0.4500, 0.3234, 0.2266],\n",
      "        [0.3038, 0.3715, 0.3247],\n",
      "        [0.3680, 0.3087, 0.3234],\n",
      "        [0.6328, 0.1839, 0.1833],\n",
      "        [0.5010, 0.2755, 0.2235],\n",
      "        [0.4357, 0.3351, 0.2292],\n",
      "        [0.3667, 0.3682, 0.2651],\n",
      "        [0.5003, 0.2073, 0.2924],\n",
      "        [0.5155, 0.2701, 0.2145],\n",
      "        [0.2753, 0.3937, 0.3310],\n",
      "        [0.3274, 0.3944, 0.2782],\n",
      "        [0.4135, 0.2717, 0.3148],\n",
      "        [0.4792, 0.3017, 0.2190],\n",
      "        [0.3502, 0.3542, 0.2956],\n",
      "        [0.4277, 0.2728, 0.2995],\n",
      "        [0.3766, 0.4210, 0.2024],\n",
      "        [0.4736, 0.2268, 0.2996],\n",
      "        [0.3425, 0.3181, 0.3394],\n",
      "        [0.4709, 0.3018, 0.2273],\n",
      "        [0.4691, 0.2508, 0.2802],\n",
      "        [0.4984, 0.2639, 0.2376],\n",
      "        [0.3871, 0.3822, 0.2306],\n",
      "        [0.4897, 0.2572, 0.2531],\n",
      "        [0.4871, 0.2682, 0.2447],\n",
      "        [0.4925, 0.2629, 0.2446],\n",
      "        [0.4508, 0.2323, 0.3169],\n",
      "        [0.3439, 0.3085, 0.3476],\n",
      "        [0.4825, 0.2856, 0.2319],\n",
      "        [0.5170, 0.2581, 0.2249],\n",
      "        [0.5128, 0.2556, 0.2316],\n",
      "        [0.3592, 0.3686, 0.2721],\n",
      "        [0.5029, 0.2690, 0.2281],\n",
      "        [0.3777, 0.3456, 0.2767],\n",
      "        [0.5259, 0.2617, 0.2124],\n",
      "        [0.4425, 0.3299, 0.2276],\n",
      "        [0.5332, 0.2591, 0.2078],\n",
      "        [0.4679, 0.2665, 0.2655],\n",
      "        [0.3943, 0.3348, 0.2709],\n",
      "        [0.5189, 0.2822, 0.1989],\n",
      "        [0.5176, 0.2561, 0.2263]], device='cuda:0', grad_fn=<SoftmaxBackward0>), tensor([[0.7862, 0.0565, 0.1573],\n",
      "        [0.7430, 0.1003, 0.1567],\n",
      "        [0.8117, 0.0890, 0.0992],\n",
      "        [0.6071, 0.1551, 0.2378],\n",
      "        [0.6791, 0.1470, 0.1739],\n",
      "        [0.7436, 0.1094, 0.1470],\n",
      "        [0.4918, 0.1905, 0.3177],\n",
      "        [0.7336, 0.1069, 0.1595],\n",
      "        [0.7522, 0.1110, 0.1368],\n",
      "        [0.6270, 0.1397, 0.2333],\n",
      "        [0.8137, 0.0821, 0.1042],\n",
      "        [0.6117, 0.1410, 0.2473],\n",
      "        [0.3020, 0.0859, 0.6121],\n",
      "        [0.8058, 0.0643, 0.1299],\n",
      "        [0.5350, 0.1559, 0.3091],\n",
      "        [0.6890, 0.1114, 0.1996],\n",
      "        [0.5988, 0.1380, 0.2632],\n",
      "        [0.5529, 0.1655, 0.2816],\n",
      "        [0.6415, 0.1627, 0.1958],\n",
      "        [0.8052, 0.0423, 0.1525],\n",
      "        [0.6549, 0.1476, 0.1975],\n",
      "        [0.7680, 0.1052, 0.1268],\n",
      "        [0.7058, 0.1212, 0.1729],\n",
      "        [0.6496, 0.1374, 0.2130],\n",
      "        [0.6953, 0.1291, 0.1756],\n",
      "        [0.4431, 0.1413, 0.4156],\n",
      "        [0.5979, 0.1575, 0.2445],\n",
      "        [0.5510, 0.1447, 0.3043],\n",
      "        [0.7598, 0.1032, 0.1370],\n",
      "        [0.5503, 0.1298, 0.3199],\n",
      "        [0.6353, 0.1428, 0.2219],\n",
      "        [0.5964, 0.1303, 0.2732],\n",
      "        [0.7407, 0.1149, 0.1444],\n",
      "        [0.3818, 0.1640, 0.4542],\n",
      "        [0.6389, 0.1500, 0.2111],\n",
      "        [0.5412, 0.1873, 0.2715],\n",
      "        [0.7552, 0.1098, 0.1350],\n",
      "        [0.8626, 0.0571, 0.0803],\n",
      "        [0.6798, 0.1411, 0.1791],\n",
      "        [0.7316, 0.1283, 0.1401],\n",
      "        [0.7627, 0.0983, 0.1390],\n",
      "        [0.7085, 0.1159, 0.1756],\n",
      "        [0.4617, 0.1677, 0.3707],\n",
      "        [0.5450, 0.1391, 0.3159],\n",
      "        [0.7426, 0.1211, 0.1364],\n",
      "        [0.7769, 0.0957, 0.1273],\n",
      "        [0.6358, 0.1459, 0.2182],\n",
      "        [0.7834, 0.0931, 0.1235],\n",
      "        [0.5926, 0.1518, 0.2556],\n",
      "        [0.8170, 0.0604, 0.1226],\n",
      "        [0.6967, 0.1294, 0.1739],\n",
      "        [0.8440, 0.0712, 0.0848],\n",
      "        [0.6195, 0.1282, 0.2523],\n",
      "        [0.6909, 0.1100, 0.1991],\n",
      "        [0.6514, 0.1388, 0.2097],\n",
      "        [0.7350, 0.1123, 0.1527]], device='cuda:0', grad_fn=<SoftmaxBackward0>), tensor([[0.1407, 0.2796, 0.1159, 0.1526, 0.1566, 0.1546],\n",
      "        [0.1521, 0.2518, 0.1260, 0.1534, 0.1450, 0.1718],\n",
      "        [0.1403, 0.2299, 0.1346, 0.1686, 0.1517, 0.1749],\n",
      "        [0.1649, 0.2042, 0.1357, 0.1655, 0.1641, 0.1656],\n",
      "        [0.1616, 0.2109, 0.1486, 0.1602, 0.1512, 0.1675],\n",
      "        [0.1594, 0.2304, 0.1331, 0.1533, 0.1510, 0.1728],\n",
      "        [0.1621, 0.2260, 0.1475, 0.1651, 0.1433, 0.1560],\n",
      "        [0.1498, 0.2273, 0.1366, 0.1702, 0.1579, 0.1582],\n",
      "        [0.1624, 0.2121, 0.1400, 0.1599, 0.1558, 0.1698],\n",
      "        [0.1472, 0.2187, 0.1416, 0.1676, 0.1594, 0.1656],\n",
      "        [0.1692, 0.2199, 0.1343, 0.1612, 0.1507, 0.1646],\n",
      "        [0.1567, 0.2275, 0.1419, 0.1628, 0.1521, 0.1590],\n",
      "        [0.1410, 0.2921, 0.1187, 0.1486, 0.1455, 0.1540],\n",
      "        [0.1413, 0.2711, 0.1234, 0.1706, 0.1552, 0.1383],\n",
      "        [0.1521, 0.2252, 0.1431, 0.1654, 0.1533, 0.1610],\n",
      "        [0.1535, 0.2505, 0.1272, 0.1621, 0.1420, 0.1647],\n",
      "        [0.1493, 0.2429, 0.1329, 0.1710, 0.1506, 0.1533],\n",
      "        [0.1646, 0.2171, 0.1432, 0.1717, 0.1546, 0.1489],\n",
      "        [0.1593, 0.2039, 0.1426, 0.1639, 0.1637, 0.1665],\n",
      "        [0.1518, 0.2858, 0.1207, 0.1364, 0.1469, 0.1584],\n",
      "        [0.1585, 0.2174, 0.1449, 0.1565, 0.1566, 0.1662],\n",
      "        [0.1724, 0.2091, 0.1433, 0.1602, 0.1489, 0.1661],\n",
      "        [0.1673, 0.2123, 0.1410, 0.1695, 0.1535, 0.1563],\n",
      "        [0.1539, 0.2263, 0.1328, 0.1616, 0.1588, 0.1666],\n",
      "        [0.1643, 0.2100, 0.1411, 0.1605, 0.1666, 0.1575],\n",
      "        [0.1563, 0.2287, 0.1467, 0.1594, 0.1624, 0.1465],\n",
      "        [0.1644, 0.2081, 0.1505, 0.1724, 0.1517, 0.1529],\n",
      "        [0.1499, 0.2313, 0.1482, 0.1567, 0.1536, 0.1603],\n",
      "        [0.1525, 0.2133, 0.1381, 0.1605, 0.1704, 0.1652],\n",
      "        [0.1727, 0.2334, 0.1302, 0.1703, 0.1454, 0.1481],\n",
      "        [0.1637, 0.2124, 0.1435, 0.1665, 0.1589, 0.1549],\n",
      "        [0.1757, 0.2290, 0.1395, 0.1578, 0.1489, 0.1492],\n",
      "        [0.1551, 0.2055, 0.1353, 0.1609, 0.1729, 0.1702],\n",
      "        [0.1588, 0.2410, 0.1555, 0.1580, 0.1464, 0.1403],\n",
      "        [0.1565, 0.2144, 0.1445, 0.1619, 0.1685, 0.1542],\n",
      "        [0.1592, 0.2099, 0.1500, 0.1632, 0.1529, 0.1648],\n",
      "        [0.1623, 0.2111, 0.1397, 0.1602, 0.1563, 0.1702],\n",
      "        [0.1578, 0.2330, 0.1355, 0.1573, 0.1716, 0.1448],\n",
      "        [0.1569, 0.2162, 0.1392, 0.1620, 0.1550, 0.1707],\n",
      "        [0.1588, 0.2048, 0.1474, 0.1610, 0.1518, 0.1762],\n",
      "        [0.1429, 0.2317, 0.1305, 0.1674, 0.1617, 0.1659],\n",
      "        [0.1517, 0.2207, 0.1371, 0.1612, 0.1575, 0.1718],\n",
      "        [0.1688, 0.2149, 0.1475, 0.1678, 0.1527, 0.1483],\n",
      "        [0.1489, 0.2462, 0.1320, 0.1614, 0.1607, 0.1508],\n",
      "        [0.1578, 0.2124, 0.1402, 0.1561, 0.1573, 0.1762],\n",
      "        [0.1550, 0.2424, 0.1318, 0.1628, 0.1401, 0.1679],\n",
      "        [0.1679, 0.1993, 0.1549, 0.1715, 0.1546, 0.1517],\n",
      "        [0.1614, 0.2118, 0.1363, 0.1635, 0.1602, 0.1668],\n",
      "        [0.1731, 0.2098, 0.1429, 0.1623, 0.1516, 0.1603],\n",
      "        [0.1649, 0.2390, 0.1312, 0.1556, 0.1486, 0.1607],\n",
      "        [0.1621, 0.2174, 0.1465, 0.1658, 0.1488, 0.1593],\n",
      "        [0.1661, 0.2196, 0.1326, 0.1612, 0.1494, 0.1709],\n",
      "        [0.1674, 0.2234, 0.1395, 0.1600, 0.1550, 0.1548],\n",
      "        [0.1651, 0.2103, 0.1441, 0.1662, 0.1568, 0.1575],\n",
      "        [0.1527, 0.2296, 0.1353, 0.1620, 0.1562, 0.1642],\n",
      "        [0.1663, 0.2148, 0.1312, 0.1639, 0.1534, 0.1704]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>), tensor([[0.1642, 0.1145, 0.0926, 0.1017, 0.1301],\n",
      "        [0.2040, 0.2256, 0.2152, 0.1973, 0.2088],\n",
      "        [0.2335, 0.2482, 0.2404, 0.2339, 0.1953],\n",
      "        [0.2742, 0.2787, 0.2898, 0.2554, 0.2822],\n",
      "        [0.2923, 0.3069, 0.2865, 0.2641, 0.2843],\n",
      "        [0.2487, 0.2545, 0.2338, 0.2313, 0.2531],\n",
      "        [0.2961, 0.3005, 0.2757, 0.2834, 0.2975],\n",
      "        [0.2504, 0.2596, 0.2463, 0.2136, 0.2532],\n",
      "        [0.2672, 0.2865, 0.2918, 0.2469, 0.2626],\n",
      "        [0.2440, 0.2369, 0.2323, 0.2456, 0.2248],\n",
      "        [0.2662, 0.2772, 0.2912, 0.2163, 0.2439],\n",
      "        [0.2893, 0.2542, 0.2303, 0.2143, 0.2593],\n",
      "        [0.1592, 0.1046, 0.1205, 0.1087, 0.1591],\n",
      "        [0.1692, 0.2037, 0.1837, 0.1506, 0.1479],\n",
      "        [0.2236, 0.2107, 0.2083, 0.2047, 0.2131],\n",
      "        [0.2044, 0.1969, 0.1731, 0.1707, 0.1986],\n",
      "        [0.2710, 0.2509, 0.2553, 0.2421, 0.2326],\n",
      "        [0.2997, 0.3073, 0.3089, 0.2608, 0.3003],\n",
      "        [0.2704, 0.2820, 0.3105, 0.2505, 0.3129],\n",
      "        [0.1162, 0.1006, 0.0988, 0.0823, 0.0892],\n",
      "        [0.2618, 0.2654, 0.2596, 0.2509, 0.2666],\n",
      "        [0.2771, 0.2996, 0.3045, 0.2415, 0.2594],\n",
      "        [0.2749, 0.3006, 0.3101, 0.2293, 0.2497],\n",
      "        [0.2856, 0.2768, 0.2723, 0.2373, 0.2739],\n",
      "        [0.2245, 0.2403, 0.2509, 0.2247, 0.2201],\n",
      "        [0.2347, 0.2307, 0.2448, 0.2105, 0.2258],\n",
      "        [0.2910, 0.3058, 0.3004, 0.2466, 0.2874],\n",
      "        [0.2241, 0.2063, 0.2269, 0.1947, 0.2215],\n",
      "        [0.2342, 0.2313, 0.2668, 0.2064, 0.2462],\n",
      "        [0.2254, 0.2156, 0.2534, 0.1600, 0.2072],\n",
      "        [0.2693, 0.3112, 0.2894, 0.2768, 0.2987],\n",
      "        [0.2078, 0.2371, 0.2374, 0.1838, 0.2299],\n",
      "        [0.2488, 0.2626, 0.2710, 0.2433, 0.2577],\n",
      "        [0.2249, 0.2296, 0.2393, 0.2473, 0.2723],\n",
      "        [0.2310, 0.2469, 0.2606, 0.2478, 0.2346],\n",
      "        [0.2906, 0.3059, 0.2968, 0.3014, 0.3026],\n",
      "        [0.2655, 0.2872, 0.2925, 0.2468, 0.2609],\n",
      "        [0.1996, 0.2373, 0.2457, 0.1700, 0.2021],\n",
      "        [0.2851, 0.3019, 0.2883, 0.2627, 0.2849],\n",
      "        [0.2958, 0.3125, 0.2869, 0.2670, 0.2794],\n",
      "        [0.2267, 0.2498, 0.2431, 0.2039, 0.2151],\n",
      "        [0.2539, 0.2448, 0.2655, 0.2406, 0.2533],\n",
      "        [0.2871, 0.2740, 0.2688, 0.2485, 0.2682],\n",
      "        [0.2426, 0.2426, 0.2127, 0.2051, 0.2024],\n",
      "        [0.2732, 0.2970, 0.2893, 0.2611, 0.2757],\n",
      "        [0.2377, 0.2390, 0.2187, 0.2011, 0.2327],\n",
      "        [0.2930, 0.3049, 0.2842, 0.2444, 0.2748],\n",
      "        [0.2528, 0.2660, 0.3053, 0.2217, 0.2503],\n",
      "        [0.2758, 0.2812, 0.3036, 0.2685, 0.2741],\n",
      "        [0.2062, 0.1913, 0.2247, 0.1526, 0.1740],\n",
      "        [0.2603, 0.2693, 0.2737, 0.2263, 0.2733],\n",
      "        [0.2606, 0.2773, 0.2878, 0.2252, 0.2426],\n",
      "        [0.2106, 0.2257, 0.2328, 0.2184, 0.1898],\n",
      "        [0.2542, 0.2621, 0.2721, 0.2115, 0.2181],\n",
      "        [0.2555, 0.2518, 0.2420, 0.2219, 0.2172],\n",
      "        [0.2961, 0.2727, 0.3260, 0.2413, 0.2654]], device='cuda:0',\n",
      "       grad_fn=<MulBackward0>)], '5%': [tensor([[0.4658, 0.0992, 0.1889],\n",
      "        [0.4833, 0.2198, 0.1511],\n",
      "        [0.4269, 0.2261, 0.2090],\n",
      "        [0.3954, 0.1945, 0.3032],\n",
      "        [0.4046, 0.2550, 0.2200],\n",
      "        [0.5013, 0.1919, 0.1925],\n",
      "        [0.3690, 0.2697, 0.2410],\n",
      "        [0.3646, 0.2334, 0.2716],\n",
      "        [0.4328, 0.2090, 0.2086],\n",
      "        [0.4228, 0.2052, 0.2750],\n",
      "        [0.4516, 0.2604, 0.1978],\n",
      "        [0.3289, 0.2005, 0.3075],\n",
      "        [0.4656, 0.0970, 0.1519],\n",
      "        [0.4344, 0.2620, 0.1533],\n",
      "        [0.3185, 0.2509, 0.2621],\n",
      "        [0.4221, 0.2040, 0.2131],\n",
      "        [0.3951, 0.2715, 0.2073],\n",
      "        [0.2821, 0.3248, 0.3048],\n",
      "        [0.3241, 0.2778, 0.2918],\n",
      "        [0.5341, 0.1140, 0.1148],\n",
      "        [0.4443, 0.2304, 0.1896],\n",
      "        [0.3692, 0.2988, 0.2002],\n",
      "        [0.3275, 0.3419, 0.2389],\n",
      "        [0.4759, 0.1580, 0.2164],\n",
      "        [0.4800, 0.2066, 0.1786],\n",
      "        [0.2430, 0.3260, 0.2507],\n",
      "        [0.3003, 0.3575, 0.2508],\n",
      "        [0.3920, 0.2227, 0.2657],\n",
      "        [0.4316, 0.2555, 0.1783],\n",
      "        [0.3137, 0.2874, 0.2385],\n",
      "        [0.3900, 0.2329, 0.2396],\n",
      "        [0.3111, 0.3645, 0.1659],\n",
      "        [0.4110, 0.2008, 0.2716],\n",
      "        [0.3157, 0.2800, 0.2857],\n",
      "        [0.3957, 0.2565, 0.2147],\n",
      "        [0.4315, 0.2232, 0.2412],\n",
      "        [0.4267, 0.2271, 0.2196],\n",
      "        [0.3116, 0.3438, 0.1842],\n",
      "        [0.4355, 0.2144, 0.2358],\n",
      "        [0.4329, 0.2299, 0.2058],\n",
      "        [0.4257, 0.2212, 0.2144],\n",
      "        [0.4094, 0.1828, 0.2773],\n",
      "        [0.3089, 0.2691, 0.3149],\n",
      "        [0.4298, 0.2434, 0.1796],\n",
      "        [0.4537, 0.2277, 0.1971],\n",
      "        [0.4633, 0.2034, 0.1979],\n",
      "        [0.3293, 0.3189, 0.2378],\n",
      "        [0.4460, 0.2194, 0.2052],\n",
      "        [0.3342, 0.3190, 0.2337],\n",
      "        [0.4828, 0.1996, 0.1762],\n",
      "        [0.4094, 0.2670, 0.2008],\n",
      "        [0.5134, 0.2009, 0.1771],\n",
      "        [0.4086, 0.2196, 0.2363],\n",
      "        [0.3658, 0.2529, 0.2064],\n",
      "        [0.4685, 0.2326, 0.1624],\n",
      "        [0.4874, 0.2110, 0.1849]], device='cuda:0', grad_fn=<ViewBackward0>), tensor([[0.6400, 0.0264, 0.0884],\n",
      "        [0.6109, 0.0653, 0.1021],\n",
      "        [0.7604, 0.0580, 0.0708],\n",
      "        [0.5458, 0.1262, 0.2159],\n",
      "        [0.6373, 0.1144, 0.1431],\n",
      "        [0.6465, 0.0842, 0.1186],\n",
      "        [0.4537, 0.1628, 0.2746],\n",
      "        [0.6471, 0.0831, 0.1251],\n",
      "        [0.7038, 0.0902, 0.1137],\n",
      "        [0.5381, 0.1103, 0.1694],\n",
      "        [0.7744, 0.0580, 0.0783],\n",
      "        [0.5087, 0.1064, 0.2020],\n",
      "        [0.1930, 0.0490, 0.2920],\n",
      "        [0.7963, 0.0353, 0.0688],\n",
      "        [0.4597, 0.1124, 0.2200],\n",
      "        [0.5911, 0.0791, 0.1493],\n",
      "        [0.5495, 0.1146, 0.2251],\n",
      "        [0.5112, 0.1324, 0.2447],\n",
      "        [0.5779, 0.1351, 0.1608],\n",
      "        [0.7309, 0.0231, 0.0866],\n",
      "        [0.5892, 0.1118, 0.1500],\n",
      "        [0.7226, 0.0753, 0.0874],\n",
      "        [0.6078, 0.0894, 0.1286],\n",
      "        [0.5841, 0.1007, 0.1487],\n",
      "        [0.6292, 0.0915, 0.1230],\n",
      "        [0.3875, 0.1174, 0.3327],\n",
      "        [0.5594, 0.1345, 0.2031],\n",
      "        [0.5163, 0.1020, 0.2437],\n",
      "        [0.6873, 0.0677, 0.1086],\n",
      "        [0.4806, 0.0929, 0.2576],\n",
      "        [0.6185, 0.1134, 0.1710],\n",
      "        [0.4457, 0.0813, 0.1708],\n",
      "        [0.6440, 0.0956, 0.1183],\n",
      "        [0.3666, 0.1252, 0.3509],\n",
      "        [0.5618, 0.1098, 0.1437],\n",
      "        [0.4728, 0.1641, 0.2510],\n",
      "        [0.7177, 0.0958, 0.1255],\n",
      "        [0.8157, 0.0387, 0.0530],\n",
      "        [0.6259, 0.1201, 0.1498],\n",
      "        [0.6638, 0.0962, 0.1158],\n",
      "        [0.6848, 0.0795, 0.1161],\n",
      "        [0.6450, 0.0825, 0.1231],\n",
      "        [0.3937, 0.1439, 0.2965],\n",
      "        [0.4823, 0.1085, 0.2597],\n",
      "        [0.7011, 0.0985, 0.1093],\n",
      "        [0.7187, 0.0762, 0.0989],\n",
      "        [0.5664, 0.1189, 0.1836],\n",
      "        [0.7055, 0.0711, 0.0973],\n",
      "        [0.5603, 0.1112, 0.2092],\n",
      "        [0.7655, 0.0363, 0.0885],\n",
      "        [0.6291, 0.1207, 0.1532],\n",
      "        [0.8064, 0.0455, 0.0590],\n",
      "        [0.5453, 0.0905, 0.1911],\n",
      "        [0.6253, 0.0831, 0.1433],\n",
      "        [0.6104, 0.1031, 0.1790],\n",
      "        [0.6934, 0.0790, 0.1103]], device='cuda:0', grad_fn=<ViewBackward0>), tensor([[0.1070, 0.2221, 0.0919, 0.1199, 0.1318, 0.1187],\n",
      "        [0.1333, 0.2127, 0.1068, 0.1436, 0.1271, 0.1488],\n",
      "        [0.1268, 0.1894, 0.1230, 0.1445, 0.1312, 0.1460],\n",
      "        [0.1385, 0.1774, 0.1287, 0.1516, 0.1493, 0.1562],\n",
      "        [0.1450, 0.1961, 0.1343, 0.1474, 0.1372, 0.1537],\n",
      "        [0.1479, 0.1908, 0.1244, 0.1391, 0.1331, 0.1557],\n",
      "        [0.1442, 0.2058, 0.1403, 0.1485, 0.1342, 0.1400],\n",
      "        [0.1341, 0.2139, 0.1168, 0.1503, 0.1478, 0.1387],\n",
      "        [0.1410, 0.1900, 0.1279, 0.1426, 0.1425, 0.1508],\n",
      "        [0.1248, 0.1959, 0.1155, 0.1455, 0.1502, 0.1447],\n",
      "        [0.1551, 0.1950, 0.1168, 0.1524, 0.1304, 0.1470],\n",
      "        [0.1378, 0.1983, 0.1237, 0.1411, 0.1393, 0.1375],\n",
      "        [0.1146, 0.2329, 0.0997, 0.1052, 0.1271, 0.1123],\n",
      "        [0.1141, 0.2307, 0.0981, 0.1434, 0.1245, 0.1155],\n",
      "        [0.1360, 0.1867, 0.1230, 0.1556, 0.1239, 0.1460],\n",
      "        [0.1394, 0.2027, 0.1051, 0.1349, 0.1242, 0.1260],\n",
      "        [0.1385, 0.2096, 0.1184, 0.1509, 0.1389, 0.1418],\n",
      "        [0.1522, 0.1902, 0.1321, 0.1555, 0.1428, 0.1323],\n",
      "        [0.1456, 0.1875, 0.1304, 0.1465, 0.1455, 0.1577],\n",
      "        [0.1267, 0.1987, 0.0962, 0.1124, 0.1177, 0.1359],\n",
      "        [0.1502, 0.1779, 0.1331, 0.1391, 0.1391, 0.1554],\n",
      "        [0.1573, 0.1817, 0.1256, 0.1457, 0.1335, 0.1436],\n",
      "        [0.1425, 0.1822, 0.1197, 0.1568, 0.1333, 0.1405],\n",
      "        [0.1417, 0.1923, 0.1212, 0.1420, 0.1395, 0.1479],\n",
      "        [0.1409, 0.1793, 0.1258, 0.1454, 0.1468, 0.1406],\n",
      "        [0.1404, 0.2071, 0.1197, 0.1377, 0.1430, 0.1321],\n",
      "        [0.1513, 0.1816, 0.1326, 0.1591, 0.1403, 0.1439],\n",
      "        [0.1366, 0.2058, 0.1226, 0.1279, 0.1327, 0.1431],\n",
      "        [0.1423, 0.1949, 0.1223, 0.1422, 0.1455, 0.1483],\n",
      "        [0.1478, 0.1957, 0.1151, 0.1397, 0.1262, 0.1225],\n",
      "        [0.1483, 0.1956, 0.1298, 0.1538, 0.1456, 0.1425],\n",
      "        [0.1529, 0.2004, 0.1224, 0.1379, 0.1267, 0.1326],\n",
      "        [0.1406, 0.1791, 0.1292, 0.1453, 0.1589, 0.1477],\n",
      "        [0.1370, 0.2103, 0.1331, 0.1411, 0.1341, 0.1299],\n",
      "        [0.1430, 0.1903, 0.1323, 0.1505, 0.1519, 0.1352],\n",
      "        [0.1542, 0.1967, 0.1395, 0.1477, 0.1432, 0.1506],\n",
      "        [0.1457, 0.1931, 0.1243, 0.1491, 0.1413, 0.1475],\n",
      "        [0.1426, 0.1951, 0.1208, 0.1311, 0.1434, 0.1272],\n",
      "        [0.1435, 0.1955, 0.1288, 0.1491, 0.1438, 0.1527],\n",
      "        [0.1417, 0.1915, 0.1370, 0.1491, 0.1364, 0.1500],\n",
      "        [0.1296, 0.1941, 0.1271, 0.1454, 0.1471, 0.1504],\n",
      "        [0.1268, 0.1963, 0.1230, 0.1387, 0.1378, 0.1524],\n",
      "        [0.1536, 0.1843, 0.1365, 0.1538, 0.1360, 0.1345],\n",
      "        [0.1291, 0.2052, 0.1184, 0.1460, 0.1369, 0.1299],\n",
      "        [0.1449, 0.1951, 0.1226, 0.1487, 0.1355, 0.1578],\n",
      "        [0.1283, 0.2143, 0.1180, 0.1495, 0.1211, 0.1433],\n",
      "        [0.1598, 0.1729, 0.1343, 0.1475, 0.1407, 0.1403],\n",
      "        [0.1453, 0.1814, 0.1203, 0.1438, 0.1489, 0.1424],\n",
      "        [0.1543, 0.1916, 0.1266, 0.1503, 0.1383, 0.1492],\n",
      "        [0.1359, 0.2025, 0.1139, 0.1351, 0.1265, 0.1315],\n",
      "        [0.1381, 0.1991, 0.1316, 0.1441, 0.1352, 0.1463],\n",
      "        [0.1427, 0.1867, 0.1116, 0.1456, 0.1291, 0.1580],\n",
      "        [0.1482, 0.1978, 0.1147, 0.1416, 0.1341, 0.1328],\n",
      "        [0.1465, 0.1864, 0.1271, 0.1516, 0.1400, 0.1353],\n",
      "        [0.1368, 0.1933, 0.1250, 0.1428, 0.1432, 0.1401],\n",
      "        [0.1415, 0.1955, 0.1195, 0.1520, 0.1335, 0.1484]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>), tensor([[0.1045, 0.0799, 0.0560, 0.0676, 0.0773],\n",
      "        [0.1338, 0.1779, 0.1502, 0.1446, 0.1345],\n",
      "        [0.2015, 0.1906, 0.1931, 0.1856, 0.1466],\n",
      "        [0.2020, 0.2100, 0.2358, 0.2068, 0.2336],\n",
      "        [0.2341, 0.2436, 0.2388, 0.2013, 0.2131],\n",
      "        [0.1816, 0.1919, 0.1758, 0.1996, 0.2026],\n",
      "        [0.2477, 0.2546, 0.2335, 0.2275, 0.2416],\n",
      "        [0.2081, 0.1995, 0.2121, 0.1656, 0.2146],\n",
      "        [0.2399, 0.2293, 0.2398, 0.2143, 0.2051],\n",
      "        [0.2198, 0.1800, 0.1756, 0.1978, 0.1751],\n",
      "        [0.2094, 0.2094, 0.2370, 0.1581, 0.1868],\n",
      "        [0.2274, 0.2132, 0.1931, 0.1654, 0.2145],\n",
      "        [0.0821, 0.0701, 0.0623, 0.0617, 0.0847],\n",
      "        [0.0892, 0.1472, 0.1161, 0.0935, 0.0955],\n",
      "        [0.1676, 0.1591, 0.1521, 0.1524, 0.1541],\n",
      "        [0.1459, 0.1372, 0.1398, 0.1416, 0.1479],\n",
      "        [0.2180, 0.1910, 0.2152, 0.1936, 0.1900],\n",
      "        [0.2589, 0.2650, 0.2358, 0.2127, 0.2526],\n",
      "        [0.2470, 0.2305, 0.2639, 0.2221, 0.2721],\n",
      "        [0.0571, 0.0536, 0.0564, 0.0356, 0.0387],\n",
      "        [0.2061, 0.2183, 0.1879, 0.1780, 0.1937],\n",
      "        [0.2263, 0.2512, 0.2720, 0.1950, 0.2119],\n",
      "        [0.2259, 0.2456, 0.2628, 0.1731, 0.2031],\n",
      "        [0.2370, 0.2019, 0.2091, 0.1859, 0.2422],\n",
      "        [0.1746, 0.1774, 0.1857, 0.1737, 0.1661],\n",
      "        [0.2044, 0.1807, 0.2201, 0.1458, 0.1911],\n",
      "        [0.2174, 0.2620, 0.2499, 0.1988, 0.2312],\n",
      "        [0.1429, 0.1354, 0.1751, 0.1436, 0.1610],\n",
      "        [0.1625, 0.1852, 0.2244, 0.1464, 0.1945],\n",
      "        [0.1754, 0.1570, 0.1702, 0.1107, 0.1501],\n",
      "        [0.2265, 0.2477, 0.2431, 0.2031, 0.2413],\n",
      "        [0.1556, 0.1834, 0.1914, 0.1435, 0.1735],\n",
      "        [0.2061, 0.2215, 0.2045, 0.2021, 0.1963],\n",
      "        [0.1665, 0.2015, 0.1765, 0.1966, 0.2170],\n",
      "        [0.1819, 0.2181, 0.1885, 0.1827, 0.1820],\n",
      "        [0.2511, 0.2675, 0.2586, 0.2559, 0.2552],\n",
      "        [0.2108, 0.2488, 0.2538, 0.1920, 0.2174],\n",
      "        [0.1242, 0.1677, 0.2025, 0.1170, 0.1461],\n",
      "        [0.2296, 0.2537, 0.2508, 0.1984, 0.2435],\n",
      "        [0.2288, 0.2414, 0.2310, 0.2194, 0.2323],\n",
      "        [0.1861, 0.2155, 0.1852, 0.1596, 0.1849],\n",
      "        [0.1455, 0.1746, 0.1983, 0.1895, 0.2065],\n",
      "        [0.2097, 0.2146, 0.2044, 0.2173, 0.2459],\n",
      "        [0.1737, 0.2104, 0.1640, 0.1633, 0.1601],\n",
      "        [0.2208, 0.2379, 0.2512, 0.2010, 0.2281],\n",
      "        [0.1882, 0.1725, 0.1591, 0.1339, 0.2039],\n",
      "        [0.2483, 0.2414, 0.2472, 0.2154, 0.2384],\n",
      "        [0.1888, 0.2168, 0.2604, 0.1869, 0.1943],\n",
      "        [0.2196, 0.2341, 0.2492, 0.2037, 0.2129],\n",
      "        [0.1634, 0.1299, 0.1457, 0.0851, 0.1253],\n",
      "        [0.2284, 0.2430, 0.2424, 0.2056, 0.2182],\n",
      "        [0.2016, 0.2202, 0.2468, 0.1633, 0.2023],\n",
      "        [0.1424, 0.1470, 0.1578, 0.1640, 0.1155],\n",
      "        [0.2067, 0.2113, 0.2128, 0.1622, 0.1561],\n",
      "        [0.2108, 0.2061, 0.1993, 0.1557, 0.1771],\n",
      "        [0.2190, 0.2293, 0.2604, 0.2014, 0.2196]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)], '95%': [tensor([[0.7185, 0.2194, 0.3435],\n",
      "        [0.6001, 0.3055, 0.2469],\n",
      "        [0.5374, 0.3390, 0.2917],\n",
      "        [0.4949, 0.2517, 0.3847],\n",
      "        [0.5146, 0.3349, 0.2661],\n",
      "        [0.6026, 0.2665, 0.2619],\n",
      "        [0.4674, 0.3532, 0.3027],\n",
      "        [0.4723, 0.3174, 0.3416],\n",
      "        [0.5642, 0.3187, 0.2629],\n",
      "        [0.5008, 0.2571, 0.3379],\n",
      "        [0.5284, 0.3417, 0.2374],\n",
      "        [0.4568, 0.3152, 0.4102],\n",
      "        [0.7495, 0.2809, 0.2752],\n",
      "        [0.5563, 0.3880, 0.2389],\n",
      "        [0.4646, 0.3316, 0.3741],\n",
      "        [0.5707, 0.3079, 0.2757],\n",
      "        [0.5140, 0.3614, 0.2557],\n",
      "        [0.3248, 0.3857, 0.3489],\n",
      "        [0.4132, 0.3347, 0.3488],\n",
      "        [0.7765, 0.2501, 0.2166],\n",
      "        [0.5695, 0.3081, 0.2475],\n",
      "        [0.4868, 0.3957, 0.2492],\n",
      "        [0.3809, 0.4180, 0.3025],\n",
      "        [0.6113, 0.2262, 0.3305],\n",
      "        [0.6153, 0.2836, 0.2468],\n",
      "        [0.3247, 0.4512, 0.4044],\n",
      "        [0.3836, 0.4313, 0.3077],\n",
      "        [0.5113, 0.3061, 0.3278],\n",
      "        [0.5256, 0.3441, 0.2515],\n",
      "        [0.4131, 0.3985, 0.3281],\n",
      "        [0.4986, 0.3043, 0.3241],\n",
      "        [0.4498, 0.4981, 0.2317],\n",
      "        [0.5306, 0.2639, 0.3305],\n",
      "        [0.3914, 0.3434, 0.3778],\n",
      "        [0.5321, 0.3575, 0.2566],\n",
      "        [0.4981, 0.2809, 0.3024],\n",
      "        [0.5398, 0.3330, 0.2699],\n",
      "        [0.4361, 0.4785, 0.2723],\n",
      "        [0.5515, 0.2947, 0.2877],\n",
      "        [0.5463, 0.3136, 0.2569],\n",
      "        [0.5385, 0.3173, 0.2874],\n",
      "        [0.5198, 0.2804, 0.3411],\n",
      "        [0.3886, 0.3264, 0.3846],\n",
      "        [0.5574, 0.3403, 0.2602],\n",
      "        [0.5725, 0.3048, 0.2555],\n",
      "        [0.5658, 0.2838, 0.2511],\n",
      "        [0.4141, 0.3848, 0.2979],\n",
      "        [0.5651, 0.3121, 0.2491],\n",
      "        [0.4015, 0.4299, 0.2860],\n",
      "        [0.5964, 0.3151, 0.2346],\n",
      "        [0.5061, 0.3586, 0.2582],\n",
      "        [0.6012, 0.2736, 0.2214],\n",
      "        [0.5329, 0.3050, 0.3041],\n",
      "        [0.5525, 0.3725, 0.3118],\n",
      "        [0.5964, 0.3232, 0.2246],\n",
      "        [0.5964, 0.2926, 0.2446]], device='cuda:0', grad_fn=<ViewBackward0>), tensor([[0.8798, 0.0931, 0.2691],\n",
      "        [0.8190, 0.1451, 0.2330],\n",
      "        [0.8689, 0.1171, 0.1250],\n",
      "        [0.6523, 0.1766, 0.2898],\n",
      "        [0.7424, 0.1656, 0.1978],\n",
      "        [0.7923, 0.1534, 0.1998],\n",
      "        [0.5597, 0.2076, 0.3518],\n",
      "        [0.7900, 0.1380, 0.2169],\n",
      "        [0.7951, 0.1408, 0.1693],\n",
      "        [0.6948, 0.1614, 0.2789],\n",
      "        [0.8633, 0.0977, 0.1266],\n",
      "        [0.6797, 0.1591, 0.3380],\n",
      "        [0.5800, 0.1310, 0.7493],\n",
      "        [0.8884, 0.0583, 0.1471],\n",
      "        [0.6305, 0.1863, 0.3832],\n",
      "        [0.7536, 0.1443, 0.2800],\n",
      "        [0.6668, 0.1597, 0.2818],\n",
      "        [0.6108, 0.1867, 0.3357],\n",
      "        [0.7015, 0.1864, 0.2345],\n",
      "        [0.8756, 0.0596, 0.2022],\n",
      "        [0.7363, 0.1691, 0.2499],\n",
      "        [0.8348, 0.1362, 0.1600],\n",
      "        [0.7747, 0.1793, 0.2107],\n",
      "        [0.7357, 0.1822, 0.2430],\n",
      "        [0.7839, 0.1598, 0.2108],\n",
      "        [0.5230, 0.1614, 0.4808],\n",
      "        [0.6565, 0.1862, 0.2716],\n",
      "        [0.6743, 0.1750, 0.3586],\n",
      "        [0.8188, 0.1361, 0.1829],\n",
      "        [0.6252, 0.1488, 0.3871],\n",
      "        [0.7042, 0.1465, 0.2431],\n",
      "        [0.7182, 0.1797, 0.3682],\n",
      "        [0.7743, 0.1512, 0.2030],\n",
      "        [0.4749, 0.1914, 0.4753],\n",
      "        [0.7353, 0.1787, 0.2607],\n",
      "        [0.5789, 0.2152, 0.3179],\n",
      "        [0.7824, 0.1271, 0.1569],\n",
      "        [0.9067, 0.0818, 0.1053],\n",
      "        [0.7314, 0.1685, 0.2068],\n",
      "        [0.7826, 0.1574, 0.1682],\n",
      "        [0.8013, 0.1311, 0.1887],\n",
      "        [0.7945, 0.1441, 0.2254],\n",
      "        [0.5530, 0.1927, 0.4196],\n",
      "        [0.6097, 0.1536, 0.3548],\n",
      "        [0.7923, 0.1405, 0.1583],\n",
      "        [0.8234, 0.1242, 0.1537],\n",
      "        [0.6746, 0.1854, 0.2646],\n",
      "        [0.8278, 0.1240, 0.1678],\n",
      "        [0.6779, 0.1621, 0.2951],\n",
      "        [0.8693, 0.0720, 0.1525],\n",
      "        [0.7183, 0.1461, 0.2075],\n",
      "        [0.8967, 0.0843, 0.1078],\n",
      "        [0.7044, 0.1842, 0.3049],\n",
      "        [0.7814, 0.1503, 0.2439],\n",
      "        [0.7080, 0.1491, 0.2385],\n",
      "        [0.8113, 0.1337, 0.1729]], device='cuda:0', grad_fn=<ViewBackward0>), tensor([[0.1805, 0.3097, 0.1356, 0.1920, 0.2056, 0.2040],\n",
      "        [0.1691, 0.2936, 0.1410, 0.1725, 0.1570, 0.2034],\n",
      "        [0.1708, 0.2708, 0.1507, 0.2022, 0.1629, 0.1919],\n",
      "        [0.1892, 0.2227, 0.1459, 0.1826, 0.1763, 0.1815],\n",
      "        [0.1724, 0.2335, 0.1652, 0.1722, 0.1636, 0.2051],\n",
      "        [0.1805, 0.2525, 0.1491, 0.1666, 0.1773, 0.1880],\n",
      "        [0.1733, 0.2392, 0.1570, 0.1824, 0.1539, 0.1698],\n",
      "        [0.1582, 0.2550, 0.1512, 0.1945, 0.1765, 0.1654],\n",
      "        [0.1835, 0.2334, 0.1594, 0.1718, 0.1773, 0.1928],\n",
      "        [0.1635, 0.2718, 0.1522, 0.1850, 0.1782, 0.1891],\n",
      "        [0.1795, 0.2534, 0.1475, 0.1860, 0.1612, 0.1787],\n",
      "        [0.1682, 0.2788, 0.1609, 0.1756, 0.1726, 0.1746],\n",
      "        [0.1776, 0.3547, 0.1247, 0.1809, 0.1797, 0.1606],\n",
      "        [0.1565, 0.3566, 0.1388, 0.1917, 0.1756, 0.1737],\n",
      "        [0.1688, 0.2405, 0.1690, 0.1840, 0.1673, 0.1744],\n",
      "        [0.1852, 0.3259, 0.1410, 0.1889, 0.1597, 0.1883],\n",
      "        [0.1737, 0.2650, 0.1461, 0.1925, 0.1712, 0.1705],\n",
      "        [0.1809, 0.2430, 0.1535, 0.1854, 0.1641, 0.1671],\n",
      "        [0.1750, 0.2238, 0.1620, 0.1771, 0.1738, 0.1777],\n",
      "        [0.1871, 0.3434, 0.1412, 0.1755, 0.1824, 0.2183],\n",
      "        [0.1819, 0.2219, 0.1595, 0.1806, 0.1763, 0.1857],\n",
      "        [0.1945, 0.2396, 0.1502, 0.1752, 0.1597, 0.1854],\n",
      "        [0.1714, 0.2432, 0.1633, 0.1929, 0.1658, 0.1730],\n",
      "        [0.1869, 0.2467, 0.1474, 0.1812, 0.1796, 0.1805],\n",
      "        [0.1756, 0.2341, 0.1566, 0.1903, 0.1849, 0.1919],\n",
      "        [0.1724, 0.2687, 0.1681, 0.1761, 0.1784, 0.1704],\n",
      "        [0.1742, 0.2382, 0.1673, 0.1842, 0.1703, 0.1772],\n",
      "        [0.1657, 0.2797, 0.1630, 0.1838, 0.1767, 0.1796],\n",
      "        [0.1744, 0.2609, 0.1453, 0.1827, 0.1824, 0.1824],\n",
      "        [0.2059, 0.2753, 0.1471, 0.1923, 0.1587, 0.1764],\n",
      "        [0.1869, 0.2316, 0.1463, 0.1847, 0.1700, 0.1664],\n",
      "        [0.1845, 0.2628, 0.1641, 0.1822, 0.1640, 0.1755],\n",
      "        [0.1793, 0.2143, 0.1501, 0.1711, 0.1957, 0.1836],\n",
      "        [0.1842, 0.2630, 0.1675, 0.1747, 0.1540, 0.1677],\n",
      "        [0.1722, 0.2350, 0.1576, 0.1813, 0.1809, 0.1632],\n",
      "        [0.1781, 0.2262, 0.1606, 0.1668, 0.1609, 0.1811],\n",
      "        [0.1868, 0.2393, 0.1537, 0.1745, 0.1693, 0.1911],\n",
      "        [0.1864, 0.2703, 0.1479, 0.1704, 0.1953, 0.1586],\n",
      "        [0.1743, 0.2387, 0.1475, 0.1739, 0.1737, 0.1833],\n",
      "        [0.1779, 0.2226, 0.1563, 0.1762, 0.1717, 0.1894],\n",
      "        [0.1598, 0.2609, 0.1598, 0.1800, 0.1788, 0.1920],\n",
      "        [0.1721, 0.2762, 0.1463, 0.1758, 0.1749, 0.1761],\n",
      "        [0.1946, 0.2302, 0.1649, 0.1780, 0.1711, 0.1651],\n",
      "        [0.1637, 0.2980, 0.1521, 0.1781, 0.1838, 0.1644],\n",
      "        [0.1792, 0.2249, 0.1566, 0.1783, 0.1676, 0.1923],\n",
      "        [0.1801, 0.2891, 0.1532, 0.1854, 0.1560, 0.1915],\n",
      "        [0.1848, 0.2227, 0.1703, 0.1940, 0.1712, 0.1688],\n",
      "        [0.1891, 0.2186, 0.1464, 0.1842, 0.1871, 0.1940],\n",
      "        [0.1912, 0.2346, 0.1548, 0.1744, 0.1634, 0.1769],\n",
      "        [0.1754, 0.2842, 0.1504, 0.1803, 0.1785, 0.1909],\n",
      "        [0.1771, 0.2474, 0.1622, 0.1793, 0.1622, 0.1803],\n",
      "        [0.1829, 0.2531, 0.1456, 0.1739, 0.1691, 0.1949],\n",
      "        [0.1835, 0.2791, 0.1503, 0.1833, 0.1630, 0.1700],\n",
      "        [0.1910, 0.2378, 0.1621, 0.1923, 0.1745, 0.1850],\n",
      "        [0.1756, 0.2653, 0.1456, 0.1763, 0.1741, 0.1860],\n",
      "        [0.1914, 0.2543, 0.1470, 0.1933, 0.1652, 0.1846]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>), tensor([[0.2441, 0.1672, 0.1574, 0.1422, 0.2537],\n",
      "        [0.2394, 0.2759, 0.2527, 0.2206, 0.2535],\n",
      "        [0.2733, 0.2725, 0.2957, 0.2751, 0.2268],\n",
      "        [0.3069, 0.3093, 0.3274, 0.3035, 0.3364],\n",
      "        [0.3230, 0.3372, 0.3234, 0.2997, 0.3012],\n",
      "        [0.3277, 0.2827, 0.3087, 0.2757, 0.2800],\n",
      "        [0.3610, 0.3218, 0.3206, 0.3198, 0.3299],\n",
      "        [0.2834, 0.3111, 0.2991, 0.2677, 0.2932],\n",
      "        [0.3272, 0.3493, 0.3387, 0.3180, 0.3129],\n",
      "        [0.2797, 0.2972, 0.2888, 0.2842, 0.2660],\n",
      "        [0.3056, 0.3338, 0.3546, 0.2659, 0.2626],\n",
      "        [0.3533, 0.2827, 0.2785, 0.2695, 0.3158],\n",
      "        [0.2465, 0.1962, 0.1957, 0.1988, 0.2162],\n",
      "        [0.2289, 0.2499, 0.2076, 0.1765, 0.2120],\n",
      "        [0.2843, 0.2742, 0.2556, 0.2571, 0.2590],\n",
      "        [0.2818, 0.2441, 0.2321, 0.1953, 0.2391],\n",
      "        [0.3029, 0.3077, 0.2885, 0.2889, 0.2796],\n",
      "        [0.3402, 0.3505, 0.3188, 0.2812, 0.3505],\n",
      "        [0.3155, 0.3122, 0.3651, 0.2958, 0.3405],\n",
      "        [0.1771, 0.1489, 0.1398, 0.1029, 0.1102],\n",
      "        [0.3068, 0.3342, 0.2972, 0.3105, 0.2836],\n",
      "        [0.3183, 0.3628, 0.3613, 0.2874, 0.2824],\n",
      "        [0.3028, 0.3261, 0.3777, 0.2510, 0.3139],\n",
      "        [0.3290, 0.3360, 0.3132, 0.2909, 0.3157],\n",
      "        [0.2825, 0.2702, 0.3086, 0.2773, 0.2673],\n",
      "        [0.2846, 0.2907, 0.3011, 0.2629, 0.2785],\n",
      "        [0.3160, 0.3535, 0.3103, 0.2818, 0.3101],\n",
      "        [0.2651, 0.2253, 0.2638, 0.2295, 0.2791],\n",
      "        [0.2738, 0.2785, 0.3340, 0.2375, 0.2887],\n",
      "        [0.3012, 0.2882, 0.2806, 0.1883, 0.2777],\n",
      "        [0.2998, 0.3559, 0.3452, 0.3209, 0.3560],\n",
      "        [0.2599, 0.2990, 0.2970, 0.2264, 0.2569],\n",
      "        [0.2737, 0.3007, 0.3247, 0.2688, 0.2833],\n",
      "        [0.2743, 0.2799, 0.2943, 0.3034, 0.3110],\n",
      "        [0.2587, 0.3042, 0.2935, 0.3031, 0.2526],\n",
      "        [0.3305, 0.3492, 0.3444, 0.3353, 0.3181],\n",
      "        [0.3247, 0.3446, 0.3474, 0.2897, 0.2883],\n",
      "        [0.2601, 0.2898, 0.2948, 0.2099, 0.2624],\n",
      "        [0.3280, 0.3534, 0.3213, 0.2762, 0.3116],\n",
      "        [0.3306, 0.3419, 0.3282, 0.3187, 0.3258],\n",
      "        [0.2712, 0.3187, 0.2498, 0.2542, 0.2839],\n",
      "        [0.2876, 0.2825, 0.2931, 0.2844, 0.3162],\n",
      "        [0.3019, 0.3055, 0.3181, 0.2880, 0.3116],\n",
      "        [0.2738, 0.2681, 0.2800, 0.2403, 0.2344],\n",
      "        [0.3524, 0.3429, 0.3507, 0.3270, 0.3100],\n",
      "        [0.2952, 0.3030, 0.2723, 0.2663, 0.2802],\n",
      "        [0.3288, 0.3429, 0.3502, 0.3090, 0.3425],\n",
      "        [0.3296, 0.3099, 0.3443, 0.2807, 0.2928],\n",
      "        [0.3008, 0.3054, 0.3503, 0.3136, 0.3266],\n",
      "        [0.2778, 0.2454, 0.3268, 0.1831, 0.2241],\n",
      "        [0.3067, 0.2920, 0.3254, 0.2788, 0.3208],\n",
      "        [0.3200, 0.3241, 0.3148, 0.2523, 0.2932],\n",
      "        [0.2803, 0.3018, 0.3120, 0.2782, 0.2574],\n",
      "        [0.3187, 0.3155, 0.3166, 0.2642, 0.2596],\n",
      "        [0.2830, 0.2960, 0.2667, 0.2813, 0.2902],\n",
      "        [0.3413, 0.3344, 0.3519, 0.2682, 0.2775]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)], 'order': ['pd', 'nd', 'mod', 'dlt']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pd': {'accuracy': 0.36142857142857143,\n",
       "  'auc_micro': 0.6952534191472246,\n",
       "  'auc_mean': 0.5613027493601995,\n",
       "  'auc_weighted': 0.6205988357561384},\n",
       " 'nd': {'accuracy': 0.2962962962962963,\n",
       "  'auc_micro': 0.21078037007240547,\n",
       "  'auc_mean': 0.44299718886983036,\n",
       "  'auc_weighted': 0.33524871355060026},\n",
       " 'mod': {'accuracy': 0.2962962962962963,\n",
       "  'auc_micro': 0.21078037007240547,\n",
       "  'auc_mean': 0.44299718886983036,\n",
       "  'auc_weighted': 0.33524871355060026},\n",
       " 'dlts': {'accuracy': [0.8035714285714286,\n",
       "   0.8928571428571429,\n",
       "   0.7857142857142857,\n",
       "   0.9464285714285714,\n",
       "   0.9107142857142857],\n",
       "  'accuracy_mean': 0.8678571428571429,\n",
       "  'auc': [0.5272727272727273,\n",
       "   0.3933333333333333,\n",
       "   0.4412878787878788,\n",
       "   0.25786163522012573,\n",
       "   0.7686274509803922],\n",
       "  'auc_mean': 0.4776766051188915}}"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_weights(df_list,scale  = None,to_torch=True):\n",
    "    #sklearn method, does'nt work for the people without values (e.g. IC patients)\n",
    "#     w = [v.shape[0]/(v.shape[1]*np.bincount(v.values.argmax(axis=1))) for v in df_list]\n",
    "    #approximate sklearn method for onehot encoded set of clases\n",
    "    getw = lambda df: df.shape[0]/(df.shape[1]*df.sum(axis=0)).values\n",
    "    w = [getw(df) for df in df_list]\n",
    "\n",
    "    if scale is not None:\n",
    "        w = [scale(ww) for ww in w]\n",
    "    if to_torch:\n",
    "        return [torch.FloatTensor(ww) for ww in w]\n",
    "    return w\n",
    "        \n",
    "def train_state(model=None,\n",
    "                model_args={},\n",
    "                state=1,\n",
    "                split=.7,\n",
    "                lr=.001,\n",
    "                epochs=1000,\n",
    "                patience=10,\n",
    "                weights=None,\n",
    "                save_path='../data/models/',\n",
    "                use_default_split=True,\n",
    "                use_bagging_split=False,\n",
    "                resample_training=False,#use bootstraping on training data after splitting\n",
    "                n_validation_trainsteps=2,\n",
    "                verbose=True,\n",
    "                balanced=True,\n",
    "                sqrt_balance_weights=False,\n",
    "                use_smote=False,\n",
    "                smote_cols = None,\n",
    "                file_suffix=''):\n",
    "    \n",
    "    ids = get_dt_ids()\n",
    "    \n",
    "    train_ids, test_ids = get_tt_split(use_default_split=use_default_split,use_bagging_split=use_bagging_split,resample_training=resample_training)\n",
    "\n",
    "    if use_smote:\n",
    "        if smote_cols is None:\n",
    "            smote_cols = Const.outcomes\n",
    "            if state == 1:\n",
    "                smote_cols = Const.primary_disease_states\n",
    "            elif state == 2:\n",
    "                smote_cols = Const.primary_disease_states2\n",
    "        dataset = DTDataset(use_smote=True,smote_ids = train_ids,smote_columns=[Const.decisions[state-1]])\n",
    "        train_ids = [i for i in dataset.processed_df.index.values if i not in test_ids]\n",
    "    else:\n",
    "        dataset = DTDataset()\n",
    "    \n",
    "    #only train on people with  IC for state 1 since other people can't have any outcomes otherwise\n",
    "    require = None\n",
    "    if state == 1:\n",
    "        require = Const.decisions[0] #we don't expect a state update if there is no treatment\n",
    "        valid_ids = dataset.get_input_state(require=require).index.values\n",
    "        train_ids = [t for t in train_ids if t in valid_ids]\n",
    "        test_ids = [t for t in test_ids if t in valid_ids]\n",
    "    xtrain = dataset.get_input_state(step=state,ids=train_ids,require=require)\n",
    "    xtest = dataset.get_input_state(step=state,ids=test_ids,require=require)\n",
    "    ytrain = dataset.get_intermediate_outcomes(step=state,ids=train_ids,require=require)\n",
    "    ytest = dataset.get_intermediate_outcomes(step=state,ids=test_ids,require=require)\n",
    "    \n",
    "    model_args = {k:v for k,v in model_args.items() if 'attention' not in k and 'embed_size' not in k}\n",
    "    if state < 3:\n",
    "        if model is None:\n",
    "                model = BayesianOutcomeSimulator(xtrain.shape[1],state=state,**model_args)\n",
    "        lfunc = state_loss\n",
    "        if weights is None:\n",
    "            weights = [1,1,.1,1]\n",
    "    else:\n",
    "        if model is None:\n",
    "                model = BayesianEndpointSimulator(xtrain.shape[1],**model_args)\n",
    "        if weights is None:\n",
    "            weights = [1,1,1,1]\n",
    "        lfunc = outcome_loss\n",
    "        \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.set_device(device)\n",
    "    \n",
    "    balance_weights=None\n",
    "    if balanced:\n",
    "        if state < 3:\n",
    "            balance_weights = [w.to(device) for w in get_weights(ytrain)]\n",
    "            if sqrt_balance_weights:\n",
    "                balance_weights = [torch.sqrt(w) for w in balance_weights]\n",
    "        else:\n",
    "            print('I dont do balancing on the outputs because Idk how that would work')\n",
    "    \n",
    "    hashcode = str(hash(','.join([str(i) for i in train_ids])))\n",
    "    save_file = save_path + 'model_' + model.identifier + '_split' + str(split) + '_resample' + str(resample_training) +  '_hash' + hashcode + file_suffix + '.tar'\n",
    "    xtrain = df_to_torch(xtrain).to(device)\n",
    "    \n",
    "    xtest = df_to_torch(xtest).to(device)\n",
    "    ytrain = [df_to_torch(t).to(device) for t in ytrain]\n",
    "    ytest= [df_to_torch(t).to(device) for t in ytest]\n",
    "    \n",
    "    model.fit_normalizer(xtrain)\n",
    "#     normalize = lambda x: (x - xtrain.mean(axis=0)+.01)/(xtrain.std(axis=0)+.01)\n",
    "#     unnormalize = lambda x: (x * (xtrain.std(axis=0) +.01)) + xtrain.mean(axis=0) - .01\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "    best_val_loss = 1000000000000000000000000000\n",
    "    best_loss_metrics = {}\n",
    "    last_epoch = False\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train(True)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        xtrain_sample = xtrain#[torch.randint(len(xtrain),(len(xtrain),) )]\n",
    "        ypred = model(xtrain_sample,n_samples=1)\n",
    "        loss = lfunc(ytrain,ypred,weights=weights,subweights=balance_weights)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if verbose:\n",
    "            print('epoch',epoch,'train loss',loss.item())\n",
    "        \n",
    "        model.eval()\n",
    "        yval = model(xtest,n_samples=1)\n",
    "        val_loss = lfunc(ytest,yval,weights=weights,subweights=balance_weights)\n",
    "        if state < 3:\n",
    "            val_metrics = state_metrics(ytest,yval)\n",
    "        else:\n",
    "            val_metrics = outcome_metrics(ytest,yval)\n",
    "        if val_loss.item() < best_val_loss:\n",
    "            best_val_loss = val_loss.item()\n",
    "            best_loss_metrics = val_metrics\n",
    "            steps_since_improvement = 0\n",
    "            torch.save(model.state_dict(),save_file)\n",
    "        else:\n",
    "            steps_since_improvement += 1\n",
    "        if verbose:\n",
    "            print('val loss',val_loss.item())\n",
    "            print('______________')\n",
    "        if steps_since_improvement > patience:\n",
    "            break\n",
    "    print('best loss',best_val_loss,best_loss_metrics)\n",
    "    model.load_state_dict(torch.load(save_file))\n",
    "    \n",
    "    #train one step on validation data\n",
    "    for i in range(n_validation_trainsteps):\n",
    "        model.train()\n",
    "        yval = model(xtest,n_samples=1)\n",
    "        val_loss = lfunc(ytest,yval,weights=weights)\n",
    "        val_loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.save(model.state_dict(),save_file)\n",
    "    \n",
    "    model.eval()\n",
    "    print(model(xtest))\n",
    "    return model,  best_val_loss, best_loss_metrics\n",
    "\n",
    "from Models import *\n",
    "t1_args = {'hidden_layers': [500,500],\n",
    "   'dropout': 0.5,\n",
    "   'input_dropout': 0.1}\n",
    "tmodel_balanced = train_state(model_args=t1_args,state=1,lr=.0001,weights=[1,1,.1,.1])\n",
    "tmodel_balanced[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7f03d6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "epoch 0 train loss 2.4485623836517334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/imblearn/over_sampling/_smote/base.py:345: FutureWarning: The parameter `n_jobs` has been deprecated in 0.10 and will be removed in 0.12. You can pass an nearest neighbors estimator where `n_jobs` is already set instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 2.0007152557373047\n",
      "______________\n",
      "epoch 1 train loss 1.951395869255066\n",
      "val loss 2.0055298805236816\n",
      "______________\n",
      "epoch 2 train loss 2.0097081661224365\n",
      "val loss 2.0284180641174316\n",
      "______________\n",
      "epoch 3 train loss 1.9616488218307495\n",
      "val loss 2.0098650455474854\n",
      "______________\n",
      "epoch 4 train loss 1.9469159841537476\n",
      "val loss 2.0174002647399902\n",
      "______________\n",
      "epoch 5 train loss 1.9538722038269043\n",
      "val loss 2.020348310470581\n",
      "______________\n",
      "epoch 6 train loss 1.9547425508499146\n",
      "val loss 2.0151419639587402\n",
      "______________\n",
      "epoch 7 train loss 1.9518719911575317\n",
      "val loss 2.011436700820923\n",
      "______________\n",
      "epoch 8 train loss 1.9423500299453735\n",
      "val loss 2.0097742080688477\n",
      "______________\n",
      "epoch 9 train loss 1.9376270771026611\n",
      "val loss 2.0125961303710938\n",
      "______________\n",
      "epoch 10 train loss 1.9403948783874512\n",
      "val loss 2.0118892192840576\n",
      "______________\n",
      "epoch 11 train loss 1.9403228759765625\n",
      "val loss 2.0074925422668457\n",
      "______________\n",
      "best loss 2.0007152557373047 {'pd': {'accuracy': 0.3333333333333333, 'auc_micro': 0.6999195494770716, 'auc_mean': 0.5650732042513266, 'auc_weighted': 0.4842640097535474}, 'nd': {'accuracy': 0.3333333333333333, 'auc_micro': 0.8601769911504424, 'auc_mean': 0.5415996640937678, 'auc_weighted': 0.5710763293310464}, 'mod': {'accuracy': 0.3333333333333333, 'auc_micro': 0.8601769911504424, 'auc_mean': 0.5415996640937678, 'auc_weighted': 0.5710763293310464}, 'dlts': {'accuracy': [0.8035714285714286, 0.8928571428571429, 0.7857142857142857, 0.9464285714285714, 0.9107142857142857], 'accuracy_mean': 0.8678571428571429, 'auc': [0.46464646464646464, 0.3766666666666667, 0.42424242424242425, 0.5408805031446541, 0.7019607843137254], 'auc_mean': 0.5016793686027871}}\n",
      "{'predictions': [tensor([[8.8173e-01, 1.1732e-01, 9.4923e-04],\n",
      "        [8.0829e-01, 1.8051e-01, 1.1197e-02],\n",
      "        [8.2941e-01, 1.5332e-01, 1.7271e-02],\n",
      "        [7.1899e-01, 2.5763e-01, 2.3374e-02],\n",
      "        [7.3330e-01, 2.3736e-01, 2.9338e-02],\n",
      "        [6.2279e-01, 3.2139e-01, 5.5825e-02],\n",
      "        [5.5600e-01, 4.1462e-01, 2.9381e-02],\n",
      "        [4.4486e-01, 4.9856e-01, 5.6581e-02],\n",
      "        [5.7487e-01, 3.5186e-01, 7.3270e-02],\n",
      "        [8.2712e-01, 1.5859e-01, 1.4293e-02],\n",
      "        [4.3427e-01, 5.0405e-01, 6.1685e-02],\n",
      "        [2.5904e-01, 6.8013e-01, 6.0822e-02],\n",
      "        [5.6616e-01, 4.3208e-01, 1.7656e-03],\n",
      "        [3.3353e-01, 6.4014e-01, 2.6326e-02],\n",
      "        [8.2318e-01, 1.7479e-01, 2.0318e-03],\n",
      "        [8.2515e-01, 1.6270e-01, 1.2151e-02],\n",
      "        [5.5185e-01, 4.2266e-01, 2.5492e-02],\n",
      "        [2.0913e-01, 7.3361e-01, 5.7259e-02],\n",
      "        [8.0448e-01, 1.7205e-01, 2.3466e-02],\n",
      "        [9.3512e-01, 6.4433e-02, 4.4787e-04],\n",
      "        [8.6765e-01, 1.1336e-01, 1.8995e-02],\n",
      "        [3.2779e-01, 6.1037e-01, 6.1848e-02],\n",
      "        [2.0091e-01, 7.4730e-01, 5.1793e-02],\n",
      "        [7.4073e-01, 2.3361e-01, 2.5659e-02],\n",
      "        [7.2976e-01, 2.4268e-01, 2.7555e-02],\n",
      "        [5.1414e-02, 9.4101e-01, 7.5752e-03],\n",
      "        [2.4627e-01, 6.8599e-01, 6.7738e-02],\n",
      "        [8.5820e-01, 1.3449e-01, 7.3044e-03],\n",
      "        [6.6993e-01, 2.8735e-01, 4.2724e-02],\n",
      "        [2.6293e-01, 7.1172e-01, 2.5346e-02],\n",
      "        [5.8167e-01, 3.8291e-01, 3.5422e-02],\n",
      "        [7.3688e-01, 2.5516e-01, 7.9671e-03],\n",
      "        [8.1642e-01, 1.6160e-01, 2.1982e-02],\n",
      "        [2.3068e-01, 7.5815e-01, 1.1162e-02],\n",
      "        [8.0471e-01, 1.7780e-01, 1.7493e-02],\n",
      "        [9.0168e-01, 8.8148e-02, 1.0170e-02],\n",
      "        [5.8339e-01, 3.4374e-01, 7.2869e-02],\n",
      "        [2.7108e-01, 6.9782e-01, 3.1101e-02],\n",
      "        [6.8068e-01, 2.7642e-01, 4.2900e-02],\n",
      "        [8.4660e-01, 1.3075e-01, 2.2656e-02],\n",
      "        [8.2009e-01, 1.6253e-01, 1.7381e-02],\n",
      "        [8.6562e-01, 1.2762e-01, 6.7606e-03],\n",
      "        [2.3152e-01, 7.2732e-01, 4.1164e-02],\n",
      "        [3.6667e-01, 5.8668e-01, 4.6651e-02],\n",
      "        [7.6299e-01, 1.9335e-01, 4.3661e-02],\n",
      "        [6.4425e-01, 3.1094e-01, 4.4815e-02],\n",
      "        [2.6317e-01, 6.8684e-01, 4.9995e-02],\n",
      "        [5.3115e-01, 4.0684e-01, 6.2013e-02],\n",
      "        [1.8212e-01, 7.9026e-01, 2.7626e-02],\n",
      "        [6.3141e-01, 3.5525e-01, 1.3343e-02],\n",
      "        [6.4411e-01, 3.0539e-01, 5.0495e-02],\n",
      "        [5.7705e-01, 3.6558e-01, 5.7367e-02],\n",
      "        [7.5924e-01, 2.3522e-01, 5.5361e-03],\n",
      "        [2.5468e-01, 6.9362e-01, 5.1708e-02],\n",
      "        [8.7368e-01, 1.1259e-01, 1.3731e-02],\n",
      "        [4.4164e-01, 5.2061e-01, 3.7749e-02]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>), tensor([[2.3005e-07, 1.0000e+00, 2.2998e-07],\n",
      "        [1.3866e-04, 9.9972e-01, 1.3784e-04],\n",
      "        [4.2107e-04, 9.9916e-01, 4.1570e-04],\n",
      "        [7.7612e-04, 9.9846e-01, 7.6574e-04],\n",
      "        [1.3529e-03, 9.9731e-01, 1.3398e-03],\n",
      "        [7.0105e-03, 9.8616e-01, 6.8252e-03],\n",
      "        [1.0127e-03, 9.9799e-01, 1.0022e-03],\n",
      "        [4.7726e-03, 9.9061e-01, 4.6219e-03],\n",
      "        [1.4471e-02, 9.7162e-01, 1.3911e-02],\n",
      "        [1.7463e-04, 9.9965e-01, 1.7328e-04],\n",
      "        [1.0770e-02, 9.7873e-01, 1.0502e-02],\n",
      "        [1.1918e-02, 9.7638e-01, 1.1706e-02],\n",
      "        [7.3062e-07, 1.0000e+00, 7.3041e-07],\n",
      "        [7.3533e-04, 9.9854e-01, 7.2630e-04],\n",
      "        [1.5475e-06, 1.0000e+00, 1.5465e-06],\n",
      "        [1.5059e-04, 9.9970e-01, 1.4954e-04],\n",
      "        [7.8854e-04, 9.9843e-01, 7.7745e-04],\n",
      "        [1.1461e-02, 9.7731e-01, 1.1226e-02],\n",
      "        [7.7364e-04, 9.9846e-01, 7.6376e-04],\n",
      "        [1.1388e-07, 1.0000e+00, 1.1386e-07],\n",
      "        [1.1077e-03, 9.9780e-01, 1.0901e-03],\n",
      "        [1.1985e-02, 9.7628e-01, 1.1739e-02],\n",
      "        [1.1006e-02, 9.7814e-01, 1.0857e-02],\n",
      "        [1.0597e-03, 9.9790e-01, 1.0438e-03],\n",
      "        [1.2848e-03, 9.9745e-01, 1.2634e-03],\n",
      "        [3.5431e-04, 9.9929e-01, 3.5365e-04],\n",
      "        [1.6719e-02, 9.6674e-01, 1.6545e-02],\n",
      "        [7.7777e-05, 9.9984e-01, 7.7439e-05],\n",
      "        [2.1891e-03, 9.9567e-01, 2.1431e-03],\n",
      "        [1.0197e-03, 9.9797e-01, 1.0110e-03],\n",
      "        [2.1655e-03, 9.9572e-01, 2.1099e-03],\n",
      "        [3.4563e-05, 9.9993e-01, 3.4479e-05],\n",
      "        [8.0999e-04, 9.9839e-01, 7.9582e-04],\n",
      "        [1.4941e-04, 9.9970e-01, 1.4901e-04],\n",
      "        [4.5193e-04, 9.9910e-01, 4.4665e-04],\n",
      "        [3.0586e-04, 9.9939e-01, 3.0292e-04],\n",
      "        [1.4770e-02, 9.7107e-01, 1.4159e-02],\n",
      "        [1.4618e-03, 9.9710e-01, 1.4420e-03],\n",
      "        [3.4329e-03, 9.9323e-01, 3.3411e-03],\n",
      "        [1.4567e-03, 9.9711e-01, 1.4367e-03],\n",
      "        [4.1752e-04, 9.9917e-01, 4.1184e-04],\n",
      "        [5.3558e-05, 9.9989e-01, 5.3343e-05],\n",
      "        [4.5146e-03, 9.9106e-01, 4.4255e-03],\n",
      "        [3.5956e-03, 9.9289e-01, 3.5123e-03],\n",
      "        [5.7740e-03, 9.8860e-01, 5.6221e-03],\n",
      "        [3.7949e-03, 9.9249e-01, 3.7176e-03],\n",
      "        [7.2877e-03, 9.8553e-01, 7.1834e-03],\n",
      "        [7.9117e-03, 9.8440e-01, 7.6879e-03],\n",
      "        [2.5484e-03, 9.9493e-01, 2.5173e-03],\n",
      "        [2.2832e-04, 9.9954e-01, 2.2725e-04],\n",
      "        [4.5378e-03, 9.9104e-01, 4.4206e-03],\n",
      "        [9.5703e-03, 9.8115e-01, 9.2781e-03],\n",
      "        [1.8960e-05, 9.9996e-01, 1.8917e-05],\n",
      "        [7.5627e-03, 9.8502e-01, 7.4142e-03],\n",
      "        [4.9078e-04, 9.9903e-01, 4.8386e-04],\n",
      "        [2.3392e-03, 9.9536e-01, 2.3010e-03]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>), tensor([[9.9982e-01, 3.5730e-05, 3.5747e-05, 3.5733e-05, 3.5733e-05, 3.5725e-05],\n",
      "        [9.8632e-01, 2.7268e-03, 2.7537e-03, 2.7314e-03, 2.7378e-03, 2.7258e-03],\n",
      "        [9.7243e-01, 5.4995e-03, 5.5670e-03, 5.5068e-03, 5.4984e-03, 5.5021e-03],\n",
      "        [9.6055e-01, 7.8720e-03, 7.9732e-03, 7.8868e-03, 7.8817e-03, 7.8380e-03],\n",
      "        [9.4761e-01, 1.0436e-02, 1.0635e-02, 1.0456e-02, 1.0446e-02, 1.0419e-02],\n",
      "        [8.5588e-01, 2.8829e-02, 2.9319e-02, 2.8821e-02, 2.8548e-02, 2.8607e-02],\n",
      "        [9.4683e-01, 1.0578e-02, 1.0779e-02, 1.0589e-02, 1.0656e-02, 1.0571e-02],\n",
      "        [8.8030e-01, 2.4015e-02, 2.4623e-02, 2.3707e-02, 2.3691e-02, 2.3662e-02],\n",
      "        [7.8349e-01, 4.2869e-02, 4.4932e-02, 4.3112e-02, 4.2950e-02, 4.2644e-02],\n",
      "        [9.8457e-01, 3.0781e-03, 3.1089e-03, 3.0837e-03, 3.0778e-03, 3.0772e-03],\n",
      "        [8.2138e-01, 3.5238e-02, 3.6829e-02, 3.5835e-02, 3.5518e-02, 3.5195e-02],\n",
      "        [8.0620e-01, 3.8518e-02, 3.9663e-02, 3.8562e-02, 3.8760e-02, 3.8299e-02],\n",
      "        [9.9967e-01, 6.6892e-05, 6.6917e-05, 6.6905e-05, 6.6902e-05, 6.6872e-05],\n",
      "        [9.6129e-01, 7.7079e-03, 7.8927e-03, 7.7245e-03, 7.7084e-03, 7.6754e-03],\n",
      "        [9.9934e-01, 1.3255e-04, 1.3275e-04, 1.3258e-04, 1.3259e-04, 1.3252e-04],\n",
      "        [9.8527e-01, 2.9449e-03, 2.9674e-03, 2.9403e-03, 2.9427e-03, 2.9384e-03],\n",
      "        [9.5582e-01, 8.7912e-03, 8.9494e-03, 8.8040e-03, 8.8488e-03, 8.7898e-03],\n",
      "        [8.0433e-01, 3.8947e-02, 4.0230e-02, 3.8629e-02, 3.9249e-02, 3.8616e-02],\n",
      "        [9.5573e-01, 8.8505e-03, 8.9706e-03, 8.8200e-03, 8.8458e-03, 8.7864e-03],\n",
      "        [9.9989e-01, 2.2447e-05, 2.2454e-05, 2.2450e-05, 2.2449e-05, 2.2446e-05],\n",
      "        [9.4849e-01, 1.0275e-02, 1.0448e-02, 1.0272e-02, 1.0284e-02, 1.0234e-02],\n",
      "        [8.1314e-01, 3.6865e-02, 3.8374e-02, 3.7250e-02, 3.7605e-02, 3.6770e-02],\n",
      "        [8.1739e-01, 3.6161e-02, 3.7483e-02, 3.6244e-02, 3.6791e-02, 3.5934e-02],\n",
      "        [9.4794e-01, 1.0395e-02, 1.0523e-02, 1.0395e-02, 1.0398e-02, 1.0351e-02],\n",
      "        [9.5075e-01, 9.8111e-03, 1.0013e-02, 9.8355e-03, 9.8137e-03, 9.7769e-03],\n",
      "        [9.8076e-01, 3.8450e-03, 3.8799e-03, 3.8290e-03, 3.8526e-03, 3.8300e-03],\n",
      "        [7.6576e-01, 4.6591e-02, 4.8230e-02, 4.6450e-02, 4.7083e-02, 4.5884e-02],\n",
      "        [9.9019e-01, 1.9582e-03, 1.9723e-03, 1.9606e-03, 1.9625e-03, 1.9570e-03],\n",
      "        [9.1781e-01, 1.6434e-02, 1.6792e-02, 1.6347e-02, 1.6355e-02, 1.6258e-02],\n",
      "        [9.5999e-01, 7.9746e-03, 8.1086e-03, 7.9800e-03, 7.9958e-03, 7.9496e-03],\n",
      "        [9.2686e-01, 1.4519e-02, 1.4945e-02, 1.4611e-02, 1.4564e-02, 1.4497e-02],\n",
      "        [9.9454e-01, 1.0907e-03, 1.0954e-03, 1.0907e-03, 1.0929e-03, 1.0892e-03],\n",
      "        [9.5695e-01, 8.5952e-03, 8.7221e-03, 8.5904e-03, 8.5857e-03, 8.5598e-03],\n",
      "        [9.8755e-01, 2.4827e-03, 2.5068e-03, 2.4836e-03, 2.4936e-03, 2.4792e-03],\n",
      "        [9.7314e-01, 5.3619e-03, 5.4281e-03, 5.3628e-03, 5.3615e-03, 5.3501e-03],\n",
      "        [9.7896e-01, 4.2005e-03, 4.2373e-03, 4.2074e-03, 4.2027e-03, 4.1970e-03],\n",
      "        [7.8271e-01, 4.3027e-02, 4.5119e-02, 4.3258e-02, 4.3065e-02, 4.2827e-02],\n",
      "        [9.4330e-01, 1.1307e-02, 1.1504e-02, 1.1348e-02, 1.1314e-02, 1.1231e-02],\n",
      "        [8.9441e-01, 2.1012e-02, 2.1569e-02, 2.0958e-02, 2.1065e-02, 2.0986e-02],\n",
      "        [9.4624e-01, 1.0702e-02, 1.0917e-02, 1.0731e-02, 1.0693e-02, 1.0714e-02],\n",
      "        [9.6797e-01, 6.3990e-03, 6.4631e-03, 6.4016e-03, 6.3961e-03, 6.3749e-03],\n",
      "        [9.9382e-01, 1.2337e-03, 1.2391e-03, 1.2349e-03, 1.2349e-03, 1.2341e-03],\n",
      "        [8.9157e-01, 2.1555e-02, 2.2187e-02, 2.1570e-02, 2.1673e-02, 2.1444e-02],\n",
      "        [8.9457e-01, 2.0944e-02, 2.1601e-02, 2.1116e-02, 2.0978e-02, 2.0793e-02],\n",
      "        [8.6929e-01, 2.6089e-02, 2.6733e-02, 2.6072e-02, 2.5892e-02, 2.5927e-02],\n",
      "        [9.0258e-01, 1.9436e-02, 1.9820e-02, 1.9498e-02, 1.9346e-02, 1.9324e-02],\n",
      "        [8.5015e-01, 2.9642e-02, 3.0771e-02, 2.9875e-02, 2.9976e-02, 2.9585e-02],\n",
      "        [8.3814e-01, 3.2192e-02, 3.3361e-02, 3.2165e-02, 3.2268e-02, 3.1869e-02],\n",
      "        [9.3060e-01, 1.3828e-02, 1.4118e-02, 1.3770e-02, 1.3893e-02, 1.3792e-02],\n",
      "        [9.8611e-01, 2.7651e-03, 2.8009e-03, 2.7780e-03, 2.7717e-03, 2.7693e-03],\n",
      "        [8.8431e-01, 2.3007e-02, 2.3731e-02, 2.3047e-02, 2.3023e-02, 2.2882e-02],\n",
      "        [8.3713e-01, 3.2147e-02, 3.3585e-02, 3.2627e-02, 3.2321e-02, 3.2187e-02],\n",
      "        [9.9693e-01, 6.1283e-04, 6.1593e-04, 6.1353e-04, 6.1342e-04, 6.1264e-04],\n",
      "        [8.5020e-01, 2.9616e-02, 3.0871e-02, 2.9655e-02, 2.9938e-02, 2.9723e-02],\n",
      "        [9.6737e-01, 6.5095e-03, 6.5855e-03, 6.5148e-03, 6.5233e-03, 6.4965e-03],\n",
      "        [9.1984e-01, 1.5929e-02, 1.6281e-02, 1.6010e-02, 1.6034e-02, 1.5903e-02]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>), tensor([[0.0028, 0.0015, 0.3193, 0.0063, 0.0029],\n",
      "        [0.0232, 0.0165, 0.4110, 0.0408, 0.0311],\n",
      "        [0.0321, 0.0328, 0.4051, 0.0622, 0.0458],\n",
      "        [0.0478, 0.0404, 0.4101, 0.0733, 0.0596],\n",
      "        [0.0573, 0.0497, 0.4002, 0.0916, 0.0693],\n",
      "        [0.1031, 0.0955, 0.4515, 0.1591, 0.1189],\n",
      "        [0.0630, 0.0464, 0.4164, 0.0686, 0.0802],\n",
      "        [0.1041, 0.0863, 0.4332, 0.1365, 0.1094],\n",
      "        [0.1409, 0.1238, 0.4507, 0.1860, 0.1536],\n",
      "        [0.0232, 0.0248, 0.3995, 0.0492, 0.0360],\n",
      "        [0.1460, 0.1112, 0.4279, 0.1495, 0.1497],\n",
      "        [0.1512, 0.1394, 0.4276, 0.1457, 0.1554],\n",
      "        [0.0046, 0.0032, 0.3729, 0.0086, 0.0059],\n",
      "        [0.0574, 0.0459, 0.4443, 0.0691, 0.0679],\n",
      "        [0.0039, 0.0034, 0.3586, 0.0084, 0.0068],\n",
      "        [0.0255, 0.0191, 0.3934, 0.0402, 0.0270],\n",
      "        [0.0483, 0.0389, 0.4001, 0.0575, 0.0679],\n",
      "        [0.1400, 0.1273, 0.4086, 0.1328, 0.1493],\n",
      "        [0.0474, 0.0393, 0.4151, 0.0809, 0.0560],\n",
      "        [0.0018, 0.0014, 0.3526, 0.0035, 0.0020],\n",
      "        [0.0454, 0.0423, 0.4267, 0.0745, 0.0633],\n",
      "        [0.1489, 0.1284, 0.4147, 0.1423, 0.1496],\n",
      "        [0.1506, 0.1371, 0.4090, 0.1342, 0.1466],\n",
      "        [0.0565, 0.0482, 0.4236, 0.0823, 0.0695],\n",
      "        [0.0531, 0.0484, 0.4091, 0.0780, 0.0801],\n",
      "        [0.0413, 0.0285, 0.3493, 0.0365, 0.0431],\n",
      "        [0.1778, 0.1588, 0.4168, 0.1692, 0.1670],\n",
      "        [0.0190, 0.0156, 0.3719, 0.0308, 0.0238],\n",
      "        [0.0725, 0.0625, 0.4257, 0.1201, 0.0821],\n",
      "        [0.0682, 0.0533, 0.3973, 0.0684, 0.0614],\n",
      "        [0.0737, 0.0549, 0.4092, 0.0915, 0.0980],\n",
      "        [0.0184, 0.0111, 0.3275, 0.0219, 0.0175],\n",
      "        [0.0448, 0.0406, 0.4198, 0.0830, 0.0570],\n",
      "        [0.0308, 0.0192, 0.3483, 0.0309, 0.0398],\n",
      "        [0.0367, 0.0308, 0.3892, 0.0608, 0.0592],\n",
      "        [0.0247, 0.0251, 0.3805, 0.0497, 0.0450],\n",
      "        [0.1390, 0.1236, 0.4515, 0.1868, 0.1532],\n",
      "        [0.0812, 0.0611, 0.3866, 0.0849, 0.0838],\n",
      "        [0.0758, 0.0687, 0.4348, 0.1159, 0.1034],\n",
      "        [0.0476, 0.0478, 0.4015, 0.0904, 0.0674],\n",
      "        [0.0365, 0.0333, 0.4309, 0.0616, 0.0511],\n",
      "        [0.0145, 0.0131, 0.3555, 0.0274, 0.0194],\n",
      "        [0.1049, 0.1022, 0.4292, 0.1091, 0.1195],\n",
      "        [0.0999, 0.0887, 0.4563, 0.1024, 0.1191],\n",
      "        [0.0903, 0.0838, 0.4384, 0.1455, 0.1100],\n",
      "        [0.0862, 0.0718, 0.4419, 0.1167, 0.0944],\n",
      "        [0.1377, 0.1235, 0.4087, 0.1279, 0.1413],\n",
      "        [0.1244, 0.0984, 0.4296, 0.1633, 0.1196],\n",
      "        [0.0787, 0.0669, 0.3872, 0.0826, 0.0849],\n",
      "        [0.0324, 0.0208, 0.3490, 0.0363, 0.0316],\n",
      "        [0.0965, 0.0824, 0.4375, 0.1286, 0.1063],\n",
      "        [0.1221, 0.1042, 0.4354, 0.1443, 0.1382],\n",
      "        [0.0115, 0.0096, 0.3686, 0.0217, 0.0154],\n",
      "        [0.1244, 0.1226, 0.4280, 0.1251, 0.1273],\n",
      "        [0.0342, 0.0330, 0.4388, 0.0536, 0.0500],\n",
      "        [0.0841, 0.0668, 0.4078, 0.0987, 0.0930]], device='cuda:0',\n",
      "       grad_fn=<MulBackward0>)], '5%': [tensor([[6.1167e-01, 4.7745e-02, 1.6231e-04],\n",
      "        [6.8360e-01, 9.4740e-02, 4.0690e-03],\n",
      "        [6.9620e-01, 8.8890e-02, 1.1287e-02],\n",
      "        [5.0381e-01, 1.2608e-01, 1.3343e-02],\n",
      "        [6.5506e-01, 1.2478e-01, 1.4747e-02],\n",
      "        [5.0406e-01, 2.3617e-01, 3.3774e-02],\n",
      "        [4.6462e-01, 2.5683e-01, 1.4366e-02],\n",
      "        [3.8976e-01, 3.6051e-01, 3.4742e-02],\n",
      "        [4.9363e-01, 2.5163e-01, 4.4395e-02],\n",
      "        [7.6462e-01, 5.2817e-02, 5.8434e-03],\n",
      "        [3.1996e-01, 4.4459e-01, 4.2128e-02],\n",
      "        [1.8640e-01, 5.8496e-01, 4.1186e-02],\n",
      "        [2.8522e-01, 1.4730e-01, 3.1368e-04],\n",
      "        [2.0713e-01, 4.0536e-01, 9.5940e-03],\n",
      "        [7.9124e-01, 5.5345e-02, 4.6527e-04],\n",
      "        [5.9403e-01, 8.0915e-02, 5.3838e-03],\n",
      "        [4.9255e-01, 2.0988e-01, 1.0874e-02],\n",
      "        [1.4360e-01, 6.0382e-01, 4.1607e-02],\n",
      "        [6.5141e-01, 9.7926e-02, 1.0154e-02],\n",
      "        [7.1665e-01, 9.9700e-03, 1.3225e-04],\n",
      "        [8.3733e-01, 5.0733e-02, 8.9083e-03],\n",
      "        [2.3809e-01, 5.0842e-01, 3.5713e-02],\n",
      "        [1.5916e-01, 6.8211e-01, 3.6371e-02],\n",
      "        [6.0810e-01, 1.4827e-01, 1.5036e-02],\n",
      "        [6.4965e-01, 1.5963e-01, 1.5710e-02],\n",
      "        [1.3831e-02, 8.9243e-01, 1.4520e-03],\n",
      "        [1.7546e-01, 6.1120e-01, 3.9578e-02],\n",
      "        [7.2633e-01, 6.6955e-02, 3.2506e-03],\n",
      "        [5.4286e-01, 1.7863e-01, 2.3457e-02],\n",
      "        [1.3251e-01, 5.8181e-01, 8.6498e-03],\n",
      "        [4.4411e-01, 2.6007e-01, 1.8417e-02],\n",
      "        [6.1255e-01, 1.3043e-01, 2.6140e-03],\n",
      "        [6.8323e-01, 1.0846e-01, 1.2302e-02],\n",
      "        [8.4150e-02, 3.6329e-01, 3.7482e-03],\n",
      "        [7.0151e-01, 7.8568e-02, 9.0254e-03],\n",
      "        [7.7924e-01, 6.5554e-02, 6.4673e-03],\n",
      "        [5.0902e-01, 2.6494e-01, 5.2030e-02],\n",
      "        [1.4200e-01, 4.8093e-01, 1.1364e-02],\n",
      "        [5.8179e-01, 1.8736e-01, 2.5216e-02],\n",
      "        [6.6015e-01, 7.0301e-02, 1.0946e-02],\n",
      "        [6.8741e-01, 6.5693e-02, 1.1221e-02],\n",
      "        [7.1074e-01, 3.6178e-02, 2.9746e-03],\n",
      "        [1.6559e-01, 6.6740e-01, 1.8752e-02],\n",
      "        [2.8648e-01, 3.2776e-01, 2.7874e-02],\n",
      "        [7.2659e-01, 1.2253e-01, 2.8458e-02],\n",
      "        [5.4159e-01, 1.7587e-01, 2.4905e-02],\n",
      "        [1.6401e-01, 5.8220e-01, 2.9110e-02],\n",
      "        [4.2334e-01, 2.9170e-01, 3.5397e-02],\n",
      "        [9.1858e-02, 6.5610e-01, 1.2081e-02],\n",
      "        [3.8281e-01, 2.0228e-01, 7.7319e-03],\n",
      "        [5.6717e-01, 2.2950e-01, 3.2448e-02],\n",
      "        [4.6071e-01, 2.5065e-01, 4.2697e-02],\n",
      "        [6.5433e-01, 9.7218e-02, 1.5439e-03],\n",
      "        [1.8745e-01, 5.5625e-01, 3.2059e-02],\n",
      "        [8.0272e-01, 5.5122e-02, 6.3100e-03],\n",
      "        [3.1731e-01, 3.2548e-01, 2.4031e-02]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>), tensor([[6.9550e-09, 1.0000e+00, 6.9539e-09],\n",
      "        [1.1180e-05, 9.9889e-01, 1.1157e-05],\n",
      "        [1.4692e-04, 9.9836e-01, 1.4559e-04],\n",
      "        [2.1755e-04, 9.9463e-01, 2.1695e-04],\n",
      "        [4.0311e-04, 9.9531e-01, 4.0573e-04],\n",
      "        [2.5465e-03, 9.8300e-01, 2.4551e-03],\n",
      "        [4.2173e-04, 9.9655e-01, 4.1647e-04],\n",
      "        [1.4688e-03, 9.8868e-01, 1.4289e-03],\n",
      "        [5.3875e-03, 9.5665e-01, 5.2966e-03],\n",
      "        [4.8181e-05, 9.9919e-01, 4.8062e-05],\n",
      "        [4.2663e-03, 9.7811e-01, 4.1493e-03],\n",
      "        [4.7533e-03, 9.7235e-01, 4.6445e-03],\n",
      "        [5.4988e-08, 9.9994e-01, 5.4982e-08],\n",
      "        [1.5373e-04, 9.9668e-01, 1.5168e-04],\n",
      "        [7.7388e-08, 9.9995e-01, 7.7378e-08],\n",
      "        [3.8274e-05, 9.9955e-01, 3.8187e-05],\n",
      "        [1.4694e-04, 9.9805e-01, 1.4509e-04],\n",
      "        [4.2309e-03, 9.6696e-01, 4.1433e-03],\n",
      "        [2.6385e-04, 9.9733e-01, 2.6150e-04],\n",
      "        [4.9418e-09, 1.0000e+00, 4.9417e-09],\n",
      "        [3.9619e-04, 9.9654e-01, 3.8512e-04],\n",
      "        [5.0882e-03, 9.6453e-01, 4.9781e-03],\n",
      "        [4.7491e-03, 9.7634e-01, 4.7391e-03],\n",
      "        [4.3263e-04, 9.9586e-01, 4.2611e-04],\n",
      "        [4.0094e-04, 9.9669e-01, 3.9501e-04],\n",
      "        [8.5908e-05, 9.9886e-01, 8.5885e-05],\n",
      "        [5.5760e-03, 9.4958e-01, 5.6717e-03],\n",
      "        [1.7190e-05, 9.9885e-01, 1.7209e-05],\n",
      "        [6.7229e-04, 9.9444e-01, 6.6038e-04],\n",
      "        [1.5076e-04, 9.9065e-01, 1.5053e-04],\n",
      "        [6.2922e-04, 9.9052e-01, 6.1963e-04],\n",
      "        [5.1034e-06, 9.9956e-01, 5.0998e-06],\n",
      "        [2.1373e-04, 9.9736e-01, 2.1080e-04],\n",
      "        [1.9490e-05, 9.9921e-01, 1.9457e-05],\n",
      "        [7.8888e-05, 9.9817e-01, 7.8474e-05],\n",
      "        [1.0981e-04, 9.9811e-01, 1.0875e-04],\n",
      "        [6.7203e-03, 9.6528e-01, 6.7842e-03],\n",
      "        [2.4407e-04, 9.9608e-01, 2.4365e-04],\n",
      "        [1.2925e-03, 9.8809e-01, 1.2614e-03],\n",
      "        [4.0548e-04, 9.9441e-01, 3.9825e-04],\n",
      "        [1.2505e-04, 9.9631e-01, 1.2444e-04],\n",
      "        [1.0716e-05, 9.9972e-01, 1.0718e-05],\n",
      "        [1.7678e-03, 9.8739e-01, 1.7394e-03],\n",
      "        [9.9092e-04, 9.9202e-01, 9.6880e-04],\n",
      "        [2.5204e-03, 9.8665e-01, 2.4829e-03],\n",
      "        [1.1552e-03, 9.8805e-01, 1.1344e-03],\n",
      "        [3.2483e-03, 9.7915e-01, 3.2009e-03],\n",
      "        [2.5645e-03, 9.7786e-01, 2.5272e-03],\n",
      "        [5.5379e-04, 9.9336e-01, 5.5048e-04],\n",
      "        [3.6735e-05, 9.9852e-01, 3.6512e-05],\n",
      "        [1.7749e-03, 9.8072e-01, 1.7614e-03],\n",
      "        [4.3595e-03, 9.7502e-01, 4.2412e-03],\n",
      "        [2.0097e-06, 9.9994e-01, 2.0092e-06],\n",
      "        [1.8352e-03, 9.8215e-01, 1.7918e-03],\n",
      "        [1.4004e-04, 9.9786e-01, 1.3841e-04],\n",
      "        [1.1538e-03, 9.9439e-01, 1.1308e-03]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>), tensor([[9.9966e-01, 4.1269e-06, 4.1277e-06, 4.1271e-06, 4.1285e-06, 4.1274e-06],\n",
      "        [9.6815e-01, 8.5949e-04, 8.8914e-04, 8.6270e-04, 8.5933e-04, 8.5912e-04],\n",
      "        [9.5723e-01, 1.8912e-03, 1.9150e-03, 1.8786e-03, 1.8755e-03, 1.8855e-03],\n",
      "        [9.2948e-01, 2.7209e-03, 2.7586e-03, 2.7662e-03, 2.7153e-03, 2.7125e-03],\n",
      "        [9.0057e-01, 5.0761e-03, 5.3358e-03, 5.1267e-03, 5.1016e-03, 5.1287e-03],\n",
      "        [8.3468e-01, 1.1542e-02, 1.1564e-02, 1.1831e-02, 1.1412e-02, 1.1575e-02],\n",
      "        [9.0260e-01, 5.5460e-03, 5.6659e-03, 5.6424e-03, 5.6496e-03, 5.5864e-03],\n",
      "        [8.7914e-01, 1.1544e-02, 1.1985e-02, 1.1409e-02, 1.1381e-02, 1.1544e-02],\n",
      "        [7.5845e-01, 2.1187e-02, 2.1806e-02, 2.1850e-02, 2.1002e-02, 2.1411e-02],\n",
      "        [9.7406e-01, 8.8218e-04, 8.8351e-04, 8.8809e-04, 8.8733e-04, 8.8220e-04],\n",
      "        [7.9819e-01, 2.3477e-02, 2.3923e-02, 2.4430e-02, 2.3137e-02, 2.2893e-02],\n",
      "        [7.8209e-01, 2.1756e-02, 2.0862e-02, 2.0740e-02, 2.1582e-02, 2.0648e-02],\n",
      "        [9.9699e-01, 8.1658e-06, 8.1700e-06, 8.1660e-06, 8.1678e-06, 8.1640e-06],\n",
      "        [9.3081e-01, 1.7597e-03, 1.8672e-03, 1.7758e-03, 1.7787e-03, 1.7805e-03],\n",
      "        [9.9562e-01, 1.6152e-05, 1.6181e-05, 1.6156e-05, 1.6153e-05, 1.6150e-05],\n",
      "        [9.7001e-01, 1.1202e-03, 1.1208e-03, 1.1182e-03, 1.1275e-03, 1.1176e-03],\n",
      "        [9.3281e-01, 4.1740e-03, 4.1598e-03, 4.1634e-03, 4.2590e-03, 4.1445e-03],\n",
      "        [7.6546e-01, 2.3348e-02, 2.3431e-02, 2.3037e-02, 2.3546e-02, 2.2450e-02],\n",
      "        [9.2722e-01, 4.1546e-03, 4.2208e-03, 4.1918e-03, 4.1508e-03, 4.1233e-03],\n",
      "        [9.9932e-01, 1.7127e-06, 1.7138e-06, 1.7128e-06, 1.7132e-06, 1.7127e-06],\n",
      "        [9.2149e-01, 5.5454e-03, 5.6686e-03, 5.5131e-03, 5.5391e-03, 5.5093e-03],\n",
      "        [7.5853e-01, 2.2255e-02, 2.3528e-02, 2.3374e-02, 2.2628e-02, 2.2008e-02],\n",
      "        [7.9712e-01, 2.0553e-02, 2.1560e-02, 2.0482e-02, 2.1092e-02, 2.0820e-02],\n",
      "        [8.5286e-01, 6.4316e-03, 6.6906e-03, 6.4904e-03, 6.4871e-03, 6.4967e-03],\n",
      "        [9.3385e-01, 3.0099e-03, 3.0642e-03, 2.9938e-03, 2.9949e-03, 2.9455e-03],\n",
      "        [9.7906e-01, 9.0236e-04, 9.0420e-04, 9.0153e-04, 8.9809e-04, 8.9425e-04],\n",
      "        [7.6710e-01, 2.3320e-02, 2.4456e-02, 2.3098e-02, 2.3570e-02, 2.4231e-02],\n",
      "        [9.7162e-01, 5.6976e-04, 5.7217e-04, 5.7151e-04, 5.6992e-04, 5.7024e-04],\n",
      "        [8.9277e-01, 7.1332e-03, 7.1532e-03, 7.1220e-03, 7.0968e-03, 7.1119e-03],\n",
      "        [9.0050e-01, 2.1419e-03, 2.1500e-03, 2.1251e-03, 2.0998e-03, 2.0876e-03],\n",
      "        [8.3888e-01, 6.8490e-03, 6.9981e-03, 6.9621e-03, 6.8887e-03, 6.8491e-03],\n",
      "        [9.8419e-01, 2.9106e-04, 2.9357e-04, 2.9254e-04, 2.9073e-04, 2.9079e-04],\n",
      "        [9.3643e-01, 2.7380e-03, 2.7467e-03, 2.7557e-03, 2.7509e-03, 2.7189e-03],\n",
      "        [9.6367e-01, 6.8270e-04, 6.8905e-04, 6.7848e-04, 6.7971e-04, 6.7980e-04],\n",
      "        [9.4575e-01, 1.5594e-03, 1.5647e-03, 1.5514e-03, 1.5588e-03, 1.5513e-03],\n",
      "        [9.5572e-01, 2.0530e-03, 2.0667e-03, 2.0599e-03, 2.0697e-03, 2.0608e-03],\n",
      "        [7.2575e-01, 2.4184e-02, 2.5929e-02, 2.5178e-02, 2.4973e-02, 2.4044e-02],\n",
      "        [9.4294e-01, 4.1692e-03, 4.2991e-03, 4.1932e-03, 4.1703e-03, 4.1720e-03],\n",
      "        [8.5265e-01, 1.2408e-02, 1.2679e-02, 1.2612e-02, 1.2559e-02, 1.2553e-02],\n",
      "        [9.0264e-01, 3.9863e-03, 4.1209e-03, 4.0891e-03, 3.9882e-03, 4.0276e-03],\n",
      "        [9.2479e-01, 2.6717e-03, 2.6707e-03, 2.7046e-03, 2.6690e-03, 2.6564e-03],\n",
      "        [9.8611e-01, 3.7672e-04, 3.7690e-04, 3.7783e-04, 3.7674e-04, 3.7683e-04],\n",
      "        [8.6401e-01, 9.5957e-03, 9.8762e-03, 9.6570e-03, 9.5689e-03, 9.4676e-03],\n",
      "        [8.8049e-01, 6.7480e-03, 6.8390e-03, 6.7048e-03, 6.8444e-03, 6.6608e-03],\n",
      "        [8.4305e-01, 1.2683e-02, 1.3181e-02, 1.2807e-02, 1.2899e-02, 1.2579e-02],\n",
      "        [8.5835e-01, 8.5034e-03, 8.5547e-03, 8.5895e-03, 8.4250e-03, 8.4957e-03],\n",
      "        [8.0291e-01, 1.6205e-02, 1.6374e-02, 1.5650e-02, 1.5836e-02, 1.5712e-02],\n",
      "        [7.6854e-01, 1.3207e-02, 1.3587e-02, 1.3388e-02, 1.3359e-02, 1.3328e-02],\n",
      "        [8.8219e-01, 6.0687e-03, 6.1382e-03, 6.0651e-03, 6.1634e-03, 6.1329e-03],\n",
      "        [9.6609e-01, 7.0603e-04, 7.1984e-04, 7.0839e-04, 7.0852e-04, 7.0739e-04],\n",
      "        [8.4739e-01, 1.1292e-02, 1.1384e-02, 1.1334e-02, 1.1191e-02, 1.1086e-02],\n",
      "        [7.8448e-01, 1.2540e-02, 1.3083e-02, 1.2902e-02, 1.2753e-02, 1.2805e-02],\n",
      "        [9.9369e-01, 8.6483e-05, 8.6655e-05, 8.6511e-05, 8.6625e-05, 8.6518e-05],\n",
      "        [8.2642e-01, 1.2063e-02, 1.2083e-02, 1.2011e-02, 1.2213e-02, 1.2091e-02],\n",
      "        [9.3016e-01, 1.9379e-03, 1.9399e-03, 1.9353e-03, 1.9212e-03, 1.9245e-03],\n",
      "        [8.5263e-01, 9.6837e-03, 1.0049e-02, 9.9955e-03, 9.6967e-03, 9.7451e-03]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>), tensor([[4.5079e-04, 3.8967e-04, 1.3514e-01, 6.6785e-04, 5.7802e-04],\n",
      "        [6.0569e-03, 3.9086e-03, 3.2355e-01, 1.8363e-02, 1.1994e-02],\n",
      "        [1.7870e-02, 1.4303e-02, 3.0510e-01, 3.4128e-02, 2.3472e-02],\n",
      "        [2.7320e-02, 1.6259e-02, 2.9424e-01, 3.9114e-02, 4.4277e-02],\n",
      "        [2.2329e-02, 1.9176e-02, 3.1390e-01, 5.1056e-02, 4.1574e-02],\n",
      "        [6.7057e-02, 5.3299e-02, 4.1230e-01, 9.7675e-02, 6.4480e-02],\n",
      "        [3.3465e-02, 2.5205e-02, 3.0992e-01, 5.1284e-02, 5.0883e-02],\n",
      "        [5.4884e-02, 3.6585e-02, 3.3851e-01, 6.9098e-02, 6.1763e-02],\n",
      "        [8.7890e-02, 7.7641e-02, 3.8239e-01, 1.1959e-01, 8.7079e-02],\n",
      "        [9.3103e-03, 1.0243e-02, 2.2002e-01, 2.0846e-02, 1.6720e-02],\n",
      "        [9.3973e-02, 7.1693e-02, 3.1433e-01, 9.9836e-02, 1.0397e-01],\n",
      "        [9.6600e-02, 7.3405e-02, 3.2131e-01, 9.3772e-02, 1.0398e-01],\n",
      "        [5.3631e-04, 3.3256e-04, 1.6353e-01, 1.1640e-03, 1.8733e-03],\n",
      "        [2.4716e-02, 2.2510e-02, 3.1504e-01, 2.8739e-02, 4.3700e-02],\n",
      "        [1.2962e-03, 8.1675e-04, 1.3326e-01, 3.8265e-03, 2.6418e-03],\n",
      "        [1.1676e-02, 8.0443e-03, 2.4583e-01, 1.8860e-02, 1.2237e-02],\n",
      "        [2.6702e-02, 1.8922e-02, 2.6936e-01, 2.9661e-02, 3.7693e-02],\n",
      "        [9.6014e-02, 8.4799e-02, 3.2260e-01, 8.8443e-02, 9.8807e-02],\n",
      "        [3.0338e-02, 2.2744e-02, 3.2941e-01, 3.3867e-02, 2.9398e-02],\n",
      "        [4.3593e-04, 2.0949e-04, 1.7132e-01, 4.0156e-04, 3.6082e-04],\n",
      "        [2.6022e-02, 2.1356e-02, 3.3326e-01, 4.9153e-02, 3.3503e-02],\n",
      "        [1.0672e-01, 8.3611e-02, 2.8690e-01, 1.1381e-01, 9.9344e-02],\n",
      "        [6.7381e-02, 7.9346e-02, 3.3392e-01, 8.0039e-02, 1.0704e-01],\n",
      "        [3.4592e-02, 3.1786e-02, 3.1831e-01, 4.5337e-02, 3.8963e-02],\n",
      "        [3.2107e-02, 2.1311e-02, 2.8758e-01, 5.8016e-02, 4.2277e-02],\n",
      "        [1.7985e-02, 1.3699e-02, 1.7061e-01, 1.4899e-02, 1.9090e-02],\n",
      "        [9.7791e-02, 9.2395e-02, 3.4850e-01, 1.0320e-01, 9.8620e-02],\n",
      "        [6.9426e-03, 6.5360e-03, 2.1332e-01, 1.9057e-02, 1.4751e-02],\n",
      "        [3.3073e-02, 3.1346e-02, 3.3595e-01, 8.7016e-02, 4.2822e-02],\n",
      "        [3.2095e-02, 1.6122e-02, 2.8965e-01, 2.2686e-02, 2.3973e-02],\n",
      "        [5.1063e-02, 3.5604e-02, 2.8667e-01, 5.6587e-02, 6.9962e-02],\n",
      "        [6.5182e-03, 2.8577e-03, 2.0889e-01, 7.3051e-03, 7.4534e-03],\n",
      "        [1.9013e-02, 2.3411e-02, 3.3689e-01, 5.2341e-02, 3.1743e-02],\n",
      "        [1.1706e-02, 5.1141e-03, 2.5919e-01, 1.2909e-02, 1.3404e-02],\n",
      "        [1.6348e-02, 1.0550e-02, 3.0943e-01, 3.8378e-02, 3.0018e-02],\n",
      "        [1.4719e-02, 1.1978e-02, 2.8064e-01, 2.1877e-02, 2.0832e-02],\n",
      "        [9.6906e-02, 9.5837e-02, 3.5002e-01, 1.2978e-01, 1.0971e-01],\n",
      "        [3.5688e-02, 2.2315e-02, 2.2599e-01, 3.8491e-02, 3.9006e-02],\n",
      "        [4.5134e-02, 3.3522e-02, 3.2505e-01, 7.2963e-02, 6.9062e-02],\n",
      "        [2.2843e-02, 2.7692e-02, 2.7994e-01, 5.3209e-02, 3.2552e-02],\n",
      "        [1.6906e-02, 9.8530e-03, 3.1592e-01, 3.2206e-02, 2.6110e-02],\n",
      "        [5.0119e-03, 3.8090e-03, 2.5197e-01, 1.0146e-02, 6.5464e-03],\n",
      "        [6.6626e-02, 6.0624e-02, 3.1771e-01, 5.4178e-02, 7.9051e-02],\n",
      "        [4.6586e-02, 4.6898e-02, 3.2924e-01, 6.0262e-02, 5.9002e-02],\n",
      "        [5.2867e-02, 5.1067e-02, 3.4395e-01, 9.6710e-02, 7.6500e-02],\n",
      "        [4.4489e-02, 4.5697e-02, 3.8263e-01, 7.4035e-02, 6.4476e-02],\n",
      "        [9.1223e-02, 7.1153e-02, 3.3167e-01, 8.0458e-02, 8.3625e-02],\n",
      "        [8.0103e-02, 4.0295e-02, 3.6957e-01, 1.0029e-01, 6.8197e-02],\n",
      "        [4.4078e-02, 3.2916e-02, 2.3589e-01, 4.1537e-02, 4.6490e-02],\n",
      "        [1.3564e-02, 6.2017e-03, 1.7957e-01, 1.3963e-02, 1.3336e-02],\n",
      "        [7.7346e-02, 4.7248e-02, 3.5609e-01, 7.4036e-02, 6.8598e-02],\n",
      "        [8.2132e-02, 6.2378e-02, 3.7771e-01, 9.5169e-02, 7.9689e-02],\n",
      "        [3.0620e-03, 1.7032e-03, 2.1579e-01, 5.9293e-03, 5.3159e-03],\n",
      "        [6.9980e-02, 6.4836e-02, 3.4043e-01, 6.4896e-02, 7.0445e-02],\n",
      "        [2.2378e-02, 1.7483e-02, 3.1539e-01, 3.0462e-02, 2.9688e-02],\n",
      "        [4.9794e-02, 3.7321e-02, 3.4393e-01, 5.8439e-02, 6.8008e-02]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)], '95%': [tensor([[0.9519, 0.3831, 0.0012],\n",
      "        [0.8960, 0.3049, 0.0199],\n",
      "        [0.8998, 0.2813, 0.0313],\n",
      "        [0.8581, 0.4646, 0.0413],\n",
      "        [0.8637, 0.3224, 0.0383],\n",
      "        [0.7208, 0.4348, 0.0617],\n",
      "        [0.7179, 0.4931, 0.0350],\n",
      "        [0.5809, 0.5755, 0.0602],\n",
      "        [0.6957, 0.4209, 0.0898],\n",
      "        [0.9363, 0.2222, 0.0170],\n",
      "        [0.5084, 0.6135, 0.0670],\n",
      "        [0.3598, 0.7736, 0.0745],\n",
      "        [0.8521, 0.7146, 0.0056],\n",
      "        [0.5484, 0.7732, 0.0405],\n",
      "        [0.9426, 0.2040, 0.0043],\n",
      "        [0.9135, 0.3853, 0.0167],\n",
      "        [0.7662, 0.4717, 0.0446],\n",
      "        [0.3045, 0.8119, 0.0744],\n",
      "        [0.8862, 0.3174, 0.0271],\n",
      "        [0.9899, 0.2831, 0.0013],\n",
      "        [0.9352, 0.1414, 0.0293],\n",
      "        [0.4114, 0.7157, 0.0736],\n",
      "        [0.2730, 0.8027, 0.0606],\n",
      "        [0.8328, 0.3546, 0.0411],\n",
      "        [0.8203, 0.3178, 0.0350],\n",
      "        [0.0926, 0.9846, 0.0127],\n",
      "        [0.3124, 0.7868, 0.0849],\n",
      "        [0.9266, 0.2599, 0.0149],\n",
      "        [0.7840, 0.4081, 0.0488],\n",
      "        [0.4051, 0.8519, 0.0423],\n",
      "        [0.7147, 0.5000, 0.0577],\n",
      "        [0.8651, 0.3778, 0.0146],\n",
      "        [0.8770, 0.2861, 0.0334],\n",
      "        [0.5935, 0.9114, 0.0266],\n",
      "        [0.9104, 0.2828, 0.0203],\n",
      "        [0.9274, 0.2003, 0.0241],\n",
      "        [0.6799, 0.4321, 0.0950],\n",
      "        [0.4825, 0.8483, 0.0326],\n",
      "        [0.7865, 0.3698, 0.0504],\n",
      "        [0.9173, 0.2870, 0.0412],\n",
      "        [0.9272, 0.2930, 0.0248],\n",
      "        [0.9587, 0.2864, 0.0130],\n",
      "        [0.3015, 0.8088, 0.0500],\n",
      "        [0.6273, 0.6826, 0.0594],\n",
      "        [0.8505, 0.2401, 0.0483],\n",
      "        [0.7943, 0.4130, 0.0544],\n",
      "        [0.3654, 0.7968, 0.0679],\n",
      "        [0.6612, 0.5247, 0.0865],\n",
      "        [0.3018, 0.8972, 0.0408],\n",
      "        [0.7883, 0.6086, 0.0203],\n",
      "        [0.7343, 0.3989, 0.0572],\n",
      "        [0.7039, 0.4557, 0.0658],\n",
      "        [0.9012, 0.3381, 0.0083],\n",
      "        [0.3939, 0.7846, 0.0721],\n",
      "        [0.9364, 0.1828, 0.0283],\n",
      "        [0.6469, 0.6435, 0.0450]], device='cuda:0', grad_fn=<ViewBackward0>), tensor([[1.6567e-06, 1.0000e+00, 1.6556e-06],\n",
      "        [5.5711e-04, 9.9998e-01, 5.5249e-04],\n",
      "        [8.3320e-04, 9.9971e-01, 8.0258e-04],\n",
      "        [2.7016e-03, 9.9957e-01, 2.6685e-03],\n",
      "        [2.3558e-03, 9.9919e-01, 2.3332e-03],\n",
      "        [8.6197e-03, 9.9500e-01, 8.3765e-03],\n",
      "        [1.7540e-03, 9.9916e-01, 1.6981e-03],\n",
      "        [5.7138e-03, 9.9710e-01, 5.6068e-03],\n",
      "        [2.2242e-02, 9.8932e-01, 2.1105e-02],\n",
      "        [4.1075e-04, 9.9990e-01, 4.0252e-04],\n",
      "        [1.1213e-02, 9.9158e-01, 1.0904e-02],\n",
      "        [1.4101e-02, 9.9060e-01, 1.3550e-02],\n",
      "        [2.8217e-05, 1.0000e+00, 2.8063e-05],\n",
      "        [1.6773e-03, 9.9969e-01, 1.6377e-03],\n",
      "        [2.6551e-05, 1.0000e+00, 2.6408e-05],\n",
      "        [2.2496e-04, 9.9992e-01, 2.2575e-04],\n",
      "        [9.8778e-04, 9.9971e-01, 9.6355e-04],\n",
      "        [1.7042e-02, 9.9163e-01, 1.6001e-02],\n",
      "        [1.3518e-03, 9.9947e-01, 1.3228e-03],\n",
      "        [4.1433e-07, 1.0000e+00, 4.1425e-07],\n",
      "        [1.7496e-03, 9.9922e-01, 1.7111e-03],\n",
      "        [1.8126e-02, 9.8993e-01, 1.7340e-02],\n",
      "        [1.2135e-02, 9.9051e-01, 1.1528e-02],\n",
      "        [2.1092e-03, 9.9914e-01, 2.0328e-03],\n",
      "        [1.6719e-03, 9.9920e-01, 1.6347e-03],\n",
      "        [5.6921e-04, 9.9983e-01, 5.6865e-04],\n",
      "        [2.5201e-02, 9.8876e-01, 2.5223e-02],\n",
      "        [5.7775e-04, 9.9997e-01, 5.6710e-04],\n",
      "        [2.8311e-03, 9.9867e-01, 2.7258e-03],\n",
      "        [4.7317e-03, 9.9970e-01, 4.6140e-03],\n",
      "        [4.8118e-03, 9.9875e-01, 4.6637e-03],\n",
      "        [2.1910e-04, 9.9999e-01, 2.1910e-04],\n",
      "        [1.3392e-03, 9.9958e-01, 1.3051e-03],\n",
      "        [3.9512e-04, 9.9996e-01, 3.9387e-04],\n",
      "        [9.2415e-04, 9.9984e-01, 9.0334e-04],\n",
      "        [9.5722e-04, 9.9978e-01, 9.3291e-04],\n",
      "        [1.8076e-02, 9.8650e-01, 1.6648e-02],\n",
      "        [1.9937e-03, 9.9951e-01, 1.9296e-03],\n",
      "        [6.0330e-03, 9.9745e-01, 5.8793e-03],\n",
      "        [2.8699e-03, 9.9920e-01, 2.7239e-03],\n",
      "        [1.8704e-03, 9.9975e-01, 1.8163e-03],\n",
      "        [1.4293e-04, 9.9998e-01, 1.4165e-04],\n",
      "        [6.3564e-03, 9.9649e-01, 6.2489e-03],\n",
      "        [4.0402e-03, 9.9804e-01, 3.9405e-03],\n",
      "        [6.8429e-03, 9.9500e-01, 6.5070e-03],\n",
      "        [6.0739e-03, 9.9771e-01, 5.7888e-03],\n",
      "        [1.0583e-02, 9.9355e-01, 1.0264e-02],\n",
      "        [1.1535e-02, 9.9491e-01, 1.0602e-02],\n",
      "        [3.3359e-03, 9.9890e-01, 3.3015e-03],\n",
      "        [7.5684e-04, 9.9993e-01, 7.2297e-04],\n",
      "        [1.0084e-02, 9.9646e-01, 9.1995e-03],\n",
      "        [1.2713e-02, 9.9140e-01, 1.2272e-02],\n",
      "        [2.9341e-05, 1.0000e+00, 2.9260e-05],\n",
      "        [8.9675e-03, 9.9637e-01, 8.8824e-03],\n",
      "        [1.0744e-03, 9.9972e-01, 1.0617e-03],\n",
      "        [2.8606e-03, 9.9772e-01, 2.7475e-03]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>), tensor([[9.9998e-01, 6.7727e-05, 6.7763e-05, 6.7889e-05, 6.7820e-05, 6.7719e-05],\n",
      "        [9.9567e-01, 6.3696e-03, 6.3899e-03, 6.3546e-03, 6.3885e-03, 6.3469e-03],\n",
      "        [9.9055e-01, 8.4923e-03, 8.6021e-03, 8.5183e-03, 8.4636e-03, 8.5385e-03],\n",
      "        [9.8633e-01, 1.4312e-02, 1.4026e-02, 1.4113e-02, 1.4227e-02, 1.3843e-02],\n",
      "        [9.7423e-01, 1.9311e-02, 2.0295e-02, 2.0460e-02, 1.9823e-02, 1.9540e-02],\n",
      "        [9.4208e-01, 3.3144e-02, 3.3058e-02, 3.2952e-02, 3.2859e-02, 3.3638e-02],\n",
      "        [9.7191e-01, 1.9561e-02, 2.0364e-02, 1.9131e-02, 1.9068e-02, 1.9279e-02],\n",
      "        [9.4216e-01, 2.4486e-02, 2.5117e-02, 2.4227e-02, 2.3500e-02, 2.3882e-02],\n",
      "        [8.9314e-01, 4.8477e-02, 4.9780e-02, 4.8130e-02, 4.7591e-02, 4.7574e-02],\n",
      "        [9.9558e-01, 5.1469e-03, 5.2229e-03, 5.2300e-03, 5.1804e-03, 5.1580e-03],\n",
      "        [8.8130e-01, 4.0062e-02, 4.1806e-02, 3.9658e-02, 4.0204e-02, 4.0402e-02],\n",
      "        [8.9393e-01, 4.3959e-02, 4.3489e-02, 4.4005e-02, 4.3371e-02, 4.4284e-02],\n",
      "        [9.9996e-01, 6.0268e-04, 6.0404e-04, 6.0020e-04, 6.0496e-04, 6.0122e-04],\n",
      "        [9.9104e-01, 1.3610e-02, 1.4064e-02, 1.4025e-02, 1.3471e-02, 1.4022e-02],\n",
      "        [9.9992e-01, 8.7620e-04, 8.7794e-04, 8.7689e-04, 8.7715e-04, 8.7549e-04],\n",
      "        [9.9440e-01, 5.9936e-03, 6.0008e-03, 5.9932e-03, 6.0188e-03, 5.9788e-03],\n",
      "        [9.7910e-01, 1.3355e-02, 1.3471e-02, 1.3410e-02, 1.3622e-02, 1.3334e-02],\n",
      "        [8.8319e-01, 4.6467e-02, 5.1159e-02, 4.5839e-02, 4.6699e-02, 4.6875e-02],\n",
      "        [9.7916e-01, 1.4656e-02, 1.5048e-02, 1.4472e-02, 1.4676e-02, 1.4386e-02],\n",
      "        [9.9999e-01, 1.3601e-04, 1.3629e-04, 1.3604e-04, 1.3605e-04, 1.3627e-04],\n",
      "        [9.7222e-01, 1.5572e-02, 1.6104e-02, 1.5536e-02, 1.5383e-02, 1.5628e-02],\n",
      "        [8.8641e-01, 4.7333e-02, 4.9290e-02, 4.8086e-02, 4.6882e-02, 4.9878e-02],\n",
      "        [8.9544e-01, 4.1936e-02, 4.2459e-02, 3.9898e-02, 4.1638e-02, 4.0793e-02],\n",
      "        [9.6729e-01, 2.9209e-02, 3.0237e-02, 2.9684e-02, 2.8837e-02, 2.9169e-02],\n",
      "        [9.8499e-01, 1.3077e-02, 1.3328e-02, 1.3239e-02, 1.3296e-02, 1.3155e-02],\n",
      "        [9.9550e-01, 4.1952e-03, 4.1769e-03, 4.1370e-03, 4.1473e-03, 4.2794e-03],\n",
      "        [8.8133e-01, 4.5691e-02, 4.7674e-02, 4.7481e-02, 4.6455e-02, 4.6498e-02],\n",
      "        [9.9715e-01, 5.6441e-03, 5.7130e-03, 5.6793e-03, 5.7328e-03, 5.6097e-03],\n",
      "        [9.6438e-01, 2.1451e-02, 2.1525e-02, 2.1249e-02, 2.1688e-02, 2.1561e-02],\n",
      "        [9.8940e-01, 2.0026e-02, 1.9604e-02, 2.0494e-02, 1.9930e-02, 1.9448e-02],\n",
      "        [9.6545e-01, 3.2091e-02, 3.2366e-02, 3.2170e-02, 3.2257e-02, 3.2106e-02],\n",
      "        [9.9854e-01, 3.1408e-03, 3.1948e-03, 3.1716e-03, 3.1712e-03, 3.1344e-03],\n",
      "        [9.8629e-01, 1.2549e-02, 1.3281e-02, 1.2820e-02, 1.2933e-02, 1.2770e-02],\n",
      "        [9.9659e-01, 7.1497e-03, 7.2754e-03, 7.3365e-03, 7.3733e-03, 7.1961e-03],\n",
      "        [9.9221e-01, 1.0731e-02, 1.0902e-02, 1.0911e-02, 1.0839e-02, 1.0867e-02],\n",
      "        [9.8965e-01, 8.7738e-03, 8.9250e-03, 8.8266e-03, 8.9316e-03, 8.8235e-03],\n",
      "        [8.7415e-01, 5.4411e-02, 5.7748e-02, 5.4171e-02, 5.4883e-02, 5.4840e-02],\n",
      "        [9.7900e-01, 1.1390e-02, 1.1574e-02, 1.1399e-02, 1.1450e-02, 1.1245e-02],\n",
      "        [9.3719e-01, 2.9698e-02, 3.0279e-02, 2.9305e-02, 2.9539e-02, 2.8588e-02],\n",
      "        [9.7979e-01, 1.9353e-02, 1.9564e-02, 1.9590e-02, 1.9319e-02, 1.9533e-02],\n",
      "        [9.8663e-01, 1.4609e-02, 1.4709e-02, 1.5026e-02, 1.4699e-02, 1.4886e-02],\n",
      "        [9.9812e-01, 2.7762e-03, 2.7769e-03, 2.7777e-03, 2.7726e-03, 2.7893e-03],\n",
      "        [9.5183e-01, 2.6629e-02, 2.8579e-02, 2.6846e-02, 2.7267e-02, 2.6673e-02],\n",
      "        [9.6620e-01, 2.4199e-02, 2.4491e-02, 2.3799e-02, 2.3679e-02, 2.3898e-02],\n",
      "        [9.3536e-01, 3.1851e-02, 3.1996e-02, 3.0887e-02, 3.0923e-02, 3.1167e-02],\n",
      "        [9.5743e-01, 2.8424e-02, 2.8410e-02, 2.8284e-02, 2.8371e-02, 2.7775e-02],\n",
      "        [9.2022e-01, 3.9562e-02, 4.0299e-02, 3.8721e-02, 4.0793e-02, 3.9406e-02],\n",
      "        [9.3331e-01, 4.5715e-02, 4.7778e-02, 4.5149e-02, 4.7214e-02, 4.5608e-02],\n",
      "        [9.6943e-01, 2.3244e-02, 2.5230e-02, 2.3212e-02, 2.3053e-02, 2.2886e-02],\n",
      "        [9.9645e-01, 6.6840e-03, 6.8164e-03, 6.8739e-03, 6.7673e-03, 6.7666e-03],\n",
      "        [9.4371e-01, 3.0437e-02, 3.1678e-02, 3.0310e-02, 3.0042e-02, 3.0142e-02],\n",
      "        [9.3617e-01, 4.3288e-02, 4.4177e-02, 4.2055e-02, 4.2765e-02, 4.3197e-02],\n",
      "        [9.9957e-01, 1.2517e-03, 1.2813e-03, 1.2566e-03, 1.2639e-03, 1.2548e-03],\n",
      "        [9.3954e-01, 3.4556e-02, 3.6831e-02, 3.3890e-02, 3.4481e-02, 3.4668e-02],\n",
      "        [9.9034e-01, 1.3958e-02, 1.3973e-02, 1.3966e-02, 1.3929e-02, 1.4013e-02],\n",
      "        [9.5083e-01, 2.8753e-02, 3.0439e-02, 2.9093e-02, 2.9403e-02, 2.9680e-02]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>), tensor([[0.0085, 0.0031, 0.4053, 0.0111, 0.0087],\n",
      "        [0.0558, 0.0288, 0.5379, 0.0595, 0.0533],\n",
      "        [0.0442, 0.0536, 0.5034, 0.0969, 0.0657],\n",
      "        [0.0802, 0.0734, 0.5393, 0.1215, 0.1157],\n",
      "        [0.0959, 0.0794, 0.4495, 0.1403, 0.1012],\n",
      "        [0.1313, 0.1190, 0.5156, 0.1795, 0.1561],\n",
      "        [0.1115, 0.0748, 0.5518, 0.0835, 0.1060],\n",
      "        [0.1450, 0.0942, 0.4915, 0.1617, 0.1080],\n",
      "        [0.1557, 0.1682, 0.4976, 0.2469, 0.1900],\n",
      "        [0.0318, 0.0429, 0.4464, 0.0780, 0.0497],\n",
      "        [0.1584, 0.1121, 0.4680, 0.1872, 0.1490],\n",
      "        [0.1899, 0.1708, 0.4967, 0.1655, 0.1991],\n",
      "        [0.0131, 0.0113, 0.6493, 0.0277, 0.0142],\n",
      "        [0.0838, 0.0518, 0.5332, 0.0997, 0.0948],\n",
      "        [0.0152, 0.0152, 0.5295, 0.0309, 0.0227],\n",
      "        [0.0384, 0.0231, 0.4517, 0.0695, 0.0530],\n",
      "        [0.0596, 0.0691, 0.5523, 0.0846, 0.0848],\n",
      "        [0.1808, 0.1614, 0.4958, 0.1606, 0.2101],\n",
      "        [0.0810, 0.0621, 0.5231, 0.1443, 0.1018],\n",
      "        [0.0062, 0.0035, 0.6705, 0.0064, 0.0041],\n",
      "        [0.0598, 0.0570, 0.4859, 0.0922, 0.0909],\n",
      "        [0.1872, 0.1772, 0.4927, 0.1816, 0.1800],\n",
      "        [0.1665, 0.1594, 0.5610, 0.1412, 0.1692],\n",
      "        [0.0687, 0.0713, 0.5020, 0.1351, 0.1023],\n",
      "        [0.0773, 0.0676, 0.5131, 0.1019, 0.1173],\n",
      "        [0.0570, 0.0372, 0.4363, 0.0429, 0.0567],\n",
      "        [0.2044, 0.1731, 0.5101, 0.1985, 0.1771],\n",
      "        [0.0290, 0.0328, 0.5476, 0.0687, 0.0536],\n",
      "        [0.0777, 0.0712, 0.5113, 0.1489, 0.1133],\n",
      "        [0.1112, 0.1276, 0.5301, 0.1152, 0.0997],\n",
      "        [0.1075, 0.0867, 0.5181, 0.1299, 0.1563],\n",
      "        [0.0275, 0.0259, 0.4608, 0.0398, 0.0360],\n",
      "        [0.0671, 0.0649, 0.4963, 0.1103, 0.0965],\n",
      "        [0.0435, 0.0393, 0.4612, 0.0607, 0.0663],\n",
      "        [0.0735, 0.0444, 0.4659, 0.0989, 0.0947],\n",
      "        [0.0538, 0.0505, 0.5004, 0.0995, 0.0735],\n",
      "        [0.1699, 0.1597, 0.5262, 0.2263, 0.1829],\n",
      "        [0.1186, 0.0699, 0.4996, 0.0962, 0.0986],\n",
      "        [0.1054, 0.1149, 0.5438, 0.1605, 0.1202],\n",
      "        [0.0595, 0.0733, 0.4605, 0.1421, 0.0872],\n",
      "        [0.0612, 0.0576, 0.5656, 0.1103, 0.0667],\n",
      "        [0.0273, 0.0314, 0.4853, 0.0479, 0.0312],\n",
      "        [0.1296, 0.1244, 0.5922, 0.1438, 0.1431],\n",
      "        [0.1108, 0.1166, 0.5542, 0.1097, 0.1629],\n",
      "        [0.1023, 0.1078, 0.4842, 0.2126, 0.1254],\n",
      "        [0.1197, 0.1001, 0.5211, 0.1460, 0.1306],\n",
      "        [0.1666, 0.1640, 0.4744, 0.1777, 0.1994],\n",
      "        [0.1654, 0.1085, 0.5168, 0.2127, 0.1423],\n",
      "        [0.0930, 0.0701, 0.4922, 0.1355, 0.1224],\n",
      "        [0.0558, 0.0317, 0.5289, 0.0620, 0.0476],\n",
      "        [0.1291, 0.1162, 0.5232, 0.1554, 0.1779],\n",
      "        [0.1708, 0.1487, 0.4865, 0.2105, 0.1624],\n",
      "        [0.0158, 0.0162, 0.5845, 0.0291, 0.0226],\n",
      "        [0.1392, 0.1193, 0.5043, 0.1633, 0.1416],\n",
      "        [0.0641, 0.0500, 0.5570, 0.0955, 0.0689],\n",
      "        [0.1285, 0.0975, 0.5319, 0.1735, 0.1342]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)], 'order': ['pd', 'nd', 'mod', 'dlt']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pd': {'accuracy': 0.3333333333333333,\n",
       "  'auc_micro': 0.6999195494770716,\n",
       "  'auc_mean': 0.5650732042513266,\n",
       "  'auc_weighted': 0.4842640097535474},\n",
       " 'nd': {'accuracy': 0.3333333333333333,\n",
       "  'auc_micro': 0.8601769911504424,\n",
       "  'auc_mean': 0.5415996640937678,\n",
       "  'auc_weighted': 0.5710763293310464},\n",
       " 'mod': {'accuracy': 0.3333333333333333,\n",
       "  'auc_micro': 0.8601769911504424,\n",
       "  'auc_mean': 0.5415996640937678,\n",
       "  'auc_weighted': 0.5710763293310464},\n",
       " 'dlts': {'accuracy': [0.8035714285714286,\n",
       "   0.8928571428571429,\n",
       "   0.7857142857142857,\n",
       "   0.9464285714285714,\n",
       "   0.9107142857142857],\n",
       "  'accuracy_mean': 0.8678571428571429,\n",
       "  'auc': [0.46464646464646464,\n",
       "   0.3766666666666667,\n",
       "   0.42424242424242425,\n",
       "   0.5408805031446541,\n",
       "   0.7019607843137254],\n",
       "  'auc_mean': 0.5016793686027871}}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmodel1_smote = train_state(model_args=t1_args,lr=.01,weights=[1,1,.1,.1],use_smote=True,balanced=False)\n",
    "tmodel1_smote[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "029a5a63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train loss 2.4509737491607666\n",
      "val loss 2.369837999343872\n",
      "______________\n",
      "epoch 1 train loss 2.367924451828003\n",
      "val loss 2.2872426509857178\n",
      "______________\n",
      "epoch 2 train loss 2.2758595943450928\n",
      "val loss 2.2009575366973877\n",
      "______________\n",
      "epoch 3 train loss 2.1895875930786133\n",
      "val loss 2.1204731464385986\n",
      "______________\n",
      "epoch 4 train loss 2.0842907428741455\n",
      "val loss 2.0539777278900146\n",
      "______________\n",
      "epoch 5 train loss 2.0182785987854004\n",
      "val loss 2.0044848918914795\n",
      "______________\n",
      "epoch 6 train loss 1.9643685817718506\n",
      "val loss 1.969146966934204\n",
      "______________\n",
      "epoch 7 train loss 1.9148519039154053\n",
      "val loss 1.9435245990753174\n",
      "______________\n",
      "epoch 8 train loss 1.8688850402832031\n",
      "val loss 1.9238097667694092\n",
      "______________\n",
      "epoch 9 train loss 1.8610708713531494\n",
      "val loss 1.9080450534820557\n",
      "______________\n",
      "epoch 10 train loss 1.8569265604019165\n",
      "val loss 1.8951029777526855\n",
      "______________\n",
      "epoch 11 train loss 1.8415250778198242\n",
      "val loss 1.885239601135254\n",
      "______________\n",
      "epoch 12 train loss 1.8208744525909424\n",
      "val loss 1.8777461051940918\n",
      "______________\n",
      "epoch 13 train loss 1.8104218244552612\n",
      "val loss 1.8698844909667969\n",
      "______________\n",
      "epoch 14 train loss 1.785670518875122\n",
      "val loss 1.863455057144165\n",
      "______________\n",
      "epoch 15 train loss 1.7706637382507324\n",
      "val loss 1.8598768711090088\n",
      "______________\n",
      "epoch 16 train loss 1.7790157794952393\n",
      "val loss 1.8576890230178833\n",
      "______________\n",
      "epoch 17 train loss 1.7645418643951416\n",
      "val loss 1.8572824001312256\n",
      "______________\n",
      "epoch 18 train loss 1.738499402999878\n",
      "val loss 1.8572421073913574\n",
      "______________\n",
      "epoch 19 train loss 1.7401100397109985\n",
      "val loss 1.85675048828125\n",
      "______________\n",
      "epoch 20 train loss 1.7387516498565674\n",
      "val loss 1.8565821647644043\n",
      "______________\n",
      "epoch 21 train loss 1.730902910232544\n",
      "val loss 1.8557689189910889\n",
      "______________\n",
      "epoch 22 train loss 1.7143973112106323\n",
      "val loss 1.8563556671142578\n",
      "______________\n",
      "epoch 23 train loss 1.6921381950378418\n",
      "val loss 1.8575725555419922\n",
      "______________\n",
      "epoch 24 train loss 1.7017998695373535\n",
      "val loss 1.859546184539795\n",
      "______________\n",
      "epoch 25 train loss 1.717839241027832\n",
      "val loss 1.8611407279968262\n",
      "______________\n",
      "epoch 26 train loss 1.6898579597473145\n",
      "val loss 1.8620853424072266\n",
      "______________\n",
      "epoch 27 train loss 1.682328701019287\n",
      "val loss 1.861680030822754\n",
      "______________\n",
      "epoch 28 train loss 1.6864607334136963\n",
      "val loss 1.8617700338363647\n",
      "______________\n",
      "epoch 29 train loss 1.6996089220046997\n",
      "val loss 1.8624324798583984\n",
      "______________\n",
      "epoch 30 train loss 1.6755841970443726\n",
      "val loss 1.8651150465011597\n",
      "______________\n",
      "epoch 31 train loss 1.687337875366211\n",
      "val loss 1.8676389455795288\n",
      "______________\n",
      "epoch 32 train loss 1.6703932285308838\n",
      "val loss 1.8669157028198242\n",
      "______________\n",
      "best loss 1.8557689189910889 {'pd': {'accuracy': 0.43047619047619046, 'auc_micro': 0.8168946098149638, 'auc_mean': 0.6421992601454057, 'auc_weighted': 0.7149197602825323}, 'nd': {'accuracy': 0.3333333333333333, 'auc_micro': 0.8558326629123089, 'auc_mean': 0.49633570373546787, 'auc_weighted': 0.5340551743853631}, 'mod': {'accuracy': 0.3333333333333333, 'auc_micro': 0.8558326629123089, 'auc_mean': 0.49633570373546787, 'auc_weighted': 0.5340551743853631}, 'dlts': {'accuracy': [0.8035714285714286, 0.8928571428571429, 0.7857142857142857, 0.9464285714285714, 0.9107142857142857], 'accuracy_mean': 0.8678571428571429, 'auc': [0.4848484848484849, 0.59, 0.5018939393939394, 0.3773584905660377, 0.6392156862745098], 'auc_mean': 0.5186633202165943}}\n",
      "{'predictions': [tensor([[9.9894e-01, 1.0554e-03, 3.0611e-07],\n",
      "        [8.4976e-01, 1.4988e-01, 3.5414e-04],\n",
      "        [7.5744e-01, 2.4184e-01, 7.2519e-04],\n",
      "        [9.6481e-01, 3.4542e-02, 6.4848e-04],\n",
      "        [7.5671e-01, 2.4241e-01, 8.7921e-04],\n",
      "        [9.9038e-01, 9.4816e-03, 1.3459e-04],\n",
      "        [2.5883e-01, 7.4020e-01, 9.6987e-04],\n",
      "        [6.8477e-01, 3.1405e-01, 1.1766e-03],\n",
      "        [9.6935e-01, 3.0283e-02, 3.6977e-04],\n",
      "        [9.1931e-01, 8.0164e-02, 5.2357e-04],\n",
      "        [8.6217e-01, 1.3706e-01, 7.6903e-04],\n",
      "        [7.0274e-01, 2.9645e-01, 8.0980e-04],\n",
      "        [1.6859e-02, 9.8314e-01, 1.3812e-06],\n",
      "        [3.2063e-02, 9.6787e-01, 6.4003e-05],\n",
      "        [2.2628e-01, 7.7355e-01, 1.7847e-04],\n",
      "        [6.6790e-01, 3.3198e-01, 1.2646e-04],\n",
      "        [3.2085e-01, 6.7836e-01, 7.9521e-04],\n",
      "        [3.0476e-02, 9.6902e-01, 5.0487e-04],\n",
      "        [4.8085e-01, 5.1724e-01, 1.9030e-03],\n",
      "        [8.2280e-01, 1.7720e-01, 7.4594e-07],\n",
      "        [9.4039e-01, 5.8828e-02, 7.7855e-04],\n",
      "        [3.0069e-01, 6.9794e-01, 1.3744e-03],\n",
      "        [3.4105e-02, 9.6544e-01, 4.5852e-04],\n",
      "        [9.8993e-01, 9.8005e-03, 2.6892e-04],\n",
      "        [7.6022e-01, 2.3918e-01, 6.0311e-04],\n",
      "        [1.1040e-04, 9.9988e-01, 6.1440e-06],\n",
      "        [2.8308e-02, 9.7129e-01, 4.0180e-04],\n",
      "        [6.7205e-02, 9.3260e-01, 1.9637e-04],\n",
      "        [4.7599e-01, 5.2271e-01, 1.2924e-03],\n",
      "        [1.6068e-02, 9.8390e-01, 2.8916e-05],\n",
      "        [9.2560e-01, 7.3236e-02, 1.1679e-03],\n",
      "        [9.9302e-03, 9.9005e-01, 2.1031e-05],\n",
      "        [9.8954e-01, 1.0129e-02, 3.2967e-04],\n",
      "        [6.8135e-03, 9.9311e-01, 8.0278e-05],\n",
      "        [5.8574e-01, 4.1344e-01, 8.1469e-04],\n",
      "        [9.7582e-01, 2.3744e-02, 4.3616e-04],\n",
      "        [9.7299e-01, 2.6646e-02, 3.6318e-04],\n",
      "        [2.9325e-02, 9.7061e-01, 6.4530e-05],\n",
      "        [9.6723e-01, 3.2247e-02, 5.2453e-04],\n",
      "        [9.6837e-01, 3.1183e-02, 4.4643e-04],\n",
      "        [8.6322e-01, 1.3599e-01, 7.9641e-04],\n",
      "        [9.5077e-01, 4.8829e-02, 4.0605e-04],\n",
      "        [3.3577e-02, 9.6598e-01, 4.4067e-04],\n",
      "        [1.1486e-01, 8.8459e-01, 5.5371e-04],\n",
      "        [9.7731e-01, 2.2141e-02, 5.4780e-04],\n",
      "        [9.4297e-01, 5.6787e-02, 2.3892e-04],\n",
      "        [1.0988e-01, 8.8931e-01, 8.0862e-04],\n",
      "        [9.6019e-01, 3.9502e-02, 3.1221e-04],\n",
      "        [5.1250e-02, 9.4833e-01, 4.1980e-04],\n",
      "        [8.5498e-01, 1.4492e-01, 1.0542e-04],\n",
      "        [5.6989e-01, 4.2881e-01, 1.2966e-03],\n",
      "        [9.8465e-01, 1.5030e-02, 3.2212e-04],\n",
      "        [8.0650e-01, 1.9326e-01, 2.3739e-04],\n",
      "        [2.5403e-01, 7.4528e-01, 6.9247e-04],\n",
      "        [6.3483e-01, 3.6390e-01, 1.2694e-03],\n",
      "        [9.6865e-01, 3.0954e-02, 3.9850e-04]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>), tensor([[9.9295e-10, 1.0000e+00, 9.9293e-10],\n",
      "        [3.0592e-06, 9.9999e-01, 3.0578e-06],\n",
      "        [5.9268e-06, 9.9999e-01, 5.9232e-06],\n",
      "        [2.3062e-05, 9.9995e-01, 2.3016e-05],\n",
      "        [1.2449e-05, 9.9998e-01, 1.2434e-05],\n",
      "        [6.9737e-06, 9.9999e-01, 6.9657e-06],\n",
      "        [1.4755e-05, 9.9997e-01, 1.4753e-05],\n",
      "        [1.3721e-05, 9.9997e-01, 1.3711e-05],\n",
      "        [1.7252e-05, 9.9997e-01, 1.7223e-05],\n",
      "        [6.4116e-06, 9.9999e-01, 6.4062e-06],\n",
      "        [1.8935e-05, 9.9996e-01, 1.8909e-05],\n",
      "        [9.3490e-06, 9.9998e-01, 9.3431e-06],\n",
      "        [1.4869e-09, 1.0000e+00, 1.4869e-09],\n",
      "        [4.7485e-07, 1.0000e+00, 4.7487e-07],\n",
      "        [4.3324e-07, 1.0000e+00, 4.3321e-07],\n",
      "        [3.8147e-07, 1.0000e+00, 3.8140e-07],\n",
      "        [7.5257e-06, 9.9998e-01, 7.5246e-06],\n",
      "        [2.7249e-05, 9.9995e-01, 2.7257e-05],\n",
      "        [2.5647e-05, 9.9995e-01, 2.5624e-05],\n",
      "        [5.9179e-11, 1.0000e+00, 5.9179e-11],\n",
      "        [2.8732e-05, 9.9994e-01, 2.8669e-05],\n",
      "        [3.1847e-05, 9.9994e-01, 3.1813e-05],\n",
      "        [2.1133e-05, 9.9996e-01, 2.1136e-05],\n",
      "        [1.2781e-05, 9.9997e-01, 1.2761e-05],\n",
      "        [5.4128e-06, 9.9999e-01, 5.4092e-06],\n",
      "        [1.2615e-06, 1.0000e+00, 1.2623e-06],\n",
      "        [2.0616e-05, 9.9996e-01, 2.0625e-05],\n",
      "        [1.6198e-06, 1.0000e+00, 1.6198e-06],\n",
      "        [1.6289e-05, 9.9997e-01, 1.6270e-05],\n",
      "        [2.8285e-07, 1.0000e+00, 2.8286e-07],\n",
      "        [5.2913e-05, 9.9989e-01, 5.2788e-05],\n",
      "        [1.9190e-07, 1.0000e+00, 1.9191e-07],\n",
      "        [1.9617e-05, 9.9996e-01, 1.9575e-05],\n",
      "        [2.1432e-06, 1.0000e+00, 2.1439e-06],\n",
      "        [6.2517e-06, 9.9999e-01, 6.2489e-06],\n",
      "        [1.8448e-05, 9.9996e-01, 1.8414e-05],\n",
      "        [1.8689e-05, 9.9996e-01, 1.8655e-05],\n",
      "        [5.2399e-07, 1.0000e+00, 5.2400e-07],\n",
      "        [2.9206e-05, 9.9994e-01, 2.9148e-05],\n",
      "        [1.8134e-05, 9.9996e-01, 1.8097e-05],\n",
      "        [1.3006e-05, 9.9997e-01, 1.2996e-05],\n",
      "        [7.3963e-06, 9.9999e-01, 7.3889e-06],\n",
      "        [1.7215e-05, 9.9997e-01, 1.7222e-05],\n",
      "        [6.8218e-06, 9.9999e-01, 6.8217e-06],\n",
      "        [3.6323e-05, 9.9993e-01, 3.6223e-05],\n",
      "        [4.6120e-06, 9.9999e-01, 4.6089e-06],\n",
      "        [2.0078e-05, 9.9996e-01, 2.0079e-05],\n",
      "        [1.0056e-05, 9.9998e-01, 1.0042e-05],\n",
      "        [1.5716e-05, 9.9997e-01, 1.5720e-05],\n",
      "        [3.9198e-07, 1.0000e+00, 3.9191e-07],\n",
      "        [2.3927e-05, 9.9995e-01, 2.3904e-05],\n",
      "        [2.4338e-05, 9.9995e-01, 2.4279e-05],\n",
      "        [1.2348e-06, 1.0000e+00, 1.2343e-06],\n",
      "        [1.0750e-05, 9.9998e-01, 1.0746e-05],\n",
      "        [1.9629e-05, 9.9996e-01, 1.9612e-05],\n",
      "        [1.5428e-05, 9.9997e-01, 1.5402e-05]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>), tensor([[9.9999e-01, 1.3750e-06, 1.3750e-06, 1.3750e-06, 1.3750e-06, 1.3754e-06],\n",
      "        [9.9886e-01, 2.2783e-04, 2.2757e-04, 2.2795e-04, 2.2770e-04, 2.2958e-04],\n",
      "        [9.9809e-01, 3.8193e-04, 3.8146e-04, 3.8195e-04, 3.8200e-04, 3.8538e-04],\n",
      "        [9.9510e-01, 9.7572e-04, 9.7385e-04, 9.7726e-04, 9.7610e-04, 9.9343e-04],\n",
      "        [9.9653e-01, 6.9323e-04, 6.9185e-04, 6.9347e-04, 6.9302e-04, 7.0139e-04],\n",
      "        [9.9797e-01, 4.0473e-04, 4.0428e-04, 4.0497e-04, 4.0524e-04, 4.0866e-04],\n",
      "        [9.9654e-01, 6.9214e-04, 6.9032e-04, 6.9209e-04, 6.9111e-04, 6.9873e-04],\n",
      "        [9.9694e-01, 6.1167e-04, 6.1044e-04, 6.1192e-04, 6.1135e-04, 6.1737e-04],\n",
      "        [9.9646e-01, 7.0613e-04, 7.0513e-04, 7.0717e-04, 7.0700e-04, 7.1676e-04],\n",
      "        [9.9769e-01, 4.6195e-04, 4.6128e-04, 4.6201e-04, 4.6191e-04, 4.6636e-04],\n",
      "        [9.9645e-01, 7.0770e-04, 7.0625e-04, 7.0864e-04, 7.0771e-04, 7.1785e-04],\n",
      "        [9.9773e-01, 4.5362e-04, 4.5296e-04, 4.5373e-04, 4.5375e-04, 4.5744e-04],\n",
      "        [9.9999e-01, 1.5789e-06, 1.5788e-06, 1.5789e-06, 1.5788e-06, 1.5792e-06],\n",
      "        [9.9965e-01, 7.0035e-05, 6.9992e-05, 7.0032e-05, 7.0004e-05, 7.0192e-05],\n",
      "        [9.9969e-01, 6.1259e-05, 6.1227e-05, 6.1252e-05, 6.1243e-05, 6.1438e-05],\n",
      "        [9.9969e-01, 6.1898e-05, 6.1872e-05, 6.1896e-05, 6.1895e-05, 6.2061e-05],\n",
      "        [9.9786e-01, 4.2849e-04, 4.2771e-04, 4.2840e-04, 4.2808e-04, 4.3206e-04],\n",
      "        [9.9570e-01, 8.6010e-04, 8.5703e-04, 8.5986e-04, 8.5856e-04, 8.6816e-04],\n",
      "        [9.9517e-01, 9.6467e-04, 9.6137e-04, 9.6467e-04, 9.6324e-04, 9.7846e-04],\n",
      "        [1.0000e+00, 2.4912e-07, 2.4912e-07, 2.4912e-07, 2.4912e-07, 2.4915e-07],\n",
      "        [9.9382e-01, 1.2327e-03, 1.2295e-03, 1.2344e-03, 1.2329e-03, 1.2533e-03],\n",
      "        [9.9515e-01, 9.6763e-04, 9.6479e-04, 9.6900e-04, 9.6699e-04, 9.8075e-04],\n",
      "        [9.9627e-01, 7.4599e-04, 7.4368e-04, 7.4626e-04, 7.4486e-04, 7.5304e-04],\n",
      "        [9.9657e-01, 6.8450e-04, 6.8370e-04, 6.8549e-04, 6.8509e-04, 6.9527e-04],\n",
      "        [9.9824e-01, 3.5189e-04, 3.5143e-04, 3.5209e-04, 3.5173e-04, 3.5481e-04],\n",
      "        [9.9949e-01, 1.0125e-04, 1.0113e-04, 1.0120e-04, 1.0116e-04, 1.0146e-04],\n",
      "        [9.9618e-01, 7.6303e-04, 7.6079e-04, 7.6305e-04, 7.6193e-04, 7.7054e-04],\n",
      "        [9.9913e-01, 1.7356e-04, 1.7340e-04, 1.7357e-04, 1.7347e-04, 1.7436e-04],\n",
      "        [9.9614e-01, 7.7134e-04, 7.6927e-04, 7.7159e-04, 7.7055e-04, 7.7938e-04],\n",
      "        [9.9975e-01, 5.0273e-05, 5.0253e-05, 5.0274e-05, 5.0263e-05, 5.0371e-05],\n",
      "        [9.9281e-01, 1.4333e-03, 1.4284e-03, 1.4363e-03, 1.4338e-03, 1.4615e-03],\n",
      "        [9.9980e-01, 4.0453e-05, 4.0436e-05, 4.0450e-05, 4.0441e-05, 4.0506e-05],\n",
      "        [9.9538e-01, 9.2019e-04, 9.1874e-04, 9.2148e-04, 9.2149e-04, 9.3816e-04],\n",
      "        [9.9912e-01, 1.7559e-04, 1.7538e-04, 1.7560e-04, 1.7546e-04, 1.7622e-04],\n",
      "        [9.9797e-01, 4.0540e-04, 4.0481e-04, 4.0549e-04, 4.0518e-04, 4.0900e-04],\n",
      "        [9.9543e-01, 9.1075e-04, 9.0902e-04, 9.1143e-04, 9.1119e-04, 9.2452e-04],\n",
      "        [9.9626e-01, 7.4578e-04, 7.4471e-04, 7.4695e-04, 7.4684e-04, 7.5746e-04],\n",
      "        [9.9965e-01, 6.9926e-05, 6.9886e-05, 6.9926e-05, 6.9904e-05, 7.0078e-05],\n",
      "        [9.9496e-01, 1.0057e-03, 1.0033e-03, 1.0067e-03, 1.0069e-03, 1.0217e-03],\n",
      "        [9.9544e-01, 9.0967e-04, 9.0788e-04, 9.1028e-04, 9.1037e-04, 9.2334e-04],\n",
      "        [9.9722e-01, 5.5392e-04, 5.5292e-04, 5.5419e-04, 5.5386e-04, 5.6030e-04],\n",
      "        [9.9775e-01, 4.4967e-04, 4.4908e-04, 4.5007e-04, 4.4968e-04, 4.5451e-04],\n",
      "        [9.9636e-01, 7.2714e-04, 7.2514e-04, 7.2756e-04, 7.2621e-04, 7.3267e-04],\n",
      "        [9.9762e-01, 4.7625e-04, 4.7539e-04, 4.7627e-04, 4.7579e-04, 4.7945e-04],\n",
      "        [9.9312e-01, 1.3691e-03, 1.3660e-03, 1.3712e-03, 1.3719e-03, 1.3970e-03],\n",
      "        [9.9853e-01, 2.9344e-04, 2.9317e-04, 2.9355e-04, 2.9358e-04, 2.9580e-04],\n",
      "        [9.9587e-01, 8.2486e-04, 8.2271e-04, 8.2541e-04, 8.2418e-04, 8.3422e-04],\n",
      "        [9.9773e-01, 4.5374e-04, 4.5320e-04, 4.5429e-04, 4.5402e-04, 4.5896e-04],\n",
      "        [9.9690e-01, 6.1949e-04, 6.1793e-04, 6.1957e-04, 6.1874e-04, 6.2557e-04],\n",
      "        [9.9972e-01, 5.6834e-05, 5.6814e-05, 5.6846e-05, 5.6835e-05, 5.7016e-05],\n",
      "        [9.9583e-01, 8.3308e-04, 8.3095e-04, 8.3376e-04, 8.3255e-04, 8.4348e-04],\n",
      "        [9.9579e-01, 8.3961e-04, 8.3814e-04, 8.4129e-04, 8.4073e-04, 8.5369e-04],\n",
      "        [9.9936e-01, 1.2877e-04, 1.2870e-04, 1.2881e-04, 1.2876e-04, 1.2944e-04],\n",
      "        [9.9731e-01, 5.3610e-04, 5.3519e-04, 5.3661e-04, 5.3593e-04, 5.4124e-04],\n",
      "        [9.9523e-01, 9.5289e-04, 9.5062e-04, 9.5341e-04, 9.5229e-04, 9.6439e-04],\n",
      "        [9.9706e-01, 5.8732e-04, 5.8621e-04, 5.8802e-04, 5.8756e-04, 5.9527e-04]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>), tensor([[0.1100, 0.0781, 0.0105, 0.0077, 0.0330],\n",
      "        [0.2103, 0.2115, 0.0590, 0.0334, 0.0934],\n",
      "        [0.1956, 0.2848, 0.0689, 0.0564, 0.1033],\n",
      "        [0.2241, 0.2400, 0.1087, 0.0884, 0.1209],\n",
      "        [0.1875, 0.2599, 0.0703, 0.0576, 0.1188],\n",
      "        [0.1876, 0.2381, 0.0623, 0.0631, 0.1213],\n",
      "        [0.2299, 0.2534, 0.0773, 0.0435, 0.1160],\n",
      "        [0.2445, 0.2269, 0.0806, 0.0643, 0.1003],\n",
      "        [0.2086, 0.2267, 0.0764, 0.0780, 0.1368],\n",
      "        [0.2142, 0.2539, 0.0786, 0.0564, 0.1029],\n",
      "        [0.2013, 0.2204, 0.0732, 0.0759, 0.1089],\n",
      "        [0.1998, 0.2204, 0.0603, 0.0576, 0.1003],\n",
      "        [0.1371, 0.0932, 0.0108, 0.0035, 0.0205],\n",
      "        [0.2168, 0.1694, 0.0378, 0.0208, 0.0600],\n",
      "        [0.1510, 0.1891, 0.0393, 0.0232, 0.0565],\n",
      "        [0.1522, 0.2346, 0.0392, 0.0236, 0.0677],\n",
      "        [0.2349, 0.2492, 0.0674, 0.0463, 0.1040],\n",
      "        [0.2282, 0.2435, 0.0786, 0.0540, 0.1011],\n",
      "        [0.2198, 0.2576, 0.1031, 0.0672, 0.1140],\n",
      "        [0.0558, 0.0720, 0.0037, 0.0028, 0.0192],\n",
      "        [0.2402, 0.2670, 0.0951, 0.0712, 0.1367],\n",
      "        [0.2152, 0.2560, 0.0787, 0.0638, 0.1094],\n",
      "        [0.2195, 0.2241, 0.0733, 0.0551, 0.0856],\n",
      "        [0.2256, 0.2444, 0.1041, 0.0800, 0.1196],\n",
      "        [0.2185, 0.2648, 0.0681, 0.0425, 0.1051],\n",
      "        [0.1718, 0.2062, 0.0436, 0.0194, 0.0422],\n",
      "        [0.2187, 0.2301, 0.0725, 0.0530, 0.0956],\n",
      "        [0.1664, 0.2332, 0.0456, 0.0303, 0.0730],\n",
      "        [0.2554, 0.2199, 0.0855, 0.0539, 0.1007],\n",
      "        [0.1697, 0.1478, 0.0265, 0.0159, 0.0414],\n",
      "        [0.2335, 0.2593, 0.1025, 0.0758, 0.1424],\n",
      "        [0.1408, 0.2040, 0.0296, 0.0137, 0.0401],\n",
      "        [0.2138, 0.2443, 0.1184, 0.0826, 0.1321],\n",
      "        [0.1671, 0.2359, 0.0448, 0.0287, 0.0623],\n",
      "        [0.2177, 0.2479, 0.0653, 0.0397, 0.1105],\n",
      "        [0.2125, 0.2673, 0.0879, 0.0604, 0.1407],\n",
      "        [0.2108, 0.2321, 0.0784, 0.0798, 0.1397],\n",
      "        [0.1627, 0.1993, 0.0351, 0.0214, 0.0609],\n",
      "        [0.2393, 0.2765, 0.0933, 0.0768, 0.1495],\n",
      "        [0.2035, 0.2812, 0.0850, 0.0688, 0.1382],\n",
      "        [0.2250, 0.2559, 0.0949, 0.0615, 0.1021],\n",
      "        [0.1896, 0.2268, 0.0859, 0.0577, 0.1141],\n",
      "        [0.2354, 0.2437, 0.0724, 0.0449, 0.0853],\n",
      "        [0.2415, 0.2497, 0.0727, 0.0527, 0.0778],\n",
      "        [0.2233, 0.2792, 0.1048, 0.0866, 0.1446],\n",
      "        [0.1821, 0.2088, 0.0453, 0.0472, 0.1058],\n",
      "        [0.2240, 0.2301, 0.0803, 0.0549, 0.1033],\n",
      "        [0.2067, 0.1900, 0.0708, 0.0712, 0.1188],\n",
      "        [0.1997, 0.2543, 0.0713, 0.0384, 0.0879],\n",
      "        [0.1457, 0.1557, 0.0253, 0.0270, 0.0598],\n",
      "        [0.2455, 0.2141, 0.0675, 0.0573, 0.1163],\n",
      "        [0.2073, 0.2287, 0.0853, 0.0863, 0.1226],\n",
      "        [0.1625, 0.1822, 0.0415, 0.0327, 0.0765],\n",
      "        [0.1948, 0.2086, 0.0751, 0.0521, 0.1158],\n",
      "        [0.2471, 0.2885, 0.0820, 0.0582, 0.1214],\n",
      "        [0.2129, 0.2157, 0.0851, 0.0885, 0.1184]], device='cuda:0',\n",
      "       grad_fn=<MulBackward0>)], '5%': [tensor([[9.9149e-01, 2.0278e-05, 1.2989e-08],\n",
      "        [5.6500e-01, 5.2650e-02, 9.1444e-05],\n",
      "        [4.5378e-01, 5.7768e-02, 1.9107e-04],\n",
      "        [9.0361e-01, 9.8167e-03, 2.6055e-04],\n",
      "        [4.1119e-01, 8.5623e-02, 2.2133e-04],\n",
      "        [9.6498e-01, 7.8603e-04, 1.5560e-05],\n",
      "        [8.4428e-02, 2.5282e-01, 3.6741e-04],\n",
      "        [4.0472e-01, 1.4826e-01, 3.6030e-04],\n",
      "        [8.5955e-01, 1.2254e-02, 1.5224e-04],\n",
      "        [7.5761e-01, 1.9166e-02, 9.6173e-05],\n",
      "        [6.9554e-01, 5.7050e-02, 1.7089e-04],\n",
      "        [5.6468e-01, 9.0002e-02, 1.9902e-04],\n",
      "        [1.8016e-03, 7.6528e-02, 1.5887e-07],\n",
      "        [4.1060e-03, 9.3883e-01, 6.0026e-06],\n",
      "        [7.3558e-02, 4.2462e-01, 3.6176e-05],\n",
      "        [2.2605e-01, 7.8096e-02, 1.6043e-05],\n",
      "        [1.3971e-01, 3.1863e-01, 3.5530e-04],\n",
      "        [8.8366e-03, 8.0631e-01, 1.7655e-04],\n",
      "        [3.0652e-01, 2.7414e-01, 3.4334e-04],\n",
      "        [7.3904e-02, 8.1562e-03, 3.9953e-08],\n",
      "        [8.3818e-01, 3.1803e-02, 2.5757e-04],\n",
      "        [6.5056e-02, 5.2825e-01, 4.6175e-04],\n",
      "        [1.0257e-02, 7.4811e-01, 1.0792e-04],\n",
      "        [9.2675e-01, 3.1266e-03, 1.2392e-04],\n",
      "        [3.3565e-01, 2.8649e-02, 1.1176e-04],\n",
      "        [1.2790e-05, 9.9907e-01, 5.6627e-07],\n",
      "        [8.0940e-03, 9.1956e-01, 7.0201e-05],\n",
      "        [9.0476e-03, 7.3453e-01, 3.3831e-05],\n",
      "        [2.4194e-01, 1.8504e-01, 3.3792e-04],\n",
      "        [2.1786e-03, 8.9930e-01, 3.9284e-06],\n",
      "        [8.1100e-01, 2.3098e-02, 2.0073e-04],\n",
      "        [2.4077e-03, 8.4335e-01, 4.1273e-06],\n",
      "        [9.5291e-01, 3.5890e-03, 1.0466e-04],\n",
      "        [8.2244e-04, 9.4041e-01, 7.8369e-06],\n",
      "        [1.7731e-01, 1.8576e-01, 2.2524e-04],\n",
      "        [9.1521e-01, 1.2358e-02, 1.6761e-04],\n",
      "        [9.2709e-01, 9.0358e-03, 1.0392e-04],\n",
      "        [1.6872e-02, 8.2712e-01, 1.3038e-05],\n",
      "        [9.0793e-01, 9.6515e-03, 1.3475e-04],\n",
      "        [8.5335e-01, 2.0223e-02, 1.6110e-04],\n",
      "        [7.1101e-01, 3.0513e-02, 1.7582e-04],\n",
      "        [7.2233e-01, 1.4704e-02, 1.5827e-04],\n",
      "        [9.5512e-03, 8.9633e-01, 6.5065e-05],\n",
      "        [2.6880e-02, 5.1614e-01, 1.1531e-04],\n",
      "        [9.4121e-01, 8.5144e-03, 1.6938e-04],\n",
      "        [7.5970e-01, 1.6828e-02, 3.5893e-05],\n",
      "        [2.4009e-02, 6.1434e-01, 1.5172e-04],\n",
      "        [8.7612e-01, 1.6858e-02, 1.1979e-04],\n",
      "        [1.9552e-02, 7.5052e-01, 1.4608e-04],\n",
      "        [5.5811e-01, 4.6676e-02, 1.8886e-05],\n",
      "        [4.8275e-01, 1.2048e-01, 3.7210e-04],\n",
      "        [9.4275e-01, 6.7725e-03, 1.1406e-04],\n",
      "        [3.4849e-01, 2.7667e-02, 2.7657e-05],\n",
      "        [3.5858e-02, 5.2820e-01, 1.7185e-04],\n",
      "        [2.5411e-01, 1.4803e-01, 4.3815e-04],\n",
      "        [9.1687e-01, 6.9550e-03, 1.0138e-04]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>), tensor([[5.1239e-11, 1.0000e+00, 5.1239e-11],\n",
      "        [3.4373e-07, 9.9999e-01, 3.4373e-07],\n",
      "        [1.2520e-06, 9.9998e-01, 1.2490e-06],\n",
      "        [4.6346e-06, 9.9990e-01, 4.6266e-06],\n",
      "        [2.4630e-06, 9.9990e-01, 2.4630e-06],\n",
      "        [8.6959e-07, 9.9999e-01, 8.6789e-07],\n",
      "        [6.9076e-06, 9.9993e-01, 6.9042e-06],\n",
      "        [2.7527e-06, 9.9994e-01, 2.7497e-06],\n",
      "        [3.6003e-06, 9.9990e-01, 3.5989e-06],\n",
      "        [1.6188e-06, 9.9999e-01, 1.6178e-06],\n",
      "        [3.6008e-06, 9.9993e-01, 3.6031e-06],\n",
      "        [2.3147e-06, 9.9997e-01, 2.3157e-06],\n",
      "        [9.5439e-11, 1.0000e+00, 9.5439e-11],\n",
      "        [2.0880e-08, 1.0000e+00, 2.0880e-08],\n",
      "        [3.5039e-08, 1.0000e+00, 3.5038e-08],\n",
      "        [4.1468e-08, 1.0000e+00, 4.1457e-08],\n",
      "        [1.6883e-06, 9.9997e-01, 1.6880e-06],\n",
      "        [3.3792e-06, 9.9987e-01, 3.3850e-06],\n",
      "        [3.3004e-06, 9.9989e-01, 3.2959e-06],\n",
      "        [1.1561e-12, 1.0000e+00, 1.1561e-12],\n",
      "        [6.7540e-06, 9.9987e-01, 6.7504e-06],\n",
      "        [7.2659e-06, 9.9989e-01, 7.2697e-06],\n",
      "        [4.4751e-06, 9.9987e-01, 4.4693e-06],\n",
      "        [4.0878e-06, 9.9993e-01, 4.0823e-06],\n",
      "        [2.0796e-06, 9.9992e-01, 2.0791e-06],\n",
      "        [1.0395e-07, 9.9999e-01, 1.0399e-07],\n",
      "        [3.9154e-06, 9.9988e-01, 3.9138e-06],\n",
      "        [3.0104e-07, 9.9998e-01, 3.0086e-07],\n",
      "        [3.5049e-06, 9.9995e-01, 3.5002e-06],\n",
      "        [2.7624e-08, 1.0000e+00, 2.7621e-08],\n",
      "        [1.3380e-05, 9.9983e-01, 1.3378e-05],\n",
      "        [1.7865e-08, 1.0000e+00, 1.7867e-08],\n",
      "        [5.4761e-06, 9.9984e-01, 5.4747e-06],\n",
      "        [1.1793e-07, 1.0000e+00, 1.1792e-07],\n",
      "        [9.6711e-07, 9.9994e-01, 9.6693e-07],\n",
      "        [9.0990e-06, 9.9992e-01, 9.1001e-06],\n",
      "        [3.8519e-06, 9.9992e-01, 3.8496e-06],\n",
      "        [9.6740e-08, 1.0000e+00, 9.6745e-08],\n",
      "        [5.9759e-06, 9.9983e-01, 5.9607e-06],\n",
      "        [3.3672e-06, 9.9986e-01, 3.3583e-06],\n",
      "        [3.8740e-06, 9.9995e-01, 3.8785e-06],\n",
      "        [2.0462e-06, 9.9996e-01, 2.0464e-06],\n",
      "        [2.7909e-06, 9.9993e-01, 2.7920e-06],\n",
      "        [1.0783e-06, 9.9996e-01, 1.0771e-06],\n",
      "        [9.7188e-06, 9.9987e-01, 9.6270e-06],\n",
      "        [5.7055e-07, 9.9993e-01, 5.7049e-07],\n",
      "        [4.1276e-06, 9.9989e-01, 4.1360e-06],\n",
      "        [3.1181e-06, 9.9990e-01, 3.0987e-06],\n",
      "        [3.5315e-06, 9.9995e-01, 3.5321e-06],\n",
      "        [5.4371e-08, 1.0000e+00, 5.4378e-08],\n",
      "        [6.2341e-06, 9.9988e-01, 6.2265e-06],\n",
      "        [6.8254e-06, 9.9990e-01, 6.8009e-06],\n",
      "        [5.6466e-08, 1.0000e+00, 5.6464e-08],\n",
      "        [2.3648e-06, 9.9993e-01, 2.3632e-06],\n",
      "        [6.1350e-06, 9.9986e-01, 6.1135e-06],\n",
      "        [4.0909e-06, 9.9995e-01, 4.0727e-06]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>), tensor([[9.9996e-01, 2.0261e-07, 2.0260e-07, 2.0260e-07, 2.0260e-07, 2.0267e-07],\n",
      "        [9.9489e-01, 5.1417e-05, 5.1392e-05, 5.1565e-05, 5.1399e-05, 5.1627e-05],\n",
      "        [9.9664e-01, 8.6385e-05, 8.6357e-05, 8.6515e-05, 8.6443e-05, 8.7587e-05],\n",
      "        [9.8879e-01, 3.4344e-04, 3.4310e-04, 3.4463e-04, 3.4368e-04, 3.5721e-04],\n",
      "        [9.9450e-01, 1.8080e-04, 1.8087e-04, 1.8095e-04, 1.8097e-04, 1.8212e-04],\n",
      "        [9.9777e-01, 1.2033e-04, 1.1997e-04, 1.2006e-04, 1.2013e-04, 1.2110e-04],\n",
      "        [9.8840e-01, 3.4152e-04, 3.3911e-04, 3.4018e-04, 3.3917e-04, 3.4676e-04],\n",
      "        [9.9402e-01, 1.3597e-04, 1.3609e-04, 1.3629e-04, 1.3612e-04, 1.3902e-04],\n",
      "        [9.9222e-01, 2.7190e-04, 2.7158e-04, 2.7238e-04, 2.7177e-04, 2.7398e-04],\n",
      "        [9.9616e-01, 1.5726e-04, 1.5725e-04, 1.5734e-04, 1.5777e-04, 1.5801e-04],\n",
      "        [9.9379e-01, 2.3980e-04, 2.3941e-04, 2.4087e-04, 2.4010e-04, 2.4768e-04],\n",
      "        [9.9604e-01, 1.6793e-04, 1.6752e-04, 1.6771e-04, 1.6771e-04, 1.6896e-04],\n",
      "        [9.9975e-01, 8.6417e-08, 8.6411e-08, 8.6413e-08, 8.6411e-08, 8.6426e-08],\n",
      "        [9.9923e-01, 1.2581e-05, 1.2573e-05, 1.2578e-05, 1.2577e-05, 1.2640e-05],\n",
      "        [9.9955e-01, 4.6626e-06, 4.6610e-06, 4.6613e-06, 4.6619e-06, 4.6728e-06],\n",
      "        [9.9884e-01, 1.5178e-05, 1.5177e-05, 1.5182e-05, 1.5180e-05, 1.5182e-05],\n",
      "        [9.9639e-01, 1.8506e-04, 1.8490e-04, 1.8512e-04, 1.8496e-04, 1.8551e-04],\n",
      "        [9.9003e-01, 2.2461e-04, 2.2406e-04, 2.2434e-04, 2.2423e-04, 2.2463e-04],\n",
      "        [9.8971e-01, 2.1982e-04, 2.1956e-04, 2.2002e-04, 2.1960e-04, 2.2158e-04],\n",
      "        [9.9997e-01, 1.8600e-08, 1.8600e-08, 1.8600e-08, 1.8600e-08, 1.8602e-08],\n",
      "        [9.8519e-01, 3.6649e-04, 3.6529e-04, 3.6581e-04, 3.6571e-04, 3.6895e-04],\n",
      "        [9.8844e-01, 3.7439e-04, 3.7328e-04, 3.7477e-04, 3.7345e-04, 3.7572e-04],\n",
      "        [9.9273e-01, 2.2396e-04, 2.2370e-04, 2.2423e-04, 2.2375e-04, 2.2516e-04],\n",
      "        [9.9126e-01, 3.1217e-04, 3.1208e-04, 3.1273e-04, 3.1365e-04, 3.1718e-04],\n",
      "        [9.9368e-01, 8.2537e-05, 8.2515e-05, 8.2599e-05, 8.2650e-05, 8.2763e-05],\n",
      "        [9.9920e-01, 1.3814e-05, 1.3804e-05, 1.3806e-05, 1.3814e-05, 1.3814e-05],\n",
      "        [9.8802e-01, 1.4891e-04, 1.4883e-04, 1.4907e-04, 1.4914e-04, 1.4947e-04],\n",
      "        [9.9849e-01, 5.6826e-05, 5.6789e-05, 5.6811e-05, 5.6825e-05, 5.6956e-05],\n",
      "        [9.9287e-01, 2.5138e-04, 2.5030e-04, 2.5045e-04, 2.5053e-04, 2.5560e-04],\n",
      "        [9.9957e-01, 1.2780e-05, 1.2775e-05, 1.2776e-05, 1.2776e-05, 1.2785e-05],\n",
      "        [9.8842e-01, 3.0727e-04, 3.0690e-04, 3.0916e-04, 3.0993e-04, 3.1084e-04],\n",
      "        [9.9928e-01, 7.4201e-06, 7.4185e-06, 7.4197e-06, 7.4184e-06, 7.4209e-06],\n",
      "        [9.9096e-01, 2.6375e-04, 2.6395e-04, 2.6398e-04, 2.6414e-04, 2.6937e-04],\n",
      "        [9.9817e-01, 2.1803e-05, 2.1794e-05, 2.1803e-05, 2.1800e-05, 2.1816e-05],\n",
      "        [9.9496e-01, 9.5965e-05, 9.5914e-05, 9.5997e-05, 9.5912e-05, 9.6319e-05],\n",
      "        [9.9040e-01, 4.1373e-04, 4.1310e-04, 4.1377e-04, 4.1370e-04, 4.1713e-04],\n",
      "        [9.9036e-01, 2.4197e-04, 2.4112e-04, 2.4138e-04, 2.4199e-04, 2.4807e-04],\n",
      "        [9.9900e-01, 1.5068e-05, 1.5067e-05, 1.5074e-05, 1.5068e-05, 1.5075e-05],\n",
      "        [9.8528e-01, 3.4835e-04, 3.4733e-04, 3.4742e-04, 3.4798e-04, 3.5169e-04],\n",
      "        [9.9096e-01, 4.2561e-04, 4.2550e-04, 4.2575e-04, 4.2574e-04, 4.3791e-04],\n",
      "        [9.9397e-01, 2.0962e-04, 2.0925e-04, 2.0974e-04, 2.0944e-04, 2.0989e-04],\n",
      "        [9.9124e-01, 1.0476e-04, 1.0462e-04, 1.0488e-04, 1.0469e-04, 1.0567e-04],\n",
      "        [9.9464e-01, 1.3752e-04, 1.3737e-04, 1.3754e-04, 1.3741e-04, 1.3855e-04],\n",
      "        [9.9600e-01, 1.7988e-04, 1.7981e-04, 1.7991e-04, 1.7998e-04, 1.8115e-04],\n",
      "        [9.8434e-01, 5.4111e-04, 5.4097e-04, 5.4204e-04, 5.4172e-04, 5.5071e-04],\n",
      "        [9.9490e-01, 2.1007e-05, 2.0972e-05, 2.0978e-05, 2.0978e-05, 2.1247e-05],\n",
      "        [9.9326e-01, 1.7287e-04, 1.7253e-04, 1.7272e-04, 1.7271e-04, 1.7446e-04],\n",
      "        [9.9162e-01, 1.9690e-04, 1.9621e-04, 1.9681e-04, 1.9698e-04, 1.9920e-04],\n",
      "        [9.9431e-01, 2.0458e-04, 2.0420e-04, 2.0468e-04, 2.0437e-04, 2.0608e-04],\n",
      "        [9.9954e-01, 7.4271e-06, 7.4252e-06, 7.4264e-06, 7.4262e-06, 7.4394e-06],\n",
      "        [9.9166e-01, 3.0853e-04, 3.0769e-04, 3.0802e-04, 3.0864e-04, 3.0976e-04],\n",
      "        [9.9375e-01, 2.9534e-04, 2.9521e-04, 2.9710e-04, 2.9611e-04, 2.9915e-04],\n",
      "        [9.9878e-01, 2.2424e-05, 2.2422e-05, 2.2437e-05, 2.2430e-05, 2.2593e-05],\n",
      "        [9.9512e-01, 2.6333e-04, 2.6299e-04, 2.6330e-04, 2.6332e-04, 2.6689e-04],\n",
      "        [9.9199e-01, 2.6933e-04, 2.6914e-04, 2.6991e-04, 2.6928e-04, 2.7142e-04],\n",
      "        [9.9558e-01, 1.5948e-04, 1.5937e-04, 1.6007e-04, 1.5984e-04, 1.6078e-04]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>), tensor([[0.0633, 0.0173, 0.0017, 0.0012, 0.0081],\n",
      "        [0.1229, 0.0580, 0.0236, 0.0138, 0.0265],\n",
      "        [0.1121, 0.1366, 0.0217, 0.0255, 0.0379],\n",
      "        [0.1414, 0.1568, 0.0752, 0.0356, 0.0536],\n",
      "        [0.1056, 0.1652, 0.0277, 0.0245, 0.0547],\n",
      "        [0.0803, 0.1069, 0.0162, 0.0206, 0.0741],\n",
      "        [0.1387, 0.1243, 0.0372, 0.0164, 0.0408],\n",
      "        [0.1061, 0.1214, 0.0525, 0.0213, 0.0498],\n",
      "        [0.0967, 0.1088, 0.0235, 0.0294, 0.0554],\n",
      "        [0.0844, 0.0979, 0.0447, 0.0341, 0.0428],\n",
      "        [0.0919, 0.1285, 0.0298, 0.0284, 0.0554],\n",
      "        [0.0983, 0.1126, 0.0278, 0.0232, 0.0392],\n",
      "        [0.0478, 0.0330, 0.0013, 0.0011, 0.0041],\n",
      "        [0.1292, 0.0547, 0.0082, 0.0090, 0.0281],\n",
      "        [0.0782, 0.0750, 0.0087, 0.0069, 0.0170],\n",
      "        [0.0578, 0.0938, 0.0138, 0.0047, 0.0284],\n",
      "        [0.1131, 0.0648, 0.0233, 0.0169, 0.0560],\n",
      "        [0.1211, 0.1547, 0.0356, 0.0283, 0.0578],\n",
      "        [0.0815, 0.1063, 0.0464, 0.0350, 0.0465],\n",
      "        [0.0136, 0.0167, 0.0012, 0.0012, 0.0021],\n",
      "        [0.1350, 0.1239, 0.0293, 0.0334, 0.0685],\n",
      "        [0.1204, 0.1260, 0.0312, 0.0341, 0.0429],\n",
      "        [0.1131, 0.1397, 0.0316, 0.0289, 0.0345],\n",
      "        [0.0995, 0.1605, 0.0473, 0.0437, 0.0515],\n",
      "        [0.1050, 0.1252, 0.0360, 0.0217, 0.0519],\n",
      "        [0.1157, 0.0655, 0.0148, 0.0102, 0.0151],\n",
      "        [0.1266, 0.1172, 0.0299, 0.0240, 0.0595],\n",
      "        [0.1224, 0.0970, 0.0276, 0.0117, 0.0197],\n",
      "        [0.1534, 0.1099, 0.0495, 0.0212, 0.0506],\n",
      "        [0.0544, 0.0299, 0.0040, 0.0053, 0.0146],\n",
      "        [0.1291, 0.1967, 0.0492, 0.0334, 0.0615],\n",
      "        [0.0561, 0.0811, 0.0100, 0.0052, 0.0093],\n",
      "        [0.1205, 0.1821, 0.0335, 0.0310, 0.0524],\n",
      "        [0.0602, 0.1341, 0.0127, 0.0088, 0.0170],\n",
      "        [0.1439, 0.1101, 0.0220, 0.0212, 0.0643],\n",
      "        [0.1415, 0.1330, 0.0553, 0.0239, 0.0628],\n",
      "        [0.1080, 0.1219, 0.0408, 0.0589, 0.0763],\n",
      "        [0.0734, 0.0791, 0.0139, 0.0061, 0.0119],\n",
      "        [0.1415, 0.1848, 0.0445, 0.0290, 0.0755],\n",
      "        [0.1226, 0.1406, 0.0264, 0.0322, 0.0619],\n",
      "        [0.1021, 0.1684, 0.0613, 0.0288, 0.0684],\n",
      "        [0.1007, 0.1294, 0.0329, 0.0376, 0.0541],\n",
      "        [0.1706, 0.0911, 0.0310, 0.0123, 0.0308],\n",
      "        [0.1341, 0.1609, 0.0318, 0.0172, 0.0227],\n",
      "        [0.1416, 0.1448, 0.0386, 0.0578, 0.0440],\n",
      "        [0.1305, 0.1122, 0.0213, 0.0199, 0.0415],\n",
      "        [0.1252, 0.1541, 0.0411, 0.0240, 0.0473],\n",
      "        [0.1533, 0.0927, 0.0286, 0.0202, 0.0461],\n",
      "        [0.1293, 0.0985, 0.0347, 0.0161, 0.0401],\n",
      "        [0.0717, 0.0546, 0.0058, 0.0053, 0.0228],\n",
      "        [0.1328, 0.1048, 0.0271, 0.0283, 0.0759],\n",
      "        [0.1351, 0.1004, 0.0436, 0.0473, 0.0549],\n",
      "        [0.0777, 0.0861, 0.0202, 0.0112, 0.0215],\n",
      "        [0.1106, 0.1236, 0.0335, 0.0173, 0.0513],\n",
      "        [0.1185, 0.1487, 0.0344, 0.0171, 0.0534],\n",
      "        [0.0999, 0.1809, 0.0341, 0.0347, 0.0617]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)], '95%': [tensor([[9.9998e-01, 8.5096e-03, 2.3107e-06],\n",
      "        [9.4713e-01, 4.3445e-01, 6.2839e-04],\n",
      "        [9.4176e-01, 5.4566e-01, 1.2234e-03],\n",
      "        [9.8985e-01, 9.5339e-02, 2.3291e-03],\n",
      "        [9.1294e-01, 5.8660e-01, 1.6614e-03],\n",
      "        [9.9920e-01, 3.4701e-02, 1.8889e-04],\n",
      "        [7.4636e-01, 9.1292e-01, 1.8108e-03],\n",
      "        [8.5134e-01, 5.9206e-01, 1.9382e-03],\n",
      "        [9.8751e-01, 1.3778e-01, 1.4734e-03],\n",
      "        [9.8074e-01, 2.4126e-01, 1.0906e-03],\n",
      "        [9.4235e-01, 3.0213e-01, 1.5914e-03],\n",
      "        [9.0913e-01, 4.3448e-01, 9.5287e-04],\n",
      "        [9.2346e-01, 9.9820e-01, 2.4342e-05],\n",
      "        [6.1100e-02, 9.9589e-01, 1.1161e-04],\n",
      "        [5.7529e-01, 9.2628e-01, 3.2764e-04],\n",
      "        [9.2188e-01, 7.7367e-01, 3.2563e-04],\n",
      "        [6.8061e-01, 8.5876e-01, 1.6691e-03],\n",
      "        [1.9193e-01, 9.9087e-01, 1.8633e-03],\n",
      "        [7.2347e-01, 6.9269e-01, 3.6887e-03],\n",
      "        [9.9184e-01, 9.2600e-01, 9.9349e-06],\n",
      "        [9.6784e-01, 1.5758e-01, 1.8844e-03],\n",
      "        [4.6996e-01, 9.3409e-01, 2.8388e-03],\n",
      "        [2.4803e-01, 9.8958e-01, 1.1596e-03],\n",
      "        [9.9679e-01, 7.2280e-02, 7.5398e-04],\n",
      "        [9.7115e-01, 6.6339e-01, 1.1641e-03],\n",
      "        [9.1169e-04, 9.9999e-01, 1.5441e-05],\n",
      "        [7.9379e-02, 9.9183e-01, 9.7675e-04],\n",
      "        [2.6525e-01, 9.9093e-01, 5.3452e-04],\n",
      "        [8.1419e-01, 7.5654e-01, 1.8307e-03],\n",
      "        [1.0067e-01, 9.9781e-01, 6.6899e-05],\n",
      "        [9.7674e-01, 1.8626e-01, 1.1853e-03],\n",
      "        [1.5525e-01, 9.9759e-01, 1.6037e-04],\n",
      "        [9.9618e-01, 4.6636e-02, 1.1510e-03],\n",
      "        [5.9501e-02, 9.9917e-01, 9.8650e-05],\n",
      "        [8.1298e-01, 8.2238e-01, 1.3554e-03],\n",
      "        [9.8649e-01, 8.2652e-02, 1.9456e-03],\n",
      "        [9.9074e-01, 7.1796e-02, 1.2277e-03],\n",
      "        [1.7282e-01, 9.8311e-01, 2.1016e-04],\n",
      "        [9.9021e-01, 9.0591e-02, 1.8786e-03],\n",
      "        [9.7899e-01, 1.4603e-01, 1.6076e-03],\n",
      "        [9.6929e-01, 2.8703e-01, 1.4788e-03],\n",
      "        [9.8525e-01, 2.7729e-01, 8.7400e-04],\n",
      "        [1.0336e-01, 9.9040e-01, 5.1288e-04],\n",
      "        [4.8234e-01, 9.7268e-01, 1.1203e-03],\n",
      "        [9.9130e-01, 5.7571e-02, 1.2522e-03],\n",
      "        [9.8310e-01, 2.3920e-01, 1.1373e-03],\n",
      "        [3.8485e-01, 9.7580e-01, 1.6884e-03],\n",
      "        [9.8265e-01, 1.2361e-01, 1.2835e-03],\n",
      "        [2.4919e-01, 9.8024e-01, 9.0711e-04],\n",
      "        [9.5327e-01, 4.4180e-01, 3.2777e-04],\n",
      "        [8.7897e-01, 5.1571e-01, 2.0622e-03],\n",
      "        [9.9311e-01, 5.6975e-02, 8.4019e-04],\n",
      "        [9.7230e-01, 6.5120e-01, 5.5273e-04],\n",
      "        [4.7135e-01, 9.6369e-01, 1.1744e-03],\n",
      "        [8.5121e-01, 7.4393e-01, 1.8006e-03],\n",
      "        [9.9294e-01, 8.2890e-02, 8.6185e-04]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>), tensor([[1.0473e-08, 1.0000e+00, 1.0470e-08],\n",
      "        [7.1904e-06, 1.0000e+00, 7.1870e-06],\n",
      "        [1.0614e-05, 1.0000e+00, 1.0640e-05],\n",
      "        [5.2074e-05, 9.9999e-01, 5.1775e-05],\n",
      "        [4.8446e-05, 1.0000e+00, 4.8279e-05],\n",
      "        [6.4179e-06, 1.0000e+00, 6.3891e-06],\n",
      "        [3.4827e-05, 9.9999e-01, 3.4535e-05],\n",
      "        [3.0515e-05, 9.9999e-01, 3.0499e-05],\n",
      "        [4.8481e-05, 9.9999e-01, 4.8201e-05],\n",
      "        [6.3057e-06, 1.0000e+00, 6.3053e-06],\n",
      "        [3.5904e-05, 9.9999e-01, 3.5826e-05],\n",
      "        [1.2902e-05, 1.0000e+00, 1.2896e-05],\n",
      "        [9.1968e-08, 1.0000e+00, 9.1998e-08],\n",
      "        [1.5669e-06, 1.0000e+00, 1.5676e-06],\n",
      "        [5.0879e-07, 1.0000e+00, 5.0948e-07],\n",
      "        [1.5983e-06, 1.0000e+00, 1.5982e-06],\n",
      "        [1.5678e-05, 1.0000e+00, 1.5659e-05],\n",
      "        [6.4885e-05, 9.9999e-01, 6.4319e-05],\n",
      "        [5.6295e-05, 9.9999e-01, 5.5927e-05],\n",
      "        [1.9379e-08, 1.0000e+00, 1.9382e-08],\n",
      "        [6.2582e-05, 9.9999e-01, 6.2481e-05],\n",
      "        [5.4700e-05, 9.9999e-01, 5.4520e-05],\n",
      "        [6.4998e-05, 9.9999e-01, 6.4532e-05],\n",
      "        [3.3944e-05, 9.9999e-01, 3.3946e-05],\n",
      "        [3.8562e-05, 1.0000e+00, 3.8505e-05],\n",
      "        [4.3702e-06, 1.0000e+00, 4.3695e-06],\n",
      "        [5.7907e-05, 9.9999e-01, 5.7959e-05],\n",
      "        [8.7293e-06, 1.0000e+00, 8.7575e-06],\n",
      "        [2.7189e-05, 9.9999e-01, 2.7155e-05],\n",
      "        [6.9957e-07, 1.0000e+00, 6.9926e-07],\n",
      "        [8.4207e-05, 9.9997e-01, 8.3705e-05],\n",
      "        [5.7871e-07, 1.0000e+00, 5.7883e-07],\n",
      "        [8.0361e-05, 9.9999e-01, 7.9969e-05],\n",
      "        [2.3390e-06, 1.0000e+00, 2.3398e-06],\n",
      "        [2.8579e-05, 1.0000e+00, 2.8642e-05],\n",
      "        [4.2388e-05, 9.9998e-01, 4.2135e-05],\n",
      "        [4.0888e-05, 9.9999e-01, 4.0818e-05],\n",
      "        [1.8178e-06, 1.0000e+00, 1.8126e-06],\n",
      "        [8.6387e-05, 9.9999e-01, 8.5928e-05],\n",
      "        [7.1564e-05, 9.9999e-01, 7.1637e-05],\n",
      "        [2.2913e-05, 9.9999e-01, 2.2840e-05],\n",
      "        [1.9761e-05, 1.0000e+00, 1.9718e-05],\n",
      "        [3.2986e-05, 9.9999e-01, 3.3015e-05],\n",
      "        [2.0965e-05, 1.0000e+00, 2.0929e-05],\n",
      "        [6.4496e-05, 9.9998e-01, 6.4023e-05],\n",
      "        [3.3667e-05, 1.0000e+00, 3.3626e-05],\n",
      "        [5.3176e-05, 9.9999e-01, 5.2997e-05],\n",
      "        [5.2230e-05, 9.9999e-01, 5.1821e-05],\n",
      "        [2.5691e-05, 9.9999e-01, 2.5682e-05],\n",
      "        [8.4304e-07, 1.0000e+00, 8.4263e-07],\n",
      "        [5.8428e-05, 9.9999e-01, 5.8178e-05],\n",
      "        [4.7945e-05, 9.9999e-01, 4.7839e-05],\n",
      "        [1.3410e-06, 1.0000e+00, 1.3411e-06],\n",
      "        [3.3436e-05, 1.0000e+00, 3.3512e-05],\n",
      "        [6.8270e-05, 9.9999e-01, 6.8318e-05],\n",
      "        [2.2564e-05, 9.9999e-01, 2.2515e-05]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>), tensor([[1.0000e+00, 7.2928e-06, 7.2920e-06, 7.2922e-06, 7.2930e-06, 7.2969e-06],\n",
      "        [9.9974e-01, 1.0148e-03, 1.0149e-03, 1.0206e-03, 1.0229e-03, 1.0334e-03],\n",
      "        [9.9956e-01, 6.6637e-04, 6.6667e-04, 6.6939e-04, 6.7339e-04, 6.8318e-04],\n",
      "        [9.9827e-01, 2.2367e-03, 2.2283e-03, 2.2395e-03, 2.2368e-03, 2.2835e-03],\n",
      "        [9.9909e-01, 1.0968e-03, 1.0959e-03, 1.0961e-03, 1.0978e-03, 1.1112e-03],\n",
      "        [9.9940e-01, 4.4497e-04, 4.4391e-04, 4.4420e-04, 4.4441e-04, 4.4886e-04],\n",
      "        [9.9829e-01, 2.3231e-03, 2.3100e-03, 2.3260e-03, 2.3108e-03, 2.3325e-03],\n",
      "        [9.9932e-01, 1.1949e-03, 1.1938e-03, 1.1959e-03, 1.1940e-03, 1.2050e-03],\n",
      "        [9.9864e-01, 1.5443e-03, 1.5432e-03, 1.5502e-03, 1.5475e-03, 1.5953e-03],\n",
      "        [9.9921e-01, 7.6346e-04, 7.6208e-04, 7.6285e-04, 7.6448e-04, 7.9004e-04],\n",
      "        [9.9879e-01, 1.2354e-03, 1.2350e-03, 1.2457e-03, 1.2391e-03, 1.2509e-03],\n",
      "        [9.9916e-01, 7.9260e-04, 7.9111e-04, 7.9153e-04, 7.9096e-04, 7.9699e-04],\n",
      "        [1.0000e+00, 5.0465e-05, 5.0463e-05, 5.0484e-05, 5.0476e-05, 5.0538e-05],\n",
      "        [9.9994e-01, 1.5332e-04, 1.5320e-04, 1.5373e-04, 1.5331e-04, 1.5394e-04],\n",
      "        [9.9998e-01, 8.9856e-05, 8.9779e-05, 8.9822e-05, 8.9792e-05, 9.0448e-05],\n",
      "        [9.9992e-01, 2.3156e-04, 2.3136e-04, 2.3149e-04, 2.3171e-04, 2.3453e-04],\n",
      "        [9.9907e-01, 7.1698e-04, 7.1658e-04, 7.1825e-04, 7.1655e-04, 7.3759e-04],\n",
      "        [9.9888e-01, 1.9883e-03, 1.9772e-03, 2.0002e-03, 1.9861e-03, 2.0188e-03],\n",
      "        [9.9890e-01, 2.0398e-03, 2.0348e-03, 2.0769e-03, 2.0606e-03, 2.0785e-03],\n",
      "        [1.0000e+00, 5.4194e-06, 5.4191e-06, 5.4196e-06, 5.4203e-06, 5.4273e-06],\n",
      "        [9.9817e-01, 2.9521e-03, 2.9435e-03, 2.9542e-03, 2.9574e-03, 3.0022e-03],\n",
      "        [9.9813e-01, 2.3145e-03, 2.3000e-03, 2.3176e-03, 2.3059e-03, 2.3238e-03],\n",
      "        [9.9888e-01, 1.4567e-03, 1.4460e-03, 1.4505e-03, 1.4503e-03, 1.4691e-03],\n",
      "        [9.9843e-01, 1.7454e-03, 1.7433e-03, 1.7470e-03, 1.7502e-03, 1.7576e-03],\n",
      "        [9.9959e-01, 1.2565e-03, 1.2567e-03, 1.2562e-03, 1.2582e-03, 1.2948e-03],\n",
      "        [9.9993e-01, 1.6105e-04, 1.6079e-04, 1.6089e-04, 1.6083e-04, 1.6099e-04],\n",
      "        [9.9925e-01, 2.3889e-03, 2.3864e-03, 2.3983e-03, 2.3948e-03, 2.4097e-03],\n",
      "        [9.9972e-01, 3.0057e-04, 3.0028e-04, 3.0116e-04, 3.0068e-04, 3.0642e-04],\n",
      "        [9.9874e-01, 1.4173e-03, 1.4120e-03, 1.4159e-03, 1.4166e-03, 1.4672e-03],\n",
      "        [9.9994e-01, 8.5885e-05, 8.5880e-05, 8.5962e-05, 8.5870e-05, 8.5973e-05],\n",
      "        [9.9846e-01, 2.3171e-03, 2.2938e-03, 2.3342e-03, 2.3018e-03, 2.3340e-03],\n",
      "        [9.9996e-01, 1.4379e-04, 1.4362e-04, 1.4368e-04, 1.4373e-04, 1.4380e-04],\n",
      "        [9.9867e-01, 1.7955e-03, 1.7934e-03, 1.8089e-03, 1.8034e-03, 1.8380e-03],\n",
      "        [9.9989e-01, 3.6633e-04, 3.6590e-04, 3.6631e-04, 3.6593e-04, 3.6702e-04],\n",
      "        [9.9952e-01, 1.0142e-03, 1.0059e-03, 1.0061e-03, 1.0057e-03, 1.0116e-03],\n",
      "        [9.9793e-01, 1.9188e-03, 1.9095e-03, 1.9135e-03, 1.9130e-03, 1.9473e-03],\n",
      "        [9.9879e-01, 1.9207e-03, 1.9166e-03, 1.9242e-03, 1.9262e-03, 1.9548e-03],\n",
      "        [9.9992e-01, 1.9918e-04, 1.9899e-04, 1.9916e-04, 1.9901e-04, 2.0087e-04],\n",
      "        [9.9826e-01, 2.9261e-03, 2.9210e-03, 2.9452e-03, 2.9418e-03, 2.9885e-03],\n",
      "        [9.9786e-01, 1.8045e-03, 1.8033e-03, 1.8060e-03, 1.8076e-03, 1.8204e-03],\n",
      "        [9.9895e-01, 1.2060e-03, 1.1933e-03, 1.1986e-03, 1.2031e-03, 1.2332e-03],\n",
      "        [9.9948e-01, 1.7505e-03, 1.7495e-03, 1.7520e-03, 1.7478e-03, 1.7569e-03],\n",
      "        [9.9931e-01, 1.0727e-03, 1.0716e-03, 1.0719e-03, 1.0714e-03, 1.0737e-03],\n",
      "        [9.9910e-01, 8.0066e-04, 7.9853e-04, 7.9989e-04, 7.9975e-04, 8.0497e-04],\n",
      "        [9.9728e-01, 3.1074e-03, 3.0944e-03, 3.0931e-03, 3.0920e-03, 3.2708e-03],\n",
      "        [9.9989e-01, 1.0160e-03, 1.0144e-03, 1.0185e-03, 1.0240e-03, 1.0248e-03],\n",
      "        [9.9913e-01, 1.3541e-03, 1.3418e-03, 1.3491e-03, 1.3439e-03, 1.3540e-03],\n",
      "        [9.9901e-01, 1.6615e-03, 1.6527e-03, 1.6757e-03, 1.6598e-03, 1.7316e-03],\n",
      "        [9.9898e-01, 1.1265e-03, 1.1251e-03, 1.1649e-03, 1.1299e-03, 1.1394e-03],\n",
      "        [9.9996e-01, 9.1132e-05, 9.1123e-05, 9.1155e-05, 9.1137e-05, 9.1293e-05],\n",
      "        [9.9846e-01, 1.6596e-03, 1.6558e-03, 1.6608e-03, 1.6629e-03, 1.6989e-03],\n",
      "        [9.9852e-01, 1.2442e-03, 1.2434e-03, 1.2517e-03, 1.2479e-03, 1.2570e-03],\n",
      "        [9.9989e-01, 2.4408e-04, 2.4266e-04, 2.4283e-04, 2.4274e-04, 2.4557e-04],\n",
      "        [9.9868e-01, 9.7685e-04, 9.7497e-04, 9.7694e-04, 9.7489e-04, 9.8090e-04],\n",
      "        [9.9865e-01, 1.6021e-03, 1.5963e-03, 1.5962e-03, 1.5971e-03, 1.6139e-03],\n",
      "        [9.9920e-01, 8.8189e-04, 8.7715e-04, 8.8289e-04, 8.7903e-04, 9.0163e-04]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>), tensor([[0.3026, 0.2694, 0.0230, 0.0399, 0.1007],\n",
      "        [0.3544, 0.4502, 0.0793, 0.0710, 0.2650],\n",
      "        [0.2690, 0.5029, 0.1008, 0.0790, 0.2251],\n",
      "        [0.3168, 0.3526, 0.2167, 0.1380, 0.2689],\n",
      "        [0.3271, 0.4196, 0.1327, 0.0933, 0.2191],\n",
      "        [0.3307, 0.4463, 0.1650, 0.1409, 0.2797],\n",
      "        [0.4181, 0.3783, 0.1620, 0.0836, 0.2306],\n",
      "        [0.3492, 0.3378, 0.1599, 0.0978, 0.2025],\n",
      "        [0.3572, 0.4938, 0.1184, 0.1259, 0.2580],\n",
      "        [0.2951, 0.3298, 0.1203, 0.1253, 0.1713],\n",
      "        [0.3354, 0.2868, 0.1200, 0.1315, 0.1987],\n",
      "        [0.2913, 0.3299, 0.0921, 0.0746, 0.1852],\n",
      "        [0.1786, 0.2888, 0.0381, 0.0333, 0.0885],\n",
      "        [0.4505, 0.3656, 0.1192, 0.0681, 0.1872],\n",
      "        [0.3378, 0.3235, 0.0819, 0.0424, 0.1317],\n",
      "        [0.2717, 0.3081, 0.1116, 0.0626, 0.1276],\n",
      "        [0.3381, 0.3868, 0.1863, 0.1073, 0.1265],\n",
      "        [0.3216, 0.3614, 0.1564, 0.0839, 0.2049],\n",
      "        [0.2732, 0.3312, 0.2152, 0.1262, 0.1808],\n",
      "        [0.5428, 0.1792, 0.0162, 0.0160, 0.1833],\n",
      "        [0.3606, 0.3577, 0.1324, 0.1033, 0.2821],\n",
      "        [0.3855, 0.4045, 0.1110, 0.1142, 0.2077],\n",
      "        [0.3118, 0.3522, 0.1271, 0.0937, 0.1698],\n",
      "        [0.3463, 0.4231, 0.2365, 0.1359, 0.3381],\n",
      "        [0.3786, 0.4803, 0.1638, 0.1497, 0.1527],\n",
      "        [0.2992, 0.3585, 0.0633, 0.0788, 0.0887],\n",
      "        [0.3367, 0.3925, 0.1250, 0.1163, 0.1366],\n",
      "        [0.3065, 0.2890, 0.0894, 0.0790, 0.1871],\n",
      "        [0.3529, 0.2606, 0.1827, 0.1684, 0.1926],\n",
      "        [0.3436, 0.2923, 0.0944, 0.0351, 0.1175],\n",
      "        [0.3985, 0.3401, 0.1911, 0.1152, 0.2940],\n",
      "        [0.2435, 0.3234, 0.0784, 0.0394, 0.0971],\n",
      "        [0.3300, 0.3917, 0.2235, 0.1746, 0.2930],\n",
      "        [0.2814, 0.2992, 0.0638, 0.0496, 0.1167],\n",
      "        [0.3535, 0.4253, 0.1666, 0.0677, 0.2373],\n",
      "        [0.3272, 0.3817, 0.1761, 0.1577, 0.2682],\n",
      "        [0.4195, 0.3925, 0.1189, 0.1372, 0.2306],\n",
      "        [0.2820, 0.3469, 0.1348, 0.0621, 0.1757],\n",
      "        [0.4027, 0.4820, 0.1868, 0.1573, 0.2062],\n",
      "        [0.3154, 0.4955, 0.1456, 0.0833, 0.2473],\n",
      "        [0.3544, 0.4833, 0.2328, 0.0994, 0.1837],\n",
      "        [0.3806, 0.3723, 0.1217, 0.1619, 0.1717],\n",
      "        [0.3408, 0.3215, 0.1002, 0.1047, 0.1629],\n",
      "        [0.3618, 0.3956, 0.1088, 0.0846, 0.1407],\n",
      "        [0.3230, 0.4218, 0.1431, 0.1444, 0.2159],\n",
      "        [0.3121, 0.3193, 0.1100, 0.0899, 0.2233],\n",
      "        [0.3652, 0.3513, 0.1268, 0.1167, 0.1757],\n",
      "        [0.4603, 0.3323, 0.1798, 0.1732, 0.2273],\n",
      "        [0.3372, 0.4173, 0.1178, 0.0789, 0.2172],\n",
      "        [0.3689, 0.3175, 0.0738, 0.0796, 0.1427],\n",
      "        [0.3011, 0.3006, 0.1398, 0.1258, 0.1890],\n",
      "        [0.2378, 0.3063, 0.1729, 0.1570, 0.2049],\n",
      "        [0.3917, 0.3475, 0.1348, 0.0615, 0.1533],\n",
      "        [0.2984, 0.3482, 0.1125, 0.0886, 0.2410],\n",
      "        [0.4723, 0.3805, 0.1593, 0.1311, 0.2468],\n",
      "        [0.3101, 0.3808, 0.1697, 0.1800, 0.1988]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)], 'order': ['pd', 'nd', 'mod', 'dlt']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pd': {'accuracy': 0.43047619047619046,\n",
       "  'auc_micro': 0.8168946098149638,\n",
       "  'auc_mean': 0.6421992601454057,\n",
       "  'auc_weighted': 0.7149197602825323},\n",
       " 'nd': {'accuracy': 0.3333333333333333,\n",
       "  'auc_micro': 0.8558326629123089,\n",
       "  'auc_mean': 0.49633570373546787,\n",
       "  'auc_weighted': 0.5340551743853631},\n",
       " 'mod': {'accuracy': 0.3333333333333333,\n",
       "  'auc_micro': 0.8558326629123089,\n",
       "  'auc_mean': 0.49633570373546787,\n",
       "  'auc_weighted': 0.5340551743853631},\n",
       " 'dlts': {'accuracy': [0.8035714285714286,\n",
       "   0.8928571428571429,\n",
       "   0.7857142857142857,\n",
       "   0.9464285714285714,\n",
       "   0.9107142857142857],\n",
       "  'accuracy_mean': 0.8678571428571429,\n",
       "  'auc': [0.4848484848484849,\n",
       "   0.59,\n",
       "   0.5018939393939394,\n",
       "   0.3773584905660377,\n",
       "   0.6392156862745098],\n",
       "  'auc_mean': 0.5186633202165943}}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmodel1 = train_state(model_args=t1_args,lr=.001,weights=[1,1,.1,.1],use_smote=False,balanced=False)\n",
    "tmodel1[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "ac5af9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train loss 2.3804824352264404\n",
      "val loss 2.3856987953186035\n",
      "______________\n",
      "epoch 1 train loss 2.372931718826294\n",
      "val loss 2.3872435092926025\n",
      "______________\n",
      "epoch 2 train loss 2.356402635574341\n",
      "val loss 2.38885498046875\n",
      "______________\n",
      "epoch 3 train loss 2.3131725788116455\n",
      "val loss 2.3912272453308105\n",
      "______________\n",
      "epoch 4 train loss 2.2799901962280273\n",
      "val loss 2.3932442665100098\n",
      "______________\n",
      "epoch 5 train loss 2.271812915802002\n",
      "val loss 2.3956217765808105\n",
      "______________\n",
      "epoch 6 train loss 2.232069730758667\n",
      "val loss 2.397284984588623\n",
      "______________\n",
      "epoch 7 train loss 2.1886913776397705\n",
      "val loss 2.3999149799346924\n",
      "______________\n",
      "epoch 8 train loss 2.1256186962127686\n",
      "val loss 2.4039223194122314\n",
      "______________\n",
      "epoch 9 train loss 2.073190689086914\n",
      "val loss 2.4085183143615723\n",
      "______________\n",
      "epoch 10 train loss 2.0284528732299805\n",
      "val loss 2.4125239849090576\n",
      "______________\n",
      "epoch 11 train loss 2.007815361022949\n",
      "val loss 2.4177825450897217\n",
      "______________\n",
      "best loss 2.3856987953186035 {'pd': {'accuracy': 0.4087427830800047, 'auc_micro': 0.8607603815937149, 'auc_mean': 0.7213919484860222, 'auc_weighted': 0.6491355274236282}, 'nd': {'accuracy': 0.312149734934545, 'auc_micro': 0.4632387706855793, 'auc_mean': 0.45452151115365275, 'auc_weighted': 0.43489857267750365}, 'mod': {'accuracy': 0.312149734934545, 'auc_micro': 0.4632387706855793, 'auc_mean': 0.45452151115365275, 'auc_weighted': 0.43489857267750365}, 'dlts': {'accuracy': [0.6190476190476191, 0.9455782312925171, 0.9387755102040817, 0.9659863945578231, 0.9523809523809523], 'accuracy_mean': 0.8843537414965986, 'auc': [0.5395061728395062, 0.5633802816901409, 0.6836734693877551, 0.5763888888888888, 0.46478873239436624], 'auc_mean': 0.5655475090401314}}\n",
      "{'predictions': [tensor([[0.4338, 0.2961, 0.2700],\n",
      "        [0.6362, 0.2176, 0.1462],\n",
      "        [0.4463, 0.3316, 0.2221],\n",
      "        [0.3670, 0.3506, 0.2824],\n",
      "        [0.3751, 0.3479, 0.2770],\n",
      "        [0.4113, 0.3058, 0.2829],\n",
      "        [0.4481, 0.3182, 0.2337],\n",
      "        [0.4139, 0.2943, 0.2917],\n",
      "        [0.6162, 0.2111, 0.1727],\n",
      "        [0.4031, 0.3156, 0.2813],\n",
      "        [0.4676, 0.2915, 0.2409],\n",
      "        [0.4374, 0.3078, 0.2548],\n",
      "        [0.4048, 0.3195, 0.2757],\n",
      "        [0.3953, 0.2993, 0.3055],\n",
      "        [0.3822, 0.3474, 0.2704],\n",
      "        [0.3974, 0.3270, 0.2756],\n",
      "        [0.5241, 0.3083, 0.1676],\n",
      "        [0.4211, 0.3203, 0.2586],\n",
      "        [0.3967, 0.3142, 0.2891],\n",
      "        [0.4894, 0.3085, 0.2020],\n",
      "        [0.4453, 0.3460, 0.2087],\n",
      "        [0.3995, 0.3172, 0.2833],\n",
      "        [0.3839, 0.3408, 0.2754],\n",
      "        [0.4154, 0.3502, 0.2343],\n",
      "        [0.6524, 0.1999, 0.1477],\n",
      "        [0.4431, 0.3077, 0.2492],\n",
      "        [0.6013, 0.2238, 0.1749],\n",
      "        [0.4876, 0.2812, 0.2312],\n",
      "        [0.3965, 0.3100, 0.2935],\n",
      "        [0.4028, 0.3070, 0.2902],\n",
      "        [0.4084, 0.3046, 0.2870],\n",
      "        [0.4233, 0.3364, 0.2404],\n",
      "        [0.3956, 0.3128, 0.2916],\n",
      "        [0.3699, 0.3516, 0.2786],\n",
      "        [0.4454, 0.2751, 0.2795],\n",
      "        [0.4327, 0.3074, 0.2599],\n",
      "        [0.4721, 0.3216, 0.2063],\n",
      "        [0.3669, 0.3207, 0.3124],\n",
      "        [0.4646, 0.3441, 0.1913],\n",
      "        [0.4005, 0.3079, 0.2916],\n",
      "        [0.3751, 0.3395, 0.2854],\n",
      "        [0.5060, 0.3004, 0.1936],\n",
      "        [0.3577, 0.3649, 0.2774],\n",
      "        [0.6230, 0.2197, 0.1573],\n",
      "        [0.4181, 0.3070, 0.2749],\n",
      "        [0.4741, 0.2774, 0.2486],\n",
      "        [0.3900, 0.3387, 0.2713],\n",
      "        [0.4668, 0.2889, 0.2443],\n",
      "        [0.3787, 0.3289, 0.2924],\n",
      "        [0.3959, 0.3268, 0.2773],\n",
      "        [0.5197, 0.3003, 0.1800],\n",
      "        [0.3600, 0.3457, 0.2943],\n",
      "        [0.5086, 0.2691, 0.2223],\n",
      "        [0.3813, 0.3241, 0.2946],\n",
      "        [0.3960, 0.3095, 0.2944],\n",
      "        [0.4294, 0.3358, 0.2348],\n",
      "        [0.4065, 0.3294, 0.2641],\n",
      "        [0.5279, 0.2581, 0.2140],\n",
      "        [0.5805, 0.2722, 0.1472],\n",
      "        [0.4044, 0.3204, 0.2752],\n",
      "        [0.3922, 0.3062, 0.3015],\n",
      "        [0.4227, 0.3112, 0.2661],\n",
      "        [0.4035, 0.3401, 0.2564],\n",
      "        [0.4156, 0.3207, 0.2637],\n",
      "        [0.6278, 0.2132, 0.1591],\n",
      "        [0.3971, 0.3056, 0.2972],\n",
      "        [0.4100, 0.3083, 0.2817],\n",
      "        [0.4366, 0.3267, 0.2368],\n",
      "        [0.3771, 0.3157, 0.3072],\n",
      "        [0.4225, 0.3110, 0.2666],\n",
      "        [0.4614, 0.3302, 0.2083],\n",
      "        [0.3708, 0.3464, 0.2828],\n",
      "        [0.3977, 0.3114, 0.2909],\n",
      "        [0.4353, 0.2944, 0.2703],\n",
      "        [0.4256, 0.3469, 0.2275],\n",
      "        [0.4778, 0.3320, 0.1902],\n",
      "        [0.4202, 0.3645, 0.2154],\n",
      "        [0.3894, 0.3587, 0.2519],\n",
      "        [0.4774, 0.3111, 0.2115],\n",
      "        [0.4254, 0.3180, 0.2566],\n",
      "        [0.4196, 0.3055, 0.2749],\n",
      "        [0.4234, 0.3016, 0.2750],\n",
      "        [0.4057, 0.3458, 0.2484],\n",
      "        [0.4349, 0.3257, 0.2394],\n",
      "        [0.4484, 0.2973, 0.2544],\n",
      "        [0.4194, 0.2985, 0.2822],\n",
      "        [0.3992, 0.3417, 0.2591],\n",
      "        [0.4610, 0.3112, 0.2278],\n",
      "        [0.4341, 0.2928, 0.2732],\n",
      "        [0.4113, 0.2935, 0.2952],\n",
      "        [0.4286, 0.3068, 0.2646],\n",
      "        [0.4190, 0.2994, 0.2816],\n",
      "        [0.3919, 0.3574, 0.2507],\n",
      "        [0.4243, 0.3032, 0.2725],\n",
      "        [0.4431, 0.3437, 0.2132],\n",
      "        [0.5143, 0.2946, 0.1911],\n",
      "        [0.4117, 0.3174, 0.2709],\n",
      "        [0.4824, 0.3238, 0.1938],\n",
      "        [0.4560, 0.2882, 0.2558],\n",
      "        [0.4193, 0.3512, 0.2295],\n",
      "        [0.4553, 0.3199, 0.2248],\n",
      "        [0.4798, 0.3169, 0.2033],\n",
      "        [0.4954, 0.2851, 0.2195],\n",
      "        [0.5122, 0.3138, 0.1741],\n",
      "        [0.3825, 0.3042, 0.3133],\n",
      "        [0.4574, 0.2942, 0.2484],\n",
      "        [0.4471, 0.2946, 0.2583],\n",
      "        [0.5517, 0.2446, 0.2036],\n",
      "        [0.4689, 0.2868, 0.2444],\n",
      "        [0.4128, 0.2907, 0.2965],\n",
      "        [0.4288, 0.3239, 0.2473],\n",
      "        [0.3850, 0.3064, 0.3087],\n",
      "        [0.4038, 0.3110, 0.2852],\n",
      "        [0.4979, 0.2773, 0.2248],\n",
      "        [0.4802, 0.2959, 0.2239],\n",
      "        [0.4097, 0.3126, 0.2777],\n",
      "        [0.3981, 0.3150, 0.2869],\n",
      "        [0.4041, 0.3200, 0.2759],\n",
      "        [0.4659, 0.3575, 0.1766],\n",
      "        [0.5690, 0.2539, 0.1771],\n",
      "        [0.4404, 0.3093, 0.2504],\n",
      "        [0.3988, 0.3251, 0.2761],\n",
      "        [0.3703, 0.3407, 0.2890],\n",
      "        [0.3784, 0.3250, 0.2965],\n",
      "        [0.3954, 0.3394, 0.2652],\n",
      "        [0.4011, 0.3050, 0.2939],\n",
      "        [0.4415, 0.3466, 0.2120],\n",
      "        [0.5054, 0.2711, 0.2235],\n",
      "        [0.5911, 0.2521, 0.1568],\n",
      "        [0.4111, 0.3139, 0.2750],\n",
      "        [0.4245, 0.3258, 0.2497],\n",
      "        [0.3860, 0.3113, 0.3027],\n",
      "        [0.4131, 0.3301, 0.2569],\n",
      "        [0.3760, 0.3330, 0.2910],\n",
      "        [0.4017, 0.3284, 0.2698],\n",
      "        [0.3710, 0.3453, 0.2837],\n",
      "        [0.3844, 0.3361, 0.2795],\n",
      "        [0.6184, 0.2294, 0.1521],\n",
      "        [0.4113, 0.3348, 0.2538],\n",
      "        [0.3677, 0.3455, 0.2868],\n",
      "        [0.4083, 0.3303, 0.2614],\n",
      "        [0.3776, 0.3381, 0.2843],\n",
      "        [0.5513, 0.2859, 0.1628],\n",
      "        [0.4744, 0.2953, 0.2304],\n",
      "        [0.3825, 0.3627, 0.2548],\n",
      "        [0.4416, 0.2890, 0.2695],\n",
      "        [0.4197, 0.3031, 0.2772]], device='cuda:0', grad_fn=<SoftmaxBackward0>), tensor([[0.3636, 0.3390, 0.2975],\n",
      "        [0.4392, 0.3353, 0.2256],\n",
      "        [0.3731, 0.3406, 0.2863],\n",
      "        [0.3577, 0.3327, 0.3096],\n",
      "        [0.3659, 0.3275, 0.3067],\n",
      "        [0.3471, 0.3529, 0.2999],\n",
      "        [0.3257, 0.3501, 0.3242],\n",
      "        [0.3437, 0.3549, 0.3014],\n",
      "        [0.3598, 0.3764, 0.2638],\n",
      "        [0.3489, 0.3438, 0.3073],\n",
      "        [0.3545, 0.3591, 0.2864],\n",
      "        [0.3512, 0.3682, 0.2806],\n",
      "        [0.3496, 0.3395, 0.3109],\n",
      "        [0.3400, 0.3581, 0.3019],\n",
      "        [0.3574, 0.3266, 0.3160],\n",
      "        [0.3546, 0.3483, 0.2972],\n",
      "        [0.3727, 0.3404, 0.2869],\n",
      "        [0.3407, 0.3395, 0.3199],\n",
      "        [0.3396, 0.3483, 0.3121],\n",
      "        [0.3526, 0.3597, 0.2877],\n",
      "        [0.3407, 0.3056, 0.3538],\n",
      "        [0.3579, 0.3462, 0.2959],\n",
      "        [0.3546, 0.3269, 0.3186],\n",
      "        [0.3685, 0.3496, 0.2819],\n",
      "        [0.3707, 0.3887, 0.2406],\n",
      "        [0.3742, 0.3473, 0.2785],\n",
      "        [0.3744, 0.3757, 0.2499],\n",
      "        [0.3669, 0.3516, 0.2815],\n",
      "        [0.3526, 0.3333, 0.3141],\n",
      "        [0.3454, 0.3518, 0.3028],\n",
      "        [0.3500, 0.3484, 0.3016],\n",
      "        [0.3450, 0.3450, 0.3100],\n",
      "        [0.3412, 0.3480, 0.3108],\n",
      "        [0.3567, 0.3491, 0.2942],\n",
      "        [0.3528, 0.3408, 0.3064],\n",
      "        [0.3488, 0.3256, 0.3256],\n",
      "        [0.3960, 0.3242, 0.2798],\n",
      "        [0.3525, 0.3395, 0.3080],\n",
      "        [0.3560, 0.3394, 0.3045],\n",
      "        [0.3459, 0.3505, 0.3036],\n",
      "        [0.3602, 0.3319, 0.3079],\n",
      "        [0.4173, 0.3572, 0.2255],\n",
      "        [0.3616, 0.3348, 0.3036],\n",
      "        [0.3499, 0.3647, 0.2854],\n",
      "        [0.3502, 0.3532, 0.2965],\n",
      "        [0.3450, 0.3677, 0.2873],\n",
      "        [0.3827, 0.3221, 0.2952],\n",
      "        [0.3486, 0.3845, 0.2669],\n",
      "        [0.3474, 0.3320, 0.3206],\n",
      "        [0.3571, 0.3450, 0.2979],\n",
      "        [0.3798, 0.3513, 0.2689],\n",
      "        [0.3585, 0.3312, 0.3104],\n",
      "        [0.3719, 0.3275, 0.3005],\n",
      "        [0.3473, 0.3503, 0.3024],\n",
      "        [0.3483, 0.3495, 0.3022],\n",
      "        [0.3609, 0.3335, 0.3056],\n",
      "        [0.3681, 0.3340, 0.2978],\n",
      "        [0.3503, 0.3502, 0.2995],\n",
      "        [0.4234, 0.3555, 0.2211],\n",
      "        [0.3555, 0.3315, 0.3130],\n",
      "        [0.3393, 0.3478, 0.3129],\n",
      "        [0.3222, 0.3321, 0.3456],\n",
      "        [0.3603, 0.3365, 0.3033],\n",
      "        [0.3471, 0.3572, 0.2956],\n",
      "        [0.3571, 0.3769, 0.2660],\n",
      "        [0.3416, 0.3561, 0.3023],\n",
      "        [0.3421, 0.3469, 0.3110],\n",
      "        [0.3572, 0.3385, 0.3043],\n",
      "        [0.3482, 0.3374, 0.3144],\n",
      "        [0.3530, 0.3340, 0.3129],\n",
      "        [0.3403, 0.3292, 0.3305],\n",
      "        [0.3569, 0.3347, 0.3085],\n",
      "        [0.3426, 0.3470, 0.3104],\n",
      "        [0.3496, 0.3526, 0.2978],\n",
      "        [0.3662, 0.3169, 0.3169],\n",
      "        [0.3515, 0.3516, 0.2970],\n",
      "        [0.3520, 0.3566, 0.2914],\n",
      "        [0.3368, 0.3313, 0.3318],\n",
      "        [0.3681, 0.3282, 0.3037],\n",
      "        [0.3575, 0.3386, 0.3039],\n",
      "        [0.3493, 0.3539, 0.2968],\n",
      "        [0.3664, 0.3462, 0.2874],\n",
      "        [0.3683, 0.3311, 0.3006],\n",
      "        [0.3723, 0.3529, 0.2747],\n",
      "        [0.3785, 0.3473, 0.2742],\n",
      "        [0.3424, 0.3550, 0.3026],\n",
      "        [0.3607, 0.3336, 0.3057],\n",
      "        [0.3474, 0.3735, 0.2790],\n",
      "        [0.3395, 0.3596, 0.3008],\n",
      "        [0.3976, 0.3250, 0.2774],\n",
      "        [0.3527, 0.3402, 0.3071],\n",
      "        [0.3432, 0.3538, 0.3030],\n",
      "        [0.3637, 0.3200, 0.3163],\n",
      "        [0.3490, 0.3517, 0.2993],\n",
      "        [0.3633, 0.3597, 0.2771],\n",
      "        [0.3464, 0.3337, 0.3199],\n",
      "        [0.3546, 0.3451, 0.3003],\n",
      "        [0.3541, 0.3433, 0.3025],\n",
      "        [0.3548, 0.3267, 0.3185],\n",
      "        [0.3694, 0.3427, 0.2879],\n",
      "        [0.3562, 0.3509, 0.2929],\n",
      "        [0.3482, 0.3431, 0.3087],\n",
      "        [0.3606, 0.3288, 0.3106],\n",
      "        [0.3437, 0.3374, 0.3189],\n",
      "        [0.3433, 0.3532, 0.3036],\n",
      "        [0.3606, 0.3428, 0.2966],\n",
      "        [0.3419, 0.3326, 0.3255],\n",
      "        [0.3605, 0.3418, 0.2978],\n",
      "        [0.3540, 0.3524, 0.2936],\n",
      "        [0.3566, 0.3530, 0.2904],\n",
      "        [0.3447, 0.3196, 0.3357],\n",
      "        [0.3405, 0.3465, 0.3131],\n",
      "        [0.3470, 0.3445, 0.3085],\n",
      "        [0.3737, 0.3298, 0.2964],\n",
      "        [0.3675, 0.3259, 0.3066],\n",
      "        [0.3538, 0.3480, 0.2983],\n",
      "        [0.3429, 0.3539, 0.3031],\n",
      "        [0.3510, 0.3398, 0.3092],\n",
      "        [0.3788, 0.3434, 0.2779],\n",
      "        [0.3500, 0.3450, 0.3050],\n",
      "        [0.3758, 0.3291, 0.2951],\n",
      "        [0.3564, 0.3269, 0.3167],\n",
      "        [0.3566, 0.3333, 0.3101],\n",
      "        [0.3519, 0.3452, 0.3029],\n",
      "        [0.3652, 0.3349, 0.2999],\n",
      "        [0.3399, 0.3464, 0.3136],\n",
      "        [0.3610, 0.3431, 0.2959],\n",
      "        [0.3505, 0.3721, 0.2774],\n",
      "        [0.3485, 0.3442, 0.3073],\n",
      "        [0.3494, 0.3558, 0.2948],\n",
      "        [0.3580, 0.3441, 0.2979],\n",
      "        [0.3476, 0.3494, 0.3030],\n",
      "        [0.3534, 0.3345, 0.3121],\n",
      "        [0.3588, 0.3451, 0.2961],\n",
      "        [0.3554, 0.3495, 0.2952],\n",
      "        [0.3483, 0.3410, 0.3107],\n",
      "        [0.3538, 0.3303, 0.3159],\n",
      "        [0.3579, 0.3646, 0.2775],\n",
      "        [0.3588, 0.3317, 0.3096],\n",
      "        [0.3678, 0.3287, 0.3035],\n",
      "        [0.3544, 0.3286, 0.3170],\n",
      "        [0.3518, 0.3336, 0.3145],\n",
      "        [0.3729, 0.3563, 0.2708],\n",
      "        [0.3396, 0.3782, 0.2821],\n",
      "        [0.3369, 0.3472, 0.3159],\n",
      "        [0.3305, 0.3256, 0.3439],\n",
      "        [0.3692, 0.3407, 0.2901]], device='cuda:0', grad_fn=<SoftmaxBackward0>), tensor([[0.2326, 0.2535, 0.2680, 0.2458],\n",
      "        [0.2104, 0.3333, 0.2210, 0.2352],\n",
      "        [0.2428, 0.2597, 0.2457, 0.2518],\n",
      "        [0.2400, 0.2554, 0.2578, 0.2468],\n",
      "        [0.2405, 0.2539, 0.2558, 0.2498],\n",
      "        [0.2325, 0.2554, 0.2685, 0.2436],\n",
      "        [0.2358, 0.2589, 0.2595, 0.2459],\n",
      "        [0.2349, 0.2499, 0.2638, 0.2514],\n",
      "        [0.2125, 0.2771, 0.2941, 0.2162],\n",
      "        [0.2432, 0.2559, 0.2539, 0.2470],\n",
      "        [0.2354, 0.2631, 0.2572, 0.2443],\n",
      "        [0.2252, 0.2501, 0.2740, 0.2507],\n",
      "        [0.2415, 0.2510, 0.2566, 0.2508],\n",
      "        [0.2369, 0.2554, 0.2540, 0.2537],\n",
      "        [0.2391, 0.2588, 0.2506, 0.2514],\n",
      "        [0.2440, 0.2602, 0.2539, 0.2420],\n",
      "        [0.2268, 0.2665, 0.2458, 0.2609],\n",
      "        [0.2326, 0.2682, 0.2595, 0.2396],\n",
      "        [0.2395, 0.2577, 0.2498, 0.2530],\n",
      "        [0.2360, 0.2662, 0.2556, 0.2422],\n",
      "        [0.2171, 0.2812, 0.2496, 0.2521],\n",
      "        [0.2403, 0.2622, 0.2593, 0.2382],\n",
      "        [0.2401, 0.2507, 0.2574, 0.2518],\n",
      "        [0.2345, 0.2666, 0.2590, 0.2398],\n",
      "        [0.2268, 0.3078, 0.2283, 0.2371],\n",
      "        [0.2435, 0.2809, 0.2482, 0.2274],\n",
      "        [0.2250, 0.3101, 0.2286, 0.2363],\n",
      "        [0.2225, 0.2796, 0.2556, 0.2422],\n",
      "        [0.2448, 0.2579, 0.2561, 0.2412],\n",
      "        [0.2360, 0.2546, 0.2640, 0.2454],\n",
      "        [0.2421, 0.2549, 0.2512, 0.2517],\n",
      "        [0.2320, 0.2761, 0.2566, 0.2354],\n",
      "        [0.2403, 0.2545, 0.2537, 0.2515],\n",
      "        [0.2383, 0.2470, 0.2678, 0.2469],\n",
      "        [0.2342, 0.2735, 0.2544, 0.2380],\n",
      "        [0.2284, 0.2550, 0.2557, 0.2609],\n",
      "        [0.2370, 0.2593, 0.2633, 0.2404],\n",
      "        [0.2388, 0.2612, 0.2513, 0.2487],\n",
      "        [0.2390, 0.2865, 0.2457, 0.2288],\n",
      "        [0.2358, 0.2555, 0.2590, 0.2498],\n",
      "        [0.2432, 0.2510, 0.2581, 0.2477],\n",
      "        [0.2204, 0.3012, 0.2494, 0.2291],\n",
      "        [0.2280, 0.2733, 0.2495, 0.2491],\n",
      "        [0.2053, 0.2774, 0.2871, 0.2301],\n",
      "        [0.2336, 0.2539, 0.2679, 0.2446],\n",
      "        [0.2308, 0.2661, 0.2588, 0.2443],\n",
      "        [0.2417, 0.2535, 0.2614, 0.2434],\n",
      "        [0.2397, 0.2654, 0.2547, 0.2402],\n",
      "        [0.2440, 0.2531, 0.2509, 0.2519],\n",
      "        [0.2372, 0.2508, 0.2619, 0.2501],\n",
      "        [0.2295, 0.2894, 0.2401, 0.2409],\n",
      "        [0.2436, 0.2529, 0.2547, 0.2488],\n",
      "        [0.2289, 0.2707, 0.2504, 0.2499],\n",
      "        [0.2385, 0.2513, 0.2589, 0.2513],\n",
      "        [0.2314, 0.2563, 0.2666, 0.2457],\n",
      "        [0.2366, 0.2612, 0.2601, 0.2421],\n",
      "        [0.2385, 0.2495, 0.2619, 0.2501],\n",
      "        [0.2220, 0.2849, 0.2505, 0.2425],\n",
      "        [0.2003, 0.3367, 0.2281, 0.2350],\n",
      "        [0.2429, 0.2519, 0.2562, 0.2491],\n",
      "        [0.2375, 0.2557, 0.2580, 0.2488],\n",
      "        [0.2327, 0.2717, 0.2463, 0.2493],\n",
      "        [0.2301, 0.2649, 0.2500, 0.2550],\n",
      "        [0.2302, 0.2681, 0.2530, 0.2487],\n",
      "        [0.2263, 0.2970, 0.2424, 0.2343],\n",
      "        [0.2336, 0.2534, 0.2578, 0.2553],\n",
      "        [0.2306, 0.2676, 0.2506, 0.2512],\n",
      "        [0.2376, 0.2727, 0.2492, 0.2404],\n",
      "        [0.2437, 0.2548, 0.2530, 0.2484],\n",
      "        [0.2326, 0.2667, 0.2427, 0.2579],\n",
      "        [0.2349, 0.2741, 0.2577, 0.2334],\n",
      "        [0.2417, 0.2591, 0.2537, 0.2455],\n",
      "        [0.2403, 0.2530, 0.2569, 0.2499],\n",
      "        [0.2425, 0.2685, 0.2559, 0.2331],\n",
      "        [0.2329, 0.2600, 0.2590, 0.2481],\n",
      "        [0.2325, 0.2740, 0.2506, 0.2429],\n",
      "        [0.2381, 0.2796, 0.2562, 0.2262],\n",
      "        [0.2425, 0.2620, 0.2517, 0.2437],\n",
      "        [0.2217, 0.2807, 0.2547, 0.2429],\n",
      "        [0.2326, 0.2736, 0.2610, 0.2328],\n",
      "        [0.2356, 0.2512, 0.2685, 0.2447],\n",
      "        [0.2363, 0.2580, 0.2667, 0.2390],\n",
      "        [0.2354, 0.2636, 0.2479, 0.2531],\n",
      "        [0.2471, 0.2651, 0.2407, 0.2471],\n",
      "        [0.2378, 0.2698, 0.2545, 0.2379],\n",
      "        [0.2391, 0.2503, 0.2611, 0.2495],\n",
      "        [0.2323, 0.2605, 0.2632, 0.2440],\n",
      "        [0.2417, 0.2555, 0.2574, 0.2455],\n",
      "        [0.2387, 0.2537, 0.2568, 0.2509],\n",
      "        [0.2363, 0.2594, 0.2632, 0.2411],\n",
      "        [0.2356, 0.2646, 0.2511, 0.2487],\n",
      "        [0.2391, 0.2510, 0.2595, 0.2504],\n",
      "        [0.2329, 0.2651, 0.2392, 0.2627],\n",
      "        [0.2329, 0.2577, 0.2618, 0.2476],\n",
      "        [0.2404, 0.2615, 0.2520, 0.2461],\n",
      "        [0.2154, 0.2936, 0.2448, 0.2463],\n",
      "        [0.2406, 0.2701, 0.2450, 0.2443],\n",
      "        [0.2255, 0.2808, 0.2545, 0.2391],\n",
      "        [0.2312, 0.2682, 0.2602, 0.2403],\n",
      "        [0.2230, 0.2844, 0.2499, 0.2426],\n",
      "        [0.2250, 0.2732, 0.2495, 0.2524],\n",
      "        [0.2327, 0.2648, 0.2524, 0.2501],\n",
      "        [0.2310, 0.2836, 0.2465, 0.2389],\n",
      "        [0.2263, 0.2873, 0.2695, 0.2169],\n",
      "        [0.2384, 0.2549, 0.2633, 0.2434],\n",
      "        [0.2402, 0.2633, 0.2575, 0.2390],\n",
      "        [0.2382, 0.2685, 0.2548, 0.2385],\n",
      "        [0.2218, 0.2918, 0.2536, 0.2328],\n",
      "        [0.2332, 0.2699, 0.2535, 0.2435],\n",
      "        [0.2346, 0.2571, 0.2630, 0.2453],\n",
      "        [0.2369, 0.2645, 0.2512, 0.2474],\n",
      "        [0.2349, 0.2522, 0.2650, 0.2479],\n",
      "        [0.2406, 0.2557, 0.2584, 0.2454],\n",
      "        [0.2283, 0.2855, 0.2457, 0.2405],\n",
      "        [0.2402, 0.2651, 0.2531, 0.2416],\n",
      "        [0.2302, 0.2718, 0.2517, 0.2463],\n",
      "        [0.2356, 0.2527, 0.2532, 0.2585],\n",
      "        [0.2405, 0.2499, 0.2605, 0.2492],\n",
      "        [0.2468, 0.2744, 0.2368, 0.2421],\n",
      "        [0.2224, 0.2830, 0.2564, 0.2381],\n",
      "        [0.2351, 0.2523, 0.2548, 0.2578],\n",
      "        [0.2401, 0.2542, 0.2544, 0.2514],\n",
      "        [0.2427, 0.2560, 0.2538, 0.2475],\n",
      "        [0.2370, 0.2486, 0.2664, 0.2481],\n",
      "        [0.2345, 0.2654, 0.2593, 0.2409],\n",
      "        [0.2394, 0.2566, 0.2570, 0.2470],\n",
      "        [0.2425, 0.2653, 0.2417, 0.2505],\n",
      "        [0.2356, 0.2653, 0.2497, 0.2493],\n",
      "        [0.2112, 0.3153, 0.2600, 0.2135],\n",
      "        [0.2428, 0.2559, 0.2494, 0.2519],\n",
      "        [0.2380, 0.2574, 0.2474, 0.2571],\n",
      "        [0.2310, 0.2558, 0.2645, 0.2487],\n",
      "        [0.2290, 0.2789, 0.2496, 0.2425],\n",
      "        [0.2324, 0.2739, 0.2495, 0.2442],\n",
      "        [0.2348, 0.2638, 0.2528, 0.2486],\n",
      "        [0.2355, 0.2594, 0.2540, 0.2511],\n",
      "        [0.2388, 0.2564, 0.2561, 0.2488],\n",
      "        [0.2088, 0.2924, 0.2743, 0.2245],\n",
      "        [0.2332, 0.2634, 0.2537, 0.2497],\n",
      "        [0.2400, 0.2542, 0.2591, 0.2466],\n",
      "        [0.2451, 0.2526, 0.2554, 0.2469],\n",
      "        [0.2408, 0.2549, 0.2567, 0.2476],\n",
      "        [0.2153, 0.2668, 0.2769, 0.2409],\n",
      "        [0.2307, 0.2786, 0.2492, 0.2414],\n",
      "        [0.2377, 0.2733, 0.2475, 0.2416],\n",
      "        [0.2287, 0.2722, 0.2544, 0.2448],\n",
      "        [0.2382, 0.2666, 0.2586, 0.2366]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>), tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4453, 0.3536, 0.3597, 0.3326, 0.2965],\n",
      "        [0.4554, 0.4170, 0.3779, 0.4147, 0.4051],\n",
      "        [0.4678, 0.4456, 0.4274, 0.4583, 0.4340],\n",
      "        [0.4748, 0.4445, 0.4310, 0.4611, 0.4385],\n",
      "        [0.4915, 0.4619, 0.4259, 0.4488, 0.4382],\n",
      "        [0.4727, 0.4396, 0.4031, 0.4125, 0.3900],\n",
      "        [0.4992, 0.4396, 0.4346, 0.4226, 0.4259],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4632, 0.4159, 0.3570, 0.3992, 0.4250],\n",
      "        [0.4753, 0.4351, 0.3977, 0.3759, 0.3734],\n",
      "        [0.4881, 0.4625, 0.4258, 0.4524, 0.4406],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4667, 0.4207, 0.4178, 0.4415, 0.4287],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4422, 0.3875, 0.3634, 0.4000, 0.3431],\n",
      "        [0.4845, 0.4449, 0.4326, 0.4558, 0.4139],\n",
      "        [0.4712, 0.4561, 0.4389, 0.4405, 0.4546],\n",
      "        [0.4537, 0.4218, 0.3471, 0.4003, 0.3195],\n",
      "        [0.3977, 0.3829, 0.3780, 0.4215, 0.3261],\n",
      "        [0.4831, 0.4610, 0.4420, 0.4669, 0.4650],\n",
      "        [0.4753, 0.4461, 0.4333, 0.4446, 0.4354],\n",
      "        [0.4486, 0.3668, 0.3941, 0.4020, 0.3973],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5238, 0.3810, 0.3603, 0.4414, 0.4135],\n",
      "        [0.3980, 0.3999, 0.3409, 0.3730, 0.3832],\n",
      "        [0.4898, 0.3698, 0.3995, 0.3841, 0.3731],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4697, 0.4551, 0.4427, 0.4379, 0.4442],\n",
      "        [0.4595, 0.4200, 0.4312, 0.4384, 0.3774],\n",
      "        [0.4738, 0.4620, 0.4425, 0.4460, 0.4597],\n",
      "        [0.4666, 0.4388, 0.4276, 0.4426, 0.4134],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4465, 0.4584, 0.4693, 0.4455, 0.4130],\n",
      "        [0.5038, 0.3966, 0.3838, 0.4059, 0.3790],\n",
      "        [0.4784, 0.4329, 0.4133, 0.4247, 0.4352],\n",
      "        [0.4204, 0.3661, 0.3556, 0.3638, 0.3802],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4732, 0.4379, 0.4306, 0.4551, 0.4450],\n",
      "        [0.4754, 0.3871, 0.3504, 0.3749, 0.2797],\n",
      "        [0.4802, 0.4203, 0.4034, 0.4461, 0.3979],\n",
      "        [0.5180, 0.3361, 0.3409, 0.3444, 0.2821],\n",
      "        [0.4835, 0.4674, 0.4361, 0.4495, 0.4427],\n",
      "        [0.4989, 0.4005, 0.4033, 0.4328, 0.3834],\n",
      "        [0.4537, 0.3969, 0.3723, 0.4419, 0.3996],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4824, 0.4441, 0.4374, 0.4492, 0.4456],\n",
      "        [0.4849, 0.4575, 0.4168, 0.4485, 0.4389],\n",
      "        [0.4830, 0.3901, 0.3399, 0.4075, 0.3364],\n",
      "        [0.4794, 0.4512, 0.4424, 0.4690, 0.4583],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4826, 0.4705, 0.4346, 0.4504, 0.4561],\n",
      "        [0.4938, 0.4577, 0.4223, 0.4455, 0.4413],\n",
      "        [0.4548, 0.4253, 0.4072, 0.4293, 0.4123],\n",
      "        [0.4648, 0.4280, 0.4123, 0.4468, 0.4239],\n",
      "        [0.5287, 0.4019, 0.3948, 0.4390, 0.3699],\n",
      "        [0.4640, 0.3726, 0.3950, 0.3515, 0.2848],\n",
      "        [0.4787, 0.4386, 0.4246, 0.4415, 0.4344],\n",
      "        [0.4796, 0.4572, 0.4352, 0.4422, 0.4580],\n",
      "        [0.4698, 0.4513, 0.4348, 0.4363, 0.4282],\n",
      "        [0.4821, 0.4279, 0.4142, 0.4287, 0.4357],\n",
      "        [0.4775, 0.4569, 0.4379, 0.4405, 0.4173],\n",
      "        [0.4039, 0.4169, 0.3348, 0.3745, 0.3661],\n",
      "        [0.4734, 0.4605, 0.4374, 0.4324, 0.4332],\n",
      "        [0.4699, 0.4451, 0.4391, 0.4498, 0.4414],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4799, 0.4299, 0.4301, 0.4395, 0.4444],\n",
      "        [0.4697, 0.4286, 0.4324, 0.4308, 0.4321],\n",
      "        [0.4610, 0.3959, 0.3778, 0.4032, 0.3788],\n",
      "        [0.4712, 0.4391, 0.4472, 0.4723, 0.4618],\n",
      "        [0.4751, 0.4644, 0.4438, 0.4501, 0.4602],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4822, 0.4256, 0.4121, 0.4310, 0.4000],\n",
      "        [0.5054, 0.3758, 0.3866, 0.4170, 0.3404],\n",
      "        [0.4802, 0.3677, 0.4088, 0.4425, 0.3772],\n",
      "        [0.4853, 0.4456, 0.4435, 0.4626, 0.4277],\n",
      "        [0.4582, 0.3889, 0.3548, 0.4264, 0.3971],\n",
      "        [0.4957, 0.4426, 0.4034, 0.4333, 0.4249],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4861, 0.4425, 0.4195, 0.4345, 0.4411],\n",
      "        [0.4702, 0.4329, 0.4322, 0.4298, 0.4179],\n",
      "        [0.4622, 0.4157, 0.3718, 0.4222, 0.3932],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4904, 0.4536, 0.4478, 0.4321, 0.4391],\n",
      "        [0.4755, 0.4324, 0.4236, 0.4399, 0.4178],\n",
      "        [0.4907, 0.3931, 0.3906, 0.4109, 0.3952],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4721, 0.4179, 0.4111, 0.4223, 0.4129],\n",
      "        [0.4604, 0.4307, 0.4266, 0.4365, 0.4472],\n",
      "        [0.4884, 0.4526, 0.4457, 0.4305, 0.4386],\n",
      "        [0.4577, 0.4203, 0.4307, 0.4439, 0.4125],\n",
      "        [0.4807, 0.4661, 0.4321, 0.4444, 0.4414],\n",
      "        [0.4681, 0.4209, 0.4141, 0.4369, 0.3833],\n",
      "        [0.4876, 0.3673, 0.3546, 0.3868, 0.3273],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4933, 0.4034, 0.3624, 0.4626, 0.4291],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4245, 0.3944, 0.3723, 0.4193, 0.3799],\n",
      "        [0.4760, 0.4137, 0.4030, 0.4218, 0.3804],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5030, 0.3910, 0.3813, 0.4446, 0.4086],\n",
      "        [0.4838, 0.3522, 0.3483, 0.4254, 0.3767],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4989, 0.4128, 0.4160, 0.4301, 0.4428],\n",
      "        [0.4499, 0.4006, 0.4191, 0.4348, 0.3831],\n",
      "        [0.4866, 0.4608, 0.4299, 0.4442, 0.4600],\n",
      "        [0.4902, 0.4623, 0.4234, 0.4510, 0.4477],\n",
      "        [0.4724, 0.3668, 0.3655, 0.3930, 0.3877],\n",
      "        [0.4495, 0.4228, 0.3571, 0.4137, 0.4091],\n",
      "        [0.4750, 0.4161, 0.4304, 0.4502, 0.4383],\n",
      "        [0.4675, 0.4680, 0.4485, 0.4436, 0.4421],\n",
      "        [0.4895, 0.4627, 0.4266, 0.4545, 0.4414],\n",
      "        [0.4326, 0.3874, 0.3431, 0.3936, 0.3419],\n",
      "        [0.5026, 0.3958, 0.3483, 0.4208, 0.3848],\n",
      "        [0.4510, 0.4072, 0.4040, 0.4000, 0.4184],\n",
      "        [0.4821, 0.4426, 0.4195, 0.4376, 0.4312],\n",
      "        [0.4796, 0.4493, 0.4413, 0.4600, 0.4554],\n",
      "        [0.4835, 0.4686, 0.4406, 0.4514, 0.4556],\n",
      "        [0.4979, 0.4382, 0.4299, 0.4604, 0.4395],\n",
      "        [0.4766, 0.4549, 0.4414, 0.4449, 0.4594],\n",
      "        [0.4734, 0.3974, 0.3705, 0.4445, 0.3687],\n",
      "        [0.4635, 0.4110, 0.4341, 0.4283, 0.3922],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4606, 0.4224, 0.4488, 0.4371, 0.4097],\n",
      "        [0.4915, 0.4526, 0.4201, 0.4374, 0.4371],\n",
      "        [0.4680, 0.4541, 0.4304, 0.4520, 0.4116],\n",
      "        [0.4856, 0.4447, 0.4229, 0.4509, 0.4445],\n",
      "        [0.4838, 0.4536, 0.4369, 0.4452, 0.4460],\n",
      "        [0.4670, 0.4289, 0.4487, 0.4413, 0.4332],\n",
      "        [0.4759, 0.4360, 0.3963, 0.4462, 0.4317],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4881, 0.4385, 0.4197, 0.4421, 0.4351],\n",
      "        [0.4718, 0.4461, 0.4217, 0.4663, 0.4427],\n",
      "        [0.4694, 0.4401, 0.4179, 0.4427, 0.4346],\n",
      "        [0.4729, 0.4566, 0.4343, 0.4610, 0.4504],\n",
      "        [0.4535, 0.3783, 0.3350, 0.3724, 0.2997],\n",
      "        [0.4942, 0.3920, 0.4324, 0.4375, 0.3955],\n",
      "        [0.4818, 0.4314, 0.4497, 0.4487, 0.4105],\n",
      "        [0.4764, 0.4396, 0.4067, 0.4200, 0.4379],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<MulBackward0>)], '5%': [tensor([[0.3969, 0.2861, 0.2454],\n",
      "        [0.5428, 0.1816, 0.1145],\n",
      "        [0.4098, 0.3081, 0.1865],\n",
      "        [0.3564, 0.3384, 0.2610],\n",
      "        [0.3624, 0.3393, 0.2615],\n",
      "        [0.4028, 0.2906, 0.2666],\n",
      "        [0.4236, 0.2849, 0.2043],\n",
      "        [0.3849, 0.2792, 0.2727],\n",
      "        [0.5191, 0.1445, 0.1182],\n",
      "        [0.3782, 0.2987, 0.2627],\n",
      "        [0.4164, 0.2697, 0.2010],\n",
      "        [0.3967, 0.2767, 0.2302],\n",
      "        [0.3819, 0.3019, 0.2598],\n",
      "        [0.3793, 0.2861, 0.2904],\n",
      "        [0.3660, 0.3300, 0.2528],\n",
      "        [0.3796, 0.3133, 0.2599],\n",
      "        [0.3855, 0.2793, 0.1283],\n",
      "        [0.4043, 0.3032, 0.2386],\n",
      "        [0.3853, 0.2961, 0.2705],\n",
      "        [0.4330, 0.2621, 0.1592],\n",
      "        [0.4086, 0.2862, 0.1703],\n",
      "        [0.3787, 0.2985, 0.2640],\n",
      "        [0.3731, 0.3270, 0.2566],\n",
      "        [0.3672, 0.3144, 0.2064],\n",
      "        [0.4477, 0.1532, 0.1091],\n",
      "        [0.3917, 0.2794, 0.2203],\n",
      "        [0.4430, 0.1713, 0.1358],\n",
      "        [0.4197, 0.2434, 0.1951],\n",
      "        [0.3799, 0.2910, 0.2775],\n",
      "        [0.3843, 0.2968, 0.2683],\n",
      "        [0.3943, 0.2910, 0.2650],\n",
      "        [0.4037, 0.3079, 0.2079],\n",
      "        [0.3749, 0.3011, 0.2770],\n",
      "        [0.3614, 0.3288, 0.2574],\n",
      "        [0.4140, 0.2515, 0.2564],\n",
      "        [0.3999, 0.2975, 0.2185],\n",
      "        [0.4253, 0.2950, 0.1659],\n",
      "        [0.3565, 0.3022, 0.2881],\n",
      "        [0.4300, 0.3085, 0.1439],\n",
      "        [0.3819, 0.2930, 0.2673],\n",
      "        [0.3596, 0.3174, 0.2664],\n",
      "        [0.4441, 0.2593, 0.1533],\n",
      "        [0.3304, 0.3255, 0.2530],\n",
      "        [0.5293, 0.1422, 0.1141],\n",
      "        [0.3919, 0.2913, 0.2520],\n",
      "        [0.4666, 0.2386, 0.2203],\n",
      "        [0.3699, 0.3139, 0.2536],\n",
      "        [0.4595, 0.2550, 0.2162],\n",
      "        [0.3649, 0.3194, 0.2712],\n",
      "        [0.3790, 0.3080, 0.2693],\n",
      "        [0.4786, 0.2492, 0.1476],\n",
      "        [0.3481, 0.3340, 0.2778],\n",
      "        [0.4469, 0.2220, 0.1859],\n",
      "        [0.3693, 0.3120, 0.2817],\n",
      "        [0.3793, 0.2914, 0.2778],\n",
      "        [0.4047, 0.3072, 0.2037],\n",
      "        [0.3929, 0.3074, 0.2494],\n",
      "        [0.4665, 0.2358, 0.1800],\n",
      "        [0.5110, 0.2155, 0.1002],\n",
      "        [0.3943, 0.3046, 0.2560],\n",
      "        [0.3798, 0.2931, 0.2825],\n",
      "        [0.4036, 0.3004, 0.2451],\n",
      "        [0.3777, 0.3211, 0.2367],\n",
      "        [0.4043, 0.2922, 0.2468],\n",
      "        [0.5332, 0.1514, 0.1066],\n",
      "        [0.3860, 0.2844, 0.2805],\n",
      "        [0.3865, 0.2931, 0.2663],\n",
      "        [0.4170, 0.2958, 0.2199],\n",
      "        [0.3611, 0.2948, 0.3030],\n",
      "        [0.3965, 0.2853, 0.2497],\n",
      "        [0.4504, 0.2861, 0.1699],\n",
      "        [0.3571, 0.3374, 0.2697],\n",
      "        [0.3813, 0.2985, 0.2677],\n",
      "        [0.4075, 0.2784, 0.2529],\n",
      "        [0.4018, 0.3072, 0.1961],\n",
      "        [0.4446, 0.2696, 0.1718],\n",
      "        [0.3806, 0.3449, 0.1853],\n",
      "        [0.3627, 0.3378, 0.2328],\n",
      "        [0.4276, 0.2533, 0.1823],\n",
      "        [0.4018, 0.2876, 0.2364],\n",
      "        [0.3950, 0.2956, 0.2551],\n",
      "        [0.4034, 0.2798, 0.2556],\n",
      "        [0.3797, 0.3273, 0.2320],\n",
      "        [0.3923, 0.3040, 0.2201],\n",
      "        [0.4245, 0.2630, 0.2151],\n",
      "        [0.3880, 0.2819, 0.2658],\n",
      "        [0.3767, 0.3216, 0.2402],\n",
      "        [0.4402, 0.2814, 0.1989],\n",
      "        [0.4171, 0.2756, 0.2591],\n",
      "        [0.3886, 0.2757, 0.2688],\n",
      "        [0.3971, 0.2889, 0.2507],\n",
      "        [0.3965, 0.2833, 0.2567],\n",
      "        [0.3787, 0.3424, 0.2257],\n",
      "        [0.4012, 0.2865, 0.2492],\n",
      "        [0.3854, 0.3282, 0.1684],\n",
      "        [0.4662, 0.2355, 0.1479],\n",
      "        [0.3915, 0.2948, 0.2473],\n",
      "        [0.4191, 0.2742, 0.1533],\n",
      "        [0.4313, 0.2673, 0.2375],\n",
      "        [0.3873, 0.3254, 0.1953],\n",
      "        [0.4390, 0.2911, 0.1928],\n",
      "        [0.4160, 0.2843, 0.1762],\n",
      "        [0.4393, 0.2615, 0.1975],\n",
      "        [0.4511, 0.2645, 0.1502],\n",
      "        [0.3625, 0.2942, 0.2971],\n",
      "        [0.4095, 0.2447, 0.2078],\n",
      "        [0.4287, 0.2721, 0.2328],\n",
      "        [0.4811, 0.2035, 0.1702],\n",
      "        [0.4392, 0.2590, 0.2100],\n",
      "        [0.3798, 0.2840, 0.2754],\n",
      "        [0.4111, 0.3092, 0.2249],\n",
      "        [0.3731, 0.2906, 0.2902],\n",
      "        [0.3788, 0.3022, 0.2661],\n",
      "        [0.4449, 0.2177, 0.1826],\n",
      "        [0.4724, 0.2607, 0.1909],\n",
      "        [0.3788, 0.2911, 0.2570],\n",
      "        [0.3881, 0.2970, 0.2791],\n",
      "        [0.3910, 0.3039, 0.2574],\n",
      "        [0.4260, 0.3098, 0.1537],\n",
      "        [0.5533, 0.2011, 0.1294],\n",
      "        [0.4148, 0.2838, 0.2232],\n",
      "        [0.3820, 0.3013, 0.2528],\n",
      "        [0.3608, 0.3296, 0.2656],\n",
      "        [0.3628, 0.3112, 0.2781],\n",
      "        [0.3839, 0.3205, 0.2470],\n",
      "        [0.3840, 0.2950, 0.2759],\n",
      "        [0.3962, 0.3104, 0.1909],\n",
      "        [0.4575, 0.2523, 0.2020],\n",
      "        [0.5140, 0.2038, 0.1182],\n",
      "        [0.3920, 0.2952, 0.2626],\n",
      "        [0.4041, 0.3066, 0.2281],\n",
      "        [0.3696, 0.2934, 0.2868],\n",
      "        [0.4051, 0.3052, 0.2279],\n",
      "        [0.3602, 0.3161, 0.2700],\n",
      "        [0.3843, 0.3123, 0.2498],\n",
      "        [0.3550, 0.3176, 0.2614],\n",
      "        [0.3725, 0.3075, 0.2584],\n",
      "        [0.5206, 0.1688, 0.1191],\n",
      "        [0.3918, 0.3072, 0.2294],\n",
      "        [0.3571, 0.3305, 0.2653],\n",
      "        [0.3958, 0.3089, 0.2464],\n",
      "        [0.3684, 0.3279, 0.2649],\n",
      "        [0.4765, 0.2086, 0.1265],\n",
      "        [0.4573, 0.2765, 0.2129],\n",
      "        [0.3702, 0.3459, 0.2403],\n",
      "        [0.4278, 0.2682, 0.2517],\n",
      "        [0.3980, 0.2882, 0.2429]], device='cuda:0', grad_fn=<ViewBackward0>), tensor([[0.3422, 0.3097, 0.2832],\n",
      "        [0.3870, 0.2959, 0.1729],\n",
      "        [0.3523, 0.3106, 0.2543],\n",
      "        [0.3483, 0.3193, 0.2945],\n",
      "        [0.3542, 0.3148, 0.2898],\n",
      "        [0.3307, 0.3407, 0.2806],\n",
      "        [0.3040, 0.3138, 0.2942],\n",
      "        [0.3292, 0.3453, 0.2844],\n",
      "        [0.3161, 0.3321, 0.2320],\n",
      "        [0.3318, 0.3284, 0.2896],\n",
      "        [0.3172, 0.3380, 0.2646],\n",
      "        [0.3331, 0.3472, 0.2478],\n",
      "        [0.3412, 0.3210, 0.3006],\n",
      "        [0.3245, 0.3392, 0.2877],\n",
      "        [0.3420, 0.3149, 0.2938],\n",
      "        [0.3463, 0.3291, 0.2819],\n",
      "        [0.3304, 0.2981, 0.2548],\n",
      "        [0.3168, 0.3207, 0.3088],\n",
      "        [0.3337, 0.3336, 0.3024],\n",
      "        [0.3249, 0.3204, 0.2484],\n",
      "        [0.3184, 0.2740, 0.3327],\n",
      "        [0.3404, 0.3337, 0.2874],\n",
      "        [0.3409, 0.3144, 0.3055],\n",
      "        [0.3537, 0.3238, 0.2586],\n",
      "        [0.2882, 0.3548, 0.2003],\n",
      "        [0.3418, 0.3205, 0.2599],\n",
      "        [0.3099, 0.3228, 0.2010],\n",
      "        [0.3386, 0.3163, 0.2583],\n",
      "        [0.3400, 0.3118, 0.2965],\n",
      "        [0.3331, 0.3389, 0.2831],\n",
      "        [0.3314, 0.3406, 0.2842],\n",
      "        [0.3285, 0.3261, 0.2935],\n",
      "        [0.3357, 0.3331, 0.3006],\n",
      "        [0.3410, 0.3324, 0.2711],\n",
      "        [0.3237, 0.3254, 0.2769],\n",
      "        [0.3329, 0.3015, 0.2902],\n",
      "        [0.3789, 0.2960, 0.2575],\n",
      "        [0.3389, 0.3264, 0.2927],\n",
      "        [0.3258, 0.3138, 0.2753],\n",
      "        [0.3363, 0.3326, 0.2859],\n",
      "        [0.3306, 0.3134, 0.2935],\n",
      "        [0.3544, 0.3314, 0.1976],\n",
      "        [0.3478, 0.3155, 0.2808],\n",
      "        [0.3064, 0.3376, 0.2350],\n",
      "        [0.3386, 0.3348, 0.2810],\n",
      "        [0.3268, 0.3453, 0.2715],\n",
      "        [0.3682, 0.3013, 0.2695],\n",
      "        [0.3251, 0.3560, 0.2373],\n",
      "        [0.3379, 0.3202, 0.3107],\n",
      "        [0.3348, 0.3375, 0.2818],\n",
      "        [0.3540, 0.3074, 0.2238],\n",
      "        [0.3479, 0.3137, 0.3034],\n",
      "        [0.3420, 0.2979, 0.2504],\n",
      "        [0.3349, 0.3416, 0.2878],\n",
      "        [0.3352, 0.3344, 0.2869],\n",
      "        [0.3363, 0.3214, 0.2838],\n",
      "        [0.3607, 0.3244, 0.2745],\n",
      "        [0.3127, 0.3206, 0.2820],\n",
      "        [0.3446, 0.3107, 0.1780],\n",
      "        [0.3469, 0.3236, 0.2917],\n",
      "        [0.3277, 0.3338, 0.2992],\n",
      "        [0.3156, 0.3173, 0.3240],\n",
      "        [0.3410, 0.3309, 0.2862],\n",
      "        [0.3351, 0.3417, 0.2776],\n",
      "        [0.3180, 0.3491, 0.2207],\n",
      "        [0.3246, 0.3409, 0.2897],\n",
      "        [0.3261, 0.3344, 0.2940],\n",
      "        [0.3303, 0.3233, 0.2887],\n",
      "        [0.3354, 0.3291, 0.3084],\n",
      "        [0.3354, 0.3085, 0.2922],\n",
      "        [0.3101, 0.2958, 0.3048],\n",
      "        [0.3424, 0.3239, 0.2959],\n",
      "        [0.3272, 0.3345, 0.3028],\n",
      "        [0.3406, 0.3401, 0.2761],\n",
      "        [0.3424, 0.2878, 0.2956],\n",
      "        [0.3222, 0.3153, 0.2741],\n",
      "        [0.3220, 0.3436, 0.2737],\n",
      "        [0.3217, 0.3199, 0.3077],\n",
      "        [0.3479, 0.2912, 0.2824],\n",
      "        [0.3477, 0.3153, 0.2853],\n",
      "        [0.3419, 0.3400, 0.2791],\n",
      "        [0.3351, 0.3313, 0.2768],\n",
      "        [0.3388, 0.3102, 0.2816],\n",
      "        [0.3391, 0.3312, 0.2593],\n",
      "        [0.3466, 0.3160, 0.2609],\n",
      "        [0.3235, 0.3492, 0.2789],\n",
      "        [0.3485, 0.3158, 0.2861],\n",
      "        [0.3189, 0.3464, 0.2524],\n",
      "        [0.3265, 0.3444, 0.2910],\n",
      "        [0.3718, 0.3011, 0.2512],\n",
      "        [0.3355, 0.3317, 0.2930],\n",
      "        [0.3366, 0.3370, 0.2859],\n",
      "        [0.3536, 0.3030, 0.2937],\n",
      "        [0.3332, 0.3347, 0.2859],\n",
      "        [0.3286, 0.3426, 0.2459],\n",
      "        [0.3333, 0.2913, 0.2752],\n",
      "        [0.3408, 0.3234, 0.2796],\n",
      "        [0.3082, 0.3113, 0.2735],\n",
      "        [0.3394, 0.3058, 0.2915],\n",
      "        [0.3426, 0.3209, 0.2572],\n",
      "        [0.3363, 0.3281, 0.2758],\n",
      "        [0.3253, 0.3193, 0.2832],\n",
      "        [0.3325, 0.3013, 0.2867],\n",
      "        [0.2754, 0.2885, 0.2546],\n",
      "        [0.3323, 0.3346, 0.2887],\n",
      "        [0.3416, 0.3214, 0.2679],\n",
      "        [0.3288, 0.3117, 0.3027],\n",
      "        [0.3295, 0.3062, 0.2624],\n",
      "        [0.3290, 0.3311, 0.2744],\n",
      "        [0.3509, 0.3279, 0.2783],\n",
      "        [0.3089, 0.2984, 0.3089],\n",
      "        [0.3281, 0.3370, 0.2935],\n",
      "        [0.3313, 0.3334, 0.2998],\n",
      "        [0.3538, 0.3027, 0.2663],\n",
      "        [0.3445, 0.3047, 0.2820],\n",
      "        [0.3337, 0.3309, 0.2804],\n",
      "        [0.3345, 0.3427, 0.2912],\n",
      "        [0.3400, 0.3246, 0.2974],\n",
      "        [0.3607, 0.3073, 0.2427],\n",
      "        [0.3112, 0.3021, 0.2662],\n",
      "        [0.3481, 0.3152, 0.2839],\n",
      "        [0.3397, 0.3135, 0.3046],\n",
      "        [0.3495, 0.3217, 0.3010],\n",
      "        [0.3396, 0.3322, 0.2919],\n",
      "        [0.3554, 0.3180, 0.2788],\n",
      "        [0.3318, 0.3354, 0.2961],\n",
      "        [0.3278, 0.3096, 0.2796],\n",
      "        [0.3243, 0.3531, 0.2456],\n",
      "        [0.2951, 0.3086, 0.2678],\n",
      "        [0.3352, 0.3419, 0.2773],\n",
      "        [0.3370, 0.3167, 0.2882],\n",
      "        [0.3353, 0.3401, 0.2887],\n",
      "        [0.3342, 0.2981, 0.2922],\n",
      "        [0.3375, 0.3256, 0.2871],\n",
      "        [0.3458, 0.3363, 0.2808],\n",
      "        [0.3351, 0.3286, 0.2959],\n",
      "        [0.3418, 0.3129, 0.3053],\n",
      "        [0.3234, 0.3304, 0.2576],\n",
      "        [0.3401, 0.3178, 0.2906],\n",
      "        [0.3603, 0.3196, 0.2834],\n",
      "        [0.3408, 0.3178, 0.3023],\n",
      "        [0.3429, 0.3183, 0.3065],\n",
      "        [0.3233, 0.3115, 0.2382],\n",
      "        [0.3162, 0.3375, 0.2648],\n",
      "        [0.3174, 0.3295, 0.2948],\n",
      "        [0.3208, 0.3137, 0.3245],\n",
      "        [0.3375, 0.3090, 0.2691]], device='cuda:0', grad_fn=<ViewBackward0>), tensor([[0.2182, 0.2367, 0.2502, 0.2288],\n",
      "        [0.1841, 0.2821, 0.1821, 0.2021],\n",
      "        [0.2271, 0.2413, 0.2261, 0.2327],\n",
      "        [0.2288, 0.2424, 0.2425, 0.2383],\n",
      "        [0.2329, 0.2469, 0.2443, 0.2384],\n",
      "        [0.2219, 0.2473, 0.2582, 0.2316],\n",
      "        [0.2173, 0.2482, 0.2392, 0.2307],\n",
      "        [0.2262, 0.2374, 0.2506, 0.2374],\n",
      "        [0.1916, 0.2574, 0.2527, 0.2002],\n",
      "        [0.2309, 0.2414, 0.2437, 0.2346],\n",
      "        [0.2181, 0.2498, 0.2420, 0.2254],\n",
      "        [0.2058, 0.2212, 0.2471, 0.2345],\n",
      "        [0.2295, 0.2410, 0.2495, 0.2386],\n",
      "        [0.2268, 0.2467, 0.2453, 0.2382],\n",
      "        [0.2325, 0.2432, 0.2398, 0.2352],\n",
      "        [0.2380, 0.2450, 0.2406, 0.2254],\n",
      "        [0.1991, 0.2420, 0.2378, 0.2461],\n",
      "        [0.2233, 0.2497, 0.2464, 0.2340],\n",
      "        [0.2351, 0.2472, 0.2395, 0.2393],\n",
      "        [0.2144, 0.2504, 0.2331, 0.2172],\n",
      "        [0.2058, 0.2488, 0.2332, 0.2220],\n",
      "        [0.2269, 0.2466, 0.2552, 0.2262],\n",
      "        [0.2340, 0.2415, 0.2427, 0.2417],\n",
      "        [0.2155, 0.2469, 0.2381, 0.2207],\n",
      "        [0.1991, 0.2538, 0.2043, 0.2070],\n",
      "        [0.2307, 0.2602, 0.2278, 0.2102],\n",
      "        [0.1833, 0.2620, 0.2022, 0.1950],\n",
      "        [0.2160, 0.2510, 0.2305, 0.2272],\n",
      "        [0.2330, 0.2430, 0.2428, 0.2275],\n",
      "        [0.2213, 0.2427, 0.2533, 0.2335],\n",
      "        [0.2305, 0.2471, 0.2400, 0.2360],\n",
      "        [0.2156, 0.2691, 0.2424, 0.2221],\n",
      "        [0.2318, 0.2485, 0.2501, 0.2351],\n",
      "        [0.2287, 0.2397, 0.2505, 0.2285],\n",
      "        [0.2229, 0.2670, 0.2381, 0.2223],\n",
      "        [0.2163, 0.2330, 0.2458, 0.2355],\n",
      "        [0.2190, 0.2422, 0.2487, 0.2173],\n",
      "        [0.2300, 0.2511, 0.2414, 0.2368],\n",
      "        [0.2224, 0.2625, 0.2161, 0.2088],\n",
      "        [0.2239, 0.2477, 0.2479, 0.2390],\n",
      "        [0.2340, 0.2419, 0.2458, 0.2348],\n",
      "        [0.1937, 0.2609, 0.2296, 0.2066],\n",
      "        [0.2172, 0.2612, 0.2340, 0.2306],\n",
      "        [0.1729, 0.2517, 0.2542, 0.2070],\n",
      "        [0.2225, 0.2362, 0.2595, 0.2341],\n",
      "        [0.2208, 0.2437, 0.2336, 0.2255],\n",
      "        [0.2274, 0.2385, 0.2424, 0.2281],\n",
      "        [0.2215, 0.2422, 0.2357, 0.2250],\n",
      "        [0.2347, 0.2472, 0.2427, 0.2402],\n",
      "        [0.2328, 0.2415, 0.2527, 0.2408],\n",
      "        [0.1984, 0.2642, 0.2059, 0.2215],\n",
      "        [0.2372, 0.2410, 0.2470, 0.2423],\n",
      "        [0.2082, 0.2399, 0.2258, 0.2277],\n",
      "        [0.2301, 0.2403, 0.2422, 0.2435],\n",
      "        [0.2185, 0.2449, 0.2518, 0.2351],\n",
      "        [0.2228, 0.2466, 0.2459, 0.2259],\n",
      "        [0.2277, 0.2410, 0.2542, 0.2396],\n",
      "        [0.2043, 0.2669, 0.2347, 0.2184],\n",
      "        [0.1720, 0.2568, 0.2040, 0.1790],\n",
      "        [0.2334, 0.2440, 0.2477, 0.2419],\n",
      "        [0.2243, 0.2437, 0.2461, 0.2402],\n",
      "        [0.2178, 0.2632, 0.2353, 0.2341],\n",
      "        [0.2191, 0.2549, 0.2322, 0.2419],\n",
      "        [0.2201, 0.2495, 0.2321, 0.2303],\n",
      "        [0.1977, 0.2560, 0.2241, 0.2012],\n",
      "        [0.2212, 0.2455, 0.2497, 0.2430],\n",
      "        [0.2201, 0.2537, 0.2389, 0.2367],\n",
      "        [0.2245, 0.2598, 0.2392, 0.2175],\n",
      "        [0.2343, 0.2476, 0.2438, 0.2396],\n",
      "        [0.2205, 0.2584, 0.2280, 0.2292],\n",
      "        [0.2265, 0.2515, 0.2324, 0.2084],\n",
      "        [0.2325, 0.2533, 0.2446, 0.2329],\n",
      "        [0.2286, 0.2424, 0.2479, 0.2372],\n",
      "        [0.2279, 0.2478, 0.2474, 0.2195],\n",
      "        [0.2224, 0.2478, 0.2364, 0.2271],\n",
      "        [0.2141, 0.2529, 0.2255, 0.2183],\n",
      "        [0.2153, 0.2413, 0.2301, 0.2110],\n",
      "        [0.2307, 0.2548, 0.2342, 0.2343],\n",
      "        [0.2000, 0.2534, 0.2285, 0.2204],\n",
      "        [0.2187, 0.2627, 0.2401, 0.2169],\n",
      "        [0.2262, 0.2431, 0.2496, 0.2342],\n",
      "        [0.2295, 0.2381, 0.2489, 0.2255],\n",
      "        [0.2253, 0.2465, 0.2344, 0.2370],\n",
      "        [0.2308, 0.2549, 0.2216, 0.2263],\n",
      "        [0.2126, 0.2514, 0.2278, 0.2289],\n",
      "        [0.2260, 0.2358, 0.2533, 0.2375],\n",
      "        [0.2137, 0.2356, 0.2477, 0.2291],\n",
      "        [0.2233, 0.2402, 0.2402, 0.2317],\n",
      "        [0.2285, 0.2407, 0.2488, 0.2340],\n",
      "        [0.2209, 0.2419, 0.2480, 0.2225],\n",
      "        [0.2306, 0.2538, 0.2380, 0.2406],\n",
      "        [0.2227, 0.2463, 0.2501, 0.2326],\n",
      "        [0.2172, 0.2477, 0.2295, 0.2492],\n",
      "        [0.2219, 0.2452, 0.2543, 0.2360],\n",
      "        [0.2299, 0.2368, 0.2302, 0.2182],\n",
      "        [0.1864, 0.2512, 0.2166, 0.1907],\n",
      "        [0.2282, 0.2528, 0.2332, 0.2288],\n",
      "        [0.1978, 0.2653, 0.2409, 0.2138],\n",
      "        [0.2198, 0.2457, 0.2450, 0.2229],\n",
      "        [0.2090, 0.2591, 0.2236, 0.2209],\n",
      "        [0.2118, 0.2597, 0.2271, 0.2331],\n",
      "        [0.2176, 0.2495, 0.2332, 0.2311],\n",
      "        [0.2186, 0.2605, 0.2311, 0.2176],\n",
      "        [0.1855, 0.2521, 0.2326, 0.1811],\n",
      "        [0.2329, 0.2499, 0.2505, 0.2293],\n",
      "        [0.2266, 0.2428, 0.2377, 0.2199],\n",
      "        [0.2192, 0.2546, 0.2425, 0.2258],\n",
      "        [0.1985, 0.2630, 0.2260, 0.2193],\n",
      "        [0.2198, 0.2543, 0.2339, 0.2244],\n",
      "        [0.2212, 0.2400, 0.2547, 0.2306],\n",
      "        [0.2213, 0.2393, 0.2452, 0.2342],\n",
      "        [0.2252, 0.2429, 0.2538, 0.2390],\n",
      "        [0.2339, 0.2436, 0.2470, 0.2321],\n",
      "        [0.2119, 0.2446, 0.2201, 0.2262],\n",
      "        [0.2236, 0.2490, 0.2335, 0.2223],\n",
      "        [0.2173, 0.2584, 0.2337, 0.2342],\n",
      "        [0.2234, 0.2416, 0.2433, 0.2461],\n",
      "        [0.2340, 0.2405, 0.2535, 0.2402],\n",
      "        [0.2234, 0.2555, 0.2288, 0.2112],\n",
      "        [0.1919, 0.2644, 0.2157, 0.2088],\n",
      "        [0.2210, 0.2384, 0.2338, 0.2573],\n",
      "        [0.2311, 0.2437, 0.2464, 0.2427],\n",
      "        [0.2304, 0.2477, 0.2463, 0.2385],\n",
      "        [0.2272, 0.2361, 0.2580, 0.2352],\n",
      "        [0.2266, 0.2545, 0.2421, 0.2264],\n",
      "        [0.2277, 0.2451, 0.2488, 0.2378],\n",
      "        [0.2254, 0.2433, 0.2297, 0.2265],\n",
      "        [0.2234, 0.2434, 0.2350, 0.2184],\n",
      "        [0.1971, 0.2792, 0.2383, 0.1959],\n",
      "        [0.2378, 0.2431, 0.2367, 0.2437],\n",
      "        [0.2186, 0.2433, 0.2321, 0.2494],\n",
      "        [0.2206, 0.2453, 0.2507, 0.2348],\n",
      "        [0.2190, 0.2644, 0.2306, 0.2245],\n",
      "        [0.2181, 0.2581, 0.2380, 0.2299],\n",
      "        [0.2223, 0.2525, 0.2456, 0.2404],\n",
      "        [0.2259, 0.2439, 0.2365, 0.2445],\n",
      "        [0.2328, 0.2396, 0.2427, 0.2427],\n",
      "        [0.1868, 0.2649, 0.2377, 0.2079],\n",
      "        [0.2240, 0.2579, 0.2378, 0.2298],\n",
      "        [0.2331, 0.2406, 0.2523, 0.2410],\n",
      "        [0.2362, 0.2418, 0.2396, 0.2339],\n",
      "        [0.2349, 0.2466, 0.2485, 0.2385],\n",
      "        [0.1906, 0.2337, 0.2520, 0.2043],\n",
      "        [0.2121, 0.2630, 0.2254, 0.2238],\n",
      "        [0.2207, 0.2591, 0.2397, 0.2279],\n",
      "        [0.2170, 0.2637, 0.2334, 0.2308],\n",
      "        [0.2159, 0.2526, 0.2467, 0.2175]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>), tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3612, 0.2548, 0.2914, 0.2643, 0.1914],\n",
      "        [0.4199, 0.3563, 0.3062, 0.3713, 0.3446],\n",
      "        [0.4448, 0.4223, 0.3966, 0.4288, 0.4016],\n",
      "        [0.4470, 0.4105, 0.4059, 0.4359, 0.4147],\n",
      "        [0.4486, 0.4294, 0.3917, 0.4111, 0.4071],\n",
      "        [0.4264, 0.3740, 0.3551, 0.3657, 0.3377],\n",
      "        [0.4625, 0.4085, 0.4029, 0.3901, 0.3841],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4209, 0.3753, 0.3134, 0.3532, 0.3722],\n",
      "        [0.4386, 0.3893, 0.3603, 0.3214, 0.3160],\n",
      "        [0.4789, 0.4400, 0.3969, 0.4296, 0.4092],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4511, 0.3904, 0.3842, 0.4149, 0.3867],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3962, 0.3352, 0.2788, 0.3457, 0.2853],\n",
      "        [0.4513, 0.4059, 0.3965, 0.4196, 0.3929],\n",
      "        [0.4575, 0.4307, 0.4204, 0.4211, 0.4463],\n",
      "        [0.3869, 0.3606, 0.2985, 0.3340, 0.2838],\n",
      "        [0.3376, 0.2999, 0.2975, 0.3496, 0.2662],\n",
      "        [0.4480, 0.4131, 0.4151, 0.4424, 0.4466],\n",
      "        [0.4533, 0.4177, 0.4028, 0.4151, 0.4180],\n",
      "        [0.3996, 0.3191, 0.3528, 0.3584, 0.3389],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4579, 0.3439, 0.3219, 0.3861, 0.3449],\n",
      "        [0.2978, 0.3150, 0.2716, 0.3062, 0.2869],\n",
      "        [0.4463, 0.3399, 0.3659, 0.3504, 0.3366],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4427, 0.4302, 0.4092, 0.4156, 0.4211],\n",
      "        [0.4161, 0.3803, 0.3884, 0.4052, 0.3304],\n",
      "        [0.4472, 0.4241, 0.4243, 0.4284, 0.4329],\n",
      "        [0.4336, 0.4156, 0.4032, 0.3919, 0.3882],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3806, 0.4013, 0.4338, 0.4100, 0.3409],\n",
      "        [0.4513, 0.3532, 0.3220, 0.3576, 0.3281],\n",
      "        [0.4488, 0.4034, 0.3799, 0.4106, 0.4050],\n",
      "        [0.3680, 0.3039, 0.3026, 0.2991, 0.2942],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4489, 0.4255, 0.4131, 0.4436, 0.4133],\n",
      "        [0.4094, 0.3238, 0.2555, 0.3069, 0.2008],\n",
      "        [0.4629, 0.3934, 0.3825, 0.4156, 0.3632],\n",
      "        [0.4244, 0.2869, 0.2726, 0.2604, 0.2263],\n",
      "        [0.4505, 0.4383, 0.4011, 0.4365, 0.4042],\n",
      "        [0.4679, 0.3530, 0.3480, 0.3757, 0.3534],\n",
      "        [0.4220, 0.3562, 0.3489, 0.3962, 0.3716],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4584, 0.4009, 0.4085, 0.4327, 0.4273],\n",
      "        [0.4641, 0.4214, 0.3938, 0.4222, 0.4048],\n",
      "        [0.3937, 0.2899, 0.2831, 0.3448, 0.2780],\n",
      "        [0.4566, 0.4232, 0.4207, 0.4458, 0.4302],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4602, 0.4397, 0.4178, 0.4233, 0.4357],\n",
      "        [0.4646, 0.4320, 0.3951, 0.4249, 0.4207],\n",
      "        [0.4091, 0.3976, 0.3746, 0.3763, 0.3744],\n",
      "        [0.4345, 0.4028, 0.3754, 0.4116, 0.3904],\n",
      "        [0.4631, 0.3608, 0.3336, 0.3752, 0.3154],\n",
      "        [0.4039, 0.2899, 0.3274, 0.3006, 0.2224],\n",
      "        [0.4427, 0.4142, 0.3909, 0.4185, 0.4063],\n",
      "        [0.4626, 0.4202, 0.4177, 0.4213, 0.4264],\n",
      "        [0.4448, 0.4239, 0.4166, 0.4044, 0.3985],\n",
      "        [0.4496, 0.3944, 0.3737, 0.3917, 0.4040],\n",
      "        [0.4424, 0.4241, 0.4025, 0.4070, 0.3963],\n",
      "        [0.3416, 0.3202, 0.2451, 0.3130, 0.2819],\n",
      "        [0.4382, 0.4245, 0.4045, 0.3980, 0.3834],\n",
      "        [0.4542, 0.4120, 0.4146, 0.4256, 0.4135],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4508, 0.3987, 0.4102, 0.4140, 0.4200],\n",
      "        [0.4432, 0.3906, 0.3945, 0.4007, 0.3990],\n",
      "        [0.4334, 0.3370, 0.3319, 0.3272, 0.3346],\n",
      "        [0.4411, 0.4164, 0.4324, 0.4541, 0.4434],\n",
      "        [0.4575, 0.4503, 0.4296, 0.4254, 0.4415],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4533, 0.3991, 0.3790, 0.4234, 0.3592],\n",
      "        [0.4505, 0.2965, 0.3373, 0.3353, 0.2670],\n",
      "        [0.4373, 0.3115, 0.3525, 0.3791, 0.3644],\n",
      "        [0.4517, 0.4060, 0.4088, 0.4192, 0.3818],\n",
      "        [0.4071, 0.3319, 0.3007, 0.3803, 0.3239],\n",
      "        [0.4281, 0.3892, 0.3715, 0.3950, 0.3866],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4483, 0.3934, 0.3766, 0.4007, 0.4047],\n",
      "        [0.4464, 0.3820, 0.3885, 0.3884, 0.3875],\n",
      "        [0.4254, 0.3725, 0.3263, 0.4019, 0.3476],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4723, 0.4183, 0.4156, 0.3976, 0.4110],\n",
      "        [0.4461, 0.3989, 0.3807, 0.3952, 0.3744],\n",
      "        [0.4581, 0.3596, 0.3589, 0.3699, 0.3218],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4335, 0.4031, 0.3835, 0.3896, 0.3517],\n",
      "        [0.4418, 0.4053, 0.3959, 0.4158, 0.4097],\n",
      "        [0.4642, 0.4053, 0.4177, 0.4035, 0.4008],\n",
      "        [0.4279, 0.3855, 0.3956, 0.4202, 0.3696],\n",
      "        [0.4641, 0.4428, 0.4038, 0.4056, 0.4108],\n",
      "        [0.4317, 0.3441, 0.3708, 0.3930, 0.3451],\n",
      "        [0.4081, 0.2869, 0.3264, 0.3374, 0.2870],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4357, 0.3518, 0.3022, 0.4057, 0.3464],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3676, 0.3536, 0.3315, 0.3642, 0.3565],\n",
      "        [0.4377, 0.3830, 0.3660, 0.3667, 0.3138],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4445, 0.3544, 0.3354, 0.4016, 0.3673],\n",
      "        [0.4226, 0.2838, 0.2849, 0.3767, 0.2935],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4359, 0.3940, 0.4020, 0.4102, 0.4035],\n",
      "        [0.3996, 0.3704, 0.3546, 0.3985, 0.3437],\n",
      "        [0.4597, 0.4458, 0.4086, 0.4172, 0.4292],\n",
      "        [0.4648, 0.4395, 0.4044, 0.4329, 0.4193],\n",
      "        [0.4393, 0.3119, 0.3228, 0.3353, 0.3367],\n",
      "        [0.4111, 0.3635, 0.2981, 0.3685, 0.3650],\n",
      "        [0.4175, 0.3876, 0.4028, 0.4139, 0.4123],\n",
      "        [0.4368, 0.4275, 0.4278, 0.4195, 0.4273],\n",
      "        [0.4664, 0.4409, 0.4083, 0.4153, 0.4147],\n",
      "        [0.3817, 0.2981, 0.3053, 0.3389, 0.2703],\n",
      "        [0.4426, 0.3304, 0.2937, 0.3425, 0.3089],\n",
      "        [0.4139, 0.3536, 0.3712, 0.3563, 0.3864],\n",
      "        [0.4498, 0.4200, 0.3908, 0.4070, 0.3985],\n",
      "        [0.4614, 0.4273, 0.4153, 0.4442, 0.4224],\n",
      "        [0.4665, 0.4246, 0.3983, 0.4263, 0.4334],\n",
      "        [0.4820, 0.4143, 0.4105, 0.4293, 0.4108],\n",
      "        [0.4565, 0.4331, 0.4083, 0.4186, 0.4390],\n",
      "        [0.4325, 0.3158, 0.3356, 0.3885, 0.3347],\n",
      "        [0.4257, 0.3581, 0.4001, 0.4090, 0.3530],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4522, 0.3984, 0.4184, 0.4116, 0.3855],\n",
      "        [0.4628, 0.4299, 0.3772, 0.4172, 0.3933],\n",
      "        [0.4322, 0.4192, 0.3755, 0.3972, 0.3922],\n",
      "        [0.4603, 0.4119, 0.3986, 0.4200, 0.4102],\n",
      "        [0.4503, 0.4245, 0.4071, 0.4094, 0.4231],\n",
      "        [0.4369, 0.3955, 0.4309, 0.4319, 0.4061],\n",
      "        [0.4554, 0.4187, 0.3760, 0.4239, 0.3863],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4759, 0.3916, 0.3822, 0.4207, 0.4099],\n",
      "        [0.4503, 0.4291, 0.3945, 0.4508, 0.4059],\n",
      "        [0.4443, 0.4091, 0.3903, 0.4138, 0.4015],\n",
      "        [0.4496, 0.4345, 0.4074, 0.4330, 0.4170],\n",
      "        [0.3753, 0.2760, 0.2460, 0.3432, 0.2537],\n",
      "        [0.4829, 0.3539, 0.4051, 0.3688, 0.3263],\n",
      "        [0.4496, 0.3945, 0.4078, 0.4091, 0.3809],\n",
      "        [0.4345, 0.3894, 0.3809, 0.4096, 0.3975],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)], '95%': [tensor([[0.4516, 0.3202, 0.3034],\n",
      "        [0.7049, 0.2738, 0.1793],\n",
      "        [0.4914, 0.3593, 0.2453],\n",
      "        [0.3835, 0.3648, 0.2985],\n",
      "        [0.3894, 0.3593, 0.2867],\n",
      "        [0.4260, 0.3154, 0.2930],\n",
      "        [0.4754, 0.3464, 0.2533],\n",
      "        [0.4440, 0.3225, 0.3099],\n",
      "        [0.7304, 0.2455, 0.2363],\n",
      "        [0.4282, 0.3274, 0.3057],\n",
      "        [0.5311, 0.3224, 0.2700],\n",
      "        [0.4801, 0.3219, 0.2834],\n",
      "        [0.4313, 0.3314, 0.2880],\n",
      "        [0.4117, 0.3108, 0.3171],\n",
      "        [0.3982, 0.3604, 0.2776],\n",
      "        [0.4186, 0.3431, 0.2892],\n",
      "        [0.5903, 0.3451, 0.2640],\n",
      "        [0.4490, 0.3383, 0.2762],\n",
      "        [0.4297, 0.3271, 0.3048],\n",
      "        [0.5708, 0.3410, 0.2211],\n",
      "        [0.5011, 0.4066, 0.2315],\n",
      "        [0.4280, 0.3397, 0.2982],\n",
      "        [0.4034, 0.3514, 0.2848],\n",
      "        [0.4673, 0.3954, 0.2561],\n",
      "        [0.7179, 0.3007, 0.2356],\n",
      "        [0.4737, 0.3528, 0.2712],\n",
      "        [0.6881, 0.3096, 0.2474],\n",
      "        [0.5478, 0.3270, 0.2616],\n",
      "        [0.4239, 0.3215, 0.3031],\n",
      "        [0.4292, 0.3288, 0.3008],\n",
      "        [0.4327, 0.3241, 0.2979],\n",
      "        [0.4836, 0.3426, 0.2578],\n",
      "        [0.4114, 0.3236, 0.3098],\n",
      "        [0.4005, 0.3708, 0.2887],\n",
      "        [0.4835, 0.2976, 0.2915],\n",
      "        [0.4856, 0.3454, 0.2675],\n",
      "        [0.5118, 0.3537, 0.2299],\n",
      "        [0.3932, 0.3376, 0.3262],\n",
      "        [0.5166, 0.3997, 0.1957],\n",
      "        [0.4223, 0.3236, 0.2979],\n",
      "        [0.4084, 0.3621, 0.2956],\n",
      "        [0.5730, 0.3594, 0.2440],\n",
      "        [0.3942, 0.3856, 0.2993],\n",
      "        [0.7450, 0.2782, 0.2060],\n",
      "        [0.4477, 0.3268, 0.2909],\n",
      "        [0.5293, 0.2838, 0.2584],\n",
      "        [0.4175, 0.3548, 0.2894],\n",
      "        [0.5298, 0.3013, 0.2554],\n",
      "        [0.4018, 0.3404, 0.2984],\n",
      "        [0.4201, 0.3418, 0.2884],\n",
      "        [0.5900, 0.3542, 0.1846],\n",
      "        [0.3803, 0.3670, 0.3001],\n",
      "        [0.5822, 0.3097, 0.2547],\n",
      "        [0.4002, 0.3336, 0.3065],\n",
      "        [0.4256, 0.3203, 0.3088],\n",
      "        [0.4781, 0.3568, 0.2525],\n",
      "        [0.4449, 0.3464, 0.2715],\n",
      "        [0.5665, 0.2940, 0.2409],\n",
      "        [0.6605, 0.3346, 0.1868],\n",
      "        [0.4254, 0.3360, 0.2767],\n",
      "        [0.4171, 0.3182, 0.3101],\n",
      "        [0.4372, 0.3330, 0.2790],\n",
      "        [0.4212, 0.3558, 0.2703],\n",
      "        [0.4494, 0.3374, 0.2701],\n",
      "        [0.7323, 0.2742, 0.1923],\n",
      "        [0.4253, 0.3114, 0.3094],\n",
      "        [0.4368, 0.3198, 0.2953],\n",
      "        [0.4702, 0.3463, 0.2454],\n",
      "        [0.3953, 0.3338, 0.3182],\n",
      "        [0.4548, 0.3309, 0.2853],\n",
      "        [0.5201, 0.3523, 0.2239],\n",
      "        [0.3859, 0.3611, 0.2935],\n",
      "        [0.4182, 0.3246, 0.3068],\n",
      "        [0.4600, 0.3092, 0.2912],\n",
      "        [0.4956, 0.3582, 0.2520],\n",
      "        [0.5462, 0.3569, 0.2271],\n",
      "        [0.4560, 0.3958, 0.2353],\n",
      "        [0.4141, 0.3917, 0.2712],\n",
      "        [0.5338, 0.3456, 0.2277],\n",
      "        [0.4557, 0.3399, 0.2721],\n",
      "        [0.4495, 0.3228, 0.2956],\n",
      "        [0.4623, 0.3175, 0.2890],\n",
      "        [0.4314, 0.3709, 0.2755],\n",
      "        [0.4733, 0.3533, 0.2609],\n",
      "        [0.5162, 0.3189, 0.2704],\n",
      "        [0.4440, 0.3155, 0.2978],\n",
      "        [0.4332, 0.3634, 0.2648],\n",
      "        [0.5053, 0.3282, 0.2554],\n",
      "        [0.4596, 0.3047, 0.2834],\n",
      "        [0.4370, 0.3213, 0.3072],\n",
      "        [0.4558, 0.3290, 0.2828],\n",
      "        [0.4459, 0.3186, 0.2947],\n",
      "        [0.4284, 0.3746, 0.2567],\n",
      "        [0.4468, 0.3180, 0.2853],\n",
      "        [0.4931, 0.3837, 0.2320],\n",
      "        [0.6156, 0.3431, 0.2016],\n",
      "        [0.4373, 0.3365, 0.2818],\n",
      "        [0.5667, 0.3502, 0.2483],\n",
      "        [0.4903, 0.3046, 0.2657],\n",
      "        [0.4609, 0.3917, 0.2363],\n",
      "        [0.4968, 0.3383, 0.2460],\n",
      "        [0.5362, 0.3595, 0.2296],\n",
      "        [0.5419, 0.3179, 0.2433],\n",
      "        [0.5708, 0.3611, 0.1821],\n",
      "        [0.4033, 0.3118, 0.3267],\n",
      "        [0.5340, 0.3196, 0.2747],\n",
      "        [0.4951, 0.3119, 0.2759],\n",
      "        [0.6106, 0.2835, 0.2492],\n",
      "        [0.5209, 0.2980, 0.2676],\n",
      "        [0.4337, 0.3189, 0.3150],\n",
      "        [0.4589, 0.3345, 0.2598],\n",
      "        [0.4013, 0.3139, 0.3212],\n",
      "        [0.4332, 0.3286, 0.2956],\n",
      "        [0.5930, 0.3129, 0.2497],\n",
      "        [0.5514, 0.2986, 0.2305],\n",
      "        [0.4388, 0.3410, 0.2984],\n",
      "        [0.4160, 0.3261, 0.3031],\n",
      "        [0.4266, 0.3331, 0.2865],\n",
      "        [0.5082, 0.3896, 0.1969],\n",
      "        [0.6578, 0.2656, 0.1849],\n",
      "        [0.4786, 0.3327, 0.2747],\n",
      "        [0.4283, 0.3441, 0.2913],\n",
      "        [0.3896, 0.3542, 0.2996],\n",
      "        [0.3968, 0.3368, 0.3114],\n",
      "        [0.4207, 0.3574, 0.2824],\n",
      "        [0.4189, 0.3147, 0.3017],\n",
      "        [0.4763, 0.3852, 0.2376],\n",
      "        [0.5369, 0.2920, 0.2509],\n",
      "        [0.6773, 0.3004, 0.1870],\n",
      "        [0.4394, 0.3219, 0.2938],\n",
      "        [0.4531, 0.3436, 0.2662],\n",
      "        [0.4130, 0.3213, 0.3194],\n",
      "        [0.4472, 0.3472, 0.2687],\n",
      "        [0.3915, 0.3495, 0.3046],\n",
      "        [0.4235, 0.3495, 0.2840],\n",
      "        [0.3973, 0.3611, 0.2899],\n",
      "        [0.4044, 0.3532, 0.2953],\n",
      "        [0.7119, 0.2564, 0.1749],\n",
      "        [0.4449, 0.3591, 0.2657],\n",
      "        [0.3909, 0.3622, 0.2934],\n",
      "        [0.4330, 0.3413, 0.2680],\n",
      "        [0.4040, 0.3510, 0.2889],\n",
      "        [0.6519, 0.3481, 0.1878],\n",
      "        [0.5074, 0.3193, 0.2421],\n",
      "        [0.4011, 0.3860, 0.2678],\n",
      "        [0.4781, 0.3098, 0.2746],\n",
      "        [0.4539, 0.3271, 0.3014]], device='cuda:0', grad_fn=<ViewBackward0>), tensor([[0.3816, 0.3591, 0.3132],\n",
      "        [0.5005, 0.4397, 0.2777],\n",
      "        [0.4013, 0.3605, 0.2980],\n",
      "        [0.3808, 0.3439, 0.3168],\n",
      "        [0.3842, 0.3387, 0.3136],\n",
      "        [0.3588, 0.3776, 0.3107],\n",
      "        [0.3615, 0.3710, 0.3396],\n",
      "        [0.3615, 0.3722, 0.3151],\n",
      "        [0.3963, 0.4507, 0.2999],\n",
      "        [0.3658, 0.3620, 0.3221],\n",
      "        [0.3733, 0.3909, 0.3168],\n",
      "        [0.3678, 0.4021, 0.2976],\n",
      "        [0.3614, 0.3555, 0.3222],\n",
      "        [0.3581, 0.3874, 0.3105],\n",
      "        [0.3776, 0.3425, 0.3246],\n",
      "        [0.3757, 0.3601, 0.3100],\n",
      "        [0.4079, 0.3649, 0.3303],\n",
      "        [0.3592, 0.3572, 0.3431],\n",
      "        [0.3469, 0.3527, 0.3300],\n",
      "        [0.3755, 0.4135, 0.3313],\n",
      "        [0.3657, 0.3203, 0.3934],\n",
      "        [0.3735, 0.3596, 0.3131],\n",
      "        [0.3679, 0.3378, 0.3312],\n",
      "        [0.3947, 0.3736, 0.3042],\n",
      "        [0.3964, 0.4772, 0.2715],\n",
      "        [0.4119, 0.3824, 0.2977],\n",
      "        [0.4143, 0.4544, 0.2893],\n",
      "        [0.3945, 0.3934, 0.3059],\n",
      "        [0.3754, 0.3468, 0.3281],\n",
      "        [0.3677, 0.3656, 0.3200],\n",
      "        [0.3667, 0.3643, 0.3108],\n",
      "        [0.3689, 0.3657, 0.3226],\n",
      "        [0.3545, 0.3537, 0.3172],\n",
      "        [0.3726, 0.3715, 0.3067],\n",
      "        [0.3764, 0.3587, 0.3337],\n",
      "        [0.3712, 0.3444, 0.3480],\n",
      "        [0.4501, 0.3348, 0.3020],\n",
      "        [0.3683, 0.3524, 0.3143],\n",
      "        [0.3984, 0.3747, 0.3275],\n",
      "        [0.3679, 0.3623, 0.3181],\n",
      "        [0.3761, 0.3527, 0.3219],\n",
      "        [0.4562, 0.4175, 0.2694],\n",
      "        [0.3865, 0.3487, 0.3174],\n",
      "        [0.4165, 0.4173, 0.3199],\n",
      "        [0.3667, 0.3672, 0.3107],\n",
      "        [0.3672, 0.3924, 0.3123],\n",
      "        [0.4192, 0.3551, 0.3031],\n",
      "        [0.3773, 0.4171, 0.2836],\n",
      "        [0.3600, 0.3413, 0.3292],\n",
      "        [0.3678, 0.3627, 0.3115],\n",
      "        [0.4293, 0.3827, 0.3004],\n",
      "        [0.3635, 0.3425, 0.3222],\n",
      "        [0.4241, 0.3613, 0.3214],\n",
      "        [0.3614, 0.3716, 0.3110],\n",
      "        [0.3661, 0.3597, 0.3155],\n",
      "        [0.3720, 0.3657, 0.3224],\n",
      "        [0.3913, 0.3448, 0.3077],\n",
      "        [0.3641, 0.3921, 0.3410],\n",
      "        [0.4930, 0.4256, 0.2692],\n",
      "        [0.3690, 0.3523, 0.3163],\n",
      "        [0.3486, 0.3581, 0.3210],\n",
      "        [0.3378, 0.3458, 0.3518],\n",
      "        [0.3729, 0.3588, 0.3227],\n",
      "        [0.3673, 0.3793, 0.3054],\n",
      "        [0.3775, 0.4396, 0.3064],\n",
      "        [0.3593, 0.3712, 0.3140],\n",
      "        [0.3517, 0.3685, 0.3161],\n",
      "        [0.3730, 0.3600, 0.3267],\n",
      "        [0.3533, 0.3502, 0.3251],\n",
      "        [0.3755, 0.3538, 0.3270],\n",
      "        [0.3753, 0.3673, 0.3510],\n",
      "        [0.3729, 0.3475, 0.3222],\n",
      "        [0.3509, 0.3632, 0.3231],\n",
      "        [0.3622, 0.3783, 0.3046],\n",
      "        [0.3898, 0.3417, 0.3393],\n",
      "        [0.3849, 0.4031, 0.3165],\n",
      "        [0.3628, 0.3905, 0.3137],\n",
      "        [0.3713, 0.3439, 0.3482],\n",
      "        [0.4020, 0.3473, 0.3422],\n",
      "        [0.3883, 0.3602, 0.3097],\n",
      "        [0.3632, 0.3622, 0.3028],\n",
      "        [0.3766, 0.3708, 0.3003],\n",
      "        [0.3966, 0.3546, 0.3241],\n",
      "        [0.3828, 0.3950, 0.3034],\n",
      "        [0.4145, 0.3709, 0.3054],\n",
      "        [0.3549, 0.3906, 0.3036],\n",
      "        [0.3700, 0.3478, 0.3203],\n",
      "        [0.3677, 0.4182, 0.2941],\n",
      "        [0.3541, 0.3739, 0.3120],\n",
      "        [0.4208, 0.3578, 0.3025],\n",
      "        [0.3697, 0.3566, 0.3156],\n",
      "        [0.3634, 0.3672, 0.3127],\n",
      "        [0.3811, 0.3329, 0.3248],\n",
      "        [0.3712, 0.3678, 0.3053],\n",
      "        [0.3812, 0.3931, 0.3025],\n",
      "        [0.3824, 0.3689, 0.3551],\n",
      "        [0.3717, 0.3630, 0.3213],\n",
      "        [0.3668, 0.3927, 0.3395],\n",
      "        [0.3911, 0.3465, 0.3398],\n",
      "        [0.4224, 0.3716, 0.3062],\n",
      "        [0.3855, 0.3631, 0.3203],\n",
      "        [0.3773, 0.3714, 0.3338],\n",
      "        [0.4020, 0.3662, 0.3250],\n",
      "        [0.4325, 0.4013, 0.3688],\n",
      "        [0.3607, 0.3657, 0.3192],\n",
      "        [0.3781, 0.3703, 0.3224],\n",
      "        [0.3611, 0.3515, 0.3398],\n",
      "        [0.3925, 0.3699, 0.3199],\n",
      "        [0.3778, 0.3885, 0.3031],\n",
      "        [0.3827, 0.3616, 0.3055],\n",
      "        [0.3746, 0.3384, 0.3725],\n",
      "        [0.3525, 0.3594, 0.3258],\n",
      "        [0.3568, 0.3600, 0.3240],\n",
      "        [0.4164, 0.3611, 0.3174],\n",
      "        [0.4052, 0.3451, 0.3184],\n",
      "        [0.3713, 0.3719, 0.3190],\n",
      "        [0.3566, 0.3660, 0.3164],\n",
      "        [0.3716, 0.3511, 0.3198],\n",
      "        [0.4275, 0.3531, 0.2981],\n",
      "        [0.3836, 0.4011, 0.3592],\n",
      "        [0.3833, 0.3529, 0.3189],\n",
      "        [0.3677, 0.3413, 0.3323],\n",
      "        [0.3748, 0.3427, 0.3136],\n",
      "        [0.3660, 0.3530, 0.3133],\n",
      "        [0.3898, 0.3432, 0.3152],\n",
      "        [0.3569, 0.3628, 0.3252],\n",
      "        [0.3829, 0.3722, 0.3200],\n",
      "        [0.3689, 0.3919, 0.3089],\n",
      "        [0.3903, 0.3797, 0.3604],\n",
      "        [0.3603, 0.3773, 0.3042],\n",
      "        [0.3837, 0.3693, 0.3121],\n",
      "        [0.3612, 0.3619, 0.3115],\n",
      "        [0.3705, 0.3545, 0.3381],\n",
      "        [0.3762, 0.3646, 0.3166],\n",
      "        [0.3742, 0.3663, 0.3165],\n",
      "        [0.3604, 0.3535, 0.3232],\n",
      "        [0.3682, 0.3474, 0.3272],\n",
      "        [0.3827, 0.4090, 0.3078],\n",
      "        [0.3727, 0.3587, 0.3238],\n",
      "        [0.3880, 0.3379, 0.3127],\n",
      "        [0.3696, 0.3449, 0.3273],\n",
      "        [0.3575, 0.3475, 0.3233],\n",
      "        [0.4361, 0.3843, 0.3186],\n",
      "        [0.3779, 0.4100, 0.2922],\n",
      "        [0.3533, 0.3672, 0.3518],\n",
      "        [0.3505, 0.3387, 0.3518],\n",
      "        [0.4039, 0.3651, 0.3094]], device='cuda:0', grad_fn=<ViewBackward0>), tensor([[0.2408, 0.2808, 0.2876, 0.2605],\n",
      "        [0.2281, 0.4115, 0.2476, 0.2536],\n",
      "        [0.2580, 0.2749, 0.2631, 0.2810],\n",
      "        [0.2573, 0.2674, 0.2692, 0.2571],\n",
      "        [0.2518, 0.2677, 0.2627, 0.2587],\n",
      "        [0.2430, 0.2664, 0.2849, 0.2559],\n",
      "        [0.2450, 0.2901, 0.2786, 0.2580],\n",
      "        [0.2401, 0.2738, 0.2792, 0.2621],\n",
      "        [0.2461, 0.3158, 0.3060, 0.2488],\n",
      "        [0.2528, 0.2697, 0.2675, 0.2634],\n",
      "        [0.2476, 0.2842, 0.2726, 0.2584],\n",
      "        [0.2428, 0.2777, 0.2973, 0.2716],\n",
      "        [0.2522, 0.2598, 0.2676, 0.2566],\n",
      "        [0.2506, 0.2711, 0.2660, 0.2555],\n",
      "        [0.2488, 0.2715, 0.2650, 0.2571],\n",
      "        [0.2531, 0.2741, 0.2672, 0.2493],\n",
      "        [0.2534, 0.2759, 0.2789, 0.2775],\n",
      "        [0.2430, 0.2840, 0.2731, 0.2534],\n",
      "        [0.2487, 0.2659, 0.2579, 0.2713],\n",
      "        [0.2550, 0.2926, 0.2823, 0.2814],\n",
      "        [0.2478, 0.2984, 0.2713, 0.2686],\n",
      "        [0.2517, 0.2782, 0.2780, 0.2465],\n",
      "        [0.2515, 0.2662, 0.2675, 0.2590],\n",
      "        [0.2433, 0.2908, 0.2838, 0.2559],\n",
      "        [0.2472, 0.3486, 0.2664, 0.2760],\n",
      "        [0.2637, 0.3067, 0.2657, 0.2492],\n",
      "        [0.2503, 0.3708, 0.2508, 0.2709],\n",
      "        [0.2373, 0.3085, 0.2738, 0.2581],\n",
      "        [0.2529, 0.2769, 0.2730, 0.2557],\n",
      "        [0.2479, 0.2719, 0.2786, 0.2531],\n",
      "        [0.2503, 0.2696, 0.2653, 0.2606],\n",
      "        [0.2452, 0.2940, 0.2660, 0.2490],\n",
      "        [0.2443, 0.2653, 0.2716, 0.2591],\n",
      "        [0.2491, 0.2656, 0.2785, 0.2585],\n",
      "        [0.2447, 0.2932, 0.2668, 0.2490],\n",
      "        [0.2463, 0.2689, 0.2782, 0.2771],\n",
      "        [0.2634, 0.2864, 0.2896, 0.2498],\n",
      "        [0.2546, 0.2772, 0.2640, 0.2598],\n",
      "        [0.2589, 0.3203, 0.2520, 0.2544],\n",
      "        [0.2455, 0.2643, 0.2686, 0.2608],\n",
      "        [0.2543, 0.2629, 0.2679, 0.2642],\n",
      "        [0.2566, 0.3412, 0.2872, 0.2431],\n",
      "        [0.2416, 0.2920, 0.2587, 0.2552],\n",
      "        [0.2243, 0.3138, 0.3261, 0.2553],\n",
      "        [0.2391, 0.2626, 0.2838, 0.2650],\n",
      "        [0.2500, 0.2866, 0.2890, 0.2621],\n",
      "        [0.2539, 0.2727, 0.2740, 0.2561],\n",
      "        [0.2523, 0.2795, 0.2696, 0.2604],\n",
      "        [0.2517, 0.2660, 0.2616, 0.2553],\n",
      "        [0.2486, 0.2657, 0.2699, 0.2545],\n",
      "        [0.2558, 0.3254, 0.2728, 0.2579],\n",
      "        [0.2521, 0.2614, 0.2661, 0.2629],\n",
      "        [0.2419, 0.3017, 0.2740, 0.2781],\n",
      "        [0.2482, 0.2638, 0.2711, 0.2582],\n",
      "        [0.2426, 0.2638, 0.2826, 0.2569],\n",
      "        [0.2481, 0.2789, 0.2765, 0.2548],\n",
      "        [0.2451, 0.2588, 0.2809, 0.2586],\n",
      "        [0.2480, 0.3065, 0.2683, 0.2678],\n",
      "        [0.2246, 0.4030, 0.2721, 0.2663],\n",
      "        [0.2499, 0.2681, 0.2662, 0.2555],\n",
      "        [0.2475, 0.2706, 0.2743, 0.2629],\n",
      "        [0.2414, 0.2832, 0.2656, 0.2570],\n",
      "        [0.2405, 0.2852, 0.2607, 0.2682],\n",
      "        [0.2443, 0.2919, 0.2662, 0.2564],\n",
      "        [0.2492, 0.3315, 0.2779, 0.2734],\n",
      "        [0.2384, 0.2666, 0.2691, 0.2633],\n",
      "        [0.2418, 0.2825, 0.2636, 0.2589],\n",
      "        [0.2494, 0.2894, 0.2657, 0.2534],\n",
      "        [0.2536, 0.2657, 0.2636, 0.2568],\n",
      "        [0.2485, 0.2846, 0.2617, 0.2651],\n",
      "        [0.2493, 0.3073, 0.2673, 0.2612],\n",
      "        [0.2535, 0.2740, 0.2613, 0.2527],\n",
      "        [0.2426, 0.2658, 0.2729, 0.2625],\n",
      "        [0.2568, 0.2860, 0.2705, 0.2447],\n",
      "        [0.2460, 0.2791, 0.2779, 0.2591],\n",
      "        [0.2582, 0.3132, 0.2660, 0.2545],\n",
      "        [0.2695, 0.3087, 0.2731, 0.2558],\n",
      "        [0.2496, 0.2763, 0.2635, 0.2626],\n",
      "        [0.2440, 0.3050, 0.2735, 0.2749],\n",
      "        [0.2423, 0.2944, 0.2710, 0.2498],\n",
      "        [0.2465, 0.2682, 0.2823, 0.2551],\n",
      "        [0.2505, 0.2648, 0.2786, 0.2591],\n",
      "        [0.2389, 0.2886, 0.2637, 0.2720],\n",
      "        [0.2711, 0.2838, 0.2509, 0.2579],\n",
      "        [0.2532, 0.3026, 0.2699, 0.2552],\n",
      "        [0.2541, 0.2616, 0.2740, 0.2670],\n",
      "        [0.2412, 0.2843, 0.2771, 0.2606],\n",
      "        [0.2471, 0.2751, 0.2706, 0.2741],\n",
      "        [0.2486, 0.2641, 0.2733, 0.2559],\n",
      "        [0.2428, 0.2879, 0.2895, 0.2514],\n",
      "        [0.2469, 0.2667, 0.2686, 0.2577],\n",
      "        [0.2472, 0.2726, 0.2758, 0.2578],\n",
      "        [0.2470, 0.2863, 0.2528, 0.2775],\n",
      "        [0.2410, 0.2683, 0.2715, 0.2642],\n",
      "        [0.2579, 0.2938, 0.2636, 0.2711],\n",
      "        [0.2391, 0.3712, 0.2763, 0.2795],\n",
      "        [0.2529, 0.2916, 0.2617, 0.2495],\n",
      "        [0.2372, 0.2946, 0.2831, 0.2687],\n",
      "        [0.2565, 0.2885, 0.2767, 0.2506],\n",
      "        [0.2366, 0.3043, 0.2717, 0.2701],\n",
      "        [0.2329, 0.2986, 0.2639, 0.2780],\n",
      "        [0.2566, 0.3007, 0.2676, 0.2629],\n",
      "        [0.2482, 0.3101, 0.2606, 0.2482],\n",
      "        [0.2491, 0.3630, 0.3104, 0.2327],\n",
      "        [0.2493, 0.2689, 0.2730, 0.2511],\n",
      "        [0.2511, 0.2838, 0.2752, 0.2549],\n",
      "        [0.2489, 0.2843, 0.2621, 0.2488],\n",
      "        [0.2332, 0.3123, 0.2938, 0.2534],\n",
      "        [0.2474, 0.2982, 0.2643, 0.2685],\n",
      "        [0.2429, 0.2736, 0.2759, 0.2580],\n",
      "        [0.2502, 0.2731, 0.2691, 0.2563],\n",
      "        [0.2443, 0.2716, 0.2738, 0.2595],\n",
      "        [0.2526, 0.2634, 0.2707, 0.2595],\n",
      "        [0.2416, 0.3113, 0.2649, 0.2679],\n",
      "        [0.2578, 0.2944, 0.2575, 0.2677],\n",
      "        [0.2403, 0.2884, 0.2661, 0.2606],\n",
      "        [0.2411, 0.2702, 0.2604, 0.2686],\n",
      "        [0.2520, 0.2619, 0.2659, 0.2574],\n",
      "        [0.2593, 0.3214, 0.2664, 0.2601],\n",
      "        [0.2359, 0.3421, 0.2874, 0.2681],\n",
      "        [0.2453, 0.2672, 0.2631, 0.2760],\n",
      "        [0.2495, 0.2636, 0.2719, 0.2609],\n",
      "        [0.2485, 0.2677, 0.2649, 0.2592],\n",
      "        [0.2471, 0.2599, 0.2773, 0.2579],\n",
      "        [0.2411, 0.2856, 0.2735, 0.2504],\n",
      "        [0.2476, 0.2650, 0.2695, 0.2562],\n",
      "        [0.2449, 0.2966, 0.2692, 0.2681],\n",
      "        [0.2489, 0.2909, 0.2717, 0.2710],\n",
      "        [0.2421, 0.3644, 0.2906, 0.2292],\n",
      "        [0.2533, 0.2732, 0.2574, 0.2659],\n",
      "        [0.2502, 0.2732, 0.2581, 0.2786],\n",
      "        [0.2406, 0.2699, 0.2747, 0.2587],\n",
      "        [0.2382, 0.3033, 0.2601, 0.2558],\n",
      "        [0.2443, 0.2863, 0.2625, 0.2565],\n",
      "        [0.2483, 0.2765, 0.2686, 0.2538],\n",
      "        [0.2440, 0.2722, 0.2640, 0.2637],\n",
      "        [0.2455, 0.2647, 0.2665, 0.2569],\n",
      "        [0.2260, 0.3463, 0.2999, 0.2391],\n",
      "        [0.2392, 0.2816, 0.2666, 0.2603],\n",
      "        [0.2486, 0.2669, 0.2675, 0.2547],\n",
      "        [0.2563, 0.2622, 0.2695, 0.2576],\n",
      "        [0.2442, 0.2634, 0.2661, 0.2571],\n",
      "        [0.2509, 0.3040, 0.3109, 0.2540],\n",
      "        [0.2449, 0.3135, 0.2644, 0.2614],\n",
      "        [0.2476, 0.2962, 0.2571, 0.2541],\n",
      "        [0.2402, 0.2911, 0.2749, 0.2537],\n",
      "        [0.2478, 0.2778, 0.2773, 0.2615]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>), tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5253, 0.4357, 0.4359, 0.4135, 0.3760],\n",
      "        [0.4921, 0.4636, 0.3910, 0.4564, 0.4459],\n",
      "        [0.4936, 0.4660, 0.4553, 0.4783, 0.4628],\n",
      "        [0.4965, 0.4552, 0.4441, 0.4743, 0.4570],\n",
      "        [0.5010, 0.4812, 0.4468, 0.4743, 0.4652],\n",
      "        [0.5073, 0.4836, 0.4423, 0.4428, 0.4140],\n",
      "        [0.5288, 0.4729, 0.4608, 0.4686, 0.4605],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5162, 0.4461, 0.4050, 0.4222, 0.4549],\n",
      "        [0.5033, 0.4870, 0.4420, 0.4561, 0.4357],\n",
      "        [0.5155, 0.4767, 0.4420, 0.4829, 0.4576],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4903, 0.4558, 0.4367, 0.4773, 0.4528],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4986, 0.4720, 0.4186, 0.4625, 0.4095],\n",
      "        [0.5188, 0.4769, 0.4608, 0.4834, 0.4424],\n",
      "        [0.4848, 0.4847, 0.4578, 0.4610, 0.4824],\n",
      "        [0.4838, 0.4757, 0.3910, 0.4238, 0.3615],\n",
      "        [0.4669, 0.4575, 0.4188, 0.4466, 0.3746],\n",
      "        [0.4976, 0.4786, 0.4590, 0.4909, 0.4830],\n",
      "        [0.4993, 0.4624, 0.4465, 0.4555, 0.4663],\n",
      "        [0.4940, 0.4186, 0.4550, 0.4513, 0.4374],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5827, 0.4212, 0.3989, 0.4667, 0.4349],\n",
      "        [0.4799, 0.4989, 0.3871, 0.4326, 0.4633],\n",
      "        [0.5416, 0.4538, 0.4442, 0.4620, 0.4278],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4958, 0.4729, 0.4668, 0.4618, 0.4674],\n",
      "        [0.5012, 0.4794, 0.4547, 0.4784, 0.4235],\n",
      "        [0.5012, 0.4903, 0.4661, 0.4593, 0.4809],\n",
      "        [0.4916, 0.4586, 0.4406, 0.4702, 0.4536],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4835, 0.4716, 0.4913, 0.4904, 0.4733],\n",
      "        [0.5691, 0.4323, 0.4072, 0.4220, 0.4017],\n",
      "        [0.5032, 0.4653, 0.4348, 0.4607, 0.4580],\n",
      "        [0.4652, 0.4322, 0.4085, 0.4139, 0.4346],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5084, 0.4862, 0.4566, 0.4816, 0.4622],\n",
      "        [0.5441, 0.4856, 0.4168, 0.4381, 0.3648],\n",
      "        [0.5238, 0.4537, 0.4257, 0.4836, 0.4325],\n",
      "        [0.5684, 0.4521, 0.3803, 0.4476, 0.3524],\n",
      "        [0.5081, 0.4811, 0.4549, 0.4780, 0.4674],\n",
      "        [0.5392, 0.4543, 0.4548, 0.4484, 0.4355],\n",
      "        [0.4821, 0.4307, 0.3895, 0.4773, 0.4239],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4969, 0.4513, 0.4431, 0.4676, 0.4616],\n",
      "        [0.5217, 0.4756, 0.4461, 0.4760, 0.4713],\n",
      "        [0.5474, 0.4481, 0.4132, 0.4619, 0.3905],\n",
      "        [0.5035, 0.4607, 0.4408, 0.4939, 0.4783],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5043, 0.4957, 0.4565, 0.4671, 0.4648],\n",
      "        [0.5261, 0.4734, 0.4425, 0.4710, 0.4679],\n",
      "        [0.4888, 0.4591, 0.4353, 0.4637, 0.4673],\n",
      "        [0.4998, 0.4447, 0.4307, 0.4816, 0.4417],\n",
      "        [0.5684, 0.4406, 0.4423, 0.4593, 0.4230],\n",
      "        [0.5841, 0.4642, 0.4813, 0.4135, 0.3556],\n",
      "        [0.5003, 0.4593, 0.4460, 0.4731, 0.4480],\n",
      "        [0.5013, 0.4777, 0.4648, 0.4580, 0.4768],\n",
      "        [0.4978, 0.4762, 0.4624, 0.4714, 0.4541],\n",
      "        [0.5113, 0.4742, 0.4429, 0.4603, 0.4809],\n",
      "        [0.5101, 0.4872, 0.4614, 0.4609, 0.4562],\n",
      "        [0.4828, 0.4719, 0.3937, 0.4297, 0.4182],\n",
      "        [0.4936, 0.4763, 0.4603, 0.4534, 0.4566],\n",
      "        [0.5021, 0.4656, 0.4637, 0.4715, 0.4812],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4924, 0.4639, 0.4588, 0.4547, 0.4557],\n",
      "        [0.5074, 0.4732, 0.4564, 0.4689, 0.4715],\n",
      "        [0.5268, 0.4398, 0.3953, 0.4571, 0.4184],\n",
      "        [0.5008, 0.4645, 0.4565, 0.4959, 0.4817],\n",
      "        [0.5074, 0.4903, 0.4687, 0.4707, 0.4879],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5273, 0.4623, 0.4536, 0.4698, 0.4290],\n",
      "        [0.5439, 0.4474, 0.4306, 0.4894, 0.4035],\n",
      "        [0.5268, 0.3954, 0.4452, 0.4793, 0.4323],\n",
      "        [0.5078, 0.4663, 0.4744, 0.4905, 0.4458],\n",
      "        [0.5341, 0.4461, 0.3752, 0.4772, 0.4250],\n",
      "        [0.5134, 0.4605, 0.4425, 0.4580, 0.4541],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5164, 0.4814, 0.4547, 0.4679, 0.5036],\n",
      "        [0.5044, 0.4705, 0.4705, 0.4620, 0.4511],\n",
      "        [0.5174, 0.4398, 0.4153, 0.4567, 0.4369],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5075, 0.4606, 0.4761, 0.4522, 0.4668],\n",
      "        [0.5093, 0.4437, 0.4249, 0.4633, 0.4466],\n",
      "        [0.5147, 0.4477, 0.4395, 0.4734, 0.4445],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5070, 0.4670, 0.4449, 0.4610, 0.4388],\n",
      "        [0.4906, 0.4699, 0.4422, 0.4626, 0.4705],\n",
      "        [0.5159, 0.4649, 0.4599, 0.4593, 0.4545],\n",
      "        [0.4738, 0.4488, 0.4444, 0.4800, 0.4439],\n",
      "        [0.5021, 0.4846, 0.4731, 0.4612, 0.4654],\n",
      "        [0.5156, 0.4823, 0.4414, 0.4872, 0.4424],\n",
      "        [0.5105, 0.4066, 0.4239, 0.4557, 0.3827],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5250, 0.4417, 0.3972, 0.5189, 0.4548],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4849, 0.4439, 0.4102, 0.4558, 0.4254],\n",
      "        [0.5206, 0.4665, 0.4350, 0.4289, 0.4049],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5117, 0.4361, 0.4070, 0.4826, 0.4473],\n",
      "        [0.5207, 0.3933, 0.3667, 0.4988, 0.4461],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5136, 0.4528, 0.4570, 0.4607, 0.4686],\n",
      "        [0.4975, 0.4615, 0.4448, 0.4791, 0.3990],\n",
      "        [0.5146, 0.4806, 0.4432, 0.4709, 0.4820],\n",
      "        [0.5230, 0.4894, 0.4429, 0.4861, 0.4790],\n",
      "        [0.5359, 0.4285, 0.4241, 0.4542, 0.4618],\n",
      "        [0.4904, 0.4371, 0.3708, 0.4615, 0.4523],\n",
      "        [0.5101, 0.4291, 0.4462, 0.4889, 0.4658],\n",
      "        [0.4948, 0.4797, 0.4677, 0.4469, 0.4588],\n",
      "        [0.5135, 0.4808, 0.4486, 0.4805, 0.4541],\n",
      "        [0.4906, 0.4115, 0.3813, 0.4243, 0.4003],\n",
      "        [0.5648, 0.4652, 0.3909, 0.4653, 0.4425],\n",
      "        [0.4952, 0.4316, 0.4529, 0.4398, 0.4345],\n",
      "        [0.4985, 0.4700, 0.4262, 0.4702, 0.4687],\n",
      "        [0.4986, 0.4592, 0.4708, 0.4780, 0.4669],\n",
      "        [0.4950, 0.4807, 0.4514, 0.4706, 0.4672],\n",
      "        [0.5444, 0.4637, 0.4514, 0.4876, 0.4673],\n",
      "        [0.5079, 0.4839, 0.4599, 0.4610, 0.4780],\n",
      "        [0.5201, 0.4498, 0.3939, 0.4616, 0.4101],\n",
      "        [0.5199, 0.4618, 0.4746, 0.4735, 0.4412],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5048, 0.4584, 0.4903, 0.4677, 0.4452],\n",
      "        [0.5110, 0.4820, 0.4518, 0.4520, 0.4612],\n",
      "        [0.5012, 0.4888, 0.4487, 0.4778, 0.4559],\n",
      "        [0.5218, 0.4683, 0.4537, 0.4747, 0.4494],\n",
      "        [0.5015, 0.4777, 0.4498, 0.4677, 0.4627],\n",
      "        [0.4754, 0.4530, 0.4788, 0.4604, 0.4599],\n",
      "        [0.5034, 0.4739, 0.4207, 0.4856, 0.4568],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5258, 0.4682, 0.4407, 0.4707, 0.4566],\n",
      "        [0.5054, 0.4722, 0.4261, 0.4908, 0.4565],\n",
      "        [0.4899, 0.4648, 0.4279, 0.4598, 0.4487],\n",
      "        [0.5025, 0.4808, 0.4496, 0.4763, 0.4642],\n",
      "        [0.5295, 0.4903, 0.4124, 0.4786, 0.3640],\n",
      "        [0.5400, 0.4449, 0.4700, 0.4797, 0.4177],\n",
      "        [0.5284, 0.4767, 0.4665, 0.4857, 0.4436],\n",
      "        [0.5047, 0.4564, 0.4174, 0.4457, 0.4645],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)], 'order': ['pd', 'nd', 'mod', 'dlt']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pd': {'accuracy': 0.4087427830800047,\n",
       "  'auc_micro': 0.8607603815937149,\n",
       "  'auc_mean': 0.7213919484860222,\n",
       "  'auc_weighted': 0.6491355274236282},\n",
       " 'nd': {'accuracy': 0.312149734934545,\n",
       "  'auc_micro': 0.4632387706855793,\n",
       "  'auc_mean': 0.45452151115365275,\n",
       "  'auc_weighted': 0.43489857267750365},\n",
       " 'mod': {'accuracy': 0.312149734934545,\n",
       "  'auc_micro': 0.4632387706855793,\n",
       "  'auc_mean': 0.45452151115365275,\n",
       "  'auc_weighted': 0.43489857267750365},\n",
       " 'dlts': {'accuracy': [0.6190476190476191,\n",
       "   0.9455782312925171,\n",
       "   0.9387755102040817,\n",
       "   0.9659863945578231,\n",
       "   0.9523809523809523],\n",
       "  'accuracy_mean': 0.8843537414965986,\n",
       "  'auc': [0.5395061728395062,\n",
       "   0.5633802816901409,\n",
       "   0.6836734693877551,\n",
       "   0.5763888888888888,\n",
       "   0.46478873239436624],\n",
       "  'auc_mean': 0.5655475090401314}}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmodel2_balanced = train_state(model_args=t1_args,state=2,lr=.001,weights=[1,1,.1,.1],use_smote=False)\n",
    "tmodel2_balanced[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "60e3da75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "epoch 0 train loss 2.3728010654449463\n",
      "val loss 2.399052143096924\n",
      "______________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/imblearn/over_sampling/_smote/base.py:345: FutureWarning: The parameter `n_jobs` has been deprecated in 0.10 and will be removed in 0.12. You can pass an nearest neighbors estimator where `n_jobs` is already set instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 train loss 2.335770606994629\n",
      "val loss 2.4041340351104736\n",
      "______________\n",
      "epoch 2 train loss 2.2979164123535156\n",
      "val loss 2.4094908237457275\n",
      "______________\n",
      "epoch 3 train loss 2.260416269302368\n",
      "val loss 2.4141788482666016\n",
      "______________\n",
      "epoch 4 train loss 2.2175753116607666\n",
      "val loss 2.41935658454895\n",
      "______________\n",
      "epoch 5 train loss 2.1588847637176514\n",
      "val loss 2.425004005432129\n",
      "______________\n",
      "epoch 6 train loss 2.1300930976867676\n",
      "val loss 2.4307374954223633\n",
      "______________\n",
      "epoch 7 train loss 2.037876605987549\n",
      "val loss 2.438628911972046\n",
      "______________\n",
      "epoch 8 train loss 2.008389949798584\n",
      "val loss 2.449005126953125\n",
      "______________\n",
      "epoch 9 train loss 1.9626109600067139\n",
      "val loss 2.461198568344116\n",
      "______________\n",
      "epoch 10 train loss 1.9318052530288696\n",
      "val loss 2.476175308227539\n",
      "______________\n",
      "epoch 11 train loss 1.886637806892395\n",
      "val loss 2.4935786724090576\n",
      "______________\n",
      "best loss 2.399052143096924 {'pd': {'accuracy': 0.3715093672675857, 'auc_micro': 0.8593340815563038, 'auc_mean': 0.5124569839895249, 'auc_weighted': 0.5256017075097881}, 'nd': {'accuracy': 0.3351292870280212, 'auc_micro': 0.5289598108747046, 'auc_mean': 0.5875004851630867, 'auc_weighted': 0.5319550544317021}, 'mod': {'accuracy': 0.3351292870280212, 'auc_micro': 0.5289598108747046, 'auc_mean': 0.5875004851630867, 'auc_weighted': 0.5319550544317021}, 'dlts': {'accuracy': [0.9183673469387755, 0.7687074829931972, 0.9523809523809523, 0.9183673469387755, 0.8027210884353742], 'accuracy_mean': 0.8721088435374149, 'auc': [0.6685185185185185, 0.6126760563380281, 0.5887755102040816, 0.6898148148148149, 0.48450704225352115], 'auc_mean': 0.6088583884257929}}\n",
      "{'predictions': [tensor([[0.4417, 0.2993, 0.2590],\n",
      "        [0.6662, 0.2108, 0.1230],\n",
      "        [0.4562, 0.3343, 0.2095],\n",
      "        [0.3877, 0.3552, 0.2571],\n",
      "        [0.4250, 0.3285, 0.2465],\n",
      "        [0.4197, 0.3057, 0.2747],\n",
      "        [0.4618, 0.3110, 0.2272],\n",
      "        [0.4149, 0.3161, 0.2690],\n",
      "        [0.5440, 0.2775, 0.1785],\n",
      "        [0.4134, 0.3205, 0.2661],\n",
      "        [0.4400, 0.3103, 0.2497],\n",
      "        [0.3949, 0.3274, 0.2777],\n",
      "        [0.4074, 0.3321, 0.2605],\n",
      "        [0.4157, 0.3052, 0.2791],\n",
      "        [0.4064, 0.3284, 0.2652],\n",
      "        [0.3960, 0.3304, 0.2736],\n",
      "        [0.5327, 0.2822, 0.1851],\n",
      "        [0.4554, 0.3164, 0.2282],\n",
      "        [0.4033, 0.3223, 0.2744],\n",
      "        [0.5318, 0.2924, 0.1759],\n",
      "        [0.5679, 0.2508, 0.1813],\n",
      "        [0.3933, 0.3275, 0.2792],\n",
      "        [0.4141, 0.3364, 0.2494],\n",
      "        [0.4470, 0.3421, 0.2109],\n",
      "        [0.6557, 0.2265, 0.1178],\n",
      "        [0.4479, 0.3142, 0.2379],\n",
      "        [0.6147, 0.2510, 0.1343],\n",
      "        [0.5138, 0.2874, 0.1987],\n",
      "        [0.4291, 0.3140, 0.2569],\n",
      "        [0.4158, 0.3037, 0.2805],\n",
      "        [0.4368, 0.3085, 0.2546],\n",
      "        [0.4992, 0.3052, 0.1956],\n",
      "        [0.3983, 0.3260, 0.2757],\n",
      "        [0.4137, 0.3413, 0.2450],\n",
      "        [0.4421, 0.2890, 0.2690],\n",
      "        [0.4595, 0.2999, 0.2406],\n",
      "        [0.4855, 0.3124, 0.2021],\n",
      "        [0.4228, 0.3223, 0.2548],\n",
      "        [0.5450, 0.2872, 0.1678],\n",
      "        [0.4172, 0.3081, 0.2747],\n",
      "        [0.4128, 0.3297, 0.2574],\n",
      "        [0.5713, 0.2925, 0.1362],\n",
      "        [0.3898, 0.3437, 0.2665],\n",
      "        [0.5487, 0.2906, 0.1607],\n",
      "        [0.4252, 0.3155, 0.2592],\n",
      "        [0.4709, 0.2923, 0.2367],\n",
      "        [0.4196, 0.3104, 0.2700],\n",
      "        [0.4563, 0.3155, 0.2282],\n",
      "        [0.4283, 0.3210, 0.2507],\n",
      "        [0.3942, 0.3281, 0.2776],\n",
      "        [0.5760, 0.2515, 0.1726],\n",
      "        [0.4007, 0.3392, 0.2601],\n",
      "        [0.4561, 0.2666, 0.2772],\n",
      "        [0.3863, 0.3455, 0.2682],\n",
      "        [0.4083, 0.3117, 0.2800],\n",
      "        [0.4708, 0.3131, 0.2161],\n",
      "        [0.4284, 0.3306, 0.2410],\n",
      "        [0.4530, 0.3222, 0.2248],\n",
      "        [0.6989, 0.2065, 0.0946],\n",
      "        [0.4311, 0.3235, 0.2454],\n",
      "        [0.4009, 0.3216, 0.2775],\n",
      "        [0.4530, 0.3047, 0.2423],\n",
      "        [0.4045, 0.3509, 0.2446],\n",
      "        [0.4102, 0.3384, 0.2514],\n",
      "        [0.5852, 0.2831, 0.1317],\n",
      "        [0.4147, 0.3252, 0.2602],\n",
      "        [0.4178, 0.3315, 0.2508],\n",
      "        [0.4659, 0.3161, 0.2180],\n",
      "        [0.4287, 0.3010, 0.2703],\n",
      "        [0.4232, 0.3288, 0.2480],\n",
      "        [0.5271, 0.2949, 0.1779],\n",
      "        [0.3980, 0.3449, 0.2571],\n",
      "        [0.3979, 0.3286, 0.2735],\n",
      "        [0.4005, 0.3124, 0.2871],\n",
      "        [0.4709, 0.3002, 0.2289],\n",
      "        [0.5792, 0.2492, 0.1716],\n",
      "        [0.4540, 0.3361, 0.2099],\n",
      "        [0.4379, 0.3466, 0.2155],\n",
      "        [0.5078, 0.2902, 0.2021],\n",
      "        [0.4517, 0.3160, 0.2323],\n",
      "        [0.4137, 0.3185, 0.2678],\n",
      "        [0.4083, 0.3113, 0.2805],\n",
      "        [0.4527, 0.3227, 0.2246],\n",
      "        [0.4600, 0.3189, 0.2211],\n",
      "        [0.5071, 0.2831, 0.2097],\n",
      "        [0.4229, 0.3137, 0.2633],\n",
      "        [0.4268, 0.3331, 0.2400],\n",
      "        [0.4362, 0.3452, 0.2185],\n",
      "        [0.4349, 0.3057, 0.2594],\n",
      "        [0.4279, 0.3166, 0.2554],\n",
      "        [0.4084, 0.3320, 0.2595],\n",
      "        [0.4236, 0.3130, 0.2634],\n",
      "        [0.4202, 0.3452, 0.2346],\n",
      "        [0.4279, 0.3141, 0.2581],\n",
      "        [0.5593, 0.2762, 0.1645],\n",
      "        [0.5579, 0.2868, 0.1554],\n",
      "        [0.4398, 0.3135, 0.2467],\n",
      "        [0.4950, 0.3257, 0.1793],\n",
      "        [0.4542, 0.3065, 0.2393],\n",
      "        [0.4742, 0.3034, 0.2224],\n",
      "        [0.5039, 0.2953, 0.2008],\n",
      "        [0.5202, 0.2674, 0.2124],\n",
      "        [0.4724, 0.3155, 0.2121],\n",
      "        [0.5517, 0.3107, 0.1376],\n",
      "        [0.3988, 0.3016, 0.2996],\n",
      "        [0.4770, 0.2815, 0.2414],\n",
      "        [0.4588, 0.3007, 0.2404],\n",
      "        [0.5216, 0.2401, 0.2383],\n",
      "        [0.4347, 0.3211, 0.2441],\n",
      "        [0.3950, 0.3154, 0.2896],\n",
      "        [0.4688, 0.3126, 0.2187],\n",
      "        [0.3897, 0.3255, 0.2848],\n",
      "        [0.4083, 0.3250, 0.2667],\n",
      "        [0.5156, 0.2747, 0.2097],\n",
      "        [0.4938, 0.2821, 0.2241],\n",
      "        [0.4004, 0.3175, 0.2821],\n",
      "        [0.4079, 0.3277, 0.2644],\n",
      "        [0.4063, 0.3349, 0.2587],\n",
      "        [0.5770, 0.2567, 0.1663],\n",
      "        [0.5454, 0.2854, 0.1692],\n",
      "        [0.4503, 0.3008, 0.2489],\n",
      "        [0.4216, 0.3171, 0.2612],\n",
      "        [0.4165, 0.3316, 0.2519],\n",
      "        [0.3811, 0.3449, 0.2739],\n",
      "        [0.4243, 0.3429, 0.2328],\n",
      "        [0.4086, 0.3151, 0.2763],\n",
      "        [0.4485, 0.3490, 0.2025],\n",
      "        [0.5113, 0.2852, 0.2035],\n",
      "        [0.5718, 0.2564, 0.1717],\n",
      "        [0.4173, 0.3265, 0.2562],\n",
      "        [0.4676, 0.3176, 0.2148],\n",
      "        [0.4024, 0.3171, 0.2806],\n",
      "        [0.4580, 0.3155, 0.2265],\n",
      "        [0.3982, 0.3328, 0.2690],\n",
      "        [0.3989, 0.3382, 0.2629],\n",
      "        [0.4117, 0.3339, 0.2544],\n",
      "        [0.4119, 0.3220, 0.2660],\n",
      "        [0.5375, 0.2975, 0.1651],\n",
      "        [0.4290, 0.3346, 0.2364],\n",
      "        [0.4230, 0.3266, 0.2504],\n",
      "        [0.4212, 0.3434, 0.2354],\n",
      "        [0.3914, 0.3488, 0.2598],\n",
      "        [0.6450, 0.2308, 0.1243],\n",
      "        [0.4890, 0.3055, 0.2055],\n",
      "        [0.4503, 0.3345, 0.2152],\n",
      "        [0.4442, 0.2900, 0.2658],\n",
      "        [0.4527, 0.3086, 0.2386]], device='cuda:0', grad_fn=<SoftmaxBackward0>), tensor([[0.4312, 0.3107, 0.2581],\n",
      "        [0.4635, 0.3021, 0.2344],\n",
      "        [0.3884, 0.3400, 0.2716],\n",
      "        [0.3755, 0.3290, 0.2954],\n",
      "        [0.3551, 0.3381, 0.3067],\n",
      "        [0.3560, 0.3469, 0.2971],\n",
      "        [0.3278, 0.3601, 0.3121],\n",
      "        [0.3397, 0.3662, 0.2941],\n",
      "        [0.4118, 0.3531, 0.2351],\n",
      "        [0.3528, 0.3418, 0.3055],\n",
      "        [0.3585, 0.3395, 0.3020],\n",
      "        [0.3675, 0.3597, 0.2728],\n",
      "        [0.3468, 0.3474, 0.3058],\n",
      "        [0.3457, 0.3559, 0.2984],\n",
      "        [0.3562, 0.3401, 0.3037],\n",
      "        [0.3670, 0.3432, 0.2897],\n",
      "        [0.4105, 0.3341, 0.2554],\n",
      "        [0.3423, 0.3426, 0.3150],\n",
      "        [0.3342, 0.3599, 0.3059],\n",
      "        [0.3611, 0.3518, 0.2871],\n",
      "        [0.3705, 0.2997, 0.3298],\n",
      "        [0.3707, 0.3393, 0.2900],\n",
      "        [0.3470, 0.3426, 0.3103],\n",
      "        [0.4040, 0.3404, 0.2556],\n",
      "        [0.3994, 0.3567, 0.2439],\n",
      "        [0.3794, 0.3399, 0.2807],\n",
      "        [0.4079, 0.3525, 0.2395],\n",
      "        [0.4123, 0.3318, 0.2560],\n",
      "        [0.3530, 0.3398, 0.3071],\n",
      "        [0.3584, 0.3463, 0.2953],\n",
      "        [0.3426, 0.3547, 0.3027],\n",
      "        [0.3292, 0.3510, 0.3198],\n",
      "        [0.3425, 0.3571, 0.3004],\n",
      "        [0.3631, 0.3456, 0.2913],\n",
      "        [0.3528, 0.3513, 0.2959],\n",
      "        [0.3749, 0.3205, 0.3046],\n",
      "        [0.3796, 0.3475, 0.2729],\n",
      "        [0.3604, 0.3469, 0.2927],\n",
      "        [0.4212, 0.3318, 0.2470],\n",
      "        [0.3518, 0.3473, 0.3009],\n",
      "        [0.3565, 0.3368, 0.3068],\n",
      "        [0.4532, 0.3095, 0.2373],\n",
      "        [0.3878, 0.3256, 0.2866],\n",
      "        [0.4415, 0.3361, 0.2224],\n",
      "        [0.3467, 0.3544, 0.2990],\n",
      "        [0.4038, 0.3200, 0.2762],\n",
      "        [0.3844, 0.3286, 0.2870],\n",
      "        [0.4064, 0.3332, 0.2604],\n",
      "        [0.3380, 0.3522, 0.3098],\n",
      "        [0.3472, 0.3484, 0.3044],\n",
      "        [0.4226, 0.3262, 0.2512],\n",
      "        [0.3564, 0.3386, 0.3050],\n",
      "        [0.4208, 0.3118, 0.2674],\n",
      "        [0.3528, 0.3454, 0.3017],\n",
      "        [0.3541, 0.3449, 0.3010],\n",
      "        [0.3723, 0.3326, 0.2950],\n",
      "        [0.3701, 0.3388, 0.2911],\n",
      "        [0.3783, 0.3298, 0.2919],\n",
      "        [0.4622, 0.2953, 0.2425],\n",
      "        [0.3373, 0.3497, 0.3130],\n",
      "        [0.3444, 0.3536, 0.3020],\n",
      "        [0.3177, 0.3467, 0.3356],\n",
      "        [0.3486, 0.3588, 0.2926],\n",
      "        [0.3489, 0.3601, 0.2911],\n",
      "        [0.4019, 0.3568, 0.2412],\n",
      "        [0.3355, 0.3597, 0.3048],\n",
      "        [0.3492, 0.3677, 0.2831],\n",
      "        [0.3555, 0.3433, 0.3012],\n",
      "        [0.3541, 0.3371, 0.3089],\n",
      "        [0.3583, 0.3588, 0.2829],\n",
      "        [0.3521, 0.3287, 0.3192],\n",
      "        [0.3648, 0.3377, 0.2975],\n",
      "        [0.3492, 0.3559, 0.2949],\n",
      "        [0.3600, 0.3402, 0.2997],\n",
      "        [0.3563, 0.3459, 0.2978],\n",
      "        [0.3498, 0.3614, 0.2888],\n",
      "        [0.3906, 0.3181, 0.2913],\n",
      "        [0.3405, 0.3383, 0.3213],\n",
      "        [0.3614, 0.3487, 0.2899],\n",
      "        [0.3431, 0.3650, 0.2919],\n",
      "        [0.3567, 0.3505, 0.2928],\n",
      "        [0.3909, 0.3292, 0.2799],\n",
      "        [0.3584, 0.3438, 0.2978],\n",
      "        [0.3871, 0.3391, 0.2739],\n",
      "        [0.4100, 0.3362, 0.2539],\n",
      "        [0.3367, 0.3650, 0.2983],\n",
      "        [0.3576, 0.3422, 0.3003],\n",
      "        [0.3831, 0.3438, 0.2732],\n",
      "        [0.3507, 0.3595, 0.2898],\n",
      "        [0.3882, 0.3214, 0.2904],\n",
      "        [0.3476, 0.3678, 0.2846],\n",
      "        [0.3351, 0.3658, 0.2991],\n",
      "        [0.3414, 0.3363, 0.3223],\n",
      "        [0.3430, 0.3577, 0.2992],\n",
      "        [0.3788, 0.3427, 0.2785],\n",
      "        [0.4139, 0.3422, 0.2439],\n",
      "        [0.3615, 0.3342, 0.3043],\n",
      "        [0.3394, 0.3692, 0.2914],\n",
      "        [0.3721, 0.3383, 0.2896],\n",
      "        [0.3644, 0.3483, 0.2873],\n",
      "        [0.3565, 0.3511, 0.2924],\n",
      "        [0.3803, 0.3335, 0.2863],\n",
      "        [0.3838, 0.3300, 0.2862],\n",
      "        [0.4098, 0.3567, 0.2335],\n",
      "        [0.3670, 0.3430, 0.2899],\n",
      "        [0.3772, 0.3227, 0.3002],\n",
      "        [0.3434, 0.3430, 0.3136],\n",
      "        [0.3982, 0.3271, 0.2747],\n",
      "        [0.3477, 0.3486, 0.3037],\n",
      "        [0.3789, 0.3326, 0.2886],\n",
      "        [0.3627, 0.3409, 0.2964],\n",
      "        [0.3620, 0.3481, 0.2899],\n",
      "        [0.3470, 0.3428, 0.3102],\n",
      "        [0.4129, 0.3383, 0.2489],\n",
      "        [0.3634, 0.3313, 0.3053],\n",
      "        [0.3678, 0.3532, 0.2790],\n",
      "        [0.3334, 0.3608, 0.3058],\n",
      "        [0.3527, 0.3464, 0.3009],\n",
      "        [0.4202, 0.3202, 0.2596],\n",
      "        [0.3977, 0.3288, 0.2735],\n",
      "        [0.3714, 0.3341, 0.2945],\n",
      "        [0.3737, 0.3329, 0.2934],\n",
      "        [0.3500, 0.3436, 0.3064],\n",
      "        [0.3665, 0.3411, 0.2924],\n",
      "        [0.3628, 0.3470, 0.2902],\n",
      "        [0.3447, 0.3574, 0.2979],\n",
      "        [0.3527, 0.3326, 0.3146],\n",
      "        [0.3888, 0.3444, 0.2668],\n",
      "        [0.4226, 0.3164, 0.2610],\n",
      "        [0.3704, 0.3512, 0.2784],\n",
      "        [0.3501, 0.3492, 0.3007],\n",
      "        [0.3528, 0.3451, 0.3022],\n",
      "        [0.3553, 0.3357, 0.3090],\n",
      "        [0.3691, 0.3384, 0.2926],\n",
      "        [0.3567, 0.3487, 0.2946],\n",
      "        [0.3570, 0.3448, 0.2982],\n",
      "        [0.3633, 0.3393, 0.2974],\n",
      "        [0.4245, 0.3190, 0.2565],\n",
      "        [0.3560, 0.3570, 0.2870],\n",
      "        [0.3631, 0.3362, 0.3008],\n",
      "        [0.3380, 0.3379, 0.3241],\n",
      "        [0.3659, 0.3304, 0.3037],\n",
      "        [0.4140, 0.3175, 0.2685],\n",
      "        [0.3839, 0.3448, 0.2713],\n",
      "        [0.3610, 0.3413, 0.2977],\n",
      "        [0.3360, 0.3344, 0.3296],\n",
      "        [0.3716, 0.3502, 0.2782]], device='cuda:0', grad_fn=<SoftmaxBackward0>), tensor([[0.2642, 0.2472, 0.2463, 0.2423],\n",
      "        [0.2419, 0.2956, 0.2209, 0.2415],\n",
      "        [0.2658, 0.2310, 0.2348, 0.2684],\n",
      "        [0.2596, 0.2552, 0.2397, 0.2455],\n",
      "        [0.2504, 0.2636, 0.2458, 0.2402],\n",
      "        [0.2630, 0.2537, 0.2477, 0.2357],\n",
      "        [0.2539, 0.2773, 0.2305, 0.2382],\n",
      "        [0.2561, 0.2569, 0.2449, 0.2421],\n",
      "        [0.2586, 0.2543, 0.2331, 0.2540],\n",
      "        [0.2550, 0.2519, 0.2489, 0.2443],\n",
      "        [0.2687, 0.2524, 0.2488, 0.2301],\n",
      "        [0.2381, 0.2588, 0.2656, 0.2375],\n",
      "        [0.2516, 0.2596, 0.2466, 0.2422],\n",
      "        [0.2565, 0.2468, 0.2499, 0.2468],\n",
      "        [0.2554, 0.2584, 0.2415, 0.2447],\n",
      "        [0.2490, 0.2533, 0.2488, 0.2489],\n",
      "        [0.2644, 0.2535, 0.2249, 0.2572],\n",
      "        [0.2508, 0.2544, 0.2457, 0.2491],\n",
      "        [0.2520, 0.2593, 0.2489, 0.2398],\n",
      "        [0.2242, 0.2699, 0.2432, 0.2627],\n",
      "        [0.2338, 0.2815, 0.2392, 0.2454],\n",
      "        [0.2513, 0.2578, 0.2481, 0.2427],\n",
      "        [0.2541, 0.2592, 0.2460, 0.2406],\n",
      "        [0.2523, 0.2503, 0.2423, 0.2551],\n",
      "        [0.2487, 0.2256, 0.2697, 0.2560],\n",
      "        [0.2591, 0.2684, 0.2402, 0.2323],\n",
      "        [0.2499, 0.2326, 0.2809, 0.2366],\n",
      "        [0.2558, 0.2572, 0.2400, 0.2469],\n",
      "        [0.2561, 0.2543, 0.2457, 0.2439],\n",
      "        [0.2585, 0.2510, 0.2497, 0.2408],\n",
      "        [0.2549, 0.2614, 0.2422, 0.2415],\n",
      "        [0.2466, 0.2740, 0.2301, 0.2493],\n",
      "        [0.2515, 0.2598, 0.2505, 0.2382],\n",
      "        [0.2516, 0.2576, 0.2498, 0.2410],\n",
      "        [0.2558, 0.2522, 0.2557, 0.2363],\n",
      "        [0.2653, 0.2450, 0.2439, 0.2458],\n",
      "        [0.2585, 0.2737, 0.2363, 0.2315],\n",
      "        [0.2566, 0.2584, 0.2517, 0.2333],\n",
      "        [0.2624, 0.2648, 0.2109, 0.2619],\n",
      "        [0.2568, 0.2469, 0.2505, 0.2458],\n",
      "        [0.2568, 0.2574, 0.2409, 0.2449],\n",
      "        [0.2627, 0.2689, 0.2090, 0.2594],\n",
      "        [0.2489, 0.2628, 0.2383, 0.2501],\n",
      "        [0.2666, 0.2721, 0.2342, 0.2272],\n",
      "        [0.2607, 0.2543, 0.2438, 0.2413],\n",
      "        [0.2449, 0.2552, 0.2548, 0.2451],\n",
      "        [0.2508, 0.2566, 0.2486, 0.2440],\n",
      "        [0.2574, 0.2476, 0.2372, 0.2577],\n",
      "        [0.2479, 0.2666, 0.2458, 0.2396],\n",
      "        [0.2502, 0.2560, 0.2526, 0.2412],\n",
      "        [0.2483, 0.2508, 0.2504, 0.2505],\n",
      "        [0.2493, 0.2634, 0.2433, 0.2441],\n",
      "        [0.2492, 0.2515, 0.2603, 0.2390],\n",
      "        [0.2493, 0.2555, 0.2525, 0.2428],\n",
      "        [0.2611, 0.2524, 0.2488, 0.2377],\n",
      "        [0.2438, 0.2683, 0.2290, 0.2589],\n",
      "        [0.2568, 0.2541, 0.2473, 0.2418],\n",
      "        [0.2448, 0.2606, 0.2637, 0.2310],\n",
      "        [0.2439, 0.2939, 0.2248, 0.2374],\n",
      "        [0.2539, 0.2593, 0.2378, 0.2490],\n",
      "        [0.2531, 0.2579, 0.2504, 0.2387],\n",
      "        [0.2415, 0.2613, 0.2444, 0.2528],\n",
      "        [0.2551, 0.2615, 0.2310, 0.2524],\n",
      "        [0.2422, 0.2566, 0.2548, 0.2464],\n",
      "        [0.2382, 0.2340, 0.2800, 0.2478],\n",
      "        [0.2548, 0.2526, 0.2487, 0.2439],\n",
      "        [0.2559, 0.2522, 0.2405, 0.2515],\n",
      "        [0.2472, 0.2680, 0.2350, 0.2498],\n",
      "        [0.2524, 0.2673, 0.2465, 0.2338],\n",
      "        [0.2595, 0.2562, 0.2320, 0.2522],\n",
      "        [0.2432, 0.2637, 0.2261, 0.2669],\n",
      "        [0.2449, 0.2629, 0.2466, 0.2456],\n",
      "        [0.2506, 0.2598, 0.2527, 0.2369],\n",
      "        [0.2492, 0.2603, 0.2556, 0.2349],\n",
      "        [0.2595, 0.2627, 0.2424, 0.2355],\n",
      "        [0.2580, 0.2772, 0.2301, 0.2347],\n",
      "        [0.2625, 0.2583, 0.2370, 0.2421],\n",
      "        [0.2475, 0.2659, 0.2345, 0.2521],\n",
      "        [0.2588, 0.2462, 0.2373, 0.2577],\n",
      "        [0.2573, 0.2577, 0.2396, 0.2453],\n",
      "        [0.2549, 0.2556, 0.2466, 0.2429],\n",
      "        [0.2502, 0.2648, 0.2493, 0.2358],\n",
      "        [0.2616, 0.2647, 0.2363, 0.2374],\n",
      "        [0.2484, 0.2584, 0.2355, 0.2577],\n",
      "        [0.2607, 0.2548, 0.2512, 0.2333],\n",
      "        [0.2572, 0.2597, 0.2413, 0.2418],\n",
      "        [0.2459, 0.2647, 0.2428, 0.2467],\n",
      "        [0.2427, 0.2550, 0.2362, 0.2660],\n",
      "        [0.2550, 0.2585, 0.2441, 0.2424],\n",
      "        [0.2613, 0.2511, 0.2386, 0.2490],\n",
      "        [0.2466, 0.2525, 0.2406, 0.2603],\n",
      "        [0.2576, 0.2593, 0.2405, 0.2426],\n",
      "        [0.2443, 0.2556, 0.2402, 0.2600],\n",
      "        [0.2590, 0.2552, 0.2436, 0.2422],\n",
      "        [0.2816, 0.2597, 0.2296, 0.2291],\n",
      "        [0.2534, 0.2459, 0.2533, 0.2473],\n",
      "        [0.2458, 0.2593, 0.2406, 0.2543],\n",
      "        [0.2315, 0.2829, 0.2319, 0.2536],\n",
      "        [0.2487, 0.2599, 0.2516, 0.2397],\n",
      "        [0.2630, 0.2537, 0.2409, 0.2423],\n",
      "        [0.2600, 0.2602, 0.2423, 0.2375],\n",
      "        [0.2492, 0.2716, 0.2344, 0.2448],\n",
      "        [0.2525, 0.2670, 0.2407, 0.2398],\n",
      "        [0.2361, 0.2774, 0.2269, 0.2596],\n",
      "        [0.2536, 0.2529, 0.2564, 0.2370],\n",
      "        [0.2638, 0.2539, 0.2511, 0.2312],\n",
      "        [0.2563, 0.2562, 0.2411, 0.2464],\n",
      "        [0.2439, 0.2653, 0.2626, 0.2282],\n",
      "        [0.2540, 0.2552, 0.2535, 0.2374],\n",
      "        [0.2447, 0.2554, 0.2520, 0.2480],\n",
      "        [0.2475, 0.2835, 0.2245, 0.2446],\n",
      "        [0.2544, 0.2593, 0.2510, 0.2353],\n",
      "        [0.2556, 0.2572, 0.2475, 0.2397],\n",
      "        [0.2711, 0.2576, 0.2459, 0.2254],\n",
      "        [0.2576, 0.2603, 0.2471, 0.2350],\n",
      "        [0.2490, 0.2527, 0.2466, 0.2517],\n",
      "        [0.2527, 0.2504, 0.2509, 0.2460],\n",
      "        [0.2507, 0.2586, 0.2477, 0.2429],\n",
      "        [0.2469, 0.2666, 0.2310, 0.2555],\n",
      "        [0.2593, 0.2655, 0.2376, 0.2376],\n",
      "        [0.2507, 0.2669, 0.2535, 0.2289],\n",
      "        [0.2487, 0.2640, 0.2416, 0.2457],\n",
      "        [0.2491, 0.2620, 0.2441, 0.2448],\n",
      "        [0.2500, 0.2575, 0.2500, 0.2424],\n",
      "        [0.2522, 0.2632, 0.2325, 0.2521],\n",
      "        [0.2532, 0.2618, 0.2477, 0.2373],\n",
      "        [0.2389, 0.2807, 0.2290, 0.2514],\n",
      "        [0.2518, 0.2523, 0.2476, 0.2483],\n",
      "        [0.2553, 0.2662, 0.2436, 0.2349],\n",
      "        [0.2520, 0.2502, 0.2480, 0.2499],\n",
      "        [0.2598, 0.2606, 0.2416, 0.2380],\n",
      "        [0.2580, 0.2506, 0.2502, 0.2412],\n",
      "        [0.2599, 0.2641, 0.2334, 0.2426],\n",
      "        [0.2547, 0.2622, 0.2380, 0.2451],\n",
      "        [0.2498, 0.2630, 0.2404, 0.2467],\n",
      "        [0.2554, 0.2560, 0.2458, 0.2428],\n",
      "        [0.2569, 0.2593, 0.2433, 0.2405],\n",
      "        [0.2560, 0.2651, 0.2471, 0.2318],\n",
      "        [0.2484, 0.2597, 0.2305, 0.2614],\n",
      "        [0.2545, 0.2620, 0.2436, 0.2399],\n",
      "        [0.2588, 0.2539, 0.2410, 0.2463],\n",
      "        [0.2597, 0.2546, 0.2400, 0.2456],\n",
      "        [0.2448, 0.2742, 0.2069, 0.2742],\n",
      "        [0.2369, 0.2576, 0.2507, 0.2547],\n",
      "        [0.2491, 0.2622, 0.2338, 0.2548],\n",
      "        [0.2431, 0.2562, 0.2465, 0.2542],\n",
      "        [0.2520, 0.2620, 0.2388, 0.2473]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>), tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3015, 0.4397, 0.3566, 0.4213, 0.4203],\n",
      "        [0.3450, 0.4343, 0.3460, 0.4197, 0.4459],\n",
      "        [0.4219, 0.4630, 0.4261, 0.4444, 0.4467],\n",
      "        [0.4329, 0.4728, 0.4171, 0.4364, 0.4767],\n",
      "        [0.4171, 0.4547, 0.4086, 0.4504, 0.4577],\n",
      "        [0.3912, 0.4815, 0.3678, 0.3890, 0.4806],\n",
      "        [0.4283, 0.4545, 0.4166, 0.4452, 0.4418],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3780, 0.4648, 0.3707, 0.4251, 0.4150],\n",
      "        [0.3854, 0.4414, 0.3420, 0.4292, 0.4506],\n",
      "        [0.4161, 0.4577, 0.4136, 0.4533, 0.4758],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4414, 0.4612, 0.4159, 0.4208, 0.4659],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3486, 0.3897, 0.3355, 0.3791, 0.4119],\n",
      "        [0.4299, 0.4754, 0.4089, 0.4459, 0.4710],\n",
      "        [0.4075, 0.4442, 0.4050, 0.4457, 0.4622],\n",
      "        [0.3426, 0.4603, 0.3574, 0.3892, 0.4095],\n",
      "        [0.4050, 0.4270, 0.3251, 0.3653, 0.4556],\n",
      "        [0.4251, 0.4387, 0.4179, 0.4612, 0.4739],\n",
      "        [0.4346, 0.4656, 0.4124, 0.4137, 0.4600],\n",
      "        [0.4098, 0.4234, 0.3669, 0.3872, 0.4148],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4052, 0.4514, 0.3661, 0.4668, 0.4454],\n",
      "        [0.2905, 0.4072, 0.2678, 0.3683, 0.4049],\n",
      "        [0.3857, 0.4544, 0.3588, 0.4609, 0.4499],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4087, 0.4525, 0.3985, 0.4235, 0.4475],\n",
      "        [0.3959, 0.4805, 0.3485, 0.3967, 0.4399],\n",
      "        [0.4132, 0.4538, 0.4111, 0.4568, 0.4633],\n",
      "        [0.4261, 0.4583, 0.4193, 0.4282, 0.4452],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3775, 0.4420, 0.3478, 0.4048, 0.4760],\n",
      "        [0.3789, 0.4256, 0.3588, 0.4293, 0.4426],\n",
      "        [0.4037, 0.4486, 0.3919, 0.4430, 0.4578],\n",
      "        [0.3785, 0.4889, 0.3147, 0.4610, 0.3600],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4325, 0.4517, 0.4167, 0.4209, 0.4666],\n",
      "        [0.3043, 0.4359, 0.3404, 0.3367, 0.3124],\n",
      "        [0.4079, 0.4625, 0.4109, 0.4276, 0.4538],\n",
      "        [0.3474, 0.4301, 0.2994, 0.3766, 0.4264],\n",
      "        [0.4105, 0.4539, 0.4010, 0.4428, 0.4583],\n",
      "        [0.4327, 0.4258, 0.4054, 0.4174, 0.4283],\n",
      "        [0.4341, 0.4692, 0.4209, 0.4498, 0.4706],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4265, 0.4783, 0.4124, 0.4444, 0.4713],\n",
      "        [0.4142, 0.4556, 0.4148, 0.4558, 0.4637],\n",
      "        [0.3975, 0.4740, 0.3790, 0.3760, 0.4464],\n",
      "        [0.4387, 0.4653, 0.4238, 0.4360, 0.4786],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4152, 0.4595, 0.4102, 0.4509, 0.4667],\n",
      "        [0.4182, 0.4564, 0.4127, 0.4456, 0.4523],\n",
      "        [0.4072, 0.4677, 0.3684, 0.4275, 0.4686],\n",
      "        [0.4165, 0.4590, 0.4010, 0.4306, 0.4676],\n",
      "        [0.4390, 0.4536, 0.4089, 0.3948, 0.4572],\n",
      "        [0.2720, 0.4021, 0.2776, 0.3871, 0.4273],\n",
      "        [0.4192, 0.4605, 0.4186, 0.4241, 0.4626],\n",
      "        [0.4160, 0.4490, 0.4017, 0.4454, 0.4563],\n",
      "        [0.4359, 0.4758, 0.4093, 0.4318, 0.4854],\n",
      "        [0.4300, 0.4526, 0.3839, 0.4263, 0.4481],\n",
      "        [0.4041, 0.4319, 0.4016, 0.4294, 0.4625],\n",
      "        [0.3021, 0.3854, 0.2565, 0.3708, 0.4120],\n",
      "        [0.4026, 0.4601, 0.4034, 0.4428, 0.4366],\n",
      "        [0.4105, 0.4476, 0.4060, 0.4429, 0.4446],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4309, 0.4743, 0.4135, 0.4480, 0.4553],\n",
      "        [0.4243, 0.4598, 0.3820, 0.4404, 0.4489],\n",
      "        [0.4196, 0.4190, 0.3456, 0.4363, 0.4501],\n",
      "        [0.4382, 0.4735, 0.4250, 0.4461, 0.4624],\n",
      "        [0.4141, 0.4614, 0.4122, 0.4633, 0.4613],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3864, 0.4573, 0.3848, 0.4039, 0.4611],\n",
      "        [0.4133, 0.3905, 0.3608, 0.3794, 0.4225],\n",
      "        [0.3927, 0.4515, 0.3839, 0.4687, 0.4393],\n",
      "        [0.4233, 0.4781, 0.4017, 0.4701, 0.4751],\n",
      "        [0.3816, 0.4615, 0.3131, 0.4188, 0.4401],\n",
      "        [0.3962, 0.4608, 0.4023, 0.4185, 0.4681],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4210, 0.4335, 0.4105, 0.4412, 0.4708],\n",
      "        [0.4061, 0.4780, 0.4056, 0.4407, 0.4524],\n",
      "        [0.4079, 0.4803, 0.4144, 0.4474, 0.4560],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4155, 0.4567, 0.4170, 0.4441, 0.4417],\n",
      "        [0.4111, 0.4686, 0.4137, 0.4205, 0.4542],\n",
      "        [0.4048, 0.4705, 0.4276, 0.4701, 0.4321],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4313, 0.4681, 0.3773, 0.3943, 0.4689],\n",
      "        [0.4194, 0.4419, 0.4256, 0.4312, 0.4406],\n",
      "        [0.4128, 0.4556, 0.4155, 0.4414, 0.4415],\n",
      "        [0.4300, 0.4698, 0.4037, 0.4479, 0.4656],\n",
      "        [0.4005, 0.4453, 0.3943, 0.4325, 0.4512],\n",
      "        [0.3221, 0.4426, 0.3403, 0.3780, 0.4418],\n",
      "        [0.3555, 0.4308, 0.3487, 0.3622, 0.4496],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4014, 0.4491, 0.3817, 0.4110, 0.4794],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4068, 0.4654, 0.3240, 0.4303, 0.4357],\n",
      "        [0.3824, 0.4296, 0.3784, 0.3858, 0.4480],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4102, 0.4545, 0.3563, 0.4224, 0.4523],\n",
      "        [0.3833, 0.3637, 0.2923, 0.4019, 0.4585],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4034, 0.4313, 0.4234, 0.4514, 0.4513],\n",
      "        [0.3980, 0.4562, 0.3720, 0.4381, 0.4505],\n",
      "        [0.4263, 0.4573, 0.4119, 0.4612, 0.4635],\n",
      "        [0.4227, 0.4524, 0.4049, 0.4381, 0.4788],\n",
      "        [0.4175, 0.4272, 0.3887, 0.3665, 0.4208],\n",
      "        [0.3984, 0.4555, 0.3630, 0.4166, 0.4171],\n",
      "        [0.4297, 0.4424, 0.4356, 0.4537, 0.4303],\n",
      "        [0.3990, 0.4606, 0.4159, 0.4433, 0.4434],\n",
      "        [0.4166, 0.4602, 0.4106, 0.4587, 0.4746],\n",
      "        [0.3853, 0.4712, 0.3538, 0.4152, 0.4021],\n",
      "        [0.4054, 0.4207, 0.3253, 0.4449, 0.3985],\n",
      "        [0.4237, 0.4362, 0.3790, 0.4217, 0.4565],\n",
      "        [0.4286, 0.4473, 0.4221, 0.4185, 0.4730],\n",
      "        [0.4336, 0.4603, 0.4122, 0.4186, 0.4737],\n",
      "        [0.4223, 0.4630, 0.4149, 0.4643, 0.4715],\n",
      "        [0.4364, 0.4582, 0.3793, 0.4503, 0.4639],\n",
      "        [0.4146, 0.4444, 0.4002, 0.4500, 0.4604],\n",
      "        [0.3907, 0.4726, 0.3820, 0.4120, 0.4464],\n",
      "        [0.3819, 0.4353, 0.3709, 0.3985, 0.4411],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3931, 0.4372, 0.3873, 0.4255, 0.4499],\n",
      "        [0.4184, 0.4568, 0.4144, 0.4443, 0.4462],\n",
      "        [0.4134, 0.4793, 0.4115, 0.4234, 0.4602],\n",
      "        [0.4401, 0.4598, 0.3909, 0.4389, 0.4625],\n",
      "        [0.4118, 0.4633, 0.3973, 0.4646, 0.4643],\n",
      "        [0.4428, 0.4734, 0.4153, 0.4353, 0.4540],\n",
      "        [0.4379, 0.4703, 0.4219, 0.4301, 0.4787],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4276, 0.4514, 0.3822, 0.4494, 0.4664],\n",
      "        [0.4273, 0.4749, 0.4128, 0.4326, 0.4793],\n",
      "        [0.4119, 0.4552, 0.4211, 0.4285, 0.4595],\n",
      "        [0.4226, 0.4613, 0.4307, 0.4391, 0.4525],\n",
      "        [0.2665, 0.3774, 0.2640, 0.3161, 0.4340],\n",
      "        [0.4234, 0.4299, 0.3756, 0.4049, 0.4364],\n",
      "        [0.4209, 0.4783, 0.4060, 0.4675, 0.4554],\n",
      "        [0.4411, 0.4606, 0.4072, 0.4072, 0.4745],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<MulBackward0>)], '5%': [tensor([[0.3924, 0.2688, 0.2358],\n",
      "        [0.5821, 0.1376, 0.0839],\n",
      "        [0.3927, 0.2976, 0.1836],\n",
      "        [0.3691, 0.3264, 0.2450],\n",
      "        [0.4199, 0.3128, 0.2275],\n",
      "        [0.3930, 0.2930, 0.2566],\n",
      "        [0.4419, 0.2740, 0.1890],\n",
      "        [0.3985, 0.2997, 0.2568],\n",
      "        [0.4903, 0.2309, 0.1357],\n",
      "        [0.3900, 0.3029, 0.2551],\n",
      "        [0.4183, 0.2818, 0.2237],\n",
      "        [0.3732, 0.3065, 0.2429],\n",
      "        [0.3832, 0.3208, 0.2431],\n",
      "        [0.4015, 0.2882, 0.2627],\n",
      "        [0.3843, 0.3168, 0.2474],\n",
      "        [0.3751, 0.3135, 0.2526],\n",
      "        [0.4767, 0.2228, 0.1470],\n",
      "        [0.4353, 0.2974, 0.2171],\n",
      "        [0.3928, 0.3124, 0.2545],\n",
      "        [0.5053, 0.2554, 0.1486],\n",
      "        [0.5138, 0.1918, 0.1165],\n",
      "        [0.3684, 0.3078, 0.2563],\n",
      "        [0.4005, 0.3234, 0.2326],\n",
      "        [0.3912, 0.2778, 0.1880],\n",
      "        [0.5065, 0.1444, 0.0879],\n",
      "        [0.4069, 0.2800, 0.2179],\n",
      "        [0.4367, 0.1760, 0.0953],\n",
      "        [0.4344, 0.2524, 0.1697],\n",
      "        [0.4114, 0.2908, 0.2470],\n",
      "        [0.4017, 0.2890, 0.2612],\n",
      "        [0.4185, 0.2937, 0.2369],\n",
      "        [0.4492, 0.2706, 0.1728],\n",
      "        [0.3811, 0.3174, 0.2629],\n",
      "        [0.3982, 0.3153, 0.2274],\n",
      "        [0.4112, 0.2670, 0.2437],\n",
      "        [0.4098, 0.2741, 0.2120],\n",
      "        [0.4444, 0.2888, 0.1813],\n",
      "        [0.4043, 0.3009, 0.2259],\n",
      "        [0.4655, 0.2672, 0.1468],\n",
      "        [0.3994, 0.2918, 0.2609],\n",
      "        [0.3981, 0.3135, 0.2488],\n",
      "        [0.5093, 0.2378, 0.0887],\n",
      "        [0.3733, 0.3190, 0.2447],\n",
      "        [0.4749, 0.2167, 0.1238],\n",
      "        [0.4123, 0.2998, 0.2453],\n",
      "        [0.3897, 0.2681, 0.2060],\n",
      "        [0.3937, 0.2798, 0.2552],\n",
      "        [0.4232, 0.2882, 0.2099],\n",
      "        [0.4079, 0.3077, 0.2412],\n",
      "        [0.3855, 0.3108, 0.2576],\n",
      "        [0.5009, 0.1882, 0.1367],\n",
      "        [0.3902, 0.3261, 0.2423],\n",
      "        [0.4218, 0.2398, 0.2329],\n",
      "        [0.3690, 0.3299, 0.2537],\n",
      "        [0.3743, 0.3015, 0.2712],\n",
      "        [0.4328, 0.2989, 0.1961],\n",
      "        [0.4275, 0.3109, 0.2265],\n",
      "        [0.4237, 0.2744, 0.1856],\n",
      "        [0.6088, 0.1155, 0.0640],\n",
      "        [0.4143, 0.3082, 0.2290],\n",
      "        [0.3826, 0.3072, 0.2575],\n",
      "        [0.4315, 0.2913, 0.2184],\n",
      "        [0.3819, 0.3219, 0.2231],\n",
      "        [0.3930, 0.3164, 0.2359],\n",
      "        [0.4112, 0.2149, 0.0804],\n",
      "        [0.3848, 0.3128, 0.2467],\n",
      "        [0.4067, 0.2980, 0.2306],\n",
      "        [0.4348, 0.2857, 0.1950],\n",
      "        [0.4039, 0.2880, 0.2541],\n",
      "        [0.4011, 0.3140, 0.2296],\n",
      "        [0.4836, 0.2605, 0.1477],\n",
      "        [0.3824, 0.3251, 0.2351],\n",
      "        [0.3791, 0.3148, 0.2572],\n",
      "        [0.3775, 0.2922, 0.2607],\n",
      "        [0.4294, 0.2834, 0.2137],\n",
      "        [0.5382, 0.2090, 0.1276],\n",
      "        [0.4397, 0.2978, 0.1836],\n",
      "        [0.4003, 0.3356, 0.2026],\n",
      "        [0.4725, 0.2557, 0.1758],\n",
      "        [0.4295, 0.2845, 0.2108],\n",
      "        [0.3962, 0.2995, 0.2521],\n",
      "        [0.3805, 0.2819, 0.2640],\n",
      "        [0.4324, 0.2930, 0.2038],\n",
      "        [0.4265, 0.2914, 0.2024],\n",
      "        [0.4851, 0.2493, 0.1792],\n",
      "        [0.4072, 0.2969, 0.2473],\n",
      "        [0.4033, 0.3244, 0.2151],\n",
      "        [0.4115, 0.2968, 0.1982],\n",
      "        [0.4247, 0.2932, 0.2423],\n",
      "        [0.3863, 0.2936, 0.2359],\n",
      "        [0.3873, 0.3091, 0.2455],\n",
      "        [0.4085, 0.3025, 0.2480],\n",
      "        [0.4062, 0.3329, 0.2133],\n",
      "        [0.4109, 0.2958, 0.2435],\n",
      "        [0.4770, 0.2350, 0.1331],\n",
      "        [0.4888, 0.1850, 0.1088],\n",
      "        [0.4100, 0.2942, 0.2345],\n",
      "        [0.4741, 0.2590, 0.1480],\n",
      "        [0.4245, 0.2883, 0.2137],\n",
      "        [0.4454, 0.2618, 0.2095],\n",
      "        [0.4783, 0.2634, 0.1697],\n",
      "        [0.4810, 0.2332, 0.1852],\n",
      "        [0.4527, 0.2859, 0.1948],\n",
      "        [0.4704, 0.2468, 0.1114],\n",
      "        [0.3831, 0.2924, 0.2755],\n",
      "        [0.4367, 0.2582, 0.2143],\n",
      "        [0.4307, 0.2760, 0.2171],\n",
      "        [0.4400, 0.2096, 0.2098],\n",
      "        [0.4120, 0.2976, 0.2177],\n",
      "        [0.3750, 0.2956, 0.2620],\n",
      "        [0.4396, 0.2984, 0.1916],\n",
      "        [0.3769, 0.3204, 0.2646],\n",
      "        [0.3947, 0.3035, 0.2542],\n",
      "        [0.4710, 0.2138, 0.1760],\n",
      "        [0.4634, 0.2574, 0.1961],\n",
      "        [0.3840, 0.3038, 0.2594],\n",
      "        [0.3913, 0.3102, 0.2552],\n",
      "        [0.3874, 0.3127, 0.2405],\n",
      "        [0.5443, 0.2205, 0.1459],\n",
      "        [0.4834, 0.2563, 0.1384],\n",
      "        [0.4132, 0.2835, 0.2263],\n",
      "        [0.3996, 0.2916, 0.2408],\n",
      "        [0.3998, 0.3056, 0.2342],\n",
      "        [0.3708, 0.3246, 0.2527],\n",
      "        [0.4049, 0.3243, 0.2099],\n",
      "        [0.3930, 0.2981, 0.2607],\n",
      "        [0.4317, 0.3222, 0.1783],\n",
      "        [0.4888, 0.2634, 0.1683],\n",
      "        [0.5254, 0.2024, 0.1347],\n",
      "        [0.4036, 0.3027, 0.2375],\n",
      "        [0.4492, 0.2836, 0.1927],\n",
      "        [0.3782, 0.3022, 0.2669],\n",
      "        [0.4326, 0.2885, 0.2050],\n",
      "        [0.3777, 0.3122, 0.2502],\n",
      "        [0.3806, 0.3242, 0.2464],\n",
      "        [0.3911, 0.3102, 0.2320],\n",
      "        [0.3877, 0.3129, 0.2488],\n",
      "        [0.5142, 0.2474, 0.1288],\n",
      "        [0.4127, 0.3167, 0.2174],\n",
      "        [0.4011, 0.3039, 0.2357],\n",
      "        [0.4032, 0.3265, 0.2221],\n",
      "        [0.3774, 0.3294, 0.2468],\n",
      "        [0.5499, 0.1619, 0.0849],\n",
      "        [0.4559, 0.2583, 0.1722],\n",
      "        [0.4243, 0.3102, 0.1939],\n",
      "        [0.4252, 0.2753, 0.2478],\n",
      "        [0.4247, 0.2816, 0.2271]], device='cuda:0', grad_fn=<ViewBackward0>), tensor([[0.3870, 0.2811, 0.2338],\n",
      "        [0.4188, 0.2210, 0.1713],\n",
      "        [0.3546, 0.3145, 0.2431],\n",
      "        [0.3536, 0.3204, 0.2800],\n",
      "        [0.3315, 0.3246, 0.2943],\n",
      "        [0.3447, 0.3310, 0.2828],\n",
      "        [0.3159, 0.3209, 0.2910],\n",
      "        [0.3261, 0.3441, 0.2845],\n",
      "        [0.3518, 0.3088, 0.2066],\n",
      "        [0.3314, 0.3295, 0.2895],\n",
      "        [0.3361, 0.3225, 0.2689],\n",
      "        [0.3432, 0.3192, 0.2380],\n",
      "        [0.3310, 0.3361, 0.2911],\n",
      "        [0.3317, 0.3461, 0.2775],\n",
      "        [0.3268, 0.3283, 0.2854],\n",
      "        [0.3579, 0.3278, 0.2658],\n",
      "        [0.3585, 0.2999, 0.2104],\n",
      "        [0.3273, 0.3256, 0.2862],\n",
      "        [0.3215, 0.3479, 0.2859],\n",
      "        [0.3219, 0.3100, 0.2595],\n",
      "        [0.2929, 0.2777, 0.2566],\n",
      "        [0.3527, 0.3179, 0.2754],\n",
      "        [0.3334, 0.3277, 0.3034],\n",
      "        [0.3724, 0.3111, 0.2302],\n",
      "        [0.3346, 0.2941, 0.1926],\n",
      "        [0.3574, 0.3057, 0.2405],\n",
      "        [0.3387, 0.2676, 0.1983],\n",
      "        [0.3754, 0.2864, 0.2161],\n",
      "        [0.3403, 0.3154, 0.2858],\n",
      "        [0.3399, 0.3344, 0.2760],\n",
      "        [0.3264, 0.3451, 0.2817],\n",
      "        [0.3029, 0.3233, 0.2870],\n",
      "        [0.3354, 0.3418, 0.2838],\n",
      "        [0.3375, 0.3312, 0.2714],\n",
      "        [0.3317, 0.3381, 0.2771],\n",
      "        [0.3372, 0.2931, 0.2732],\n",
      "        [0.3557, 0.3233, 0.2464],\n",
      "        [0.3431, 0.3253, 0.2735],\n",
      "        [0.4068, 0.2804, 0.1920],\n",
      "        [0.3385, 0.3295, 0.2793],\n",
      "        [0.3335, 0.3237, 0.2881],\n",
      "        [0.3468, 0.2717, 0.1928],\n",
      "        [0.3606, 0.3042, 0.2794],\n",
      "        [0.3860, 0.2761, 0.1831],\n",
      "        [0.3341, 0.3413, 0.2784],\n",
      "        [0.3657, 0.2861, 0.2504],\n",
      "        [0.3625, 0.3069, 0.2579],\n",
      "        [0.3673, 0.2997, 0.2271],\n",
      "        [0.3233, 0.3380, 0.2954],\n",
      "        [0.3333, 0.3306, 0.2824],\n",
      "        [0.3565, 0.2858, 0.2288],\n",
      "        [0.3421, 0.3215, 0.2931],\n",
      "        [0.3470, 0.2782, 0.2383],\n",
      "        [0.3367, 0.3277, 0.2857],\n",
      "        [0.3382, 0.3273, 0.2839],\n",
      "        [0.3420, 0.2978, 0.2789],\n",
      "        [0.3599, 0.3144, 0.2650],\n",
      "        [0.3579, 0.2982, 0.2467],\n",
      "        [0.4247, 0.2111, 0.1731],\n",
      "        [0.3241, 0.3342, 0.2984],\n",
      "        [0.3262, 0.3448, 0.2829],\n",
      "        [0.2962, 0.3233, 0.3143],\n",
      "        [0.3354, 0.3451, 0.2739],\n",
      "        [0.3309, 0.3467, 0.2677],\n",
      "        [0.3207, 0.3008, 0.1825],\n",
      "        [0.3282, 0.3478, 0.2886],\n",
      "        [0.3324, 0.3445, 0.2670],\n",
      "        [0.3480, 0.3211, 0.2697],\n",
      "        [0.3368, 0.3166, 0.2892],\n",
      "        [0.3335, 0.3392, 0.2650],\n",
      "        [0.3391, 0.2986, 0.2960],\n",
      "        [0.3438, 0.3164, 0.2760],\n",
      "        [0.3354, 0.3357, 0.2758],\n",
      "        [0.3295, 0.3248, 0.2806],\n",
      "        [0.3309, 0.3098, 0.2677],\n",
      "        [0.3242, 0.3203, 0.2506],\n",
      "        [0.3585, 0.2854, 0.2517],\n",
      "        [0.3196, 0.3089, 0.2948],\n",
      "        [0.3484, 0.3037, 0.2456],\n",
      "        [0.3248, 0.3384, 0.2817],\n",
      "        [0.3437, 0.3258, 0.2778],\n",
      "        [0.3601, 0.3133, 0.2589],\n",
      "        [0.3308, 0.3175, 0.2780],\n",
      "        [0.3560, 0.3217, 0.2524],\n",
      "        [0.3779, 0.3067, 0.2275],\n",
      "        [0.3256, 0.3454, 0.2805],\n",
      "        [0.3279, 0.3127, 0.2828],\n",
      "        [0.3633, 0.3005, 0.2548],\n",
      "        [0.3403, 0.3360, 0.2741],\n",
      "        [0.3608, 0.2964, 0.2687],\n",
      "        [0.3311, 0.3421, 0.2663],\n",
      "        [0.3118, 0.3328, 0.2828],\n",
      "        [0.3022, 0.3307, 0.3009],\n",
      "        [0.3271, 0.3483, 0.2800],\n",
      "        [0.3462, 0.2989, 0.2377],\n",
      "        [0.3565, 0.3050, 0.1953],\n",
      "        [0.3397, 0.3244, 0.2817],\n",
      "        [0.2644, 0.3189, 0.2447],\n",
      "        [0.3484, 0.3149, 0.2718],\n",
      "        [0.3364, 0.3089, 0.2683],\n",
      "        [0.3285, 0.3270, 0.2657],\n",
      "        [0.3544, 0.3073, 0.2608],\n",
      "        [0.3540, 0.3153, 0.2687],\n",
      "        [0.3596, 0.3001, 0.1865],\n",
      "        [0.3469, 0.3283, 0.2743],\n",
      "        [0.3346, 0.3010, 0.2686],\n",
      "        [0.3225, 0.3267, 0.2888],\n",
      "        [0.3569, 0.3056, 0.2329],\n",
      "        [0.3203, 0.3252, 0.2833],\n",
      "        [0.3640, 0.3100, 0.2592],\n",
      "        [0.3242, 0.3232, 0.2772],\n",
      "        [0.3455, 0.3283, 0.2692],\n",
      "        [0.3304, 0.3257, 0.2862],\n",
      "        [0.3668, 0.2921, 0.2333],\n",
      "        [0.3509, 0.3011, 0.2771],\n",
      "        [0.3428, 0.3284, 0.2697],\n",
      "        [0.3218, 0.3505, 0.2855],\n",
      "        [0.3308, 0.3221, 0.2910],\n",
      "        [0.3738, 0.2898, 0.2170],\n",
      "        [0.3497, 0.2761, 0.2424],\n",
      "        [0.3590, 0.3178, 0.2706],\n",
      "        [0.3482, 0.3212, 0.2690],\n",
      "        [0.3264, 0.3202, 0.2931],\n",
      "        [0.3503, 0.3203, 0.2794],\n",
      "        [0.3525, 0.3299, 0.2707],\n",
      "        [0.3293, 0.3395, 0.2844],\n",
      "        [0.3247, 0.3043, 0.2895],\n",
      "        [0.3527, 0.3083, 0.2431],\n",
      "        [0.3240, 0.2840, 0.2401],\n",
      "        [0.3586, 0.3347, 0.2614],\n",
      "        [0.3218, 0.3138, 0.2830],\n",
      "        [0.3307, 0.3347, 0.2843],\n",
      "        [0.3411, 0.3164, 0.2777],\n",
      "        [0.3498, 0.3141, 0.2741],\n",
      "        [0.3313, 0.3356, 0.2770],\n",
      "        [0.3368, 0.3299, 0.2814],\n",
      "        [0.3436, 0.3237, 0.2842],\n",
      "        [0.3660, 0.2574, 0.2077],\n",
      "        [0.3407, 0.3486, 0.2651],\n",
      "        [0.3377, 0.3270, 0.2793],\n",
      "        [0.3275, 0.3246, 0.3017],\n",
      "        [0.3430, 0.3193, 0.2927],\n",
      "        [0.3342, 0.2599, 0.2368],\n",
      "        [0.3578, 0.3007, 0.2458],\n",
      "        [0.3353, 0.3129, 0.2754],\n",
      "        [0.3094, 0.3191, 0.3086],\n",
      "        [0.3464, 0.3274, 0.2545]], device='cuda:0', grad_fn=<ViewBackward0>), tensor([[0.2410, 0.2338, 0.2281, 0.2198],\n",
      "        [0.2011, 0.2368, 0.1758, 0.1971],\n",
      "        [0.2318, 0.2147, 0.2169, 0.2265],\n",
      "        [0.2440, 0.2378, 0.2288, 0.2348],\n",
      "        [0.2360, 0.2550, 0.2346, 0.2290],\n",
      "        [0.2457, 0.2417, 0.2378, 0.2279],\n",
      "        [0.2255, 0.2382, 0.2118, 0.2164],\n",
      "        [0.2441, 0.2454, 0.2362, 0.2359],\n",
      "        [0.2258, 0.2177, 0.2096, 0.2225],\n",
      "        [0.2437, 0.2441, 0.2352, 0.2353],\n",
      "        [0.2447, 0.2319, 0.2363, 0.2068],\n",
      "        [0.2115, 0.2276, 0.2365, 0.2188],\n",
      "        [0.2419, 0.2490, 0.2395, 0.2353],\n",
      "        [0.2429, 0.2347, 0.2380, 0.2288],\n",
      "        [0.2406, 0.2437, 0.2345, 0.2278],\n",
      "        [0.2311, 0.2410, 0.2373, 0.2328],\n",
      "        [0.2379, 0.2258, 0.1874, 0.2271],\n",
      "        [0.2363, 0.2311, 0.2313, 0.2355],\n",
      "        [0.2380, 0.2511, 0.2386, 0.2371],\n",
      "        [0.2080, 0.2379, 0.2165, 0.2335],\n",
      "        [0.2174, 0.2469, 0.2019, 0.2070],\n",
      "        [0.2421, 0.2456, 0.2289, 0.2272],\n",
      "        [0.2384, 0.2544, 0.2334, 0.2264],\n",
      "        [0.2317, 0.2321, 0.2278, 0.2169],\n",
      "        [0.2006, 0.1956, 0.2474, 0.2180],\n",
      "        [0.2417, 0.2384, 0.2272, 0.2132],\n",
      "        [0.2135, 0.1969, 0.2331, 0.1918],\n",
      "        [0.2415, 0.2273, 0.2045, 0.2276],\n",
      "        [0.2454, 0.2409, 0.2331, 0.2271],\n",
      "        [0.2477, 0.2411, 0.2382, 0.2337],\n",
      "        [0.2430, 0.2526, 0.2277, 0.2272],\n",
      "        [0.2202, 0.2599, 0.2117, 0.2277],\n",
      "        [0.2376, 0.2454, 0.2446, 0.2316],\n",
      "        [0.2379, 0.2417, 0.2337, 0.2289],\n",
      "        [0.2464, 0.2370, 0.2422, 0.2205],\n",
      "        [0.2453, 0.2237, 0.2253, 0.2249],\n",
      "        [0.2431, 0.2404, 0.2185, 0.2145],\n",
      "        [0.2467, 0.2392, 0.2429, 0.2171],\n",
      "        [0.2377, 0.2340, 0.1804, 0.2477],\n",
      "        [0.2396, 0.2366, 0.2431, 0.2349],\n",
      "        [0.2435, 0.2494, 0.2318, 0.2313],\n",
      "        [0.2210, 0.2362, 0.1864, 0.2218],\n",
      "        [0.2350, 0.2492, 0.2238, 0.2337],\n",
      "        [0.2404, 0.2531, 0.1993, 0.1859],\n",
      "        [0.2408, 0.2386, 0.2303, 0.2333],\n",
      "        [0.2210, 0.2422, 0.2382, 0.2280],\n",
      "        [0.2351, 0.2440, 0.2303, 0.2297],\n",
      "        [0.2223, 0.2351, 0.2296, 0.2384],\n",
      "        [0.2379, 0.2540, 0.2358, 0.2285],\n",
      "        [0.2385, 0.2454, 0.2391, 0.2289],\n",
      "        [0.2258, 0.2255, 0.2297, 0.2137],\n",
      "        [0.2366, 0.2530, 0.2361, 0.2332],\n",
      "        [0.2392, 0.2190, 0.2323, 0.2216],\n",
      "        [0.2392, 0.2431, 0.2430, 0.2292],\n",
      "        [0.2508, 0.2426, 0.2364, 0.2293],\n",
      "        [0.2221, 0.2540, 0.2046, 0.2435],\n",
      "        [0.2416, 0.2451, 0.2294, 0.2299],\n",
      "        [0.2293, 0.2434, 0.2366, 0.2103],\n",
      "        [0.2006, 0.2514, 0.1853, 0.2089],\n",
      "        [0.2455, 0.2506, 0.2254, 0.2410],\n",
      "        [0.2482, 0.2425, 0.2346, 0.2284],\n",
      "        [0.2301, 0.2477, 0.2308, 0.2385],\n",
      "        [0.2405, 0.2470, 0.2183, 0.2373],\n",
      "        [0.2196, 0.2459, 0.2466, 0.2283],\n",
      "        [0.1901, 0.2009, 0.2345, 0.2224],\n",
      "        [0.2456, 0.2407, 0.2361, 0.2351],\n",
      "        [0.2440, 0.2437, 0.2318, 0.2323],\n",
      "        [0.2307, 0.2477, 0.2239, 0.2322],\n",
      "        [0.2403, 0.2560, 0.2399, 0.2231],\n",
      "        [0.2423, 0.2431, 0.2207, 0.2302],\n",
      "        [0.2216, 0.2430, 0.2048, 0.2391],\n",
      "        [0.2332, 0.2564, 0.2387, 0.2312],\n",
      "        [0.2429, 0.2445, 0.2409, 0.2297],\n",
      "        [0.2280, 0.2393, 0.2448, 0.2262],\n",
      "        [0.2405, 0.2462, 0.2327, 0.2138],\n",
      "        [0.2431, 0.2567, 0.2074, 0.2167],\n",
      "        [0.2569, 0.2305, 0.2180, 0.2239],\n",
      "        [0.2342, 0.2464, 0.2181, 0.2410],\n",
      "        [0.2404, 0.2295, 0.2190, 0.2266],\n",
      "        [0.2473, 0.2496, 0.2229, 0.2314],\n",
      "        [0.2445, 0.2488, 0.2393, 0.2280],\n",
      "        [0.2293, 0.2487, 0.2365, 0.2166],\n",
      "        [0.2464, 0.2490, 0.2158, 0.2298],\n",
      "        [0.2218, 0.2467, 0.2179, 0.2308],\n",
      "        [0.2235, 0.2344, 0.2329, 0.2111],\n",
      "        [0.2456, 0.2507, 0.2327, 0.2306],\n",
      "        [0.2353, 0.2488, 0.2297, 0.2272],\n",
      "        [0.2166, 0.2426, 0.2169, 0.2522],\n",
      "        [0.2453, 0.2408, 0.2290, 0.2307],\n",
      "        [0.2299, 0.2402, 0.2206, 0.2274],\n",
      "        [0.2402, 0.2366, 0.2319, 0.2410],\n",
      "        [0.2427, 0.2427, 0.2310, 0.2306],\n",
      "        [0.2287, 0.2454, 0.2254, 0.2510],\n",
      "        [0.2483, 0.2471, 0.2330, 0.2270],\n",
      "        [0.2528, 0.2393, 0.2148, 0.2041],\n",
      "        [0.2241, 0.2233, 0.2255, 0.2150],\n",
      "        [0.2292, 0.2449, 0.2179, 0.2389],\n",
      "        [0.2041, 0.2554, 0.2195, 0.2280],\n",
      "        [0.2339, 0.2507, 0.2369, 0.2217],\n",
      "        [0.2433, 0.2290, 0.2264, 0.2236],\n",
      "        [0.2493, 0.2442, 0.2259, 0.2180],\n",
      "        [0.2244, 0.2432, 0.2183, 0.2277],\n",
      "        [0.2361, 0.2458, 0.2299, 0.2199],\n",
      "        [0.2098, 0.2517, 0.1999, 0.2254],\n",
      "        [0.2401, 0.2440, 0.2453, 0.2352],\n",
      "        [0.2373, 0.2392, 0.2360, 0.2070],\n",
      "        [0.2481, 0.2400, 0.2254, 0.2308],\n",
      "        [0.2150, 0.2284, 0.2492, 0.2088],\n",
      "        [0.2454, 0.2419, 0.2326, 0.2249],\n",
      "        [0.2338, 0.2392, 0.2390, 0.2253],\n",
      "        [0.2313, 0.2645, 0.2058, 0.2224],\n",
      "        [0.2457, 0.2437, 0.2376, 0.2303],\n",
      "        [0.2458, 0.2461, 0.2337, 0.2361],\n",
      "        [0.2412, 0.2333, 0.2220, 0.1917],\n",
      "        [0.2442, 0.2347, 0.2356, 0.2154],\n",
      "        [0.2330, 0.2349, 0.2362, 0.2369],\n",
      "        [0.2407, 0.2398, 0.2341, 0.2358],\n",
      "        [0.2347, 0.2497, 0.2348, 0.2209],\n",
      "        [0.2141, 0.2349, 0.2064, 0.2298],\n",
      "        [0.2382, 0.2429, 0.2039, 0.2118],\n",
      "        [0.2309, 0.2501, 0.2327, 0.2087],\n",
      "        [0.2335, 0.2511, 0.2247, 0.2369],\n",
      "        [0.2391, 0.2577, 0.2320, 0.2305],\n",
      "        [0.2392, 0.2500, 0.2373, 0.2333],\n",
      "        [0.2362, 0.2502, 0.2211, 0.2359],\n",
      "        [0.2446, 0.2512, 0.2322, 0.2287],\n",
      "        [0.2233, 0.2642, 0.2094, 0.2284],\n",
      "        [0.2311, 0.2272, 0.2285, 0.2327],\n",
      "        [0.2161, 0.2398, 0.2328, 0.2094],\n",
      "        [0.2332, 0.2384, 0.2409, 0.2376],\n",
      "        [0.2436, 0.2426, 0.2279, 0.2225],\n",
      "        [0.2410, 0.2380, 0.2355, 0.2345],\n",
      "        [0.2419, 0.2555, 0.2207, 0.2159],\n",
      "        [0.2481, 0.2473, 0.2238, 0.2297],\n",
      "        [0.2330, 0.2484, 0.2334, 0.2362],\n",
      "        [0.2519, 0.2455, 0.2302, 0.2286],\n",
      "        [0.2497, 0.2451, 0.2320, 0.2291],\n",
      "        [0.2115, 0.2336, 0.2144, 0.2158],\n",
      "        [0.2307, 0.2521, 0.2174, 0.2483],\n",
      "        [0.2394, 0.2535, 0.2346, 0.2280],\n",
      "        [0.2442, 0.2443, 0.2322, 0.2261],\n",
      "        [0.2475, 0.2445, 0.2274, 0.2359],\n",
      "        [0.2069, 0.2333, 0.1782, 0.2343],\n",
      "        [0.2076, 0.2540, 0.2293, 0.2405],\n",
      "        [0.2300, 0.2375, 0.2176, 0.2420],\n",
      "        [0.2285, 0.2427, 0.2358, 0.2321],\n",
      "        [0.2322, 0.2426, 0.2181, 0.2301]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>), tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1991, 0.3027, 0.2295, 0.3407, 0.3360],\n",
      "        [0.3078, 0.3867, 0.3034, 0.3526, 0.4085],\n",
      "        [0.4027, 0.4324, 0.3907, 0.4261, 0.4034],\n",
      "        [0.3984, 0.4369, 0.3888, 0.4045, 0.4208],\n",
      "        [0.3860, 0.4368, 0.3860, 0.4204, 0.4185],\n",
      "        [0.3434, 0.4396, 0.3278, 0.3311, 0.4134],\n",
      "        [0.4000, 0.4254, 0.3875, 0.4136, 0.4168],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3527, 0.4273, 0.3325, 0.3577, 0.3707],\n",
      "        [0.3441, 0.4077, 0.2858, 0.3963, 0.3985],\n",
      "        [0.3820, 0.4178, 0.3881, 0.4306, 0.4467],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4144, 0.4361, 0.3814, 0.3935, 0.4480],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2557, 0.3014, 0.2654, 0.3195, 0.3570],\n",
      "        [0.3943, 0.4561, 0.3854, 0.4186, 0.4581],\n",
      "        [0.3852, 0.4152, 0.3830, 0.4119, 0.4428],\n",
      "        [0.2859, 0.4088, 0.3095, 0.3293, 0.3234],\n",
      "        [0.3016, 0.3723, 0.2553, 0.2880, 0.3823],\n",
      "        [0.4041, 0.4217, 0.3867, 0.4393, 0.4460],\n",
      "        [0.4084, 0.4423, 0.3826, 0.3823, 0.4293],\n",
      "        [0.3408, 0.3618, 0.3226, 0.3178, 0.3700],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3561, 0.4262, 0.3325, 0.4248, 0.3933],\n",
      "        [0.2412, 0.2864, 0.2282, 0.2567, 0.3487],\n",
      "        [0.3319, 0.4037, 0.3157, 0.4127, 0.3882],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3734, 0.4094, 0.3799, 0.3943, 0.4185],\n",
      "        [0.3448, 0.4384, 0.3100, 0.3384, 0.3990],\n",
      "        [0.3959, 0.4277, 0.3844, 0.4468, 0.4440],\n",
      "        [0.3818, 0.4152, 0.3808, 0.4092, 0.4258],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3205, 0.3885, 0.2898, 0.3817, 0.4147],\n",
      "        [0.3376, 0.3715, 0.2864, 0.3871, 0.3919],\n",
      "        [0.3711, 0.4252, 0.3691, 0.4128, 0.4340],\n",
      "        [0.3335, 0.4221, 0.2512, 0.3959, 0.2948],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4109, 0.4286, 0.4008, 0.3965, 0.4330],\n",
      "        [0.2419, 0.3541, 0.2833, 0.3079, 0.2715],\n",
      "        [0.3672, 0.4292, 0.3645, 0.3919, 0.4196],\n",
      "        [0.2674, 0.3562, 0.2487, 0.3153, 0.3783],\n",
      "        [0.3738, 0.4411, 0.3748, 0.4112, 0.4265],\n",
      "        [0.3796, 0.3690, 0.3767, 0.3753, 0.3943],\n",
      "        [0.3951, 0.4394, 0.3806, 0.4201, 0.4436],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3911, 0.4420, 0.3788, 0.4027, 0.4454],\n",
      "        [0.3753, 0.4314, 0.3818, 0.4247, 0.4364],\n",
      "        [0.3177, 0.4088, 0.2924, 0.2821, 0.3765],\n",
      "        [0.4130, 0.4326, 0.3966, 0.4094, 0.4434],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3857, 0.4259, 0.3802, 0.4266, 0.4388],\n",
      "        [0.3972, 0.4143, 0.3780, 0.4191, 0.4294],\n",
      "        [0.3730, 0.3904, 0.3305, 0.3896, 0.4222],\n",
      "        [0.3703, 0.4203, 0.3769, 0.4013, 0.4394],\n",
      "        [0.3975, 0.3948, 0.3795, 0.3254, 0.4044],\n",
      "        [0.1930, 0.2971, 0.2210, 0.2849, 0.3060],\n",
      "        [0.3880, 0.4278, 0.3813, 0.4013, 0.4354],\n",
      "        [0.4073, 0.4284, 0.3774, 0.4196, 0.4251],\n",
      "        [0.4014, 0.4309, 0.3680, 0.3838, 0.4438],\n",
      "        [0.4103, 0.4237, 0.3361, 0.3842, 0.4284],\n",
      "        [0.3641, 0.3945, 0.3634, 0.4156, 0.4076],\n",
      "        [0.2552, 0.2685, 0.1804, 0.2762, 0.3332],\n",
      "        [0.3631, 0.4339, 0.3698, 0.4198, 0.4138],\n",
      "        [0.3805, 0.4196, 0.3763, 0.4076, 0.4218],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4123, 0.4283, 0.3763, 0.4216, 0.4249],\n",
      "        [0.3783, 0.4155, 0.3628, 0.4054, 0.4101],\n",
      "        [0.3428, 0.3680, 0.2911, 0.3902, 0.3690],\n",
      "        [0.4168, 0.4487, 0.3977, 0.4197, 0.4392],\n",
      "        [0.3874, 0.4358, 0.3869, 0.4439, 0.4475],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3119, 0.4380, 0.3323, 0.3730, 0.3839],\n",
      "        [0.3420, 0.3352, 0.3156, 0.3115, 0.3419],\n",
      "        [0.3346, 0.3889, 0.3185, 0.4328, 0.3730],\n",
      "        [0.3855, 0.4399, 0.3693, 0.4457, 0.4425],\n",
      "        [0.3255, 0.3844, 0.2690, 0.3490, 0.3739],\n",
      "        [0.3382, 0.4341, 0.3651, 0.3987, 0.4223],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3931, 0.4026, 0.3773, 0.4034, 0.4323],\n",
      "        [0.3696, 0.4286, 0.3634, 0.3982, 0.4315],\n",
      "        [0.3758, 0.4420, 0.3840, 0.4125, 0.4056],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3880, 0.4225, 0.3835, 0.4219, 0.4087],\n",
      "        [0.3825, 0.4404, 0.3880, 0.3804, 0.4142],\n",
      "        [0.3427, 0.4350, 0.3775, 0.4248, 0.3857],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3820, 0.4018, 0.3164, 0.3601, 0.4221],\n",
      "        [0.3617, 0.4168, 0.3923, 0.4022, 0.3901],\n",
      "        [0.3652, 0.4278, 0.3907, 0.4287, 0.4056],\n",
      "        [0.3865, 0.4360, 0.3500, 0.4313, 0.4491],\n",
      "        [0.3696, 0.4303, 0.3650, 0.3837, 0.4051],\n",
      "        [0.2754, 0.3956, 0.2986, 0.3517, 0.3960],\n",
      "        [0.2812, 0.3792, 0.2680, 0.2908, 0.4087],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3202, 0.3698, 0.3085, 0.3668, 0.4390],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3612, 0.4379, 0.3027, 0.3654, 0.3809],\n",
      "        [0.3274, 0.3789, 0.3323, 0.3404, 0.4184],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3622, 0.4147, 0.2923, 0.3682, 0.4020],\n",
      "        [0.3008, 0.3047, 0.2222, 0.3400, 0.3885],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3723, 0.3993, 0.3931, 0.4230, 0.4117],\n",
      "        [0.3563, 0.4207, 0.3102, 0.4115, 0.3977],\n",
      "        [0.3794, 0.4392, 0.3839, 0.4368, 0.4475],\n",
      "        [0.3772, 0.4189, 0.3730, 0.4106, 0.4604],\n",
      "        [0.3537, 0.3375, 0.3463, 0.2993, 0.3272],\n",
      "        [0.3376, 0.3905, 0.3172, 0.3610, 0.3705],\n",
      "        [0.3923, 0.4034, 0.4054, 0.4080, 0.4041],\n",
      "        [0.3692, 0.4330, 0.4013, 0.4070, 0.4102],\n",
      "        [0.3754, 0.4197, 0.3915, 0.4203, 0.4466],\n",
      "        [0.3281, 0.4113, 0.2954, 0.3485, 0.3187],\n",
      "        [0.3301, 0.3540, 0.2572, 0.3735, 0.3260],\n",
      "        [0.3781, 0.4033, 0.3405, 0.3851, 0.4154],\n",
      "        [0.3961, 0.4006, 0.3851, 0.3848, 0.4341],\n",
      "        [0.4034, 0.4410, 0.3876, 0.3787, 0.4346],\n",
      "        [0.3834, 0.4422, 0.3775, 0.4440, 0.4434],\n",
      "        [0.4048, 0.4190, 0.3291, 0.4137, 0.4168],\n",
      "        [0.3880, 0.4297, 0.3578, 0.4208, 0.4431],\n",
      "        [0.3501, 0.4053, 0.3271, 0.3555, 0.4193],\n",
      "        [0.3257, 0.3953, 0.3259, 0.3375, 0.3885],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3414, 0.3801, 0.3326, 0.3669, 0.4239],\n",
      "        [0.3879, 0.4280, 0.4037, 0.4359, 0.4207],\n",
      "        [0.3702, 0.4432, 0.3899, 0.3789, 0.4310],\n",
      "        [0.4091, 0.4299, 0.3694, 0.3976, 0.4270],\n",
      "        [0.3797, 0.4438, 0.3613, 0.4324, 0.4330],\n",
      "        [0.3869, 0.4277, 0.3862, 0.3995, 0.4209],\n",
      "        [0.4096, 0.4346, 0.3832, 0.4017, 0.4528],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4042, 0.4284, 0.3605, 0.4202, 0.4395],\n",
      "        [0.3999, 0.4448, 0.3807, 0.4084, 0.4468],\n",
      "        [0.3873, 0.4199, 0.3775, 0.3766, 0.4259],\n",
      "        [0.3962, 0.4353, 0.3949, 0.4112, 0.4358],\n",
      "        [0.1898, 0.2867, 0.2089, 0.2429, 0.3406],\n",
      "        [0.3398, 0.4222, 0.3272, 0.3379, 0.3699],\n",
      "        [0.3792, 0.4369, 0.3711, 0.4198, 0.4128],\n",
      "        [0.4132, 0.4199, 0.3663, 0.3831, 0.4463],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)], '95%': [tensor([[0.4915, 0.3222, 0.2956],\n",
      "        [0.7788, 0.2794, 0.1478],\n",
      "        [0.4933, 0.3865, 0.2598],\n",
      "        [0.4125, 0.3753, 0.2712],\n",
      "        [0.4562, 0.3424, 0.2560],\n",
      "        [0.4464, 0.3255, 0.2824],\n",
      "        [0.5263, 0.3339, 0.2490],\n",
      "        [0.4397, 0.3280, 0.2800],\n",
      "        [0.6276, 0.3217, 0.2102],\n",
      "        [0.4316, 0.3445, 0.2838],\n",
      "        [0.4765, 0.3439, 0.2750],\n",
      "        [0.4401, 0.3498, 0.2835],\n",
      "        [0.4297, 0.3508, 0.2742],\n",
      "        [0.4329, 0.3241, 0.2950],\n",
      "        [0.4286, 0.3415, 0.2803],\n",
      "        [0.4238, 0.3467, 0.2876],\n",
      "        [0.6235, 0.3317, 0.1942],\n",
      "        [0.4787, 0.3286, 0.2434],\n",
      "        [0.4266, 0.3301, 0.2848],\n",
      "        [0.5775, 0.3258, 0.1864],\n",
      "        [0.6930, 0.2977, 0.1863],\n",
      "        [0.4163, 0.3512, 0.2861],\n",
      "        [0.4255, 0.3572, 0.2644],\n",
      "        [0.5094, 0.3719, 0.2595],\n",
      "        [0.7756, 0.3305, 0.1395],\n",
      "        [0.4845, 0.3367, 0.2632],\n",
      "        [0.7079, 0.3265, 0.2515],\n",
      "        [0.5574, 0.3436, 0.2353],\n",
      "        [0.4486, 0.3245, 0.2682],\n",
      "        [0.4394, 0.3153, 0.2882],\n",
      "        [0.4531, 0.3307, 0.2633],\n",
      "        [0.5470, 0.3348, 0.2226],\n",
      "        [0.4076, 0.3362, 0.2871],\n",
      "        [0.4382, 0.3715, 0.2623],\n",
      "        [0.4793, 0.2999, 0.2843],\n",
      "        [0.5011, 0.3258, 0.2700],\n",
      "        [0.5193, 0.3414, 0.2286],\n",
      "        [0.4617, 0.3324, 0.2713],\n",
      "        [0.5886, 0.3523, 0.1836],\n",
      "        [0.4396, 0.3265, 0.2894],\n",
      "        [0.4302, 0.3388, 0.2685],\n",
      "        [0.6629, 0.3637, 0.1808],\n",
      "        [0.4256, 0.3592, 0.2743],\n",
      "        [0.6488, 0.3443, 0.1813],\n",
      "        [0.4485, 0.3313, 0.2649],\n",
      "        [0.5222, 0.3269, 0.2866],\n",
      "        [0.4510, 0.3457, 0.2815],\n",
      "        [0.4918, 0.3415, 0.2483],\n",
      "        [0.4429, 0.3339, 0.2593],\n",
      "        [0.4113, 0.3423, 0.2892],\n",
      "        [0.6785, 0.3117, 0.2011],\n",
      "        [0.4201, 0.3492, 0.2738],\n",
      "        [0.5141, 0.2849, 0.3148],\n",
      "        [0.4035, 0.3582, 0.2859],\n",
      "        [0.4232, 0.3357, 0.2901],\n",
      "        [0.4958, 0.3428, 0.2367],\n",
      "        [0.4530, 0.3343, 0.2462],\n",
      "        [0.5140, 0.3613, 0.2326],\n",
      "        [0.8218, 0.2921, 0.1350],\n",
      "        [0.4510, 0.3356, 0.2598],\n",
      "        [0.4192, 0.3348, 0.2898],\n",
      "        [0.4767, 0.3219, 0.2622],\n",
      "        [0.4308, 0.3782, 0.2584],\n",
      "        [0.4447, 0.3450, 0.2642],\n",
      "        [0.7100, 0.3767, 0.1716],\n",
      "        [0.4348, 0.3432, 0.2779],\n",
      "        [0.4578, 0.3474, 0.2628],\n",
      "        [0.5029, 0.3368, 0.2304],\n",
      "        [0.4499, 0.3193, 0.2867],\n",
      "        [0.4477, 0.3368, 0.2622],\n",
      "        [0.5817, 0.3324, 0.1856],\n",
      "        [0.4240, 0.3593, 0.2681],\n",
      "        [0.4209, 0.3412, 0.2848],\n",
      "        [0.4402, 0.3220, 0.3001],\n",
      "        [0.5031, 0.3412, 0.2405],\n",
      "        [0.6538, 0.2756, 0.1949],\n",
      "        [0.5135, 0.3548, 0.2155],\n",
      "        [0.4614, 0.3821, 0.2301],\n",
      "        [0.5496, 0.3209, 0.2155],\n",
      "        [0.4965, 0.3276, 0.2531],\n",
      "        [0.4422, 0.3367, 0.2810],\n",
      "        [0.4507, 0.3372, 0.3012],\n",
      "        [0.4911, 0.3488, 0.2334],\n",
      "        [0.4870, 0.3489, 0.2499],\n",
      "        [0.5639, 0.3001, 0.2200],\n",
      "        [0.4491, 0.3335, 0.2703],\n",
      "        [0.4562, 0.3500, 0.2601],\n",
      "        [0.4982, 0.3626, 0.2304],\n",
      "        [0.4571, 0.3159, 0.2718],\n",
      "        [0.4562, 0.3394, 0.2794],\n",
      "        [0.4282, 0.3499, 0.2702],\n",
      "        [0.4456, 0.3203, 0.2740],\n",
      "        [0.4528, 0.3635, 0.2439],\n",
      "        [0.4499, 0.3259, 0.2678],\n",
      "        [0.6043, 0.3237, 0.2068],\n",
      "        [0.7141, 0.3504, 0.1937],\n",
      "        [0.4632, 0.3329, 0.2642],\n",
      "        [0.5724, 0.3569, 0.1832],\n",
      "        [0.4805, 0.3272, 0.2598],\n",
      "        [0.5296, 0.3187, 0.2453],\n",
      "        [0.5643, 0.3183, 0.2157],\n",
      "        [0.5751, 0.3016, 0.2369],\n",
      "        [0.5125, 0.3266, 0.2344],\n",
      "        [0.6132, 0.3847, 0.1564],\n",
      "        [0.4160, 0.3234, 0.3040],\n",
      "        [0.5322, 0.3099, 0.2694],\n",
      "        [0.5011, 0.3331, 0.2521],\n",
      "        [0.5730, 0.2923, 0.2735],\n",
      "        [0.4640, 0.3436, 0.2673],\n",
      "        [0.4235, 0.3352, 0.2955],\n",
      "        [0.5055, 0.3334, 0.2388],\n",
      "        [0.4092, 0.3390, 0.2976],\n",
      "        [0.4290, 0.3362, 0.2752],\n",
      "        [0.5962, 0.3068, 0.2258],\n",
      "        [0.5445, 0.3172, 0.2365],\n",
      "        [0.4336, 0.3305, 0.2928],\n",
      "        [0.4284, 0.3401, 0.2770],\n",
      "        [0.4359, 0.3515, 0.2711],\n",
      "        [0.6262, 0.2975, 0.1830],\n",
      "        [0.5867, 0.3390, 0.1898],\n",
      "        [0.4896, 0.3253, 0.2714],\n",
      "        [0.4570, 0.3286, 0.2833],\n",
      "        [0.4435, 0.3436, 0.2617],\n",
      "        [0.4084, 0.3657, 0.2776],\n",
      "        [0.4452, 0.3614, 0.2467],\n",
      "        [0.4343, 0.3311, 0.2809],\n",
      "        [0.4944, 0.3734, 0.2138],\n",
      "        [0.5615, 0.3094, 0.2118],\n",
      "        [0.6630, 0.2962, 0.1940],\n",
      "        [0.4534, 0.3417, 0.2628],\n",
      "        [0.5148, 0.3349, 0.2287],\n",
      "        [0.4184, 0.3383, 0.2974],\n",
      "        [0.4976, 0.3413, 0.2381],\n",
      "        [0.4291, 0.3511, 0.2816],\n",
      "        [0.4267, 0.3560, 0.2703],\n",
      "        [0.4519, 0.3558, 0.2644],\n",
      "        [0.4338, 0.3385, 0.2758],\n",
      "        [0.6086, 0.3192, 0.1805],\n",
      "        [0.4461, 0.3625, 0.2546],\n",
      "        [0.4389, 0.3412, 0.2648],\n",
      "        [0.4391, 0.3617, 0.2566],\n",
      "        [0.4073, 0.3568, 0.2718],\n",
      "        [0.7519, 0.3159, 0.1474],\n",
      "        [0.5673, 0.3324, 0.2293],\n",
      "        [0.4867, 0.3622, 0.2331],\n",
      "        [0.4692, 0.3080, 0.2794],\n",
      "        [0.4762, 0.3401, 0.2567]], device='cuda:0', grad_fn=<ViewBackward0>), tensor([[0.4760, 0.3390, 0.2806],\n",
      "        [0.6029, 0.3645, 0.2766],\n",
      "        [0.4305, 0.3696, 0.3129],\n",
      "        [0.3934, 0.3512, 0.3094],\n",
      "        [0.3669, 0.3584, 0.3243],\n",
      "        [0.3815, 0.3613, 0.3036],\n",
      "        [0.3617, 0.3781, 0.3346],\n",
      "        [0.3626, 0.3775, 0.3104],\n",
      "        [0.4640, 0.3901, 0.2739],\n",
      "        [0.3723, 0.3676, 0.3203],\n",
      "        [0.3835, 0.3674, 0.3203],\n",
      "        [0.4326, 0.3736, 0.2980],\n",
      "        [0.3634, 0.3565, 0.3277],\n",
      "        [0.3682, 0.3737, 0.3052],\n",
      "        [0.3695, 0.3701, 0.3219],\n",
      "        [0.3988, 0.3574, 0.2978],\n",
      "        [0.4511, 0.3649, 0.2944],\n",
      "        [0.3818, 0.3646, 0.3276],\n",
      "        [0.3500, 0.3749, 0.3209],\n",
      "        [0.3835, 0.4025, 0.3413],\n",
      "        [0.4303, 0.3394, 0.3972],\n",
      "        [0.4003, 0.3582, 0.3010],\n",
      "        [0.3739, 0.3585, 0.3294],\n",
      "        [0.4394, 0.3585, 0.2853],\n",
      "        [0.4988, 0.4463, 0.3007],\n",
      "        [0.4414, 0.3566, 0.2974],\n",
      "        [0.4940, 0.4204, 0.3059],\n",
      "        [0.4704, 0.3429, 0.2914],\n",
      "        [0.3795, 0.3546, 0.3247],\n",
      "        [0.3835, 0.3662, 0.3090],\n",
      "        [0.3657, 0.3786, 0.3065],\n",
      "        [0.3473, 0.3833, 0.3654],\n",
      "        [0.3598, 0.3710, 0.3053],\n",
      "        [0.3864, 0.3701, 0.3084],\n",
      "        [0.3723, 0.3703, 0.3121],\n",
      "        [0.4063, 0.3385, 0.3408],\n",
      "        [0.4149, 0.3780, 0.2861],\n",
      "        [0.3808, 0.3686, 0.3059],\n",
      "        [0.5015, 0.3493, 0.2641],\n",
      "        [0.3789, 0.3588, 0.3100],\n",
      "        [0.3799, 0.3542, 0.3270],\n",
      "        [0.5276, 0.4153, 0.2764],\n",
      "        [0.4044, 0.3409, 0.3119],\n",
      "        [0.5195, 0.3835, 0.2539],\n",
      "        [0.3729, 0.3791, 0.3067],\n",
      "        [0.4471, 0.3321, 0.3160],\n",
      "        [0.4233, 0.3485, 0.3035],\n",
      "        [0.4521, 0.3744, 0.2747],\n",
      "        [0.3594, 0.3718, 0.3249],\n",
      "        [0.3769, 0.3645, 0.3226],\n",
      "        [0.4580, 0.3754, 0.2762],\n",
      "        [0.3721, 0.3569, 0.3163],\n",
      "        [0.4672, 0.3664, 0.2976],\n",
      "        [0.3731, 0.3545, 0.3174],\n",
      "        [0.3749, 0.3580, 0.3181],\n",
      "        [0.4009, 0.3456, 0.3276],\n",
      "        [0.4000, 0.3531, 0.3098],\n",
      "        [0.4503, 0.3541, 0.3086],\n",
      "        [0.5989, 0.3297, 0.2676],\n",
      "        [0.3589, 0.3609, 0.3222],\n",
      "        [0.3620, 0.3696, 0.3180],\n",
      "        [0.3446, 0.3604, 0.3554],\n",
      "        [0.3714, 0.3748, 0.3004],\n",
      "        [0.3783, 0.3696, 0.3062],\n",
      "        [0.5080, 0.4398, 0.2691],\n",
      "        [0.3518, 0.3744, 0.3187],\n",
      "        [0.3715, 0.3889, 0.3013],\n",
      "        [0.3858, 0.3788, 0.3013],\n",
      "        [0.3757, 0.3518, 0.3227],\n",
      "        [0.3856, 0.3794, 0.2982],\n",
      "        [0.3924, 0.3435, 0.3380],\n",
      "        [0.3905, 0.3556, 0.3085],\n",
      "        [0.3689, 0.3683, 0.3024],\n",
      "        [0.3945, 0.3661, 0.3196],\n",
      "        [0.3987, 0.3750, 0.3465],\n",
      "        [0.4223, 0.3796, 0.3101],\n",
      "        [0.4389, 0.3378, 0.3470],\n",
      "        [0.3884, 0.3613, 0.3359],\n",
      "        [0.4070, 0.3769, 0.3170],\n",
      "        [0.3722, 0.3836, 0.3054],\n",
      "        [0.3831, 0.3580, 0.3096],\n",
      "        [0.4114, 0.3479, 0.2902],\n",
      "        [0.3873, 0.3690, 0.3169],\n",
      "        [0.4133, 0.3559, 0.3023],\n",
      "        [0.4480, 0.3667, 0.2783],\n",
      "        [0.3576, 0.3824, 0.3071],\n",
      "        [0.3817, 0.3631, 0.3242],\n",
      "        [0.4350, 0.3687, 0.2959],\n",
      "        [0.3664, 0.3721, 0.3048],\n",
      "        [0.4149, 0.3511, 0.3088],\n",
      "        [0.3660, 0.3890, 0.3002],\n",
      "        [0.3625, 0.3811, 0.3134],\n",
      "        [0.3594, 0.3589, 0.3457],\n",
      "        [0.3588, 0.3782, 0.3145],\n",
      "        [0.4616, 0.3815, 0.3026],\n",
      "        [0.4667, 0.4124, 0.2951],\n",
      "        [0.3766, 0.3549, 0.3201],\n",
      "        [0.3792, 0.3999, 0.4046],\n",
      "        [0.3984, 0.3526, 0.3073],\n",
      "        [0.3884, 0.3727, 0.3214],\n",
      "        [0.3736, 0.3967, 0.3146],\n",
      "        [0.4262, 0.3559, 0.3051],\n",
      "        [0.4116, 0.3563, 0.3037],\n",
      "        [0.4586, 0.4320, 0.2869],\n",
      "        [0.3847, 0.3577, 0.3016],\n",
      "        [0.4075, 0.3491, 0.3264],\n",
      "        [0.3693, 0.3602, 0.3326],\n",
      "        [0.4504, 0.3662, 0.2896],\n",
      "        [0.3811, 0.3703, 0.3214],\n",
      "        [0.4089, 0.3475, 0.3001],\n",
      "        [0.3794, 0.3628, 0.3257],\n",
      "        [0.3985, 0.3626, 0.3040],\n",
      "        [0.3846, 0.3592, 0.3156],\n",
      "        [0.4643, 0.3621, 0.2756],\n",
      "        [0.4068, 0.3567, 0.3174],\n",
      "        [0.3895, 0.3639, 0.2995],\n",
      "        [0.3545, 0.3775, 0.3127],\n",
      "        [0.3681, 0.3616, 0.3196],\n",
      "        [0.4697, 0.3720, 0.2867],\n",
      "        [0.4682, 0.3592, 0.3046],\n",
      "        [0.3924, 0.3485, 0.3108],\n",
      "        [0.3997, 0.3534, 0.3103],\n",
      "        [0.3674, 0.3595, 0.3240],\n",
      "        [0.3882, 0.3604, 0.3134],\n",
      "        [0.3875, 0.3669, 0.3039],\n",
      "        [0.3615, 0.3699, 0.3151],\n",
      "        [0.3761, 0.3767, 0.3511],\n",
      "        [0.4218, 0.3652, 0.2921],\n",
      "        [0.4386, 0.3657, 0.3188],\n",
      "        [0.3844, 0.3692, 0.2881],\n",
      "        [0.3770, 0.3792, 0.3118],\n",
      "        [0.3668, 0.3673, 0.3178],\n",
      "        [0.3878, 0.3637, 0.3252],\n",
      "        [0.3890, 0.3634, 0.3039],\n",
      "        [0.3718, 0.3744, 0.3084],\n",
      "        [0.3717, 0.3646, 0.3057],\n",
      "        [0.3836, 0.3497, 0.3128],\n",
      "        [0.5076, 0.3485, 0.2856],\n",
      "        [0.3707, 0.3878, 0.3044],\n",
      "        [0.3824, 0.3612, 0.3073],\n",
      "        [0.3635, 0.3484, 0.3353],\n",
      "        [0.3797, 0.3479, 0.3216],\n",
      "        [0.4744, 0.3647, 0.3300],\n",
      "        [0.4347, 0.3666, 0.2869],\n",
      "        [0.3932, 0.3649, 0.3123],\n",
      "        [0.3525, 0.3602, 0.3595],\n",
      "        [0.4065, 0.3759, 0.2946]], device='cuda:0', grad_fn=<ViewBackward0>), tensor([[0.2692, 0.2847, 0.2672, 0.2634],\n",
      "        [0.2774, 0.3479, 0.2558, 0.3052],\n",
      "        [0.3083, 0.2680, 0.2650, 0.2842],\n",
      "        [0.2708, 0.2627, 0.2526, 0.2646],\n",
      "        [0.2671, 0.2780, 0.2547, 0.2477],\n",
      "        [0.2756, 0.2634, 0.2641, 0.2501],\n",
      "        [0.2748, 0.2981, 0.2564, 0.2633],\n",
      "        [0.2683, 0.2670, 0.2547, 0.2502],\n",
      "        [0.2749, 0.2829, 0.2503, 0.2922],\n",
      "        [0.2675, 0.2589, 0.2585, 0.2615],\n",
      "        [0.2852, 0.2760, 0.2696, 0.2462],\n",
      "        [0.2577, 0.2788, 0.2974, 0.2552],\n",
      "        [0.2612, 0.2706, 0.2542, 0.2501],\n",
      "        [0.2681, 0.2604, 0.2686, 0.2560],\n",
      "        [0.2650, 0.2698, 0.2536, 0.2591],\n",
      "        [0.2572, 0.2682, 0.2649, 0.2634],\n",
      "        [0.3049, 0.2846, 0.2467, 0.3030],\n",
      "        [0.2710, 0.2658, 0.2676, 0.2703],\n",
      "        [0.2632, 0.2624, 0.2570, 0.2576],\n",
      "        [0.2471, 0.2909, 0.2683, 0.3012],\n",
      "        [0.2589, 0.3190, 0.2569, 0.2855],\n",
      "        [0.2677, 0.2684, 0.2552, 0.2611],\n",
      "        [0.2680, 0.2696, 0.2512, 0.2556],\n",
      "        [0.2730, 0.2809, 0.2609, 0.2703],\n",
      "        [0.2592, 0.2639, 0.3234, 0.3115],\n",
      "        [0.2914, 0.2761, 0.2592, 0.2550],\n",
      "        [0.2944, 0.2565, 0.3475, 0.2753],\n",
      "        [0.2861, 0.2862, 0.2522, 0.2555],\n",
      "        [0.2745, 0.2701, 0.2557, 0.2565],\n",
      "        [0.2743, 0.2594, 0.2595, 0.2462],\n",
      "        [0.2653, 0.2843, 0.2511, 0.2503],\n",
      "        [0.2629, 0.3044, 0.2436, 0.2722],\n",
      "        [0.2575, 0.2731, 0.2615, 0.2526],\n",
      "        [0.2708, 0.2685, 0.2695, 0.2493],\n",
      "        [0.2688, 0.2661, 0.2717, 0.2573],\n",
      "        [0.2942, 0.2713, 0.2645, 0.2702],\n",
      "        [0.2891, 0.2989, 0.2533, 0.2594],\n",
      "        [0.2736, 0.2721, 0.2703, 0.2429],\n",
      "        [0.2850, 0.2865, 0.2271, 0.3130],\n",
      "        [0.2625, 0.2583, 0.2575, 0.2622],\n",
      "        [0.2677, 0.2755, 0.2513, 0.2616],\n",
      "        [0.3095, 0.2914, 0.2504, 0.2991],\n",
      "        [0.2667, 0.2816, 0.2472, 0.2670],\n",
      "        [0.2942, 0.3442, 0.2347, 0.2624],\n",
      "        [0.2727, 0.2632, 0.2559, 0.2566],\n",
      "        [0.2683, 0.2771, 0.2652, 0.2654],\n",
      "        [0.2698, 0.2709, 0.2732, 0.2620],\n",
      "        [0.2762, 0.2575, 0.2544, 0.2787],\n",
      "        [0.2622, 0.2757, 0.2528, 0.2470],\n",
      "        [0.2616, 0.2677, 0.2637, 0.2581],\n",
      "        [0.2704, 0.2856, 0.2721, 0.2770],\n",
      "        [0.2671, 0.2711, 0.2523, 0.2533],\n",
      "        [0.2826, 0.2739, 0.2737, 0.2533],\n",
      "        [0.2582, 0.2686, 0.2641, 0.2498],\n",
      "        [0.2716, 0.2589, 0.2575, 0.2511],\n",
      "        [0.2668, 0.2943, 0.2515, 0.2801],\n",
      "        [0.2763, 0.2634, 0.2509, 0.2558],\n",
      "        [0.2676, 0.2929, 0.2852, 0.2454],\n",
      "        [0.2778, 0.3553, 0.2397, 0.2872],\n",
      "        [0.2643, 0.2709, 0.2395, 0.2601],\n",
      "        [0.2662, 0.2645, 0.2576, 0.2560],\n",
      "        [0.2547, 0.2653, 0.2611, 0.2673],\n",
      "        [0.2607, 0.2766, 0.2433, 0.2708],\n",
      "        [0.2512, 0.2687, 0.2763, 0.2547],\n",
      "        [0.2717, 0.2782, 0.3432, 0.3068],\n",
      "        [0.2661, 0.2593, 0.2539, 0.2655],\n",
      "        [0.2657, 0.2722, 0.2580, 0.2652],\n",
      "        [0.2673, 0.2828, 0.2494, 0.2665],\n",
      "        [0.2679, 0.2744, 0.2555, 0.2427],\n",
      "        [0.2784, 0.2755, 0.2418, 0.2679],\n",
      "        [0.2691, 0.2800, 0.2486, 0.2827],\n",
      "        [0.2533, 0.2723, 0.2592, 0.2524],\n",
      "        [0.2588, 0.2692, 0.2590, 0.2512],\n",
      "        [0.2630, 0.2755, 0.2684, 0.2439],\n",
      "        [0.2669, 0.2859, 0.2695, 0.2496],\n",
      "        [0.2873, 0.3028, 0.2381, 0.2553],\n",
      "        [0.3019, 0.2624, 0.2528, 0.2651],\n",
      "        [0.2632, 0.2746, 0.2472, 0.2724],\n",
      "        [0.2732, 0.2756, 0.2543, 0.2840],\n",
      "        [0.2723, 0.2704, 0.2519, 0.2604],\n",
      "        [0.2625, 0.2721, 0.2564, 0.2514],\n",
      "        [0.2785, 0.2737, 0.2613, 0.2540],\n",
      "        [0.2705, 0.2812, 0.2473, 0.2542],\n",
      "        [0.2682, 0.2749, 0.2487, 0.2792],\n",
      "        [0.2863, 0.2789, 0.2795, 0.2614],\n",
      "        [0.2683, 0.2715, 0.2520, 0.2478],\n",
      "        [0.2644, 0.2842, 0.2607, 0.2596],\n",
      "        [0.2728, 0.2650, 0.2507, 0.2902],\n",
      "        [0.2699, 0.2646, 0.2551, 0.2609],\n",
      "        [0.2811, 0.2698, 0.2634, 0.2627],\n",
      "        [0.2636, 0.2655, 0.2612, 0.2747],\n",
      "        [0.2696, 0.2706, 0.2538, 0.2544],\n",
      "        [0.2542, 0.2667, 0.2509, 0.2820],\n",
      "        [0.2720, 0.2699, 0.2467, 0.2518],\n",
      "        [0.3049, 0.2739, 0.2493, 0.2534],\n",
      "        [0.2717, 0.2892, 0.2847, 0.2766],\n",
      "        [0.2582, 0.2742, 0.2512, 0.2773],\n",
      "        [0.2583, 0.3157, 0.2612, 0.2768],\n",
      "        [0.2588, 0.2775, 0.2675, 0.2564],\n",
      "        [0.2832, 0.2740, 0.2643, 0.2651],\n",
      "        [0.2804, 0.2776, 0.2604, 0.2522],\n",
      "        [0.2703, 0.2966, 0.2533, 0.2766],\n",
      "        [0.2790, 0.2799, 0.2568, 0.2546],\n",
      "        [0.2504, 0.3471, 0.2557, 0.3203],\n",
      "        [0.2646, 0.2613, 0.2655, 0.2471],\n",
      "        [0.2968, 0.2692, 0.2668, 0.2434],\n",
      "        [0.2721, 0.2679, 0.2534, 0.2583],\n",
      "        [0.2679, 0.2925, 0.2815, 0.2560],\n",
      "        [0.2647, 0.2697, 0.2670, 0.2560],\n",
      "        [0.2644, 0.2692, 0.2648, 0.2618],\n",
      "        [0.2660, 0.3098, 0.2369, 0.2581],\n",
      "        [0.2641, 0.2679, 0.2641, 0.2452],\n",
      "        [0.2617, 0.2702, 0.2597, 0.2554],\n",
      "        [0.3006, 0.2833, 0.2649, 0.2438],\n",
      "        [0.2732, 0.2814, 0.2679, 0.2452],\n",
      "        [0.2669, 0.2571, 0.2742, 0.2596],\n",
      "        [0.2679, 0.2576, 0.2626, 0.2640],\n",
      "        [0.2668, 0.2771, 0.2603, 0.2538],\n",
      "        [0.2668, 0.3002, 0.2520, 0.2873],\n",
      "        [0.2862, 0.3022, 0.2546, 0.2533],\n",
      "        [0.2618, 0.2906, 0.2763, 0.2528],\n",
      "        [0.2673, 0.2756, 0.2572, 0.2579],\n",
      "        [0.2583, 0.2783, 0.2533, 0.2519],\n",
      "        [0.2645, 0.2769, 0.2598, 0.2512],\n",
      "        [0.2621, 0.2799, 0.2433, 0.2671],\n",
      "        [0.2667, 0.2788, 0.2561, 0.2469],\n",
      "        [0.2678, 0.2979, 0.2433, 0.2846],\n",
      "        [0.2761, 0.2626, 0.2665, 0.2624],\n",
      "        [0.2721, 0.3088, 0.2637, 0.2570],\n",
      "        [0.2599, 0.2663, 0.2562, 0.2642],\n",
      "        [0.2892, 0.2719, 0.2500, 0.2522],\n",
      "        [0.2704, 0.2706, 0.2616, 0.2542],\n",
      "        [0.2766, 0.2927, 0.2434, 0.2507],\n",
      "        [0.2674, 0.2797, 0.2507, 0.2516],\n",
      "        [0.2684, 0.2830, 0.2508, 0.2511],\n",
      "        [0.2748, 0.2691, 0.2512, 0.2535],\n",
      "        [0.2822, 0.2676, 0.2532, 0.2514],\n",
      "        [0.2830, 0.2791, 0.2750, 0.2747],\n",
      "        [0.2635, 0.2766, 0.2440, 0.2726],\n",
      "        [0.2643, 0.2788, 0.2570, 0.2444],\n",
      "        [0.2666, 0.2680, 0.2594, 0.2590],\n",
      "        [0.2741, 0.2617, 0.2536, 0.2591],\n",
      "        [0.2785, 0.3010, 0.2386, 0.3277],\n",
      "        [0.2550, 0.2811, 0.2669, 0.2756],\n",
      "        [0.2733, 0.2902, 0.2458, 0.2797],\n",
      "        [0.2584, 0.2806, 0.2607, 0.2675],\n",
      "        [0.2689, 0.2769, 0.2556, 0.2630]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>), tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3149, 0.5279, 0.4245, 0.4677, 0.5631],\n",
      "        [0.4107, 0.4859, 0.4192, 0.4687, 0.4908],\n",
      "        [0.4579, 0.4898, 0.4454, 0.4658, 0.4639],\n",
      "        [0.4482, 0.5016, 0.4365, 0.4611, 0.5034],\n",
      "        [0.4369, 0.4729, 0.4474, 0.4738, 0.5047],\n",
      "        [0.4394, 0.5338, 0.4014, 0.4187, 0.5169],\n",
      "        [0.4506, 0.4725, 0.4304, 0.4766, 0.4592],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4204, 0.4912, 0.4323, 0.4362, 0.4519],\n",
      "        [0.4269, 0.4699, 0.4079, 0.4714, 0.4824],\n",
      "        [0.4468, 0.4756, 0.4321, 0.4675, 0.4859],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4860, 0.5037, 0.4424, 0.4386, 0.4840],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4355, 0.5010, 0.4011, 0.4612, 0.4968],\n",
      "        [0.4500, 0.5031, 0.4397, 0.4882, 0.5086],\n",
      "        [0.4495, 0.4738, 0.4253, 0.4657, 0.4918],\n",
      "        [0.3874, 0.5205, 0.4203, 0.4540, 0.4645],\n",
      "        [0.4721, 0.4967, 0.3836, 0.4282, 0.5389],\n",
      "        [0.4520, 0.4764, 0.4514, 0.4856, 0.4942],\n",
      "        [0.4596, 0.4982, 0.4418, 0.4261, 0.4652],\n",
      "        [0.4779, 0.4683, 0.4377, 0.4734, 0.4663],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4461, 0.5109, 0.4137, 0.5099, 0.4904],\n",
      "        [0.3850, 0.4952, 0.3803, 0.4656, 0.5243],\n",
      "        [0.3947, 0.4959, 0.4188, 0.5021, 0.4876],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4315, 0.4869, 0.4288, 0.4593, 0.4842],\n",
      "        [0.4447, 0.5220, 0.3877, 0.4412, 0.5056],\n",
      "        [0.4383, 0.4710, 0.4423, 0.4901, 0.4876],\n",
      "        [0.4485, 0.4946, 0.4568, 0.4690, 0.4879],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4284, 0.4892, 0.4003, 0.4365, 0.5186],\n",
      "        [0.4244, 0.4670, 0.4041, 0.4658, 0.5006],\n",
      "        [0.4265, 0.4794, 0.4264, 0.4703, 0.4715],\n",
      "        [0.4441, 0.5348, 0.3656, 0.5481, 0.3748],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4556, 0.4858, 0.4453, 0.4484, 0.4884],\n",
      "        [0.3962, 0.4629, 0.4424, 0.4145, 0.4576],\n",
      "        [0.4510, 0.4991, 0.4481, 0.4539, 0.4889],\n",
      "        [0.3877, 0.5242, 0.3432, 0.4262, 0.5247],\n",
      "        [0.4531, 0.4835, 0.4285, 0.4797, 0.4985],\n",
      "        [0.4832, 0.4655, 0.4455, 0.4530, 0.4705],\n",
      "        [0.4766, 0.4915, 0.4675, 0.4807, 0.4953],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4477, 0.4875, 0.4496, 0.4648, 0.4971],\n",
      "        [0.4566, 0.4850, 0.4379, 0.4813, 0.4744],\n",
      "        [0.4157, 0.5733, 0.4590, 0.4258, 0.5145],\n",
      "        [0.4582, 0.4965, 0.4557, 0.4560, 0.4872],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4397, 0.4932, 0.4250, 0.4786, 0.4953],\n",
      "        [0.4399, 0.4796, 0.4322, 0.4708, 0.4721],\n",
      "        [0.4571, 0.5282, 0.3952, 0.4710, 0.5030],\n",
      "        [0.4354, 0.4856, 0.4368, 0.4531, 0.4842],\n",
      "        [0.5225, 0.5247, 0.4735, 0.4594, 0.5052],\n",
      "        [0.3906, 0.5371, 0.3507, 0.4783, 0.4848],\n",
      "        [0.4312, 0.4949, 0.4485, 0.4555, 0.4876],\n",
      "        [0.4458, 0.4794, 0.4253, 0.4804, 0.4709],\n",
      "        [0.4608, 0.5001, 0.4255, 0.4363, 0.5026],\n",
      "        [0.4848, 0.4870, 0.4024, 0.4603, 0.4824],\n",
      "        [0.4356, 0.4749, 0.4388, 0.4638, 0.4802],\n",
      "        [0.4249, 0.4994, 0.3347, 0.4543, 0.5207],\n",
      "        [0.4232, 0.4734, 0.4317, 0.4663, 0.4594],\n",
      "        [0.4457, 0.4871, 0.4285, 0.4664, 0.4675],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4609, 0.4931, 0.4442, 0.4657, 0.4815],\n",
      "        [0.4551, 0.4874, 0.3972, 0.4663, 0.4735],\n",
      "        [0.4627, 0.4652, 0.4024, 0.4789, 0.5006],\n",
      "        [0.4521, 0.4850, 0.4592, 0.4652, 0.4878],\n",
      "        [0.4438, 0.4746, 0.4308, 0.4821, 0.4907],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4104, 0.5010, 0.4067, 0.4653, 0.4619],\n",
      "        [0.4459, 0.4446, 0.3991, 0.4227, 0.4590],\n",
      "        [0.4390, 0.5124, 0.4137, 0.5098, 0.5020],\n",
      "        [0.4629, 0.5177, 0.4280, 0.4887, 0.5067],\n",
      "        [0.4346, 0.5366, 0.4047, 0.4904, 0.4832],\n",
      "        [0.4538, 0.5061, 0.4317, 0.4497, 0.4904],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4551, 0.4632, 0.4358, 0.4671, 0.4946],\n",
      "        [0.4360, 0.5059, 0.4447, 0.4653, 0.4881],\n",
      "        [0.4533, 0.5083, 0.4437, 0.4720, 0.4922],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4600, 0.4674, 0.4345, 0.4709, 0.4815],\n",
      "        [0.4641, 0.4878, 0.4443, 0.4498, 0.4760],\n",
      "        [0.4355, 0.5148, 0.4584, 0.5143, 0.4859],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4787, 0.5101, 0.4289, 0.4348, 0.5030],\n",
      "        [0.4406, 0.4956, 0.4536, 0.4739, 0.4721],\n",
      "        [0.4263, 0.4728, 0.4457, 0.4767, 0.4877],\n",
      "        [0.4615, 0.4937, 0.4208, 0.4673, 0.4945],\n",
      "        [0.4197, 0.4853, 0.4188, 0.4709, 0.4678],\n",
      "        [0.3789, 0.4859, 0.4123, 0.4481, 0.4991],\n",
      "        [0.4058, 0.5211, 0.3751, 0.4213, 0.5511],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4456, 0.4930, 0.3999, 0.4702, 0.5389],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4473, 0.5103, 0.3859, 0.4901, 0.4894],\n",
      "        [0.4247, 0.4830, 0.4288, 0.4206, 0.5027],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4459, 0.5032, 0.4002, 0.4565, 0.4853],\n",
      "        [0.4435, 0.4583, 0.3342, 0.4622, 0.5359],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4383, 0.4525, 0.4421, 0.4842, 0.4661],\n",
      "        [0.4347, 0.5013, 0.3965, 0.4786, 0.4906],\n",
      "        [0.4503, 0.4872, 0.4264, 0.4980, 0.4864],\n",
      "        [0.4613, 0.4666, 0.4249, 0.4626, 0.5024],\n",
      "        [0.4631, 0.5026, 0.4309, 0.4304, 0.4885],\n",
      "        [0.4439, 0.5150, 0.3765, 0.4475, 0.4473],\n",
      "        [0.4711, 0.4640, 0.4465, 0.4775, 0.4596],\n",
      "        [0.4249, 0.4777, 0.4416, 0.4851, 0.4728],\n",
      "        [0.4515, 0.4793, 0.4447, 0.4752, 0.5254],\n",
      "        [0.4332, 0.5493, 0.4010, 0.4740, 0.4362],\n",
      "        [0.4036, 0.4989, 0.3760, 0.5066, 0.4194],\n",
      "        [0.4700, 0.4622, 0.4188, 0.4582, 0.4752],\n",
      "        [0.4509, 0.4790, 0.4511, 0.4502, 0.5167],\n",
      "        [0.4645, 0.4750, 0.4268, 0.4564, 0.5010],\n",
      "        [0.4605, 0.4937, 0.4465, 0.4951, 0.4866],\n",
      "        [0.4794, 0.4809, 0.3992, 0.4664, 0.4807],\n",
      "        [0.4469, 0.4618, 0.4157, 0.4746, 0.4853],\n",
      "        [0.4366, 0.5292, 0.4128, 0.4615, 0.4754],\n",
      "        [0.4483, 0.4856, 0.3869, 0.4652, 0.4590],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4262, 0.4728, 0.4019, 0.4397, 0.4801],\n",
      "        [0.4398, 0.4968, 0.4537, 0.4737, 0.4914],\n",
      "        [0.4345, 0.5126, 0.4459, 0.4553, 0.4989],\n",
      "        [0.4588, 0.5002, 0.4278, 0.4695, 0.4839],\n",
      "        [0.4277, 0.4863, 0.4338, 0.4944, 0.4868],\n",
      "        [0.4723, 0.5085, 0.4454, 0.4558, 0.4628],\n",
      "        [0.4611, 0.4997, 0.4431, 0.4635, 0.5030],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4688, 0.4843, 0.4130, 0.4847, 0.4907],\n",
      "        [0.4563, 0.5013, 0.4335, 0.4516, 0.5117],\n",
      "        [0.4545, 0.4901, 0.4494, 0.4575, 0.4856],\n",
      "        [0.4494, 0.4962, 0.4655, 0.4521, 0.4907],\n",
      "        [0.3938, 0.4808, 0.3802, 0.4407, 0.5222],\n",
      "        [0.4541, 0.5087, 0.4161, 0.4633, 0.4752],\n",
      "        [0.4571, 0.5094, 0.4452, 0.5063, 0.5071],\n",
      "        [0.4649, 0.4978, 0.4400, 0.4418, 0.5021],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)], 'order': ['pd', 'nd', 'mod', 'dlt']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pd': {'accuracy': 0.3715093672675857,\n",
       "  'auc_micro': 0.8593340815563038,\n",
       "  'auc_mean': 0.5124569839895249,\n",
       "  'auc_weighted': 0.5256017075097881},\n",
       " 'nd': {'accuracy': 0.3351292870280212,\n",
       "  'auc_micro': 0.5289598108747046,\n",
       "  'auc_mean': 0.5875004851630867,\n",
       "  'auc_weighted': 0.5319550544317021},\n",
       " 'mod': {'accuracy': 0.3351292870280212,\n",
       "  'auc_micro': 0.5289598108747046,\n",
       "  'auc_mean': 0.5875004851630867,\n",
       "  'auc_weighted': 0.5319550544317021},\n",
       " 'dlts': {'accuracy': [0.9183673469387755,\n",
       "   0.7687074829931972,\n",
       "   0.9523809523809523,\n",
       "   0.9183673469387755,\n",
       "   0.8027210884353742],\n",
       "  'accuracy_mean': 0.8721088435374149,\n",
       "  'auc': [0.6685185185185185,\n",
       "   0.6126760563380281,\n",
       "   0.5887755102040816,\n",
       "   0.6898148148148149,\n",
       "   0.48450704225352115],\n",
       "  'auc_mean': 0.6088583884257929}}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmodel2_smote = train_state(model_args=t1_args,state=2,lr=.001,weights=[1,1,.1,.1],use_smote=True)\n",
    "tmodel2_smote[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "710a716f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train loss 2.3837714195251465\n",
      "val loss 2.290266752243042\n",
      "______________\n",
      "epoch 1 train loss 2.282294511795044\n",
      "val loss 2.1886394023895264\n",
      "______________\n",
      "epoch 2 train loss 2.173572063446045\n",
      "val loss 2.0944511890411377\n",
      "______________\n",
      "epoch 3 train loss 2.072824001312256\n",
      "val loss 2.017432689666748\n",
      "______________\n",
      "epoch 4 train loss 1.9809283018112183\n",
      "val loss 1.9605162143707275\n",
      "______________\n",
      "epoch 5 train loss 1.921317458152771\n",
      "val loss 1.9201730489730835\n",
      "______________\n",
      "epoch 6 train loss 1.8728001117706299\n",
      "val loss 1.88919198513031\n",
      "______________\n",
      "epoch 7 train loss 1.8413349390029907\n",
      "val loss 1.862000823020935\n",
      "______________\n",
      "epoch 8 train loss 1.8096041679382324\n",
      "val loss 1.8403205871582031\n",
      "______________\n",
      "epoch 9 train loss 1.7978054285049438\n",
      "val loss 1.8286479711532593\n",
      "______________\n",
      "epoch 10 train loss 1.7951533794403076\n",
      "val loss 1.8247299194335938\n",
      "______________\n",
      "epoch 11 train loss 1.7685106992721558\n",
      "val loss 1.8276957273483276\n",
      "______________\n",
      "epoch 12 train loss 1.7714344263076782\n",
      "val loss 1.837754249572754\n",
      "______________\n",
      "epoch 13 train loss 1.7611418962478638\n",
      "val loss 1.8500396013259888\n",
      "______________\n",
      "epoch 14 train loss 1.7516859769821167\n",
      "val loss 1.857056975364685\n",
      "______________\n",
      "epoch 15 train loss 1.7568191289901733\n",
      "val loss 1.8551487922668457\n",
      "______________\n",
      "epoch 16 train loss 1.7575207948684692\n",
      "val loss 1.8488768339157104\n",
      "______________\n",
      "epoch 17 train loss 1.7442463636398315\n",
      "val loss 1.845578670501709\n",
      "______________\n",
      "epoch 18 train loss 1.7368007898330688\n",
      "val loss 1.8460053205490112\n",
      "______________\n",
      "epoch 19 train loss 1.7340493202209473\n",
      "val loss 1.8525185585021973\n",
      "______________\n",
      "epoch 20 train loss 1.7187459468841553\n",
      "val loss 1.861533761024475\n",
      "______________\n",
      "epoch 21 train loss 1.727723479270935\n",
      "val loss 1.8683916330337524\n",
      "______________\n",
      "best loss 1.8247299194335938 {'pd': {'accuracy': 0.3333333333333333, 'auc_micro': 0.8947577628133184, 'auc_mean': 0.585274565586227, 'auc_weighted': 0.605061871600574}, 'nd': {'accuracy': 0.35877961700746513, 'auc_micro': 0.7628605200945627, 'auc_mean': 0.5110004655207351, 'auc_weighted': 0.5572855873517524}, 'mod': {'accuracy': 0.35877961700746513, 'auc_micro': 0.7628605200945627, 'auc_mean': 0.5110004655207351, 'auc_weighted': 0.5572855873517524}, 'dlts': {'accuracy': [0.9183673469387755, 0.9659863945578231, 0.9523809523809523, 0.9795918367346939, 0.9659863945578231], 'accuracy_mean': 0.9564625850340136, 'auc': [0.5814814814814815, 0.7563380281690141, 0.6275510204081632, 0.6296296296296295, 0.5521126760563381], 'auc_mean': 0.6294225671489253}}\n",
      "{'predictions': [tensor([[9.9979e-01, 1.0647e-04, 1.0454e-04],\n",
      "        [1.0000e+00, 2.7223e-08, 2.7214e-08],\n",
      "        [9.9991e-01, 4.3559e-05, 4.3190e-05],\n",
      "        [9.9769e-01, 1.1742e-03, 1.1400e-03],\n",
      "        [9.9465e-01, 2.7274e-03, 2.6229e-03],\n",
      "        [9.9871e-01, 6.5035e-04, 6.3496e-04],\n",
      "        [9.9991e-01, 4.7064e-05, 4.6649e-05],\n",
      "        [9.9925e-01, 3.8057e-04, 3.7385e-04],\n",
      "        [1.0000e+00, 5.7666e-07, 5.7566e-07],\n",
      "        [9.9802e-01, 1.0070e-03, 9.7739e-04],\n",
      "        [9.9983e-01, 8.3769e-05, 8.2812e-05],\n",
      "        [9.9997e-01, 1.3045e-05, 1.2975e-05],\n",
      "        [9.9752e-01, 1.2600e-03, 1.2247e-03],\n",
      "        [9.9937e-01, 3.2021e-04, 3.1444e-04],\n",
      "        [9.9681e-01, 1.6184e-03, 1.5670e-03],\n",
      "        [9.9841e-01, 8.0816e-04, 7.8537e-04],\n",
      "        [1.0000e+00, 1.5893e-06, 1.5859e-06],\n",
      "        [9.9626e-01, 1.9027e-03, 1.8339e-03],\n",
      "        [9.9836e-01, 8.2959e-04, 8.1022e-04],\n",
      "        [9.9999e-01, 6.3437e-06, 6.3079e-06],\n",
      "        [9.9999e-01, 4.0092e-06, 3.9924e-06],\n",
      "        [9.9818e-01, 9.2139e-04, 8.9396e-04],\n",
      "        [9.9585e-01, 2.1106e-03, 2.0370e-03],\n",
      "        [9.9989e-01, 5.7458e-05, 5.6878e-05],\n",
      "        [1.0000e+00, 1.0592e-08, 1.0589e-08],\n",
      "        [9.9977e-01, 1.1432e-04, 1.1242e-04],\n",
      "        [1.0000e+00, 2.4927e-08, 2.4917e-08],\n",
      "        [9.9991e-01, 4.3876e-05, 4.3475e-05],\n",
      "        [9.9928e-01, 3.6119e-04, 3.5443e-04],\n",
      "        [9.9883e-01, 5.9022e-04, 5.7593e-04],\n",
      "        [9.9781e-01, 1.1115e-03, 1.0814e-03],\n",
      "        [9.9962e-01, 1.9207e-04, 1.8849e-04],\n",
      "        [9.9790e-01, 1.0642e-03, 1.0369e-03],\n",
      "        [9.9818e-01, 9.2030e-04, 8.9473e-04],\n",
      "        [9.9970e-01, 1.5088e-04, 1.4896e-04],\n",
      "        [9.9967e-01, 1.6588e-04, 1.6251e-04],\n",
      "        [9.9945e-01, 2.7935e-04, 2.7289e-04],\n",
      "        [9.9789e-01, 1.0704e-03, 1.0398e-03],\n",
      "        [9.9988e-01, 6.2512e-05, 6.1803e-05],\n",
      "        [9.9916e-01, 4.2526e-04, 4.1610e-04],\n",
      "        [9.9110e-01, 4.5518e-03, 4.3526e-03],\n",
      "        [1.0000e+00, 6.3908e-08, 6.3870e-08],\n",
      "        [9.9878e-01, 6.1601e-04, 6.0037e-04],\n",
      "        [1.0000e+00, 4.1449e-07, 4.1396e-07],\n",
      "        [9.9810e-01, 9.6095e-04, 9.3581e-04],\n",
      "        [9.9994e-01, 2.7838e-05, 2.7615e-05],\n",
      "        [9.9896e-01, 5.2704e-04, 5.1587e-04],\n",
      "        [9.9978e-01, 1.0862e-04, 1.0708e-04],\n",
      "        [9.9493e-01, 2.5814e-03, 2.4858e-03],\n",
      "        [9.9853e-01, 7.4391e-04, 7.2575e-04],\n",
      "        [9.9999e-01, 2.5441e-06, 2.5360e-06],\n",
      "        [9.8764e-01, 6.3378e-03, 6.0172e-03],\n",
      "        [9.9996e-01, 1.8524e-05, 1.8403e-05],\n",
      "        [9.9672e-01, 1.6653e-03, 1.6098e-03],\n",
      "        [9.9823e-01, 8.9489e-04, 8.7130e-04],\n",
      "        [9.9943e-01, 2.8696e-04, 2.8055e-04],\n",
      "        [9.9565e-01, 2.2156e-03, 2.1371e-03],\n",
      "        [9.9994e-01, 3.1640e-05, 3.1349e-05],\n",
      "        [1.0000e+00, 2.2549e-08, 2.2542e-08],\n",
      "        [9.9468e-01, 2.7112e-03, 2.6136e-03],\n",
      "        [9.9823e-01, 8.9667e-04, 8.7434e-04],\n",
      "        [9.9817e-01, 9.2886e-04, 9.0293e-04],\n",
      "        [9.9784e-01, 1.0916e-03, 1.0636e-03],\n",
      "        [9.9934e-01, 3.3389e-04, 3.2780e-04],\n",
      "        [1.0000e+00, 3.4996e-08, 3.4977e-08],\n",
      "        [9.9849e-01, 7.6301e-04, 7.4657e-04],\n",
      "        [9.9838e-01, 8.2080e-04, 8.0307e-04],\n",
      "        [9.9843e-01, 7.9529e-04, 7.7320e-04],\n",
      "        [9.9781e-01, 1.1095e-03, 1.0790e-03],\n",
      "        [9.9818e-01, 9.1889e-04, 9.0006e-04],\n",
      "        [9.9959e-01, 2.0849e-04, 2.0472e-04],\n",
      "        [9.9428e-01, 2.9189e-03, 2.7975e-03],\n",
      "        [9.9783e-01, 1.0995e-03, 1.0705e-03],\n",
      "        [9.9962e-01, 1.9018e-04, 1.8714e-04],\n",
      "        [9.9957e-01, 2.1492e-04, 2.1088e-04],\n",
      "        [9.9996e-01, 2.0631e-05, 2.0502e-05],\n",
      "        [9.9991e-01, 4.5522e-05, 4.5026e-05],\n",
      "        [9.9589e-01, 2.0953e-03, 2.0101e-03],\n",
      "        [9.9998e-01, 1.1426e-05, 1.1354e-05],\n",
      "        [9.9884e-01, 5.8772e-04, 5.7488e-04],\n",
      "        [9.9887e-01, 5.6989e-04, 5.5606e-04],\n",
      "        [9.9917e-01, 4.1865e-04, 4.1042e-04],\n",
      "        [9.9721e-01, 1.4202e-03, 1.3738e-03],\n",
      "        [9.9937e-01, 3.1685e-04, 3.1042e-04],\n",
      "        [9.9996e-01, 1.8497e-05, 1.8385e-05],\n",
      "        [9.9923e-01, 3.8910e-04, 3.8222e-04],\n",
      "        [9.9870e-01, 6.5964e-04, 6.4359e-04],\n",
      "        [9.9987e-01, 6.4742e-05, 6.4046e-05],\n",
      "        [9.9906e-01, 4.7396e-04, 4.6462e-04],\n",
      "        [9.9969e-01, 1.5590e-04, 1.5356e-04],\n",
      "        [9.9919e-01, 4.1004e-04, 4.0281e-04],\n",
      "        [9.9927e-01, 3.6568e-04, 3.5937e-04],\n",
      "        [9.9494e-01, 2.5843e-03, 2.4794e-03],\n",
      "        [9.9867e-01, 6.7282e-04, 6.5781e-04],\n",
      "        [9.9998e-01, 9.3828e-06, 9.3423e-06],\n",
      "        [9.9999e-01, 2.8010e-06, 2.7898e-06],\n",
      "        [9.9864e-01, 6.8909e-04, 6.6992e-04],\n",
      "        [9.9998e-01, 9.2969e-06, 9.2518e-06],\n",
      "        [9.9976e-01, 1.2078e-04, 1.1898e-04],\n",
      "        [9.9986e-01, 6.9301e-05, 6.8485e-05],\n",
      "        [9.9962e-01, 1.8907e-04, 1.8626e-04],\n",
      "        [9.9983e-01, 8.3475e-05, 8.2403e-05],\n",
      "        [9.9962e-01, 1.9299e-04, 1.8867e-04],\n",
      "        [9.9998e-01, 1.1570e-05, 1.1499e-05],\n",
      "        [9.9899e-01, 5.1185e-04, 5.0014e-04],\n",
      "        [9.9977e-01, 1.1810e-04, 1.1638e-04],\n",
      "        [9.9769e-01, 1.1716e-03, 1.1367e-03],\n",
      "        [9.9997e-01, 1.5240e-05, 1.5132e-05],\n",
      "        [9.9977e-01, 1.1446e-04, 1.1290e-04],\n",
      "        [9.9911e-01, 4.5102e-04, 4.4175e-04],\n",
      "        [9.9926e-01, 3.7265e-04, 3.6570e-04],\n",
      "        [9.9742e-01, 1.3113e-03, 1.2720e-03],\n",
      "        [9.9759e-01, 1.2206e-03, 1.1846e-03],\n",
      "        [9.9997e-01, 1.4861e-05, 1.4761e-05],\n",
      "        [9.9954e-01, 2.3299e-04, 2.2863e-04],\n",
      "        [9.9894e-01, 5.3651e-04, 5.2541e-04],\n",
      "        [9.9768e-01, 1.1736e-03, 1.1444e-03],\n",
      "        [9.9761e-01, 1.2137e-03, 1.1797e-03],\n",
      "        [9.9984e-01, 8.2381e-05, 8.1396e-05],\n",
      "        [9.9985e-01, 7.3226e-05, 7.2179e-05],\n",
      "        [9.9924e-01, 3.8248e-04, 3.7533e-04],\n",
      "        [9.9788e-01, 1.0757e-03, 1.0447e-03],\n",
      "        [9.9325e-01, 3.4486e-03, 3.3045e-03],\n",
      "        [9.9572e-01, 2.1837e-03, 2.1011e-03],\n",
      "        [9.9685e-01, 1.6000e-03, 1.5472e-03],\n",
      "        [9.9851e-01, 7.5273e-04, 7.3512e-04],\n",
      "        [9.9905e-01, 4.8135e-04, 4.7032e-04],\n",
      "        [9.9996e-01, 2.0407e-05, 2.0266e-05],\n",
      "        [9.9999e-01, 4.9276e-06, 4.9012e-06],\n",
      "        [9.9925e-01, 3.7782e-04, 3.7029e-04],\n",
      "        [9.9897e-01, 5.1820e-04, 5.0775e-04],\n",
      "        [9.9830e-01, 8.6142e-04, 8.3882e-04],\n",
      "        [9.9782e-01, 1.1045e-03, 1.0711e-03],\n",
      "        [9.9677e-01, 1.6454e-03, 1.5875e-03],\n",
      "        [9.9685e-01, 1.6023e-03, 1.5527e-03],\n",
      "        [9.9652e-01, 1.7682e-03, 1.7120e-03],\n",
      "        [9.9579e-01, 2.1423e-03, 2.0660e-03],\n",
      "        [1.0000e+00, 1.4795e-06, 1.4754e-06],\n",
      "        [9.9760e-01, 1.2136e-03, 1.1814e-03],\n",
      "        [9.9474e-01, 2.6824e-03, 2.5779e-03],\n",
      "        [9.9817e-01, 9.2673e-04, 9.0408e-04],\n",
      "        [9.9704e-01, 1.5057e-03, 1.4590e-03],\n",
      "        [1.0000e+00, 1.1018e-08, 1.1015e-08],\n",
      "        [9.9984e-01, 8.1359e-05, 8.0456e-05],\n",
      "        [9.9769e-01, 1.1743e-03, 1.1395e-03],\n",
      "        [9.9908e-01, 4.6566e-04, 4.5604e-04],\n",
      "        [9.9892e-01, 5.4555e-04, 5.3262e-04]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>), tensor([[9.0617e-01, 8.7240e-02, 6.5897e-03],\n",
      "        [9.4332e-01, 5.6469e-02, 2.0833e-04],\n",
      "        [7.9740e-01, 1.9685e-01, 5.7444e-03],\n",
      "        [6.7737e-01, 2.9724e-01, 2.5382e-02],\n",
      "        [6.2854e-01, 3.3421e-01, 3.7254e-02],\n",
      "        [5.4264e-01, 4.3309e-01, 2.4275e-02],\n",
      "        [6.0318e-01, 3.8952e-01, 7.3011e-03],\n",
      "        [4.9074e-01, 4.9097e-01, 1.8286e-02],\n",
      "        [7.6378e-01, 2.3479e-01, 1.4273e-03],\n",
      "        [5.0727e-01, 4.6720e-01, 2.5530e-02],\n",
      "        [7.8877e-01, 2.0392e-01, 7.3061e-03],\n",
      "        [7.8363e-01, 2.1314e-01, 3.2277e-03],\n",
      "        [4.5750e-01, 5.1390e-01, 2.8604e-02],\n",
      "        [5.3701e-01, 4.4586e-01, 1.7132e-02],\n",
      "        [6.8417e-01, 2.8936e-01, 2.6469e-02],\n",
      "        [6.1286e-01, 3.6353e-01, 2.3610e-02],\n",
      "        [8.3425e-01, 1.6452e-01, 1.2328e-03],\n",
      "        [6.2099e-01, 3.4569e-01, 3.3321e-02],\n",
      "        [4.5959e-01, 5.1631e-01, 2.4103e-02],\n",
      "        [6.1660e-01, 3.7957e-01, 3.8348e-03],\n",
      "        [8.1819e-01, 1.7964e-01, 2.1668e-03],\n",
      "        [5.9587e-01, 3.7891e-01, 2.5215e-02],\n",
      "        [6.3960e-01, 3.2939e-01, 3.1003e-02],\n",
      "        [7.6196e-01, 2.3162e-01, 6.4203e-03],\n",
      "        [5.3739e-01, 4.6232e-01, 2.9325e-04],\n",
      "        [7.9160e-01, 1.9932e-01, 9.0798e-03],\n",
      "        [6.4809e-01, 3.5155e-01, 3.5789e-04],\n",
      "        [7.1122e-01, 2.8246e-01, 6.3227e-03],\n",
      "        [5.3863e-01, 4.4054e-01, 2.0833e-02],\n",
      "        [5.8088e-01, 3.9685e-01, 2.2270e-02],\n",
      "        [5.1204e-01, 4.6009e-01, 2.7869e-02],\n",
      "        [6.5280e-01, 3.3445e-01, 1.2758e-02],\n",
      "        [4.7003e-01, 5.0343e-01, 2.6538e-02],\n",
      "        [6.5638e-01, 3.2083e-01, 2.2785e-02],\n",
      "        [7.5057e-01, 2.3848e-01, 1.0954e-02],\n",
      "        [8.7705e-01, 1.1370e-01, 9.2488e-03],\n",
      "        [6.4427e-01, 3.3953e-01, 1.6202e-02],\n",
      "        [6.4564e-01, 3.2927e-01, 2.5086e-02],\n",
      "        [7.2535e-01, 2.6703e-01, 7.6224e-03],\n",
      "        [5.8521e-01, 3.9568e-01, 1.9106e-02],\n",
      "        [6.0060e-01, 3.5711e-01, 4.2293e-02],\n",
      "        [8.5594e-01, 1.4371e-01, 3.5680e-04],\n",
      "        [7.6153e-01, 2.2320e-01, 1.5272e-02],\n",
      "        [9.2721e-01, 7.2042e-02, 7.5219e-04],\n",
      "        [5.1298e-01, 4.5950e-01, 2.7520e-02],\n",
      "        [7.8948e-01, 2.0580e-01, 4.7188e-03],\n",
      "        [6.8988e-01, 2.9058e-01, 1.9541e-02],\n",
      "        [4.9583e-01, 4.9262e-01, 1.1549e-02],\n",
      "        [5.2159e-01, 4.4080e-01, 3.7612e-02],\n",
      "        [5.5842e-01, 4.1775e-01, 2.3826e-02],\n",
      "        [8.6448e-01, 1.3371e-01, 1.8131e-03],\n",
      "        [5.8762e-01, 3.6320e-01, 4.9177e-02],\n",
      "        [9.4310e-01, 5.4426e-02, 2.4774e-03],\n",
      "        [5.3006e-01, 4.3914e-01, 3.0797e-02],\n",
      "        [5.4820e-01, 4.2564e-01, 2.6168e-02],\n",
      "        [6.1446e-01, 3.6848e-01, 1.7064e-02],\n",
      "        [6.3639e-01, 3.3046e-01, 3.3148e-02],\n",
      "        [7.0230e-01, 2.9212e-01, 5.5819e-03],\n",
      "        [9.2120e-01, 7.8583e-02, 2.1332e-04],\n",
      "        [5.2214e-01, 4.3964e-01, 3.8217e-02],\n",
      "        [4.8406e-01, 4.9101e-01, 2.4935e-02],\n",
      "        [5.9889e-01, 3.7487e-01, 2.6243e-02],\n",
      "        [5.6129e-01, 4.1275e-01, 2.5961e-02],\n",
      "        [5.1101e-01, 4.7259e-01, 1.6400e-02],\n",
      "        [5.2332e-01, 4.7625e-01, 4.3310e-04],\n",
      "        [4.6787e-01, 5.0922e-01, 2.2907e-02],\n",
      "        [4.4655e-01, 5.3020e-01, 2.3253e-02],\n",
      "        [5.6640e-01, 4.1079e-01, 2.2811e-02],\n",
      "        [5.6722e-01, 4.0466e-01, 2.8118e-02],\n",
      "        [5.6821e-01, 4.0771e-01, 2.4082e-02],\n",
      "        [6.0817e-01, 3.7744e-01, 1.4392e-02],\n",
      "        [6.3777e-01, 3.2435e-01, 3.7882e-02],\n",
      "        [4.8097e-01, 4.9222e-01, 2.6812e-02],\n",
      "        [6.6507e-01, 3.2164e-01, 1.3297e-02],\n",
      "        [7.8541e-01, 2.0307e-01, 1.1517e-02],\n",
      "        [7.2358e-01, 2.7195e-01, 4.4654e-03],\n",
      "        [6.4770e-01, 3.4549e-01, 6.8183e-03],\n",
      "        [5.0953e-01, 4.5351e-01, 3.6960e-02],\n",
      "        [8.6332e-01, 1.3366e-01, 3.0153e-03],\n",
      "        [5.1941e-01, 4.5979e-01, 2.0798e-02],\n",
      "        [5.8003e-01, 3.9863e-01, 2.1340e-02],\n",
      "        [7.2238e-01, 2.5886e-01, 1.8758e-02],\n",
      "        [6.5789e-01, 3.1603e-01, 2.6073e-02],\n",
      "        [5.4309e-01, 4.4006e-01, 1.6846e-02],\n",
      "        [8.5847e-01, 1.3768e-01, 3.8499e-03],\n",
      "        [4.6682e-01, 5.1418e-01, 1.8998e-02],\n",
      "        [6.7791e-01, 3.0332e-01, 1.8774e-02],\n",
      "        [4.5332e-01, 5.3862e-01, 8.0607e-03],\n",
      "        [5.0330e-01, 4.7577e-01, 2.0933e-02],\n",
      "        [8.8942e-01, 1.0267e-01, 7.9141e-03],\n",
      "        [4.5075e-01, 5.3083e-01, 1.8426e-02],\n",
      "        [4.6317e-01, 5.1833e-01, 1.8498e-02],\n",
      "        [5.7979e-01, 3.8695e-01, 3.3262e-02],\n",
      "        [4.9663e-01, 4.7977e-01, 2.3607e-02],\n",
      "        [6.5691e-01, 3.3971e-01, 3.3833e-03],\n",
      "        [8.6828e-01, 1.3004e-01, 1.6786e-03],\n",
      "        [6.0969e-01, 3.6838e-01, 2.1925e-02],\n",
      "        [7.3264e-01, 2.6341e-01, 3.9479e-03],\n",
      "        [7.0505e-01, 2.8379e-01, 1.1163e-02],\n",
      "        [8.3607e-01, 1.5813e-01, 5.8005e-03],\n",
      "        [6.7043e-01, 3.1811e-01, 1.1455e-02],\n",
      "        [7.6224e-01, 2.2866e-01, 9.0907e-03],\n",
      "        [7.7847e-01, 2.0887e-01, 1.2663e-02],\n",
      "        [5.8862e-01, 4.0643e-01, 4.9497e-03],\n",
      "        [5.5877e-01, 4.2063e-01, 2.0603e-02],\n",
      "        [8.5267e-01, 1.3899e-01, 8.3381e-03],\n",
      "        [5.9458e-01, 3.7543e-01, 2.9991e-02],\n",
      "        [8.4933e-01, 1.4645e-01, 4.2199e-03],\n",
      "        [5.9328e-01, 3.9597e-01, 1.0747e-02],\n",
      "        [5.0669e-01, 4.7462e-01, 1.8692e-02],\n",
      "        [7.2199e-01, 2.6232e-01, 1.5689e-02],\n",
      "        [5.0149e-01, 4.7087e-01, 2.7640e-02],\n",
      "        [4.7088e-01, 5.0075e-01, 2.8368e-02],\n",
      "        [9.1144e-01, 8.5731e-02, 2.8258e-03],\n",
      "        [7.9093e-01, 1.9691e-01, 1.2158e-02],\n",
      "        [5.3776e-01, 4.4138e-01, 2.0859e-02],\n",
      "        [4.4029e-01, 5.3277e-01, 2.6940e-02],\n",
      "        [4.6959e-01, 5.0217e-01, 2.8240e-02],\n",
      "        [6.6655e-01, 3.2466e-01, 8.7871e-03],\n",
      "        [8.1610e-01, 1.7586e-01, 8.0346e-03],\n",
      "        [7.8172e-01, 2.0245e-01, 1.5822e-02],\n",
      "        [7.2673e-01, 2.4817e-01, 2.5097e-02],\n",
      "        [5.7350e-01, 3.8513e-01, 4.1371e-02],\n",
      "        [5.4570e-01, 4.2050e-01, 3.3800e-02],\n",
      "        [5.4039e-01, 4.2578e-01, 3.3825e-02],\n",
      "        [4.7949e-01, 4.9668e-01, 2.3826e-02],\n",
      "        [5.6695e-01, 4.1491e-01, 1.8143e-02],\n",
      "        [6.0732e-01, 3.8669e-01, 5.9916e-03],\n",
      "        [8.5868e-01, 1.3893e-01, 2.3890e-03],\n",
      "        [5.0826e-01, 4.7333e-01, 1.8404e-02],\n",
      "        [5.6640e-01, 4.1475e-01, 1.8850e-02],\n",
      "        [5.5991e-01, 4.1550e-01, 2.4592e-02],\n",
      "        [7.2043e-01, 2.5722e-01, 2.2355e-02],\n",
      "        [5.6623e-01, 4.0120e-01, 3.2569e-02],\n",
      "        [5.1465e-01, 4.5328e-01, 3.2064e-02],\n",
      "        [6.9751e-01, 2.7537e-01, 2.7118e-02],\n",
      "        [6.1700e-01, 3.5065e-01, 3.2342e-02],\n",
      "        [7.1882e-01, 2.7908e-01, 2.1066e-03],\n",
      "        [4.6288e-01, 5.0874e-01, 2.8380e-02],\n",
      "        [6.4371e-01, 3.1909e-01, 3.7195e-02],\n",
      "        [5.4895e-01, 4.2411e-01, 2.6940e-02],\n",
      "        [6.1406e-01, 3.5521e-01, 3.0724e-02],\n",
      "        [5.0141e-01, 4.9831e-01, 2.8071e-04],\n",
      "        [7.4986e-01, 2.4178e-01, 8.3609e-03],\n",
      "        [5.7755e-01, 3.9327e-01, 2.9185e-02],\n",
      "        [7.6918e-01, 2.1266e-01, 1.8158e-02],\n",
      "        [5.6991e-01, 4.0852e-01, 2.1576e-02]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>), tensor([[2.4081e-02, 9.4974e-01, 1.3311e-02, 1.2866e-02],\n",
      "        [5.3822e-04, 9.9890e-01, 2.8143e-04, 2.7930e-04],\n",
      "        [1.2708e-02, 9.7204e-01, 7.7209e-03, 7.5341e-03],\n",
      "        [6.1140e-02, 8.6713e-01, 3.6615e-02, 3.5112e-02],\n",
      "        [8.9018e-02, 8.0267e-01, 5.5874e-02, 5.2442e-02],\n",
      "        [6.2902e-02, 8.7019e-01, 3.4512e-02, 3.2396e-02],\n",
      "        [1.5122e-02, 9.6838e-01, 8.3646e-03, 8.1328e-03],\n",
      "        [5.5070e-02, 8.8821e-01, 2.9265e-02, 2.7456e-02],\n",
      "        [3.8222e-03, 9.9255e-01, 1.8244e-03, 1.8004e-03],\n",
      "        [6.8165e-02, 8.5974e-01, 3.7284e-02, 3.4815e-02],\n",
      "        [2.4930e-02, 9.4939e-01, 1.3155e-02, 1.2525e-02],\n",
      "        [9.0628e-03, 9.8039e-01, 5.3444e-03, 5.2044e-03],\n",
      "        [6.8765e-02, 8.5232e-01, 4.0791e-02, 3.8127e-02],\n",
      "        [5.0724e-02, 8.9989e-01, 2.5406e-02, 2.3982e-02],\n",
      "        [6.3442e-02, 8.5918e-01, 3.9518e-02, 3.7857e-02],\n",
      "        [6.5026e-02, 8.6442e-01, 3.6443e-02, 3.4115e-02],\n",
      "        [2.6170e-03, 9.9474e-01, 1.3311e-03, 1.3119e-03],\n",
      "        [8.4264e-02, 8.1868e-01, 4.9616e-02, 4.7440e-02],\n",
      "        [6.0168e-02, 8.7243e-01, 3.4968e-02, 3.2438e-02],\n",
      "        [6.2638e-03, 9.8633e-01, 3.7410e-03, 3.6608e-03],\n",
      "        [4.8456e-03, 9.8912e-01, 3.0437e-03, 2.9957e-03],\n",
      "        [6.5601e-02, 8.6047e-01, 3.8368e-02, 3.5559e-02],\n",
      "        [7.6039e-02, 8.3340e-01, 4.6351e-02, 4.4209e-02],\n",
      "        [1.7948e-02, 9.6172e-01, 1.0285e-02, 1.0046e-02],\n",
      "        [4.2078e-04, 9.9912e-01, 2.2874e-04, 2.2738e-04],\n",
      "        [3.5926e-02, 9.2728e-01, 1.8890e-02, 1.7900e-02],\n",
      "        [5.1446e-04, 9.9887e-01, 3.0986e-04, 3.0749e-04],\n",
      "        [1.4042e-02, 9.6973e-01, 8.2492e-03, 7.9755e-03],\n",
      "        [5.9728e-02, 8.8098e-01, 3.0456e-02, 2.8833e-02],\n",
      "        [6.2070e-02, 8.7513e-01, 3.2316e-02, 3.0482e-02],\n",
      "        [7.4081e-02, 8.4565e-01, 4.1434e-02, 3.8832e-02],\n",
      "        [2.9489e-02, 9.3621e-01, 1.7601e-02, 1.6701e-02],\n",
      "        [6.7289e-02, 8.5697e-01, 3.9290e-02, 3.6449e-02],\n",
      "        [5.8691e-02, 8.7157e-01, 3.5742e-02, 3.3994e-02],\n",
      "        [4.2865e-02, 9.1427e-01, 2.1977e-02, 2.0888e-02],\n",
      "        [2.5114e-02, 9.4617e-01, 1.4577e-02, 1.4144e-02],\n",
      "        [4.2614e-02, 9.1082e-01, 2.3866e-02, 2.2704e-02],\n",
      "        [5.7598e-02, 8.7334e-01, 3.5658e-02, 3.3404e-02],\n",
      "        [2.0170e-02, 9.5617e-01, 1.2102e-02, 1.1560e-02],\n",
      "        [5.2665e-02, 8.9470e-01, 2.7034e-02, 2.5597e-02],\n",
      "        [1.0189e-01, 7.7721e-01, 6.2326e-02, 5.8577e-02],\n",
      "        [7.8066e-04, 9.9824e-01, 4.9121e-04, 4.8807e-04],\n",
      "        [4.4718e-02, 9.0210e-01, 2.7047e-02, 2.6135e-02],\n",
      "        [2.1718e-03, 9.9529e-01, 1.2767e-03, 1.2636e-03],\n",
      "        [7.0618e-02, 8.5319e-01, 3.9460e-02, 3.6735e-02],\n",
      "        [1.6000e-02, 9.6787e-01, 8.1982e-03, 7.9310e-03],\n",
      "        [4.9760e-02, 8.9209e-01, 2.9849e-02, 2.8296e-02],\n",
      "        [2.7584e-02, 9.4179e-01, 1.5653e-02, 1.4969e-02],\n",
      "        [8.8976e-02, 8.0528e-01, 5.4517e-02, 5.1227e-02],\n",
      "        [5.6452e-02, 8.7948e-01, 3.2976e-02, 3.1089e-02],\n",
      "        [5.7657e-03, 9.8782e-01, 3.2306e-03, 3.1880e-03],\n",
      "        [1.1298e-01, 7.4521e-01, 7.3372e-02, 6.8437e-02],\n",
      "        [1.1014e-02, 9.7760e-01, 5.7585e-03, 5.6299e-03],\n",
      "        [7.1762e-02, 8.4666e-01, 4.2190e-02, 3.9391e-02],\n",
      "        [6.7055e-02, 8.6138e-01, 3.6931e-02, 3.4629e-02],\n",
      "        [3.6030e-02, 9.2033e-01, 2.2404e-02, 2.1235e-02],\n",
      "        [7.6405e-02, 8.3495e-01, 4.5691e-02, 4.2956e-02],\n",
      "        [1.9690e-02, 9.6087e-01, 9.8651e-03, 9.5734e-03],\n",
      "        [5.1234e-04, 9.9890e-01, 2.9556e-04, 2.9406e-04],\n",
      "        [8.7018e-02, 8.1268e-01, 5.1664e-02, 4.8642e-02],\n",
      "        [6.3791e-02, 8.6529e-01, 3.6896e-02, 3.4024e-02],\n",
      "        [6.3967e-02, 8.6209e-01, 3.7897e-02, 3.6049e-02],\n",
      "        [5.5371e-02, 8.8006e-01, 3.3005e-02, 3.1559e-02],\n",
      "        [4.0373e-02, 9.1339e-01, 2.3736e-02, 2.2501e-02],\n",
      "        [6.2137e-04, 9.9862e-01, 3.7819e-04, 3.7556e-04],\n",
      "        [6.3207e-02, 8.6924e-01, 3.4933e-02, 3.2623e-02],\n",
      "        [5.4307e-02, 8.8286e-01, 3.2231e-02, 3.0605e-02],\n",
      "        [5.2829e-02, 8.8864e-01, 3.0150e-02, 2.8384e-02],\n",
      "        [7.2109e-02, 8.4726e-01, 4.1506e-02, 3.9123e-02],\n",
      "        [4.9888e-02, 8.9135e-01, 2.9962e-02, 2.8796e-02],\n",
      "        [3.7207e-02, 9.2041e-01, 2.1876e-02, 2.0504e-02],\n",
      "        [9.0741e-02, 7.9854e-01, 5.7094e-02, 5.3622e-02],\n",
      "        [6.9185e-02, 8.5293e-01, 4.0372e-02, 3.7508e-02],\n",
      "        [4.8252e-02, 9.0563e-01, 2.3717e-02, 2.2405e-02],\n",
      "        [2.9383e-02, 9.3587e-01, 1.7683e-02, 1.7062e-02],\n",
      "        [1.7419e-02, 9.6429e-01, 9.2811e-03, 9.0088e-03],\n",
      "        [1.3276e-02, 9.7140e-01, 7.7718e-03, 7.5549e-03],\n",
      "        [7.2768e-02, 8.3652e-01, 4.6962e-02, 4.3747e-02],\n",
      "        [1.1754e-02, 9.7515e-01, 6.6273e-03, 6.4693e-03],\n",
      "        [4.6845e-02, 8.9912e-01, 2.7837e-02, 2.6201e-02],\n",
      "        [6.4307e-02, 8.7032e-01, 3.3583e-02, 3.1787e-02],\n",
      "        [5.2595e-02, 8.9087e-01, 2.9029e-02, 2.7502e-02],\n",
      "        [6.3157e-02, 8.5895e-01, 4.0069e-02, 3.7825e-02],\n",
      "        [3.6235e-02, 9.2042e-01, 2.2286e-02, 2.1064e-02],\n",
      "        [1.0302e-02, 9.7891e-01, 5.4614e-03, 5.3273e-03],\n",
      "        [5.8666e-02, 8.8260e-01, 3.0220e-02, 2.8510e-02],\n",
      "        [5.1745e-02, 8.8728e-01, 3.0989e-02, 2.9990e-02],\n",
      "        [2.2489e-02, 9.5242e-01, 1.2747e-02, 1.2349e-02],\n",
      "        [6.5033e-02, 8.7060e-01, 3.3180e-02, 3.1183e-02],\n",
      "        [2.4130e-02, 9.4803e-01, 1.4164e-02, 1.3675e-02],\n",
      "        [4.0668e-02, 9.1345e-01, 2.3432e-02, 2.2453e-02],\n",
      "        [5.7011e-02, 8.8612e-01, 2.9255e-02, 2.7610e-02],\n",
      "        [7.2139e-02, 8.3620e-01, 4.7254e-02, 4.4406e-02],\n",
      "        [5.9604e-02, 8.7637e-01, 3.3155e-02, 3.0872e-02],\n",
      "        [8.0430e-03, 9.8369e-01, 4.1697e-03, 4.1015e-03],\n",
      "        [5.3896e-03, 9.8834e-01, 3.1600e-03, 3.1103e-03],\n",
      "        [5.2765e-02, 8.8939e-01, 2.9717e-02, 2.8123e-02],\n",
      "        [9.0547e-03, 9.8175e-01, 4.6658e-03, 4.5294e-03],\n",
      "        [4.3938e-02, 9.1414e-01, 2.1534e-02, 2.0387e-02],\n",
      "        [1.6103e-02, 9.6546e-01, 9.3590e-03, 9.0793e-03],\n",
      "        [3.0823e-02, 9.3360e-01, 1.8046e-02, 1.7531e-02],\n",
      "        [3.0206e-02, 9.3921e-01, 1.5506e-02, 1.5078e-02],\n",
      "        [4.6945e-02, 9.0526e-01, 2.4532e-02, 2.3259e-02],\n",
      "        [1.5098e-02, 9.6888e-01, 8.2087e-03, 7.8150e-03],\n",
      "        [5.9182e-02, 8.7963e-01, 3.1663e-02, 2.9526e-02],\n",
      "        [3.0435e-02, 9.3806e-01, 1.6080e-02, 1.5429e-02],\n",
      "        [8.5548e-02, 8.2512e-01, 4.5784e-02, 4.3544e-02],\n",
      "        [2.0480e-02, 9.5963e-01, 1.0137e-02, 9.7572e-03],\n",
      "        [3.8249e-02, 9.2426e-01, 1.9126e-02, 1.8364e-02],\n",
      "        [4.8000e-02, 8.9985e-01, 2.6889e-02, 2.5265e-02],\n",
      "        [3.8730e-02, 9.1544e-01, 2.3689e-02, 2.2139e-02],\n",
      "        [7.4119e-02, 8.4238e-01, 4.3332e-02, 4.0167e-02],\n",
      "        [6.9268e-02, 8.5282e-01, 4.0388e-02, 3.7521e-02],\n",
      "        [1.4594e-02, 9.7033e-01, 7.6385e-03, 7.4375e-03],\n",
      "        [3.7017e-02, 9.2143e-01, 2.1271e-02, 2.0285e-02],\n",
      "        [4.6881e-02, 8.9899e-01, 2.7673e-02, 2.6455e-02],\n",
      "        [7.3048e-02, 8.4837e-01, 4.0545e-02, 3.8034e-02],\n",
      "        [6.8933e-02, 8.5224e-01, 4.0699e-02, 3.8131e-02],\n",
      "        [2.2323e-02, 9.5147e-01, 1.3349e-02, 1.2857e-02],\n",
      "        [2.7641e-02, 9.4372e-01, 1.4633e-02, 1.4002e-02],\n",
      "        [4.2486e-02, 9.0928e-01, 2.4783e-02, 2.3452e-02],\n",
      "        [5.6422e-02, 8.7522e-01, 3.4926e-02, 3.3432e-02],\n",
      "        [9.3497e-02, 7.9358e-01, 5.8444e-02, 5.4480e-02],\n",
      "        [8.2657e-02, 8.2140e-01, 4.9626e-02, 4.6314e-02],\n",
      "        [7.1709e-02, 8.4259e-01, 4.4156e-02, 4.1548e-02],\n",
      "        [6.2447e-02, 8.6879e-01, 3.5755e-02, 3.3005e-02],\n",
      "        [3.6673e-02, 9.2118e-01, 2.1494e-02, 2.0656e-02],\n",
      "        [1.6851e-02, 9.6626e-01, 8.5636e-03, 8.3253e-03],\n",
      "        [1.0777e-02, 9.7872e-01, 5.3031e-03, 5.2003e-03],\n",
      "        [5.0360e-02, 8.9757e-01, 2.6762e-02, 2.5311e-02],\n",
      "        [5.1391e-02, 8.9200e-01, 2.8877e-02, 2.7730e-02],\n",
      "        [6.3164e-02, 8.6977e-01, 3.4555e-02, 3.2508e-02],\n",
      "        [6.4154e-02, 8.6031e-01, 3.8663e-02, 3.6875e-02],\n",
      "        [6.9422e-02, 8.4904e-01, 4.1959e-02, 3.9583e-02],\n",
      "        [7.5842e-02, 8.3779e-01, 4.4828e-02, 4.1539e-02],\n",
      "        [7.3442e-02, 8.4065e-01, 4.3842e-02, 4.2069e-02],\n",
      "        [7.4509e-02, 8.3473e-01, 4.6467e-02, 4.4294e-02],\n",
      "        [5.7957e-03, 9.8882e-01, 2.7096e-03, 2.6719e-03],\n",
      "        [5.4700e-02, 8.8003e-01, 3.3405e-02, 3.1865e-02],\n",
      "        [8.9655e-02, 8.0302e-01, 5.5354e-02, 5.1972e-02],\n",
      "        [6.3612e-02, 8.6460e-01, 3.6720e-02, 3.5063e-02],\n",
      "        [6.9390e-02, 8.4991e-01, 4.1276e-02, 3.9426e-02],\n",
      "        [4.1371e-04, 9.9918e-01, 2.0211e-04, 2.0123e-04],\n",
      "        [2.3436e-02, 9.5180e-01, 1.2591e-02, 1.2169e-02],\n",
      "        [5.8755e-02, 8.6856e-01, 3.7413e-02, 3.5270e-02],\n",
      "        [4.6704e-02, 9.0135e-01, 2.6494e-02, 2.5457e-02],\n",
      "        [5.1865e-02, 8.9199e-01, 2.8960e-02, 2.7189e-02]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>), tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0023, 0.0004, 0.0025, 0.0019, 0.0025],\n",
      "        [0.0320, 0.0102, 0.0336, 0.0241, 0.0234],\n",
      "        [0.0882, 0.0398, 0.0792, 0.0846, 0.0911],\n",
      "        [0.1211, 0.0614, 0.1096, 0.1127, 0.1125],\n",
      "        [0.0805, 0.0326, 0.0929, 0.0771, 0.0738],\n",
      "        [0.0337, 0.0078, 0.0299, 0.0214, 0.0321],\n",
      "        [0.0685, 0.0231, 0.0732, 0.0572, 0.0571],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0398, 0.0147, 0.0395, 0.0355, 0.0324],\n",
      "        [0.0225, 0.0060, 0.0208, 0.0173, 0.0181],\n",
      "        [0.0967, 0.0396, 0.1005, 0.0952, 0.0833],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1017, 0.0437, 0.0871, 0.0784, 0.0894],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0104, 0.0019, 0.0099, 0.0084, 0.0104],\n",
      "        [0.1132, 0.0483, 0.1064, 0.0990, 0.1078],\n",
      "        [0.0903, 0.0334, 0.0910, 0.0755, 0.0676],\n",
      "        [0.0229, 0.0040, 0.0207, 0.0133, 0.0138],\n",
      "        [0.0204, 0.0037, 0.0154, 0.0093, 0.0149],\n",
      "        [0.0887, 0.0407, 0.0977, 0.0894, 0.0740],\n",
      "        [0.1080, 0.0478, 0.0984, 0.0935, 0.0969],\n",
      "        [0.0329, 0.0103, 0.0324, 0.0266, 0.0342],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0454, 0.0212, 0.0517, 0.0406, 0.0400],\n",
      "        [0.0045, 0.0004, 0.0024, 0.0019, 0.0027],\n",
      "        [0.0284, 0.0077, 0.0286, 0.0249, 0.0278],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1004, 0.0390, 0.1014, 0.0849, 0.0819],\n",
      "        [0.0622, 0.0169, 0.0610, 0.0444, 0.0516],\n",
      "        [0.0977, 0.0367, 0.0946, 0.0830, 0.0763],\n",
      "        [0.0868, 0.0367, 0.0766, 0.0774, 0.0744],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0550, 0.0160, 0.0535, 0.0498, 0.0483],\n",
      "        [0.0595, 0.0278, 0.0628, 0.0588, 0.0576],\n",
      "        [0.0970, 0.0365, 0.0879, 0.0697, 0.0811],\n",
      "        [0.0373, 0.0183, 0.0337, 0.0274, 0.0264],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1426, 0.0665, 0.1332, 0.1264, 0.1373],\n",
      "        [0.0034, 0.0005, 0.0034, 0.0022, 0.0035],\n",
      "        [0.0671, 0.0249, 0.0631, 0.0524, 0.0677],\n",
      "        [0.0075, 0.0014, 0.0076, 0.0051, 0.0071],\n",
      "        [0.0897, 0.0373, 0.1022, 0.0887, 0.0795],\n",
      "        [0.0308, 0.0073, 0.0355, 0.0234, 0.0268],\n",
      "        [0.0730, 0.0314, 0.0701, 0.0695, 0.0679],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1257, 0.0574, 0.1060, 0.1027, 0.1081],\n",
      "        [0.0822, 0.0348, 0.0884, 0.0782, 0.0648],\n",
      "        [0.0120, 0.0036, 0.0147, 0.0094, 0.0119],\n",
      "        [0.1526, 0.0832, 0.1440, 0.1429, 0.1389],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0983, 0.0465, 0.1116, 0.1078, 0.0892],\n",
      "        [0.0836, 0.0363, 0.1011, 0.0853, 0.0799],\n",
      "        [0.0577, 0.0279, 0.0651, 0.0549, 0.0522],\n",
      "        [0.1103, 0.0518, 0.1047, 0.1055, 0.1186],\n",
      "        [0.0256, 0.0095, 0.0380, 0.0259, 0.0269],\n",
      "        [0.0024, 0.0004, 0.0027, 0.0020, 0.0022],\n",
      "        [0.1183, 0.0558, 0.1239, 0.1155, 0.1148],\n",
      "        [0.0934, 0.0346, 0.0940, 0.0789, 0.0755],\n",
      "        [0.0922, 0.0373, 0.0766, 0.0678, 0.0860],\n",
      "        [0.0901, 0.0348, 0.0845, 0.0804, 0.0795],\n",
      "        [0.0659, 0.0252, 0.0682, 0.0598, 0.0482],\n",
      "        [0.0046, 0.0004, 0.0027, 0.0020, 0.0028],\n",
      "        [0.0814, 0.0300, 0.0896, 0.0737, 0.0724],\n",
      "        [0.0848, 0.0298, 0.0790, 0.0712, 0.0804],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1025, 0.0399, 0.0897, 0.0812, 0.0870],\n",
      "        [0.0893, 0.0328, 0.0794, 0.0695, 0.0791],\n",
      "        [0.0570, 0.0279, 0.0558, 0.0515, 0.0492],\n",
      "        [0.1252, 0.0647, 0.1175, 0.1182, 0.1077],\n",
      "        [0.0987, 0.0371, 0.0934, 0.0846, 0.0795],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0571, 0.0172, 0.0504, 0.0407, 0.0490],\n",
      "        [0.0276, 0.0071, 0.0251, 0.0220, 0.0256],\n",
      "        [0.0287, 0.0088, 0.0343, 0.0279, 0.0309],\n",
      "        [0.1078, 0.0592, 0.1082, 0.1016, 0.1045],\n",
      "        [0.0209, 0.0061, 0.0181, 0.0169, 0.0171],\n",
      "        [0.0833, 0.0295, 0.0775, 0.0694, 0.0777],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0711, 0.0280, 0.0831, 0.0771, 0.0613],\n",
      "        [0.1030, 0.0419, 0.0834, 0.0707, 0.0870],\n",
      "        [0.0638, 0.0265, 0.0664, 0.0526, 0.0503],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0708, 0.0241, 0.0752, 0.0598, 0.0564],\n",
      "        [0.0716, 0.0271, 0.0682, 0.0564, 0.0620],\n",
      "        [0.0338, 0.0102, 0.0371, 0.0299, 0.0300],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0528, 0.0159, 0.0448, 0.0461, 0.0451],\n",
      "        [0.0720, 0.0227, 0.0649, 0.0557, 0.0582],\n",
      "        [0.0697, 0.0235, 0.0741, 0.0585, 0.0547],\n",
      "        [0.1134, 0.0536, 0.1098, 0.0907, 0.0951],\n",
      "        [0.0804, 0.0326, 0.0971, 0.0777, 0.0676],\n",
      "        [0.0247, 0.0042, 0.0169, 0.0186, 0.0209],\n",
      "        [0.0118, 0.0037, 0.0144, 0.0114, 0.0124],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0208, 0.0059, 0.0221, 0.0145, 0.0158],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0341, 0.0123, 0.0294, 0.0255, 0.0327],\n",
      "        [0.0582, 0.0172, 0.0493, 0.0395, 0.0477],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0535, 0.0257, 0.0550, 0.0487, 0.0526],\n",
      "        [0.0278, 0.0092, 0.0247, 0.0200, 0.0156],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0764, 0.0271, 0.0751, 0.0634, 0.0620],\n",
      "        [0.0676, 0.0266, 0.0610, 0.0532, 0.0577],\n",
      "        [0.1033, 0.0388, 0.0952, 0.0830, 0.0871],\n",
      "        [0.0949, 0.0390, 0.1037, 0.0962, 0.0858],\n",
      "        [0.0241, 0.0083, 0.0272, 0.0216, 0.0245],\n",
      "        [0.0608, 0.0239, 0.0542, 0.0496, 0.0534],\n",
      "        [0.0772, 0.0269, 0.0712, 0.0595, 0.0669],\n",
      "        [0.0945, 0.0363, 0.1004, 0.0834, 0.0794],\n",
      "        [0.0959, 0.0388, 0.0964, 0.0946, 0.0842],\n",
      "        [0.0435, 0.0163, 0.0370, 0.0338, 0.0356],\n",
      "        [0.0404, 0.0176, 0.0363, 0.0337, 0.0340],\n",
      "        [0.0770, 0.0265, 0.0610, 0.0627, 0.0587],\n",
      "        [0.0971, 0.0382, 0.0816, 0.0901, 0.0903],\n",
      "        [0.1289, 0.0670, 0.1286, 0.1247, 0.1150],\n",
      "        [0.1098, 0.0499, 0.1124, 0.1114, 0.0994],\n",
      "        [0.1138, 0.0474, 0.1029, 0.0973, 0.0962],\n",
      "        [0.0924, 0.0328, 0.0897, 0.0754, 0.0704],\n",
      "        [0.0652, 0.0245, 0.0671, 0.0584, 0.0647],\n",
      "        [0.0294, 0.0066, 0.0320, 0.0231, 0.0262],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0767, 0.0252, 0.0704, 0.0602, 0.0671],\n",
      "        [0.0794, 0.0349, 0.0970, 0.0844, 0.0779],\n",
      "        [0.0981, 0.0410, 0.0803, 0.0733, 0.0909],\n",
      "        [0.1062, 0.0459, 0.0994, 0.0976, 0.0909],\n",
      "        [0.0957, 0.0480, 0.1175, 0.1038, 0.0814],\n",
      "        [0.1014, 0.0461, 0.0890, 0.0788, 0.0893],\n",
      "        [0.1115, 0.0498, 0.0956, 0.0960, 0.1067],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0978, 0.0374, 0.0873, 0.0817, 0.0874],\n",
      "        [0.1189, 0.0622, 0.1093, 0.1155, 0.1174],\n",
      "        [0.0876, 0.0344, 0.0801, 0.0795, 0.0762],\n",
      "        [0.0983, 0.0452, 0.0905, 0.0943, 0.0966],\n",
      "        [0.0023, 0.0002, 0.0027, 0.0018, 0.0020],\n",
      "        [0.0399, 0.0131, 0.0423, 0.0315, 0.0365],\n",
      "        [0.0900, 0.0476, 0.0946, 0.0839, 0.0842],\n",
      "        [0.0748, 0.0268, 0.0602, 0.0590, 0.0731],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<MulBackward0>)], '5%': [tensor([[9.9971e-01, 1.9330e-05, 1.9288e-05],\n",
      "        [1.0000e+00, 6.5480e-10, 6.5471e-10],\n",
      "        [9.9854e-01, 2.5297e-06, 2.5216e-06],\n",
      "        [9.9490e-01, 4.5689e-04, 4.4612e-04],\n",
      "        [9.9235e-01, 9.4877e-04, 9.1302e-04],\n",
      "        [9.9781e-01, 1.7774e-04, 1.7540e-04],\n",
      "        [9.9971e-01, 1.1895e-05, 1.1840e-05],\n",
      "        [9.9866e-01, 1.0996e-04, 1.0913e-04],\n",
      "        [9.9996e-01, 4.8756e-08, 4.8746e-08],\n",
      "        [9.9676e-01, 3.1644e-04, 2.9948e-04],\n",
      "        [9.9912e-01, 1.1503e-05, 1.1385e-05],\n",
      "        [9.9994e-01, 1.9331e-06, 1.9298e-06],\n",
      "        [9.9538e-01, 7.5319e-04, 7.4039e-04],\n",
      "        [9.9881e-01, 6.4533e-05, 6.3672e-05],\n",
      "        [9.9318e-01, 8.5002e-04, 7.9616e-04],\n",
      "        [9.9669e-01, 2.1105e-04, 2.0509e-04],\n",
      "        [1.0000e+00, 2.0331e-07, 2.0315e-07],\n",
      "        [9.9463e-01, 7.9650e-04, 7.7905e-04],\n",
      "        [9.9632e-01, 2.4186e-04, 2.3663e-04],\n",
      "        [9.9998e-01, 5.9389e-07, 5.9095e-07],\n",
      "        [9.9999e-01, 4.3287e-07, 4.3246e-07],\n",
      "        [9.9643e-01, 4.1283e-04, 4.0033e-04],\n",
      "        [9.9240e-01, 7.1526e-04, 6.9647e-04],\n",
      "        [9.9965e-01, 1.0999e-05, 1.0886e-05],\n",
      "        [9.9963e-01, 2.8341e-10, 2.8338e-10],\n",
      "        [9.9958e-01, 2.9285e-05, 2.8675e-05],\n",
      "        [9.9993e-01, 2.0932e-09, 2.0931e-09],\n",
      "        [9.9982e-01, 8.3368e-06, 8.2765e-06],\n",
      "        [9.9901e-01, 1.4730e-04, 1.4581e-04],\n",
      "        [9.9686e-01, 3.7598e-04, 3.7202e-04],\n",
      "        [9.9688e-01, 4.2662e-04, 4.2276e-04],\n",
      "        [9.9563e-01, 3.7317e-05, 3.6879e-05],\n",
      "        [9.9593e-01, 4.4469e-04, 4.3545e-04],\n",
      "        [9.9509e-01, 3.6049e-04, 3.5501e-04],\n",
      "        [9.9938e-01, 4.4332e-05, 4.3575e-05],\n",
      "        [9.9907e-01, 4.4140e-05, 4.3340e-05],\n",
      "        [9.9931e-01, 5.3335e-05, 5.3034e-05],\n",
      "        [9.9679e-01, 4.5726e-04, 4.3642e-04],\n",
      "        [9.9974e-01, 1.2524e-05, 1.2480e-05],\n",
      "        [9.9836e-01, 1.8405e-04, 1.7793e-04],\n",
      "        [9.8265e-01, 1.9874e-03, 1.9144e-03],\n",
      "        [1.0000e+00, 3.6262e-09, 3.6249e-09],\n",
      "        [9.9822e-01, 2.4662e-04, 2.3753e-04],\n",
      "        [1.0000e+00, 2.1667e-08, 2.1646e-08],\n",
      "        [9.9724e-01, 3.9218e-04, 3.8411e-04],\n",
      "        [9.9950e-01, 4.7823e-06, 4.7628e-06],\n",
      "        [9.9841e-01, 1.2905e-04, 1.2792e-04],\n",
      "        [9.9949e-01, 2.2079e-05, 2.1564e-05],\n",
      "        [9.8886e-01, 1.0295e-03, 9.9060e-04],\n",
      "        [9.9756e-01, 2.6585e-04, 2.6390e-04],\n",
      "        [9.9999e-01, 1.5147e-07, 1.5128e-07],\n",
      "        [9.8166e-01, 2.2837e-03, 2.1261e-03],\n",
      "        [9.9994e-01, 3.4022e-06, 3.3834e-06],\n",
      "        [9.9336e-01, 1.0286e-03, 9.9520e-04],\n",
      "        [9.9766e-01, 1.5017e-04, 1.4656e-04],\n",
      "        [9.9897e-01, 6.3136e-05, 6.1170e-05],\n",
      "        [9.9468e-01, 7.3381e-04, 7.2159e-04],\n",
      "        [9.9981e-01, 4.9761e-06, 4.9612e-06],\n",
      "        [1.0000e+00, 1.4630e-09, 1.4629e-09],\n",
      "        [9.9109e-01, 1.5195e-03, 1.4882e-03],\n",
      "        [9.9580e-01, 2.5153e-04, 2.5135e-04],\n",
      "        [9.9713e-01, 2.2465e-04, 2.1551e-04],\n",
      "        [9.9474e-01, 4.0358e-04, 3.9468e-04],\n",
      "        [9.9848e-01, 7.8146e-05, 7.7123e-05],\n",
      "        [1.0000e+00, 2.4811e-09, 2.4803e-09],\n",
      "        [9.9710e-01, 1.8857e-04, 1.8588e-04],\n",
      "        [9.9724e-01, 1.6878e-04, 1.6552e-04],\n",
      "        [9.9753e-01, 2.0490e-04, 2.0282e-04],\n",
      "        [9.9600e-01, 2.9796e-04, 2.9192e-04],\n",
      "        [9.9800e-01, 3.5554e-04, 3.5218e-04],\n",
      "        [9.9910e-01, 3.8649e-05, 3.7994e-05],\n",
      "        [9.9092e-01, 1.1077e-03, 1.0948e-03],\n",
      "        [9.9724e-01, 2.2288e-04, 2.2205e-04],\n",
      "        [9.9931e-01, 8.4464e-05, 8.2822e-05],\n",
      "        [9.9880e-01, 4.9791e-05, 4.8741e-05],\n",
      "        [9.9990e-01, 3.3512e-06, 3.3458e-06],\n",
      "        [9.9979e-01, 1.4447e-05, 1.4331e-05],\n",
      "        [9.9444e-01, 8.5461e-04, 8.0636e-04],\n",
      "        [9.9996e-01, 1.7695e-06, 1.7586e-06],\n",
      "        [9.9867e-01, 1.5427e-04, 1.5089e-04],\n",
      "        [9.9776e-01, 1.8915e-04, 1.8827e-04],\n",
      "        [9.9899e-01, 1.3345e-04, 1.2984e-04],\n",
      "        [9.9549e-01, 6.3867e-04, 6.1378e-04],\n",
      "        [9.9856e-01, 5.7022e-05, 5.6354e-05],\n",
      "        [9.9988e-01, 2.0808e-06, 2.0677e-06],\n",
      "        [9.9842e-01, 9.6655e-05, 9.6090e-05],\n",
      "        [9.9804e-01, 1.9906e-04, 1.9649e-04],\n",
      "        [9.9978e-01, 1.4714e-05, 1.4539e-05],\n",
      "        [9.9875e-01, 2.1026e-04, 2.0984e-04],\n",
      "        [9.9893e-01, 3.6972e-05, 3.6371e-05],\n",
      "        [9.9792e-01, 9.0634e-05, 8.9741e-05],\n",
      "        [9.9897e-01, 1.3945e-04, 1.3877e-04],\n",
      "        [9.8980e-01, 1.2169e-03, 1.2088e-03],\n",
      "        [9.9742e-01, 1.5535e-04, 1.5406e-04],\n",
      "        [9.9860e-01, 1.3624e-06, 1.3538e-06],\n",
      "        [9.9996e-01, 1.6768e-07, 1.6717e-07],\n",
      "        [9.9788e-01, 2.5862e-04, 2.5566e-04],\n",
      "        [9.9995e-01, 1.1376e-06, 1.1338e-06],\n",
      "        [9.9939e-01, 3.9604e-05, 3.9436e-05],\n",
      "        [9.9974e-01, 2.4381e-05, 2.4211e-05],\n",
      "        [9.9933e-01, 6.9508e-05, 6.8871e-05],\n",
      "        [9.9982e-01, 1.7773e-05, 1.7636e-05],\n",
      "        [9.9944e-01, 5.4364e-05, 5.3404e-05],\n",
      "        [9.9995e-01, 1.3976e-06, 1.3937e-06],\n",
      "        [9.9731e-01, 2.7397e-04, 2.6712e-04],\n",
      "        [9.9962e-01, 4.1101e-05, 4.0647e-05],\n",
      "        [9.9656e-01, 3.7823e-04, 3.7000e-04],\n",
      "        [9.9991e-01, 3.5280e-06, 3.4902e-06],\n",
      "        [9.9904e-01, 2.6802e-05, 2.6417e-05],\n",
      "        [9.9880e-01, 1.3212e-04, 1.2991e-04],\n",
      "        [9.9900e-01, 8.2058e-05, 7.9348e-05],\n",
      "        [9.9728e-01, 5.0229e-04, 4.8758e-04],\n",
      "        [9.9539e-01, 4.3515e-04, 4.2443e-04],\n",
      "        [9.9982e-01, 3.6227e-06, 3.6098e-06],\n",
      "        [9.9836e-01, 3.7932e-05, 3.6873e-05],\n",
      "        [9.9868e-01, 1.3536e-04, 1.3421e-04],\n",
      "        [9.9597e-01, 4.8952e-04, 4.7969e-04],\n",
      "        [9.9682e-01, 6.3761e-04, 6.2013e-04],\n",
      "        [9.9964e-01, 1.3161e-05, 1.3073e-05],\n",
      "        [9.9971e-01, 1.6105e-05, 1.5769e-05],\n",
      "        [9.9850e-01, 1.2544e-04, 1.2397e-04],\n",
      "        [9.9675e-01, 5.3975e-04, 5.2762e-04],\n",
      "        [9.8990e-01, 1.2775e-03, 1.2067e-03],\n",
      "        [9.9412e-01, 8.1849e-04, 7.9062e-04],\n",
      "        [9.9570e-01, 4.0301e-04, 3.9149e-04],\n",
      "        [9.9852e-01, 2.8003e-04, 2.6953e-04],\n",
      "        [9.9729e-01, 1.2696e-04, 1.2460e-04],\n",
      "        [9.9900e-01, 3.0229e-06, 3.0078e-06],\n",
      "        [9.9998e-01, 4.6524e-07, 4.6420e-07],\n",
      "        [9.9828e-01, 1.0268e-04, 1.0132e-04],\n",
      "        [9.9816e-01, 1.6860e-04, 1.6734e-04],\n",
      "        [9.9577e-01, 3.4106e-04, 3.3373e-04],\n",
      "        [9.9802e-01, 3.4369e-04, 3.4132e-04],\n",
      "        [9.9511e-01, 8.1716e-04, 7.7819e-04],\n",
      "        [9.9444e-01, 7.0712e-04, 6.9638e-04],\n",
      "        [9.9376e-01, 8.8089e-04, 8.2498e-04],\n",
      "        [9.8962e-01, 1.0848e-03, 1.0086e-03],\n",
      "        [9.9998e-01, 2.2526e-07, 2.2498e-07],\n",
      "        [9.9605e-01, 4.4264e-04, 4.2608e-04],\n",
      "        [9.9327e-01, 1.3774e-03, 1.3289e-03],\n",
      "        [9.9789e-01, 3.7218e-04, 3.6645e-04],\n",
      "        [9.9326e-01, 5.8379e-04, 5.7412e-04],\n",
      "        [9.9989e-01, 7.7171e-10, 7.7162e-10],\n",
      "        [9.9943e-01, 8.6361e-06, 8.5968e-06],\n",
      "        [9.9579e-01, 3.4939e-04, 3.4283e-04],\n",
      "        [9.9665e-01, 1.6116e-04, 1.5777e-04],\n",
      "        [9.9811e-01, 1.6777e-04, 1.6435e-04]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>), tensor([[8.4774e-01, 3.7338e-02, 2.3971e-03],\n",
      "        [7.3021e-01, 5.1787e-03, 1.8115e-05],\n",
      "        [6.2146e-01, 8.6564e-02, 9.2773e-04],\n",
      "        [5.2639e-01, 2.2135e-01, 1.4248e-02],\n",
      "        [4.9963e-01, 2.0358e-01, 1.8420e-02],\n",
      "        [3.6907e-01, 3.2830e-01, 1.4112e-02],\n",
      "        [4.4296e-01, 2.2231e-01, 4.5421e-03],\n",
      "        [3.7050e-01, 2.6346e-01, 6.8861e-03],\n",
      "        [4.5223e-01, 4.4987e-02, 1.6885e-04],\n",
      "        [3.5393e-01, 3.0145e-01, 1.0880e-02],\n",
      "        [6.6996e-01, 9.4218e-02, 2.6112e-03],\n",
      "        [6.0621e-01, 9.8556e-02, 1.1076e-03],\n",
      "        [2.9832e-01, 3.6207e-01, 1.7858e-02],\n",
      "        [3.2981e-01, 2.0571e-01, 7.4533e-03],\n",
      "        [5.2075e-01, 1.8336e-01, 2.0545e-02],\n",
      "        [4.2343e-01, 2.7608e-01, 1.0527e-02],\n",
      "        [6.2824e-01, 3.0964e-02, 3.8621e-04],\n",
      "        [4.6049e-01, 2.8157e-01, 2.3044e-02],\n",
      "        [2.9809e-01, 3.1820e-01, 1.3683e-02],\n",
      "        [5.3454e-01, 1.6575e-01, 1.4354e-03],\n",
      "        [6.0988e-01, 3.7824e-02, 7.6227e-04],\n",
      "        [4.2888e-01, 2.7364e-01, 1.3970e-02],\n",
      "        [5.9167e-01, 1.8403e-01, 1.7228e-02],\n",
      "        [5.8141e-01, 1.0606e-01, 2.4899e-03],\n",
      "        [1.2154e-01, 1.6493e-01, 4.4766e-05],\n",
      "        [6.8860e-01, 9.9145e-02, 4.3457e-03],\n",
      "        [2.6039e-01, 1.1946e-01, 1.0336e-04],\n",
      "        [5.7361e-01, 9.5107e-02, 2.2825e-03],\n",
      "        [3.2685e-01, 3.2018e-01, 1.3608e-02],\n",
      "        [4.5417e-01, 3.0516e-01, 1.4127e-02],\n",
      "        [3.7305e-01, 2.8768e-01, 1.5017e-02],\n",
      "        [5.2255e-01, 1.6778e-01, 5.4377e-03],\n",
      "        [2.6776e-01, 3.6655e-01, 1.2236e-02],\n",
      "        [5.0648e-01, 1.6319e-01, 1.4054e-02],\n",
      "        [5.6378e-01, 1.0900e-01, 4.5885e-03],\n",
      "        [7.4234e-01, 3.0969e-02, 3.7954e-03],\n",
      "        [4.8038e-01, 1.8969e-01, 6.5732e-03],\n",
      "        [6.0342e-01, 2.4542e-01, 1.4757e-02],\n",
      "        [3.2925e-01, 1.1201e-01, 3.0063e-03],\n",
      "        [4.2426e-01, 2.3823e-01, 1.1463e-02],\n",
      "        [5.0732e-01, 2.9482e-01, 2.8245e-02],\n",
      "        [5.3573e-01, 2.0738e-02, 9.6977e-05],\n",
      "        [6.3617e-01, 1.3154e-01, 6.5839e-03],\n",
      "        [7.5323e-01, 2.2392e-02, 1.7209e-04],\n",
      "        [4.1626e-01, 2.3054e-01, 1.5025e-02],\n",
      "        [5.4895e-01, 6.0186e-02, 1.7379e-03],\n",
      "        [5.5158e-01, 1.8097e-01, 8.0948e-03],\n",
      "        [2.8917e-01, 2.4702e-01, 4.2628e-03],\n",
      "        [4.3219e-01, 3.1993e-01, 2.3987e-02],\n",
      "        [4.1839e-01, 3.6108e-01, 1.4232e-02],\n",
      "        [7.3433e-01, 4.0154e-02, 2.7444e-04],\n",
      "        [4.2866e-01, 2.7706e-01, 2.8454e-02],\n",
      "        [8.6971e-01, 1.9647e-02, 5.0839e-04],\n",
      "        [4.5336e-01, 3.2955e-01, 2.1864e-02],\n",
      "        [4.0549e-01, 2.2212e-01, 1.0892e-02],\n",
      "        [4.5602e-01, 2.0689e-01, 7.4619e-03],\n",
      "        [5.3737e-01, 2.1443e-01, 1.3885e-02],\n",
      "        [5.6669e-01, 1.8850e-01, 2.1697e-03],\n",
      "        [7.7149e-01, 2.4105e-02, 4.3795e-05],\n",
      "        [3.9874e-01, 3.6397e-01, 2.6716e-02],\n",
      "        [3.0959e-01, 4.0202e-01, 1.0934e-02],\n",
      "        [4.3703e-01, 2.7798e-01, 1.5663e-02],\n",
      "        [4.2462e-01, 2.8247e-01, 1.2128e-02],\n",
      "        [3.6364e-01, 3.2321e-01, 8.5658e-03],\n",
      "        [2.2279e-01, 1.2017e-01, 6.0968e-05],\n",
      "        [3.8832e-01, 2.8392e-01, 1.0295e-02],\n",
      "        [3.2597e-01, 3.8252e-01, 1.2277e-02],\n",
      "        [4.6413e-01, 2.5549e-01, 1.4355e-02],\n",
      "        [3.7306e-01, 2.8527e-01, 1.3847e-02],\n",
      "        [4.2198e-01, 2.4237e-01, 1.4972e-02],\n",
      "        [4.8647e-01, 1.9183e-01, 5.2563e-03],\n",
      "        [4.8600e-01, 2.4425e-01, 1.8319e-02],\n",
      "        [3.4899e-01, 3.5264e-01, 1.6754e-02],\n",
      "        [3.2489e-01, 2.2468e-01, 5.1293e-03],\n",
      "        [6.1629e-01, 7.3684e-02, 4.0440e-03],\n",
      "        [4.9832e-01, 1.1851e-01, 1.0553e-03],\n",
      "        [4.0430e-01, 2.1156e-01, 2.2256e-03],\n",
      "        [4.2341e-01, 3.4317e-01, 2.2154e-02],\n",
      "        [7.0156e-01, 5.8239e-02, 9.4576e-04],\n",
      "        [3.1933e-01, 2.9130e-01, 1.0796e-02],\n",
      "        [3.5435e-01, 3.0648e-01, 1.3343e-02],\n",
      "        [6.5367e-01, 1.1089e-01, 6.6085e-03],\n",
      "        [5.3873e-01, 1.6839e-01, 1.2950e-02],\n",
      "        [3.9842e-01, 2.7039e-01, 9.1984e-03],\n",
      "        [7.6086e-01, 5.2304e-02, 8.7044e-04],\n",
      "        [2.6462e-01, 4.0030e-01, 9.5539e-03],\n",
      "        [4.7161e-01, 2.1224e-01, 9.1422e-03],\n",
      "        [2.0385e-01, 2.9983e-01, 3.3033e-03],\n",
      "        [3.2754e-01, 2.7879e-01, 1.2747e-02],\n",
      "        [7.4092e-01, 5.7111e-02, 4.4408e-03],\n",
      "        [2.5392e-01, 2.8700e-01, 9.1135e-03],\n",
      "        [3.0982e-01, 3.7759e-01, 8.8944e-03],\n",
      "        [4.9786e-01, 2.5505e-01, 2.0311e-02],\n",
      "        [2.7603e-01, 3.5296e-01, 1.4179e-02],\n",
      "        [3.9758e-01, 1.5067e-01, 1.2829e-03],\n",
      "        [6.4044e-01, 3.4874e-02, 3.4365e-04],\n",
      "        [4.9249e-01, 2.3875e-01, 1.0118e-02],\n",
      "        [3.8961e-01, 8.8416e-02, 8.6325e-04],\n",
      "        [6.0040e-01, 1.2634e-01, 3.7046e-03],\n",
      "        [5.1136e-01, 5.1126e-02, 2.2779e-03],\n",
      "        [5.0193e-01, 1.2959e-01, 5.6906e-03],\n",
      "        [6.2226e-01, 1.0623e-01, 3.7619e-03],\n",
      "        [6.4091e-01, 1.2651e-01, 6.9514e-03],\n",
      "        [2.7646e-01, 2.6800e-01, 1.8665e-03],\n",
      "        [4.7391e-01, 2.7564e-01, 1.5159e-02],\n",
      "        [7.1923e-01, 7.1684e-02, 4.0671e-03],\n",
      "        [4.2389e-01, 2.8788e-01, 2.0013e-02],\n",
      "        [6.7735e-01, 2.5681e-02, 8.9146e-04],\n",
      "        [3.8588e-01, 1.8096e-01, 4.9686e-03],\n",
      "        [3.5256e-01, 3.4273e-01, 7.6928e-03],\n",
      "        [5.3732e-01, 1.0670e-01, 5.4490e-03],\n",
      "        [3.7624e-01, 3.0440e-01, 1.6718e-02],\n",
      "        [3.7087e-01, 3.6805e-01, 1.8845e-02],\n",
      "        [7.8245e-01, 2.9065e-02, 1.0794e-03],\n",
      "        [6.1400e-01, 8.7726e-02, 4.3298e-03],\n",
      "        [3.6814e-01, 3.1068e-01, 8.8644e-03],\n",
      "        [2.9363e-01, 3.9467e-01, 1.5230e-02],\n",
      "        [3.9463e-01, 3.3617e-01, 1.6883e-02],\n",
      "        [5.2082e-01, 1.1559e-01, 3.3139e-03],\n",
      "        [7.0416e-01, 6.2206e-02, 2.3252e-03],\n",
      "        [5.8099e-01, 1.5061e-01, 1.0917e-02],\n",
      "        [5.3898e-01, 1.5127e-01, 1.3131e-02],\n",
      "        [4.9912e-01, 3.2601e-01, 2.9491e-02],\n",
      "        [4.4679e-01, 3.0150e-01, 1.6778e-02],\n",
      "        [4.0683e-01, 2.7686e-01, 1.7350e-02],\n",
      "        [3.4392e-01, 4.0435e-01, 1.2466e-02],\n",
      "        [3.7666e-01, 3.0316e-01, 1.1686e-02],\n",
      "        [3.1077e-01, 1.8694e-01, 2.6674e-03],\n",
      "        [5.9897e-01, 4.7197e-02, 4.8881e-04],\n",
      "        [3.4743e-01, 3.2346e-01, 8.6423e-03],\n",
      "        [5.0496e-01, 2.6042e-01, 7.7725e-03],\n",
      "        [4.4074e-01, 2.1590e-01, 1.7368e-02],\n",
      "        [5.9222e-01, 1.3811e-01, 1.1900e-02],\n",
      "        [4.4995e-01, 3.1774e-01, 1.9122e-02],\n",
      "        [4.2297e-01, 2.9675e-01, 1.8030e-02],\n",
      "        [5.7849e-01, 1.7868e-01, 1.7465e-02],\n",
      "        [4.8223e-01, 2.6406e-01, 1.8425e-02],\n",
      "        [5.7756e-01, 1.0029e-01, 5.6889e-04],\n",
      "        [3.2230e-01, 2.7089e-01, 1.6288e-02],\n",
      "        [5.2397e-01, 1.9869e-01, 2.0598e-02],\n",
      "        [3.9119e-01, 3.3090e-01, 1.8597e-02],\n",
      "        [4.0518e-01, 2.4671e-01, 1.7038e-02],\n",
      "        [1.6875e-01, 2.6158e-01, 5.6124e-05],\n",
      "        [5.4527e-01, 1.2252e-01, 2.9354e-03],\n",
      "        [4.2868e-01, 2.3397e-01, 1.4940e-02],\n",
      "        [6.0493e-01, 1.3112e-01, 1.1899e-02],\n",
      "        [5.1253e-01, 2.9244e-01, 9.3964e-03]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>), tensor([[8.6240e-03, 8.9875e-01, 4.8975e-03, 4.3326e-03],\n",
      "        [5.4732e-05, 9.9262e-01, 3.4829e-05, 3.4740e-05],\n",
      "        [5.5125e-03, 8.5675e-01, 2.7228e-03, 2.6446e-03],\n",
      "        [3.6224e-02, 7.7401e-01, 2.0396e-02, 2.0023e-02],\n",
      "        [4.8006e-02, 7.3588e-01, 3.4990e-02, 3.4172e-02],\n",
      "        [3.0187e-02, 8.4213e-01, 1.8781e-02, 1.7847e-02],\n",
      "        [6.8084e-03, 9.1927e-01, 4.2899e-03, 4.3157e-03],\n",
      "        [3.6632e-02, 7.9382e-01, 1.5513e-02, 1.4534e-02],\n",
      "        [2.7250e-04, 9.6331e-01, 1.3569e-04, 1.3513e-04],\n",
      "        [2.6465e-02, 8.0301e-01, 1.6031e-02, 1.5956e-02],\n",
      "        [1.2732e-02, 9.0215e-01, 4.0548e-03, 4.0973e-03],\n",
      "        [3.3326e-03, 9.6517e-01, 2.3316e-03, 2.3764e-03],\n",
      "        [4.2033e-02, 7.5827e-01, 2.6792e-02, 2.4835e-02],\n",
      "        [1.5760e-02, 8.3410e-01, 1.0034e-02, 9.7756e-03],\n",
      "        [4.5789e-02, 7.6854e-01, 2.6972e-02, 2.5190e-02],\n",
      "        [3.3880e-02, 7.4855e-01, 1.6921e-02, 1.6890e-02],\n",
      "        [6.3440e-04, 9.8509e-01, 2.1761e-04, 2.1277e-04],\n",
      "        [3.7767e-02, 7.3559e-01, 2.2718e-02, 2.1699e-02],\n",
      "        [3.3590e-02, 7.9710e-01, 1.9668e-02, 1.8144e-02],\n",
      "        [1.7347e-03, 9.6498e-01, 1.2133e-03, 1.1809e-03],\n",
      "        [1.8041e-03, 9.8272e-01, 1.1524e-03, 1.1178e-03],\n",
      "        [4.2598e-02, 7.7812e-01, 2.6039e-02, 2.2576e-02],\n",
      "        [4.6393e-02, 7.9231e-01, 2.6935e-02, 2.6590e-02],\n",
      "        [5.0442e-03, 9.0832e-01, 3.6713e-03, 3.6607e-03],\n",
      "        [3.0687e-05, 9.0157e-01, 2.3683e-05, 2.3614e-05],\n",
      "        [1.6535e-02, 8.5970e-01, 1.0012e-02, 8.9795e-03],\n",
      "        [6.5487e-05, 9.8183e-01, 3.6620e-05, 3.6436e-05],\n",
      "        [2.9774e-03, 9.2072e-01, 1.6682e-03, 1.6732e-03],\n",
      "        [2.5161e-02, 8.4275e-01, 1.2985e-02, 1.2914e-02],\n",
      "        [1.9195e-02, 8.1780e-01, 1.4186e-02, 1.3723e-02],\n",
      "        [3.2896e-02, 7.3897e-01, 2.0854e-02, 1.9314e-02],\n",
      "        [1.0881e-02, 7.7757e-01, 6.8411e-03, 6.4516e-03],\n",
      "        [2.9274e-02, 7.4944e-01, 1.7439e-02, 1.6189e-02],\n",
      "        [3.1028e-02, 7.7212e-01, 1.8799e-02, 1.7740e-02],\n",
      "        [1.7453e-02, 8.3560e-01, 9.8126e-03, 9.2985e-03],\n",
      "        [8.0569e-03, 8.9654e-01, 5.4225e-03, 5.2010e-03],\n",
      "        [1.8086e-02, 8.6332e-01, 1.0528e-02, 1.0308e-02],\n",
      "        [3.3877e-02, 8.0602e-01, 2.2392e-02, 1.9742e-02],\n",
      "        [6.3298e-03, 9.2265e-01, 3.5770e-03, 3.4993e-03],\n",
      "        [2.5362e-02, 8.1634e-01, 1.1865e-02, 1.2031e-02],\n",
      "        [5.8005e-02, 7.2361e-01, 4.1863e-02, 4.1729e-02],\n",
      "        [1.5359e-04, 9.9374e-01, 7.7570e-05, 7.6852e-05],\n",
      "        [2.6073e-02, 8.8234e-01, 1.7601e-02, 1.6857e-02],\n",
      "        [3.6784e-04, 9.7545e-01, 2.3521e-04, 2.3058e-04],\n",
      "        [3.8433e-02, 7.9079e-01, 2.1069e-02, 2.0591e-02],\n",
      "        [4.8766e-03, 9.4016e-01, 2.9607e-03, 2.7630e-03],\n",
      "        [2.0746e-02, 8.5735e-01, 1.2741e-02, 1.2069e-02],\n",
      "        [1.0466e-02, 9.1465e-01, 6.4237e-03, 6.6430e-03],\n",
      "        [5.4801e-02, 7.5711e-01, 3.2799e-02, 2.9831e-02],\n",
      "        [3.0158e-02, 8.3282e-01, 1.5872e-02, 1.4851e-02],\n",
      "        [1.1347e-03, 9.6303e-01, 5.6825e-04, 5.6213e-04],\n",
      "        [7.1859e-02, 6.8382e-01, 4.8541e-02, 4.8298e-02],\n",
      "        [1.9795e-03, 9.5940e-01, 1.3235e-03, 1.2859e-03],\n",
      "        [3.8645e-02, 7.6861e-01, 2.5953e-02, 2.5534e-02],\n",
      "        [3.7219e-02, 8.2984e-01, 1.8986e-02, 1.7409e-02],\n",
      "        [1.3617e-02, 8.8336e-01, 8.8199e-03, 8.3222e-03],\n",
      "        [4.2646e-02, 7.9485e-01, 2.7030e-02, 2.5879e-02],\n",
      "        [6.6544e-03, 9.5805e-01, 3.1059e-03, 3.0992e-03],\n",
      "        [4.9470e-05, 9.9612e-01, 3.8098e-05, 3.7948e-05],\n",
      "        [6.6115e-02, 7.4861e-01, 3.9524e-02, 3.5456e-02],\n",
      "        [3.1438e-02, 8.0537e-01, 1.5122e-02, 1.3613e-02],\n",
      "        [2.3359e-02, 8.4094e-01, 1.4697e-02, 1.3815e-02],\n",
      "        [3.4644e-02, 8.1068e-01, 2.2913e-02, 2.1494e-02],\n",
      "        [2.2040e-02, 8.8757e-01, 1.1860e-02, 1.1625e-02],\n",
      "        [5.1634e-05, 9.9496e-01, 3.6179e-05, 3.6000e-05],\n",
      "        [2.8576e-02, 8.0867e-01, 1.7634e-02, 1.7219e-02],\n",
      "        [2.3260e-02, 8.2272e-01, 1.1755e-02, 1.1481e-02],\n",
      "        [2.3734e-02, 7.9221e-01, 1.6979e-02, 1.5682e-02],\n",
      "        [3.8015e-02, 7.7876e-01, 2.1555e-02, 1.9339e-02],\n",
      "        [2.0252e-02, 8.5614e-01, 1.4064e-02, 1.2787e-02],\n",
      "        [1.6424e-02, 8.7570e-01, 9.8068e-03, 9.0402e-03],\n",
      "        [5.2405e-02, 7.1612e-01, 3.4985e-02, 3.5220e-02],\n",
      "        [4.7183e-02, 7.9066e-01, 2.1472e-02, 2.0919e-02],\n",
      "        [1.9116e-02, 8.6086e-01, 1.1676e-02, 1.1792e-02],\n",
      "        [1.2413e-02, 8.7382e-01, 7.7726e-03, 7.8027e-03],\n",
      "        [4.1251e-03, 9.4130e-01, 2.3452e-03, 2.2598e-03],\n",
      "        [3.2513e-03, 9.2706e-01, 2.0573e-03, 2.0150e-03],\n",
      "        [4.1714e-02, 7.6599e-01, 2.5409e-02, 2.4037e-02],\n",
      "        [4.1029e-03, 9.6219e-01, 2.0890e-03, 2.1039e-03],\n",
      "        [2.5617e-02, 8.1161e-01, 1.5511e-02, 1.5348e-02],\n",
      "        [4.3385e-02, 7.7093e-01, 2.4227e-02, 2.3818e-02],\n",
      "        [3.0143e-02, 8.6339e-01, 1.1106e-02, 1.1134e-02],\n",
      "        [4.1810e-02, 8.2655e-01, 2.3392e-02, 2.0181e-02],\n",
      "        [1.5800e-02, 8.7422e-01, 1.0599e-02, 1.0426e-02],\n",
      "        [3.0581e-03, 9.5267e-01, 1.8198e-03, 1.7799e-03],\n",
      "        [2.2409e-02, 7.7756e-01, 1.3828e-02, 1.2551e-02],\n",
      "        [2.4573e-02, 8.4201e-01, 1.8257e-02, 1.7819e-02],\n",
      "        [6.9540e-03, 9.1688e-01, 3.4602e-03, 3.3955e-03],\n",
      "        [3.1997e-02, 7.8225e-01, 1.7851e-02, 1.7598e-02],\n",
      "        [8.1848e-03, 9.1216e-01, 5.7472e-03, 5.5855e-03],\n",
      "        [1.3163e-02, 8.5641e-01, 7.8165e-03, 7.6998e-03],\n",
      "        [2.1803e-02, 8.5996e-01, 1.2980e-02, 1.1481e-02],\n",
      "        [4.4769e-02, 7.7795e-01, 3.0968e-02, 2.8781e-02],\n",
      "        [2.7276e-02, 7.9071e-01, 1.8121e-02, 1.7364e-02],\n",
      "        [4.1484e-03, 9.1460e-01, 1.5075e-03, 1.4822e-03],\n",
      "        [1.6017e-03, 9.6905e-01, 9.5689e-04, 9.2897e-04],\n",
      "        [1.9957e-02, 8.0994e-01, 1.2801e-02, 1.1255e-02],\n",
      "        [2.2238e-03, 9.5970e-01, 1.0939e-03, 1.0606e-03],\n",
      "        [1.7493e-02, 8.1346e-01, 9.7730e-03, 9.4193e-03],\n",
      "        [6.0494e-03, 9.3792e-01, 2.8653e-03, 2.8152e-03],\n",
      "        [1.3244e-02, 8.7519e-01, 9.6129e-03, 8.5782e-03],\n",
      "        [8.3365e-03, 9.0132e-01, 5.5653e-03, 5.4464e-03],\n",
      "        [2.3997e-02, 8.1745e-01, 1.4274e-02, 1.4217e-02],\n",
      "        [8.5786e-03, 8.9637e-01, 3.5914e-03, 3.4190e-03],\n",
      "        [3.7054e-02, 8.2812e-01, 2.3292e-02, 2.1734e-02],\n",
      "        [1.3684e-02, 8.9728e-01, 7.0081e-03, 6.6403e-03],\n",
      "        [5.2579e-02, 7.3167e-01, 2.6290e-02, 2.4509e-02],\n",
      "        [8.9222e-03, 8.9943e-01, 3.8494e-03, 3.8013e-03],\n",
      "        [2.6041e-02, 8.3183e-01, 1.1792e-02, 1.1271e-02],\n",
      "        [1.7775e-02, 8.6627e-01, 1.2795e-02, 1.1992e-02],\n",
      "        [1.9997e-02, 8.5418e-01, 1.0356e-02, 1.0262e-02],\n",
      "        [3.8935e-02, 7.8106e-01, 2.6811e-02, 2.4805e-02],\n",
      "        [3.8568e-02, 7.6768e-01, 2.0776e-02, 1.9590e-02],\n",
      "        [4.7831e-03, 9.3259e-01, 2.9989e-03, 2.9537e-03],\n",
      "        [1.0129e-02, 8.5842e-01, 6.8554e-03, 6.5077e-03],\n",
      "        [1.7653e-02, 8.4601e-01, 1.0476e-02, 1.0503e-02],\n",
      "        [4.1773e-02, 7.5355e-01, 2.6679e-02, 2.6011e-02],\n",
      "        [3.8555e-02, 7.7692e-01, 2.0293e-02, 2.0432e-02],\n",
      "        [9.2997e-03, 9.1719e-01, 4.4811e-03, 4.3909e-03],\n",
      "        [8.3358e-03, 9.3947e-01, 5.5433e-03, 5.5566e-03],\n",
      "        [2.1200e-02, 8.9026e-01, 1.2301e-02, 1.2114e-02],\n",
      "        [3.7395e-02, 7.7995e-01, 2.4110e-02, 2.2564e-02],\n",
      "        [5.2145e-02, 7.1117e-01, 3.6456e-02, 3.3833e-02],\n",
      "        [4.8423e-02, 7.8201e-01, 2.8528e-02, 2.5386e-02],\n",
      "        [3.1465e-02, 7.3639e-01, 2.4712e-02, 2.1871e-02],\n",
      "        [2.7016e-02, 8.0869e-01, 1.7418e-02, 1.6325e-02],\n",
      "        [2.0143e-02, 8.7616e-01, 9.8202e-03, 9.7051e-03],\n",
      "        [4.4114e-03, 8.9670e-01, 2.6707e-03, 2.6050e-03],\n",
      "        [1.9933e-03, 9.3703e-01, 9.8160e-04, 9.8702e-04],\n",
      "        [2.4372e-02, 8.2489e-01, 1.2038e-02, 1.2328e-02],\n",
      "        [3.0035e-02, 8.2576e-01, 1.3783e-02, 1.3367e-02],\n",
      "        [2.8613e-02, 7.3258e-01, 1.8364e-02, 1.7737e-02],\n",
      "        [2.3244e-02, 8.3101e-01, 1.6172e-02, 1.6317e-02],\n",
      "        [4.7613e-02, 7.4775e-01, 2.3153e-02, 2.3002e-02],\n",
      "        [3.7910e-02, 7.5320e-01, 2.3481e-02, 2.0858e-02],\n",
      "        [4.8459e-02, 7.6572e-01, 2.9258e-02, 2.8947e-02],\n",
      "        [4.8379e-02, 6.7365e-01, 2.9368e-02, 2.6768e-02],\n",
      "        [1.2193e-03, 9.6509e-01, 7.5441e-04, 7.5246e-04],\n",
      "        [3.1395e-02, 8.1528e-01, 1.8143e-02, 1.7084e-02],\n",
      "        [5.6891e-02, 7.5086e-01, 3.6660e-02, 3.4326e-02],\n",
      "        [2.9344e-02, 7.9877e-01, 1.8859e-02, 1.8295e-02],\n",
      "        [4.2024e-02, 7.3120e-01, 2.7430e-02, 2.7348e-02],\n",
      "        [2.1981e-05, 9.6307e-01, 1.4605e-05, 1.4578e-05],\n",
      "        [4.3266e-03, 9.2039e-01, 3.0249e-03, 2.8915e-03],\n",
      "        [2.6188e-02, 8.0482e-01, 1.7629e-02, 1.6800e-02],\n",
      "        [2.8505e-02, 7.7081e-01, 1.2605e-02, 1.3181e-02],\n",
      "        [1.9398e-02, 8.7786e-01, 1.2165e-02, 1.1519e-02]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>), tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [5.6533e-04, 1.4639e-04, 5.6492e-04, 3.7307e-04, 2.1693e-04],\n",
      "        [1.4965e-02, 2.6796e-03, 1.4056e-02, 1.1220e-02, 1.0385e-02],\n",
      "        [5.8741e-02, 1.7106e-02, 4.7484e-02, 5.2601e-02, 6.5284e-02],\n",
      "        [6.8980e-02, 3.2721e-02, 5.9753e-02, 6.4278e-02, 8.0340e-02],\n",
      "        [4.2477e-02, 1.3060e-02, 4.6887e-02, 3.5260e-02, 3.6806e-02],\n",
      "        [1.3699e-02, 4.1368e-03, 1.1751e-02, 8.8741e-03, 2.2425e-02],\n",
      "        [4.3572e-02, 1.3725e-02, 3.1509e-02, 2.5516e-02, 2.2915e-02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [1.5185e-02, 6.6068e-03, 1.5600e-02, 1.2083e-02, 1.3648e-02],\n",
      "        [7.7330e-03, 2.2034e-03, 6.7452e-03, 4.3075e-03, 5.9571e-03],\n",
      "        [5.9047e-02, 2.4797e-02, 5.8807e-02, 6.8927e-02, 6.9711e-02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [5.5236e-02, 2.7407e-02, 6.0776e-02, 5.0412e-02, 8.0620e-02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [2.2127e-03, 3.6255e-04, 1.6565e-03, 3.3170e-03, 2.6407e-03],\n",
      "        [6.8278e-02, 3.0260e-02, 7.3282e-02, 6.2659e-02, 5.8446e-02],\n",
      "        [4.9611e-02, 1.8244e-02, 5.9314e-02, 3.6714e-02, 3.3224e-02],\n",
      "        [7.1114e-03, 9.5487e-04, 5.3335e-03, 3.8625e-03, 4.6494e-03],\n",
      "        [7.4386e-03, 8.8662e-04, 4.8376e-03, 3.1531e-03, 3.9377e-03],\n",
      "        [4.4090e-02, 2.3249e-02, 4.9192e-02, 6.3945e-02, 4.6506e-02],\n",
      "        [6.7394e-02, 2.4542e-02, 5.0311e-02, 6.6548e-02, 5.7926e-02],\n",
      "        [1.4365e-02, 2.7287e-03, 1.2292e-02, 1.3965e-02, 1.5650e-02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [2.0860e-02, 8.1799e-03, 2.3740e-02, 2.6480e-02, 1.8538e-02],\n",
      "        [6.6544e-04, 1.0946e-04, 6.2304e-04, 3.8889e-04, 1.1673e-03],\n",
      "        [1.2059e-02, 2.3590e-03, 1.3563e-02, 6.6036e-03, 1.0580e-02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [5.3193e-02, 1.8987e-02, 6.4675e-02, 5.4490e-02, 5.1718e-02],\n",
      "        [2.7025e-02, 6.2393e-03, 2.5980e-02, 1.3715e-02, 2.5676e-02],\n",
      "        [5.5555e-02, 1.7783e-02, 5.9368e-02, 3.9554e-02, 4.8556e-02],\n",
      "        [4.4720e-02, 1.5120e-02, 3.4278e-02, 4.9266e-02, 4.8919e-02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [2.2734e-02, 5.3509e-03, 2.9380e-02, 1.9621e-02, 1.8760e-02],\n",
      "        [1.8836e-02, 1.0357e-02, 2.5949e-02, 2.1480e-02, 2.1951e-02],\n",
      "        [5.5678e-02, 1.8257e-02, 4.7875e-02, 3.7937e-02, 5.1136e-02],\n",
      "        [1.0253e-02, 5.7120e-03, 1.6560e-02, 7.8264e-03, 1.0320e-02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [1.0375e-01, 3.8732e-02, 9.1447e-02, 6.4181e-02, 8.6906e-02],\n",
      "        [8.7209e-04, 1.0462e-04, 1.2317e-03, 4.9816e-04, 8.6895e-04],\n",
      "        [4.3257e-02, 1.3541e-02, 3.2533e-02, 2.3270e-02, 3.3482e-02],\n",
      "        [1.7274e-03, 2.2679e-04, 1.6039e-03, 5.4491e-04, 1.2412e-03],\n",
      "        [5.0379e-02, 1.7331e-02, 5.4276e-02, 5.2440e-02, 4.4437e-02],\n",
      "        [1.0494e-02, 2.2635e-03, 1.7187e-02, 5.2158e-03, 6.9791e-03],\n",
      "        [3.9388e-02, 8.4746e-03, 3.1240e-02, 2.7458e-02, 3.0270e-02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [8.0245e-02, 2.6696e-02, 7.9381e-02, 7.0258e-02, 6.8509e-02],\n",
      "        [5.1163e-02, 2.0944e-02, 5.0804e-02, 3.4328e-02, 4.5659e-02],\n",
      "        [3.4789e-03, 6.5935e-04, 5.4251e-03, 3.5500e-03, 7.3870e-03],\n",
      "        [8.8363e-02, 4.6666e-02, 7.9314e-02, 8.6373e-02, 8.0083e-02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [8.5569e-02, 2.6055e-02, 7.0533e-02, 5.7926e-02, 5.0702e-02],\n",
      "        [4.8575e-02, 1.8191e-02, 4.5537e-02, 4.7887e-02, 4.7378e-02],\n",
      "        [2.5896e-02, 9.6704e-03, 3.4692e-02, 2.0452e-02, 2.8868e-02],\n",
      "        [6.9648e-02, 2.6395e-02, 5.1667e-02, 4.8921e-02, 7.1433e-02],\n",
      "        [1.0952e-02, 3.6912e-03, 9.7669e-03, 1.0301e-02, 1.1303e-02],\n",
      "        [6.2998e-04, 6.6308e-05, 4.0602e-04, 3.7246e-04, 7.4993e-04],\n",
      "        [7.0973e-02, 3.0851e-02, 7.4743e-02, 7.0561e-02, 6.2706e-02],\n",
      "        [5.9796e-02, 1.6465e-02, 5.5490e-02, 2.8348e-02, 3.7457e-02],\n",
      "        [5.5777e-02, 1.8711e-02, 3.1783e-02, 2.5821e-02, 3.9157e-02],\n",
      "        [4.7476e-02, 1.8645e-02, 4.7469e-02, 3.8761e-02, 3.6639e-02],\n",
      "        [3.9185e-02, 1.0159e-02, 3.1619e-02, 2.9258e-02, 2.4110e-02],\n",
      "        [5.3331e-04, 3.7329e-05, 6.7661e-04, 2.6421e-04, 7.2592e-04],\n",
      "        [4.1555e-02, 1.5325e-02, 3.7032e-02, 3.5525e-02, 3.3636e-02],\n",
      "        [4.7690e-02, 1.3308e-02, 4.2552e-02, 3.6983e-02, 3.5438e-02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [6.4062e-02, 1.7323e-02, 5.1548e-02, 3.5508e-02, 5.0962e-02],\n",
      "        [4.9725e-02, 1.8861e-02, 4.0584e-02, 3.1684e-02, 5.2531e-02],\n",
      "        [2.6596e-02, 1.1733e-02, 2.9354e-02, 2.6975e-02, 2.3282e-02],\n",
      "        [7.3999e-02, 3.6052e-02, 7.4500e-02, 5.0641e-02, 6.2253e-02],\n",
      "        [6.6386e-02, 1.4708e-02, 4.6826e-02, 4.1207e-02, 4.4648e-02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [3.0948e-02, 7.0757e-03, 2.1784e-02, 1.7238e-02, 2.6575e-02],\n",
      "        [1.8752e-02, 2.8693e-03, 8.9530e-03, 6.3690e-03, 8.4606e-03],\n",
      "        [1.1590e-02, 4.6089e-03, 9.3201e-03, 7.4960e-03, 8.5435e-03],\n",
      "        [4.0480e-02, 3.3462e-02, 7.5583e-02, 6.8250e-02, 8.1396e-02],\n",
      "        [6.8776e-03, 2.0715e-03, 6.5052e-03, 5.7626e-03, 4.8306e-03],\n",
      "        [3.2194e-02, 1.1371e-02, 3.3927e-02, 2.6782e-02, 4.3159e-02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [3.9766e-02, 1.2878e-02, 4.0505e-02, 4.5419e-02, 3.1025e-02],\n",
      "        [5.3406e-02, 2.2704e-02, 4.9816e-02, 3.9628e-02, 5.9612e-02],\n",
      "        [4.6782e-02, 1.1399e-02, 2.7094e-02, 2.4949e-02, 2.2410e-02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [3.6947e-02, 1.2791e-02, 4.3034e-02, 2.9929e-02, 3.5368e-02],\n",
      "        [4.5938e-02, 1.2303e-02, 3.7000e-02, 3.1039e-02, 3.0750e-02],\n",
      "        [1.1036e-02, 5.1022e-03, 1.4732e-02, 1.5440e-02, 1.3223e-02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [1.9295e-02, 8.9117e-03, 2.6175e-02, 2.1904e-02, 2.0855e-02],\n",
      "        [4.0670e-02, 1.1852e-02, 3.4033e-02, 2.2867e-02, 2.2631e-02],\n",
      "        [3.7660e-02, 1.1814e-02, 3.7318e-02, 2.5511e-02, 2.9901e-02],\n",
      "        [5.6952e-02, 2.8564e-02, 6.0422e-02, 7.1362e-02, 6.5452e-02],\n",
      "        [4.3909e-02, 1.9994e-02, 5.2523e-02, 3.9337e-02, 2.9123e-02],\n",
      "        [8.7708e-03, 1.1166e-03, 5.2574e-03, 6.2470e-03, 9.0021e-03],\n",
      "        [5.1520e-03, 8.6631e-04, 4.0496e-03, 1.9463e-03, 3.8325e-03],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [5.3639e-03, 1.6840e-03, 1.0698e-02, 2.7520e-03, 2.5050e-03],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [1.2226e-02, 5.9919e-03, 1.1859e-02, 1.2851e-02, 1.4967e-02],\n",
      "        [3.0853e-02, 7.1100e-03, 2.5233e-02, 1.6286e-02, 2.9759e-02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [3.0529e-02, 1.4607e-02, 3.1924e-02, 1.6408e-02, 2.8833e-02],\n",
      "        [7.9753e-03, 3.9681e-03, 7.5737e-03, 5.5481e-03, 4.4129e-03],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [3.6037e-02, 1.0504e-02, 2.7287e-02, 3.3578e-02, 2.7142e-02],\n",
      "        [2.4976e-02, 1.0256e-02, 2.6508e-02, 2.2944e-02, 3.3851e-02],\n",
      "        [6.6426e-02, 1.8467e-02, 5.1142e-02, 5.9040e-02, 5.0692e-02],\n",
      "        [5.4192e-02, 2.5224e-02, 6.6667e-02, 4.4139e-02, 3.5640e-02],\n",
      "        [8.3685e-03, 2.3021e-03, 1.3446e-02, 8.5843e-03, 1.2453e-02],\n",
      "        [3.4684e-02, 1.1419e-02, 2.8752e-02, 2.1997e-02, 2.3330e-02],\n",
      "        [4.2120e-02, 1.5516e-02, 3.4406e-02, 3.1118e-02, 3.5465e-02],\n",
      "        [6.2996e-02, 2.2165e-02, 5.2777e-02, 3.8929e-02, 4.3622e-02],\n",
      "        [6.7862e-02, 1.9263e-02, 4.9816e-02, 7.2434e-02, 6.1352e-02],\n",
      "        [1.9966e-02, 6.6478e-03, 1.3888e-02, 9.7869e-03, 1.3516e-02],\n",
      "        [1.8002e-02, 5.1782e-03, 1.2506e-02, 7.1220e-03, 1.1471e-02],\n",
      "        [5.0685e-02, 1.8042e-02, 3.4573e-02, 3.3740e-02, 2.8490e-02],\n",
      "        [6.4295e-02, 2.1748e-02, 4.7988e-02, 4.8786e-02, 4.4399e-02],\n",
      "        [7.3762e-02, 3.7133e-02, 9.3423e-02, 8.9619e-02, 7.9958e-02],\n",
      "        [5.6755e-02, 2.5592e-02, 7.0333e-02, 7.2442e-02, 5.7427e-02],\n",
      "        [7.2858e-02, 2.8088e-02, 7.1901e-02, 4.4885e-02, 6.2828e-02],\n",
      "        [5.6038e-02, 2.1295e-02, 5.8462e-02, 3.6488e-02, 3.3646e-02],\n",
      "        [2.8327e-02, 1.0600e-02, 1.9576e-02, 3.0312e-02, 3.2950e-02],\n",
      "        [7.8427e-03, 1.9718e-03, 1.2528e-02, 9.7288e-03, 1.1670e-02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [3.9254e-02, 1.1020e-02, 4.1556e-02, 2.9665e-02, 2.2872e-02],\n",
      "        [4.9566e-02, 2.5292e-02, 6.4364e-02, 4.0781e-02, 4.7341e-02],\n",
      "        [5.5460e-02, 2.1142e-02, 4.1846e-02, 4.3450e-02, 5.0683e-02],\n",
      "        [6.2468e-02, 3.1803e-02, 7.1936e-02, 4.2482e-02, 5.3486e-02],\n",
      "        [6.4222e-02, 2.5166e-02, 7.8735e-02, 5.9927e-02, 5.4606e-02],\n",
      "        [6.0142e-02, 2.7986e-02, 5.9045e-02, 3.4290e-02, 4.8008e-02],\n",
      "        [6.4347e-02, 3.1129e-02, 6.7283e-02, 7.1770e-02, 6.6615e-02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [6.5541e-02, 1.8229e-02, 4.2531e-02, 3.6778e-02, 6.1060e-02],\n",
      "        [6.0168e-02, 4.2673e-02, 5.9996e-02, 8.5009e-02, 6.5056e-02],\n",
      "        [6.2253e-02, 1.7711e-02, 4.4177e-02, 4.1456e-02, 3.9161e-02],\n",
      "        [6.0271e-02, 2.5070e-02, 6.7149e-02, 5.5329e-02, 6.6598e-02],\n",
      "        [1.8962e-04, 1.5646e-05, 2.1667e-04, 2.3502e-04, 2.7103e-04],\n",
      "        [1.1467e-02, 4.5259e-03, 1.5315e-02, 7.4506e-03, 1.7958e-02],\n",
      "        [5.0795e-02, 2.6238e-02, 4.0394e-02, 4.3465e-02, 5.0281e-02],\n",
      "        [4.0613e-02, 1.5523e-02, 2.6646e-02, 2.6581e-02, 4.4871e-02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)], '95%': [tensor([[9.9996e-01, 1.4846e-04, 1.4592e-04],\n",
      "        [1.0000e+00, 1.8455e-06, 1.8358e-06],\n",
      "        [9.9999e-01, 7.4701e-04, 7.1129e-04],\n",
      "        [9.9910e-01, 2.5657e-03, 2.5286e-03],\n",
      "        [9.9814e-01, 3.8373e-03, 3.8175e-03],\n",
      "        [9.9965e-01, 1.1140e-03, 1.0740e-03],\n",
      "        [9.9998e-01, 1.4774e-04, 1.4643e-04],\n",
      "        [9.9978e-01, 6.6891e-04, 6.6620e-04],\n",
      "        [1.0000e+00, 2.1362e-05, 2.1189e-05],\n",
      "        [9.9938e-01, 1.6450e-03, 1.5918e-03],\n",
      "        [9.9998e-01, 4.4218e-04, 4.3949e-04],\n",
      "        [1.0000e+00, 2.9271e-05, 2.9203e-05],\n",
      "        [9.9851e-01, 2.3410e-03, 2.2782e-03],\n",
      "        [9.9987e-01, 5.9981e-04, 5.9078e-04],\n",
      "        [9.9835e-01, 3.4328e-03, 3.3332e-03],\n",
      "        [9.9958e-01, 1.6819e-03, 1.6279e-03],\n",
      "        [1.0000e+00, 2.4089e-06, 2.4078e-06],\n",
      "        [9.9842e-01, 2.7213e-03, 2.6531e-03],\n",
      "        [9.9952e-01, 1.8595e-03, 1.8202e-03],\n",
      "        [1.0000e+00, 1.1095e-05, 1.0973e-05],\n",
      "        [1.0000e+00, 6.5680e-06, 6.5452e-06],\n",
      "        [9.9919e-01, 1.8079e-03, 1.7612e-03],\n",
      "        [9.9859e-01, 3.9388e-03, 3.6585e-03],\n",
      "        [9.9998e-01, 1.7552e-04, 1.7435e-04],\n",
      "        [1.0000e+00, 1.8877e-04, 1.8083e-04],\n",
      "        [9.9994e-01, 2.1101e-04, 2.0935e-04],\n",
      "        [1.0000e+00, 3.4129e-05, 3.3763e-05],\n",
      "        [9.9998e-01, 9.2041e-05, 9.1300e-05],\n",
      "        [9.9971e-01, 4.9885e-04, 4.9019e-04],\n",
      "        [9.9925e-01, 1.5848e-03, 1.5503e-03],\n",
      "        [9.9915e-01, 1.5864e-03, 1.5367e-03],\n",
      "        [9.9993e-01, 2.2333e-03, 2.1403e-03],\n",
      "        [9.9912e-01, 2.0665e-03, 2.0026e-03],\n",
      "        [9.9928e-01, 2.4924e-03, 2.4205e-03],\n",
      "        [9.9991e-01, 3.0965e-04, 3.0576e-04],\n",
      "        [9.9991e-01, 4.6456e-04, 4.6060e-04],\n",
      "        [9.9989e-01, 3.4915e-04, 3.4062e-04],\n",
      "        [9.9911e-01, 1.6238e-03, 1.5814e-03],\n",
      "        [9.9998e-01, 1.3119e-04, 1.3070e-04],\n",
      "        [9.9964e-01, 8.4935e-04, 8.0812e-04],\n",
      "        [9.9610e-01, 8.9071e-03, 8.4464e-03],\n",
      "        [1.0000e+00, 1.1093e-06, 1.1068e-06],\n",
      "        [9.9952e-01, 9.0577e-04, 8.7004e-04],\n",
      "        [1.0000e+00, 1.4386e-06, 1.4278e-06],\n",
      "        [9.9922e-01, 1.3950e-03, 1.3601e-03],\n",
      "        [9.9999e-01, 2.5286e-04, 2.4823e-04],\n",
      "        [9.9974e-01, 8.0387e-04, 7.9023e-04],\n",
      "        [9.9996e-01, 2.5535e-04, 2.5014e-04],\n",
      "        [9.9798e-01, 5.7582e-03, 5.3798e-03],\n",
      "        [9.9947e-01, 1.2491e-03, 1.1905e-03],\n",
      "        [1.0000e+00, 5.6226e-06, 5.6026e-06],\n",
      "        [9.9559e-01, 9.2217e-03, 9.1154e-03],\n",
      "        [9.9999e-01, 3.1106e-05, 3.0692e-05],\n",
      "        [9.9798e-01, 3.4118e-03, 3.2244e-03],\n",
      "        [9.9970e-01, 1.1819e-03, 1.1559e-03],\n",
      "        [9.9988e-01, 5.2609e-04, 5.0773e-04],\n",
      "        [9.9854e-01, 2.7764e-03, 2.5432e-03],\n",
      "        [9.9999e-01, 9.6433e-05, 9.6030e-05],\n",
      "        [1.0000e+00, 2.1347e-07, 2.1335e-07],\n",
      "        [9.9699e-01, 4.4987e-03, 4.4094e-03],\n",
      "        [9.9950e-01, 2.1220e-03, 2.0754e-03],\n",
      "        [9.9956e-01, 1.4476e-03, 1.4247e-03],\n",
      "        [9.9920e-01, 2.6926e-03, 2.5721e-03],\n",
      "        [9.9984e-01, 7.6483e-04, 7.5341e-04],\n",
      "        [1.0000e+00, 3.3219e-08, 3.3214e-08],\n",
      "        [9.9963e-01, 1.4872e-03, 1.4085e-03],\n",
      "        [9.9967e-01, 1.4015e-03, 1.3631e-03],\n",
      "        [9.9959e-01, 1.2441e-03, 1.2295e-03],\n",
      "        [9.9941e-01, 2.0561e-03, 1.9411e-03],\n",
      "        [9.9929e-01, 1.0007e-03, 9.9928e-04],\n",
      "        [9.9992e-01, 4.5135e-04, 4.4772e-04],\n",
      "        [9.9780e-01, 4.6257e-03, 4.4532e-03],\n",
      "        [9.9956e-01, 1.4080e-03, 1.3494e-03],\n",
      "        [9.9983e-01, 3.4685e-04, 3.4251e-04],\n",
      "        [9.9990e-01, 6.0578e-04, 5.8960e-04],\n",
      "        [9.9999e-01, 4.8446e-05, 4.8105e-05],\n",
      "        [9.9997e-01, 1.0698e-04, 1.0554e-04],\n",
      "        [9.9835e-01, 2.8415e-03, 2.7137e-03],\n",
      "        [1.0000e+00, 1.9514e-05, 1.9106e-05],\n",
      "        [9.9969e-01, 6.6979e-04, 6.5565e-04],\n",
      "        [9.9962e-01, 1.1262e-03, 1.1101e-03],\n",
      "        [9.9974e-01, 5.1151e-04, 4.9928e-04],\n",
      "        [9.9875e-01, 2.3179e-03, 2.1927e-03],\n",
      "        [9.9989e-01, 7.3007e-04, 7.1437e-04],\n",
      "        [1.0000e+00, 5.9295e-05, 5.8829e-05],\n",
      "        [9.9981e-01, 8.0059e-04, 7.7457e-04],\n",
      "        [9.9960e-01, 9.8229e-04, 9.7385e-04],\n",
      "        [9.9997e-01, 1.1058e-04, 1.0968e-04],\n",
      "        [9.9958e-01, 6.2925e-04, 6.2407e-04],\n",
      "        [9.9993e-01, 5.4395e-04, 5.2690e-04],\n",
      "        [9.9982e-01, 1.0624e-03, 1.0184e-03],\n",
      "        [9.9972e-01, 5.2112e-04, 5.0911e-04],\n",
      "        [9.9757e-01, 5.1899e-03, 5.0216e-03],\n",
      "        [9.9969e-01, 1.3056e-03, 1.2708e-03],\n",
      "        [1.0000e+00, 7.2484e-04, 6.7944e-04],\n",
      "        [1.0000e+00, 1.7847e-05, 1.7638e-05],\n",
      "        [9.9949e-01, 1.0722e-03, 1.0491e-03],\n",
      "        [1.0000e+00, 2.5223e-05, 2.5111e-05],\n",
      "        [9.9992e-01, 3.0861e-04, 3.0369e-04],\n",
      "        [9.9995e-01, 1.3276e-04, 1.3220e-04],\n",
      "        [9.9986e-01, 3.3428e-04, 3.3110e-04],\n",
      "        [9.9996e-01, 9.1499e-05, 9.1230e-05],\n",
      "        [9.9989e-01, 2.8219e-04, 2.7586e-04],\n",
      "        [1.0000e+00, 2.4591e-05, 2.4373e-05],\n",
      "        [9.9946e-01, 1.3921e-03, 1.2948e-03],\n",
      "        [9.9992e-01, 1.8862e-04, 1.8730e-04],\n",
      "        [9.9925e-01, 1.7352e-03, 1.7028e-03],\n",
      "        [9.9999e-01, 4.6007e-05, 4.5159e-05],\n",
      "        [9.9995e-01, 4.8699e-04, 4.7350e-04],\n",
      "        [9.9974e-01, 6.0601e-04, 5.9622e-04],\n",
      "        [9.9984e-01, 5.0522e-04, 4.9820e-04],\n",
      "        [9.9901e-01, 1.3749e-03, 1.3473e-03],\n",
      "        [9.9914e-01, 2.3175e-03, 2.2970e-03],\n",
      "        [9.9999e-01, 9.3057e-05, 9.1598e-05],\n",
      "        [9.9993e-01, 8.3067e-04, 8.0845e-04],\n",
      "        [9.9973e-01, 6.6749e-04, 6.5188e-04],\n",
      "        [9.9903e-01, 2.0212e-03, 2.0055e-03],\n",
      "        [9.9874e-01, 1.6381e-03, 1.5554e-03],\n",
      "        [9.9997e-01, 1.8186e-04, 1.8131e-04],\n",
      "        [9.9997e-01, 1.4805e-04, 1.4351e-04],\n",
      "        [9.9975e-01, 7.5744e-04, 7.4262e-04],\n",
      "        [9.9893e-01, 1.6540e-03, 1.5999e-03],\n",
      "        [9.9753e-01, 5.1556e-03, 4.8539e-03],\n",
      "        [9.9839e-01, 2.9504e-03, 2.9265e-03],\n",
      "        [9.9921e-01, 2.2498e-03, 2.0531e-03],\n",
      "        [9.9945e-01, 7.4876e-04, 7.2770e-04],\n",
      "        [9.9975e-01, 1.4024e-03, 1.3103e-03],\n",
      "        [9.9999e-01, 5.0813e-04, 4.9232e-04],\n",
      "        [1.0000e+00, 1.1926e-05, 1.1870e-05],\n",
      "        [9.9980e-01, 8.7291e-04, 8.5466e-04],\n",
      "        [9.9966e-01, 9.2580e-04, 9.1359e-04],\n",
      "        [9.9933e-01, 2.1280e-03, 2.0980e-03],\n",
      "        [9.9932e-01, 9.9594e-04, 9.8732e-04],\n",
      "        [9.9840e-01, 2.4754e-03, 2.4157e-03],\n",
      "        [9.9860e-01, 2.8173e-03, 2.7463e-03],\n",
      "        [9.9829e-01, 3.2064e-03, 3.0340e-03],\n",
      "        [9.9791e-01, 5.3823e-03, 5.0008e-03],\n",
      "        [1.0000e+00, 7.8300e-06, 7.7807e-06],\n",
      "        [9.9913e-01, 2.0145e-03, 1.9375e-03],\n",
      "        [9.9729e-01, 3.4458e-03, 3.2588e-03],\n",
      "        [9.9926e-01, 1.0815e-03, 1.0326e-03],\n",
      "        [9.9884e-01, 3.4694e-03, 3.2689e-03],\n",
      "        [1.0000e+00, 5.4733e-05, 5.4036e-05],\n",
      "        [9.9998e-01, 2.8933e-04, 2.8079e-04],\n",
      "        [9.9931e-01, 2.1246e-03, 2.0856e-03],\n",
      "        [9.9968e-01, 1.7228e-03, 1.5932e-03],\n",
      "        [9.9967e-01, 9.5787e-04, 9.2716e-04]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>), tensor([[9.5940e-01, 1.4310e-01, 8.5399e-03],\n",
      "        [9.9479e-01, 2.6729e-01, 1.1414e-03],\n",
      "        [9.1181e-01, 3.5187e-01, 2.4355e-02],\n",
      "        [7.6180e-01, 4.4661e-01, 3.8882e-02],\n",
      "        [7.7738e-01, 4.6103e-01, 3.8841e-02],\n",
      "        [6.5377e-01, 6.0581e-01, 2.9959e-02],\n",
      "        [7.7141e-01, 5.4767e-01, 1.2510e-02],\n",
      "        [7.1236e-01, 6.1144e-01, 2.6772e-02],\n",
      "        [9.5446e-01, 5.4622e-01, 9.2789e-03],\n",
      "        [6.7003e-01, 6.1673e-01, 3.2761e-02],\n",
      "        [9.0366e-01, 3.1888e-01, 1.2173e-02],\n",
      "        [8.9984e-01, 3.9052e-01, 5.0242e-03],\n",
      "        [6.1452e-01, 6.7161e-01, 3.5941e-02],\n",
      "        [7.8113e-01, 6.4296e-01, 2.4309e-02],\n",
      "        [7.8434e-01, 4.5813e-01, 4.2952e-02],\n",
      "        [7.1375e-01, 5.5374e-01, 3.9241e-02],\n",
      "        [9.6863e-01, 3.6869e-01, 3.0669e-03],\n",
      "        [6.9190e-01, 5.0713e-01, 4.2653e-02],\n",
      "        [6.5264e-01, 6.8325e-01, 3.5041e-02],\n",
      "        [8.2928e-01, 4.6081e-01, 5.6294e-03],\n",
      "        [9.6017e-01, 3.8597e-01, 5.0425e-03],\n",
      "        [7.0229e-01, 5.3383e-01, 3.7886e-02],\n",
      "        [7.8634e-01, 3.6261e-01, 4.0225e-02],\n",
      "        [8.9270e-01, 4.1005e-01, 1.3035e-02],\n",
      "        [8.3481e-01, 8.7841e-01, 1.1290e-02],\n",
      "        [8.9541e-01, 3.0503e-01, 1.2930e-02],\n",
      "        [8.8037e-01, 7.3901e-01, 1.9606e-03],\n",
      "        [9.0134e-01, 4.0673e-01, 1.3560e-02],\n",
      "        [6.6608e-01, 6.5294e-01, 2.4072e-02],\n",
      "        [6.6684e-01, 5.3161e-01, 3.7058e-02],\n",
      "        [6.8669e-01, 6.1097e-01, 3.3604e-02],\n",
      "        [8.2628e-01, 4.6399e-01, 3.2614e-02],\n",
      "        [6.1739e-01, 7.1952e-01, 3.3752e-02],\n",
      "        [8.1516e-01, 4.5450e-01, 3.8741e-02],\n",
      "        [8.8239e-01, 4.1489e-01, 1.8260e-02],\n",
      "        [9.6192e-01, 2.4066e-01, 1.7000e-02],\n",
      "        [7.9837e-01, 5.1269e-01, 1.8746e-02],\n",
      "        [7.3532e-01, 3.7390e-01, 3.4109e-02],\n",
      "        [8.8473e-01, 6.6233e-01, 2.0758e-02],\n",
      "        [7.3875e-01, 5.5087e-01, 2.7052e-02],\n",
      "        [6.7791e-01, 4.4973e-01, 5.5597e-02],\n",
      "        [9.7897e-01, 4.6400e-01, 1.7585e-03],\n",
      "        [8.5954e-01, 3.4524e-01, 2.5194e-02],\n",
      "        [9.7744e-01, 2.4516e-01, 1.3493e-03],\n",
      "        [7.4959e-01, 5.6065e-01, 3.9772e-02],\n",
      "        [9.3783e-01, 4.4335e-01, 9.2704e-03],\n",
      "        [8.0825e-01, 4.4208e-01, 2.2228e-02],\n",
      "        [7.3987e-01, 7.0298e-01, 1.4336e-02],\n",
      "        [6.3267e-01, 5.3479e-01, 5.0073e-02],\n",
      "        [6.1196e-01, 5.6159e-01, 3.2638e-02],\n",
      "        [9.5890e-01, 2.6406e-01, 4.2593e-03],\n",
      "        [6.7289e-01, 5.2708e-01, 5.8185e-02],\n",
      "        [9.7895e-01, 1.2653e-01, 4.0958e-03],\n",
      "        [6.4662e-01, 5.0829e-01, 4.8111e-02],\n",
      "        [7.6128e-01, 5.6727e-01, 3.2534e-02],\n",
      "        [7.7868e-01, 5.3233e-01, 2.2610e-02],\n",
      "        [7.6786e-01, 4.3335e-01, 4.4412e-02],\n",
      "        [8.0910e-01, 4.2871e-01, 8.1326e-03],\n",
      "        [9.7584e-01, 2.2838e-01, 4.9669e-04],\n",
      "        [5.8772e-01, 5.6881e-01, 5.2343e-02],\n",
      "        [5.8456e-01, 6.6622e-01, 3.6271e-02],\n",
      "        [6.9836e-01, 5.2342e-01, 2.9527e-02],\n",
      "        [6.9437e-01, 5.4808e-01, 3.5045e-02],\n",
      "        [6.5727e-01, 6.2011e-01, 2.7161e-02],\n",
      "        [8.7967e-01, 7.7710e-01, 9.0531e-04],\n",
      "        [6.9474e-01, 5.8676e-01, 2.8336e-02],\n",
      "        [5.9743e-01, 6.5775e-01, 3.4507e-02],\n",
      "        [7.0555e-01, 5.1532e-01, 2.9366e-02],\n",
      "        [7.0027e-01, 5.9427e-01, 3.6430e-02],\n",
      "        [7.3618e-01, 5.5805e-01, 3.2509e-02],\n",
      "        [8.0253e-01, 5.0117e-01, 2.3078e-02],\n",
      "        [7.3708e-01, 4.6749e-01, 5.4593e-02],\n",
      "        [6.2443e-01, 6.2683e-01, 3.2085e-02],\n",
      "        [7.6849e-01, 6.6342e-01, 1.5440e-02],\n",
      "        [9.2245e-01, 3.6035e-01, 2.2511e-02],\n",
      "        [8.7929e-01, 4.9680e-01, 9.6252e-03],\n",
      "        [7.7810e-01, 5.8981e-01, 1.0958e-02],\n",
      "        [6.1828e-01, 5.3903e-01, 4.9504e-02],\n",
      "        [9.4016e-01, 2.8929e-01, 7.1242e-03],\n",
      "        [6.9598e-01, 6.6085e-01, 2.9473e-02],\n",
      "        [6.7580e-01, 6.2339e-01, 2.9021e-02],\n",
      "        [8.8103e-01, 3.3048e-01, 2.2113e-02],\n",
      "        [8.1730e-01, 4.4144e-01, 3.5806e-02],\n",
      "        [7.1595e-01, 5.6842e-01, 3.2897e-02],\n",
      "        [9.4489e-01, 2.2794e-01, 1.1206e-02],\n",
      "        [5.8556e-01, 7.1822e-01, 2.6697e-02],\n",
      "        [7.7295e-01, 5.1368e-01, 2.7300e-02],\n",
      "        [6.9076e-01, 7.9065e-01, 9.7183e-03],\n",
      "        [7.0894e-01, 6.5125e-01, 2.8732e-02],\n",
      "        [9.3612e-01, 2.3461e-01, 1.2856e-02],\n",
      "        [6.9798e-01, 7.3918e-01, 2.5926e-02],\n",
      "        [6.0779e-01, 6.6926e-01, 2.3024e-02],\n",
      "        [7.1975e-01, 4.7219e-01, 4.3115e-02],\n",
      "        [6.2539e-01, 7.1286e-01, 3.7241e-02],\n",
      "        [8.4135e-01, 6.0074e-01, 9.1561e-03],\n",
      "        [9.6451e-01, 3.5679e-01, 3.0306e-03],\n",
      "        [7.4740e-01, 4.9227e-01, 3.0728e-02],\n",
      "        [9.0977e-01, 6.0075e-01, 5.4935e-03],\n",
      "        [8.6938e-01, 3.9019e-01, 2.6168e-02],\n",
      "        [9.4560e-01, 4.7865e-01, 9.9874e-03],\n",
      "        [8.6384e-01, 4.8414e-01, 1.6905e-02],\n",
      "        [8.8965e-01, 3.6576e-01, 1.6362e-02],\n",
      "        [8.6825e-01, 3.3760e-01, 1.6845e-02],\n",
      "        [7.2934e-01, 7.2207e-01, 7.8261e-03],\n",
      "        [7.0563e-01, 4.9621e-01, 3.3025e-02],\n",
      "        [9.2529e-01, 2.7251e-01, 1.7135e-02],\n",
      "        [6.8277e-01, 5.4258e-01, 3.8526e-02],\n",
      "        [9.7143e-01, 3.1627e-01, 8.7353e-03],\n",
      "        [8.0925e-01, 6.0153e-01, 1.8232e-02],\n",
      "        [6.3885e-01, 6.2631e-01, 1.8881e-02],\n",
      "        [8.8161e-01, 4.4296e-01, 1.8263e-02],\n",
      "        [6.6404e-01, 5.9099e-01, 3.5222e-02],\n",
      "        [6.0623e-01, 5.9729e-01, 4.2163e-02],\n",
      "        [9.7016e-01, 2.1301e-01, 9.1167e-03],\n",
      "        [9.0595e-01, 3.4962e-01, 2.1012e-02],\n",
      "        [6.7344e-01, 6.1433e-01, 2.2533e-02],\n",
      "        [5.8798e-01, 6.8527e-01, 3.7218e-02],\n",
      "        [6.3038e-01, 5.8898e-01, 3.8102e-02],\n",
      "        [8.7054e-01, 4.7268e-01, 1.2277e-02],\n",
      "        [9.3485e-01, 2.7055e-01, 1.1326e-02],\n",
      "        [8.3900e-01, 4.0405e-01, 1.7845e-02],\n",
      "        [8.3503e-01, 4.2125e-01, 3.5170e-02],\n",
      "        [6.2154e-01, 4.4715e-01, 5.6610e-02],\n",
      "        [6.7295e-01, 5.1651e-01, 4.3993e-02],\n",
      "        [7.0272e-01, 5.6428e-01, 4.6738e-02],\n",
      "        [5.7910e-01, 6.3028e-01, 2.7303e-02],\n",
      "        [6.9088e-01, 6.1050e-01, 2.6529e-02],\n",
      "        [8.1097e-01, 6.7497e-01, 1.8378e-02],\n",
      "        [9.5246e-01, 3.9726e-01, 5.1625e-03],\n",
      "        [6.6445e-01, 6.3433e-01, 2.5502e-02],\n",
      "        [7.3160e-01, 4.7804e-01, 2.9866e-02],\n",
      "        [7.6394e-01, 5.0950e-01, 4.7574e-02],\n",
      "        [8.4837e-01, 3.8657e-01, 2.8583e-02],\n",
      "        [6.6570e-01, 5.0786e-01, 4.7197e-02],\n",
      "        [6.7525e-01, 5.3845e-01, 4.5659e-02],\n",
      "        [8.0059e-01, 3.9354e-01, 4.1717e-02],\n",
      "        [7.0067e-01, 4.8007e-01, 4.4371e-02],\n",
      "        [8.9742e-01, 4.2158e-01, 5.1909e-03],\n",
      "        [7.0882e-01, 6.5353e-01, 4.1081e-02],\n",
      "        [7.6734e-01, 4.1919e-01, 4.9323e-02],\n",
      "        [6.4818e-01, 5.7690e-01, 4.0474e-02],\n",
      "        [7.3465e-01, 5.6961e-01, 4.7637e-02],\n",
      "        [7.3820e-01, 8.3112e-01, 6.2494e-03],\n",
      "        [8.7309e-01, 4.4568e-01, 1.4315e-02],\n",
      "        [7.4992e-01, 5.4521e-01, 3.0775e-02],\n",
      "        [8.4783e-01, 3.5552e-01, 3.7093e-02],\n",
      "        [6.7902e-01, 4.6211e-01, 2.9665e-02]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>), tensor([[5.7150e-02, 9.8240e-01, 2.4651e-02, 2.0902e-02],\n",
      "        [3.3639e-03, 9.9986e-01, 1.5493e-03, 1.4338e-03],\n",
      "        [5.5683e-02, 9.8844e-01, 4.0413e-02, 3.8174e-02],\n",
      "        [9.9010e-02, 9.2172e-01, 6.0677e-02, 5.5946e-02],\n",
      "        [1.2389e-01, 8.8283e-01, 7.3048e-02, 7.0926e-02],\n",
      "        [8.5676e-02, 9.3326e-01, 4.3977e-02, 4.1379e-02],\n",
      "        [4.0365e-02, 9.8477e-01, 1.7738e-02, 1.7337e-02],\n",
      "        [1.1490e-01, 9.3602e-01, 5.6465e-02, 5.1075e-02],\n",
      "        [2.4089e-02, 9.9946e-01, 8.2488e-03, 8.1332e-03],\n",
      "        [8.8659e-02, 9.4175e-01, 5.2576e-02, 5.3123e-02],\n",
      "        [4.3817e-02, 9.7869e-01, 3.0498e-02, 2.8920e-02],\n",
      "        [2.1038e-02, 9.9132e-01, 8.3285e-03, 8.1854e-03],\n",
      "        [1.3247e-01, 8.9840e-01, 5.9383e-02, 5.5256e-02],\n",
      "        [9.3412e-02, 9.6544e-01, 3.6290e-02, 3.4297e-02],\n",
      "        [1.1524e-01, 8.9804e-01, 6.0863e-02, 5.8589e-02],\n",
      "        [1.2188e-01, 9.3171e-01, 5.8511e-02, 5.4380e-02],\n",
      "        [5.7179e-03, 9.9894e-01, 4.3949e-03, 4.4201e-03],\n",
      "        [1.3920e-01, 9.1748e-01, 6.4997e-02, 6.3671e-02],\n",
      "        [9.3755e-02, 9.2372e-01, 5.9233e-02, 5.0314e-02],\n",
      "        [1.7669e-02, 9.9603e-01, 8.7072e-03, 8.6424e-03],\n",
      "        [8.5041e-03, 9.9606e-01, 4.2996e-03, 4.2335e-03],\n",
      "        [9.8505e-02, 9.1383e-01, 6.5444e-02, 6.3388e-02],\n",
      "        [1.0577e-01, 9.0198e-01, 5.5089e-02, 5.5527e-02],\n",
      "        [5.3092e-02, 9.8763e-01, 1.9740e-02, 1.9307e-02],\n",
      "        [5.6358e-02, 9.9992e-01, 2.1392e-02, 2.0676e-02],\n",
      "        [8.4134e-02, 9.6747e-01, 3.4951e-02, 3.1945e-02],\n",
      "        [1.1453e-02, 9.9986e-01, 5.6721e-03, 5.6434e-03],\n",
      "        [3.9207e-02, 9.9327e-01, 2.3233e-02, 2.2833e-02],\n",
      "        [8.5137e-02, 9.4502e-01, 3.4718e-02, 3.5161e-02],\n",
      "        [9.0790e-02, 9.5263e-01, 4.9238e-02, 4.5829e-02],\n",
      "        [1.3477e-01, 9.2794e-01, 6.2111e-02, 5.8279e-02],\n",
      "        [1.0299e-01, 9.7507e-01, 6.0804e-02, 5.8637e-02],\n",
      "        [1.3232e-01, 9.3350e-01, 6.1025e-02, 5.8301e-02],\n",
      "        [1.0151e-01, 9.3243e-01, 6.5249e-02, 6.2007e-02],\n",
      "        [9.1289e-02, 9.6190e-01, 3.9646e-02, 4.0214e-02],\n",
      "        [5.8406e-02, 9.8087e-01, 2.3873e-02, 2.3974e-02],\n",
      "        [6.8373e-02, 9.6150e-01, 3.8656e-02, 3.5716e-02],\n",
      "        [9.4756e-02, 9.2036e-01, 5.3812e-02, 5.0145e-02],\n",
      "        [3.3649e-02, 9.8681e-01, 2.2990e-02, 2.0711e-02],\n",
      "        [9.3844e-02, 9.5116e-01, 4.3519e-02, 4.1026e-02],\n",
      "        [1.3092e-01, 8.5938e-01, 7.5380e-02, 6.9641e-02],\n",
      "        [2.8597e-03, 9.9969e-01, 1.5441e-03, 1.5299e-03],\n",
      "        [5.6473e-02, 9.3788e-01, 3.4018e-02, 3.2991e-02],\n",
      "        [1.7690e-02, 9.9908e-01, 6.5810e-03, 6.5267e-03],\n",
      "        [1.0250e-01, 9.1337e-01, 4.8249e-02, 4.4502e-02],\n",
      "        [2.8946e-02, 9.8765e-01, 1.6330e-02, 1.5560e-02],\n",
      "        [7.5790e-02, 9.5430e-01, 4.0356e-02, 4.0495e-02],\n",
      "        [5.0360e-02, 9.7570e-01, 2.2475e-02, 2.2604e-02],\n",
      "        [1.3712e-01, 8.8252e-01, 6.1550e-02, 6.1806e-02],\n",
      "        [8.4230e-02, 9.3749e-01, 4.6392e-02, 4.3574e-02],\n",
      "        [1.8192e-02, 9.9773e-01, 7.7170e-03, 7.4757e-03],\n",
      "        [1.4365e-01, 8.1893e-01, 8.9221e-02, 9.2450e-02],\n",
      "        [1.8164e-02, 9.9543e-01, 1.1360e-02, 1.1105e-02],\n",
      "        [1.1309e-01, 9.0363e-01, 6.3583e-02, 6.2548e-02],\n",
      "        [8.8466e-02, 9.2599e-01, 4.0749e-02, 4.0186e-02],\n",
      "        [4.9338e-02, 9.6709e-01, 3.3709e-02, 3.2770e-02],\n",
      "        [9.3803e-02, 9.0469e-01, 5.5441e-02, 5.2058e-02],\n",
      "        [2.0412e-02, 9.8653e-01, 1.1338e-02, 1.1241e-02],\n",
      "        [1.9505e-03, 9.9988e-01, 9.6739e-04, 9.6431e-04],\n",
      "        [1.1254e-01, 8.5544e-01, 7.0858e-02, 6.3866e-02],\n",
      "        [8.5684e-02, 9.3784e-01, 4.5714e-02, 4.4384e-02],\n",
      "        [7.3608e-02, 9.4906e-01, 4.2799e-02, 4.0864e-02],\n",
      "        [9.4225e-02, 9.1732e-01, 5.2190e-02, 5.1433e-02],\n",
      "        [5.4026e-02, 9.5438e-01, 2.9474e-02, 2.8935e-02],\n",
      "        [2.4693e-03, 9.9987e-01, 1.3508e-03, 1.3444e-03],\n",
      "        [8.9210e-02, 9.3612e-01, 5.3918e-02, 5.3543e-02],\n",
      "        [7.8033e-02, 9.5352e-01, 5.3671e-02, 5.1366e-02],\n",
      "        [8.7822e-02, 9.4503e-01, 6.0661e-02, 5.0530e-02],\n",
      "        [1.0044e-01, 9.1370e-01, 6.4626e-02, 5.8008e-02],\n",
      "        [6.4404e-02, 9.5154e-01, 4.3214e-02, 3.9674e-02],\n",
      "        [5.7178e-02, 9.6288e-01, 3.0581e-02, 2.8474e-02],\n",
      "        [1.2573e-01, 8.7838e-01, 8.4770e-02, 7.4719e-02],\n",
      "        [1.0254e-01, 9.0907e-01, 6.0792e-02, 4.8796e-02],\n",
      "        [7.8577e-02, 9.5761e-01, 2.8803e-02, 2.8025e-02],\n",
      "        [5.7544e-02, 9.7084e-01, 3.5245e-02, 3.3390e-02],\n",
      "        [3.4920e-02, 9.9069e-01, 1.4137e-02, 1.3664e-02],\n",
      "        [3.4240e-02, 9.9268e-01, 2.0993e-02, 2.0715e-02],\n",
      "        [9.9719e-02, 9.0548e-01, 6.7059e-02, 5.9992e-02],\n",
      "        [2.2581e-02, 9.9172e-01, 1.0945e-02, 1.0282e-02],\n",
      "        [1.0210e-01, 9.4357e-01, 4.7409e-02, 4.4327e-02],\n",
      "        [1.2908e-01, 9.0542e-01, 5.1986e-02, 4.8201e-02],\n",
      "        [7.3748e-02, 9.4762e-01, 3.6887e-02, 3.1933e-02],\n",
      "        [8.2870e-02, 9.1211e-01, 5.1921e-02, 4.8694e-02],\n",
      "        [5.4998e-02, 9.6318e-01, 3.6701e-02, 3.4080e-02],\n",
      "        [2.8561e-02, 9.9303e-01, 1.4130e-02, 1.3720e-02],\n",
      "        [1.2184e-01, 9.5121e-01, 5.3193e-02, 4.9246e-02],\n",
      "        [8.1047e-02, 9.3855e-01, 4.6977e-02, 4.5681e-02],\n",
      "        [4.0075e-02, 9.8624e-01, 2.1812e-02, 2.1236e-02],\n",
      "        [1.0295e-01, 9.2782e-01, 5.2629e-02, 4.8333e-02],\n",
      "        [4.5013e-02, 9.8052e-01, 2.5425e-02, 2.3738e-02],\n",
      "        [7.6450e-02, 9.7228e-01, 3.9031e-02, 3.6528e-02],\n",
      "        [7.3491e-02, 9.5421e-01, 3.3958e-02, 3.3431e-02],\n",
      "        [1.0429e-01, 8.9506e-01, 6.5018e-02, 5.9130e-02],\n",
      "        [1.1024e-01, 9.3724e-01, 5.2167e-02, 5.0521e-02],\n",
      "        [4.7029e-02, 9.9239e-01, 1.7070e-02, 1.6630e-02],\n",
      "        [1.6343e-02, 9.9616e-01, 8.7354e-03, 8.5438e-03],\n",
      "        [1.1118e-01, 9.5063e-01, 4.7469e-02, 4.4721e-02],\n",
      "        [1.8351e-02, 9.9531e-01, 1.0684e-02, 9.5654e-03],\n",
      "        [9.7531e-02, 9.6312e-01, 4.7366e-02, 4.0133e-02],\n",
      "        [2.8253e-02, 9.8817e-01, 1.9173e-02, 1.8061e-02],\n",
      "        [5.7117e-02, 9.6856e-01, 3.4656e-02, 3.2510e-02],\n",
      "        [5.4182e-02, 9.8053e-01, 2.2311e-02, 2.1946e-02],\n",
      "        [9.7930e-02, 9.4385e-01, 4.2775e-02, 4.2302e-02],\n",
      "        [4.7667e-02, 9.8444e-01, 2.8744e-02, 2.7217e-02],\n",
      "        [9.1245e-02, 9.1235e-01, 4.0780e-02, 3.9223e-02],\n",
      "        [6.5348e-02, 9.7371e-01, 2.3286e-02, 2.1547e-02],\n",
      "        [1.2294e-01, 8.9058e-01, 6.8791e-02, 6.6295e-02],\n",
      "        [5.0893e-02, 9.8245e-01, 2.4867e-02, 2.5079e-02],\n",
      "        [9.5000e-02, 9.5090e-01, 3.4870e-02, 3.4140e-02],\n",
      "        [7.3808e-02, 9.5294e-01, 3.6779e-02, 3.3314e-02],\n",
      "        [6.8377e-02, 9.5506e-01, 3.1171e-02, 3.2000e-02],\n",
      "        [1.1682e-01, 9.1419e-01, 5.5769e-02, 5.4303e-02],\n",
      "        [1.0220e-01, 9.1991e-01, 6.7394e-02, 6.2725e-02],\n",
      "        [2.8440e-02, 9.8926e-01, 1.3656e-02, 1.3045e-02],\n",
      "        [7.0882e-02, 9.7651e-01, 3.6517e-02, 3.4167e-02],\n",
      "        [8.0653e-02, 9.6520e-01, 3.8962e-02, 3.7180e-02],\n",
      "        [1.3227e-01, 8.9626e-01, 5.5307e-02, 5.0136e-02],\n",
      "        [1.0783e-01, 9.2072e-01, 6.0164e-02, 5.6043e-02],\n",
      "        [4.6086e-02, 9.8125e-01, 2.0992e-02, 2.0623e-02],\n",
      "        [3.4375e-02, 9.7914e-01, 1.5288e-02, 1.4794e-02],\n",
      "        [5.2025e-02, 9.5328e-01, 3.0682e-02, 2.6622e-02],\n",
      "        [9.1768e-02, 9.1593e-01, 6.7057e-02, 6.4680e-02],\n",
      "        [1.3656e-01, 8.7782e-01, 8.4697e-02, 7.8220e-02],\n",
      "        [1.0999e-01, 8.8958e-01, 6.1554e-02, 5.6988e-02],\n",
      "        [1.2264e-01, 9.1974e-01, 7.4282e-02, 7.2786e-02],\n",
      "        [8.8398e-02, 9.3875e-01, 5.3526e-02, 5.2722e-02],\n",
      "        [6.5860e-02, 9.6675e-01, 3.2473e-02, 3.2050e-02],\n",
      "        [5.1782e-02, 9.9031e-01, 2.6194e-02, 2.5320e-02],\n",
      "        [3.2626e-02, 9.9598e-01, 1.5106e-02, 1.5240e-02],\n",
      "        [8.8642e-02, 9.5068e-01, 4.9755e-02, 4.6257e-02],\n",
      "        [9.3579e-02, 9.3464e-01, 4.8403e-02, 4.5377e-02],\n",
      "        [1.2241e-01, 9.3389e-01, 6.0343e-02, 6.0851e-02],\n",
      "        [7.8569e-02, 9.4427e-01, 4.6510e-02, 4.3839e-02],\n",
      "        [1.1898e-01, 9.0612e-01, 7.2423e-02, 6.5513e-02],\n",
      "        [1.0238e-01, 9.1362e-01, 7.4450e-02, 6.6928e-02],\n",
      "        [1.0200e-01, 8.9311e-01, 7.3976e-02, 6.2262e-02],\n",
      "        [1.5474e-01, 8.9765e-01, 9.5015e-02, 7.6481e-02],\n",
      "        [2.3990e-02, 9.9732e-01, 6.7594e-03, 6.5614e-03],\n",
      "        [8.3161e-02, 9.3214e-01, 5.7874e-02, 4.8699e-02],\n",
      "        [1.2005e-01, 8.6707e-01, 6.9827e-02, 6.3271e-02],\n",
      "        [9.9545e-02, 9.3153e-01, 5.4825e-02, 5.3415e-02],\n",
      "        [1.2214e-01, 8.9896e-01, 7.0809e-02, 6.6464e-02],\n",
      "        [1.7688e-02, 9.9995e-01, 9.9449e-03, 9.2964e-03],\n",
      "        [3.9350e-02, 9.8970e-01, 2.3091e-02, 2.2341e-02],\n",
      "        [8.2555e-02, 9.3671e-01, 5.4998e-02, 4.9745e-02],\n",
      "        [1.0439e-01, 9.4374e-01, 4.7090e-02, 4.6855e-02],\n",
      "        [6.0324e-02, 9.6029e-01, 3.6013e-02, 3.0180e-02]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>), tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0140, 0.0029, 0.0128, 0.0071, 0.0087],\n",
      "        [0.0788, 0.0424, 0.0811, 0.0664, 0.0612],\n",
      "        [0.1313, 0.0600, 0.1209, 0.1437, 0.1184],\n",
      "        [0.1723, 0.0796, 0.1501, 0.1331, 0.1584],\n",
      "        [0.0909, 0.0388, 0.1391, 0.1350, 0.1029],\n",
      "        [0.0714, 0.0146, 0.0596, 0.0481, 0.0493],\n",
      "        [0.1209, 0.0410, 0.1009, 0.0909, 0.1129],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0750, 0.0254, 0.0763, 0.0961, 0.0489],\n",
      "        [0.0343, 0.0100, 0.0402, 0.0269, 0.0294],\n",
      "        [0.1537, 0.0710, 0.1672, 0.1814, 0.1336],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1356, 0.0713, 0.1457, 0.1101, 0.1278],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0242, 0.0096, 0.0220, 0.0252, 0.0219],\n",
      "        [0.1516, 0.0676, 0.1398, 0.1513, 0.1399],\n",
      "        [0.1370, 0.0442, 0.1513, 0.1182, 0.1233],\n",
      "        [0.0623, 0.0125, 0.0267, 0.0285, 0.0305],\n",
      "        [0.0315, 0.0115, 0.0497, 0.0162, 0.0412],\n",
      "        [0.1156, 0.0657, 0.1287, 0.1934, 0.1287],\n",
      "        [0.1729, 0.0629, 0.1305, 0.1347, 0.1376],\n",
      "        [0.0560, 0.0199, 0.0636, 0.0643, 0.0451],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0640, 0.0518, 0.0810, 0.0602, 0.0493],\n",
      "        [0.0139, 0.0051, 0.0113, 0.0249, 0.0308],\n",
      "        [0.0504, 0.0148, 0.0806, 0.0503, 0.0449],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1423, 0.0494, 0.1366, 0.1880, 0.1053],\n",
      "        [0.1259, 0.0880, 0.0841, 0.0828, 0.0771],\n",
      "        [0.1407, 0.0481, 0.1706, 0.1109, 0.1144],\n",
      "        [0.1122, 0.0846, 0.1018, 0.1131, 0.1184],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0874, 0.0300, 0.1041, 0.0920, 0.0986],\n",
      "        [0.0992, 0.0276, 0.0783, 0.0755, 0.0641],\n",
      "        [0.1561, 0.0427, 0.1470, 0.1214, 0.1125],\n",
      "        [0.0798, 0.0299, 0.0638, 0.0565, 0.0507],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1644, 0.0885, 0.1853, 0.1749, 0.1939],\n",
      "        [0.0130, 0.0018, 0.0088, 0.0101, 0.0116],\n",
      "        [0.0877, 0.0460, 0.0991, 0.0803, 0.1261],\n",
      "        [0.0171, 0.0041, 0.0233, 0.0093, 0.0114],\n",
      "        [0.1340, 0.0657, 0.1483, 0.1187, 0.1204],\n",
      "        [0.0614, 0.0151, 0.0673, 0.0665, 0.0774],\n",
      "        [0.1239, 0.0886, 0.1062, 0.1130, 0.1119],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1807, 0.0838, 0.1296, 0.1823, 0.1494],\n",
      "        [0.1146, 0.0481, 0.1308, 0.1251, 0.0814],\n",
      "        [0.0268, 0.0069, 0.0220, 0.0356, 0.0211],\n",
      "        [0.1814, 0.1118, 0.1681, 0.2151, 0.1640],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1588, 0.0734, 0.1658, 0.1610, 0.1321],\n",
      "        [0.0956, 0.0464, 0.1301, 0.1179, 0.1197],\n",
      "        [0.0785, 0.0393, 0.1018, 0.0760, 0.0836],\n",
      "        [0.1704, 0.0653, 0.1445, 0.1554, 0.1361],\n",
      "        [0.0427, 0.0197, 0.0598, 0.0902, 0.0445],\n",
      "        [0.0087, 0.0016, 0.0093, 0.0085, 0.0050],\n",
      "        [0.1656, 0.0624, 0.1739, 0.1818, 0.1474],\n",
      "        [0.1128, 0.0739, 0.1420, 0.1106, 0.0950],\n",
      "        [0.1366, 0.0471, 0.1199, 0.0965, 0.1367],\n",
      "        [0.1312, 0.0641, 0.1270, 0.1017, 0.1073],\n",
      "        [0.0930, 0.0312, 0.0991, 0.1144, 0.0607],\n",
      "        [0.0176, 0.0007, 0.0294, 0.0059, 0.0062],\n",
      "        [0.1344, 0.0483, 0.1349, 0.1003, 0.1098],\n",
      "        [0.1406, 0.0408, 0.0852, 0.0977, 0.1163],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1402, 0.0514, 0.1385, 0.1072, 0.1417],\n",
      "        [0.0917, 0.0389, 0.1119, 0.1209, 0.1402],\n",
      "        [0.1064, 0.0500, 0.0861, 0.0706, 0.0954],\n",
      "        [0.1641, 0.0887, 0.1626, 0.1525, 0.1556],\n",
      "        [0.1327, 0.0547, 0.1121, 0.1256, 0.1156],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0829, 0.0530, 0.0866, 0.0912, 0.0718],\n",
      "        [0.0619, 0.0191, 0.0529, 0.0396, 0.0520],\n",
      "        [0.0463, 0.0216, 0.0549, 0.0550, 0.0810],\n",
      "        [0.1474, 0.0660, 0.1929, 0.1565, 0.1647],\n",
      "        [0.0523, 0.0078, 0.0328, 0.0313, 0.0222],\n",
      "        [0.1282, 0.0498, 0.1306, 0.1223, 0.1034],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1007, 0.0379, 0.1369, 0.1184, 0.1207],\n",
      "        [0.1546, 0.0729, 0.1250, 0.1106, 0.1258],\n",
      "        [0.1166, 0.0492, 0.1172, 0.0834, 0.0797],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1266, 0.0329, 0.1479, 0.0868, 0.0934],\n",
      "        [0.1233, 0.0426, 0.0905, 0.0885, 0.0904],\n",
      "        [0.0747, 0.0194, 0.0550, 0.0595, 0.0565],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0925, 0.0241, 0.0900, 0.1378, 0.0795],\n",
      "        [0.1012, 0.0360, 0.1100, 0.1114, 0.1170],\n",
      "        [0.1078, 0.0288, 0.1094, 0.1173, 0.0787],\n",
      "        [0.1558, 0.0996, 0.1369, 0.1632, 0.1688],\n",
      "        [0.1111, 0.0519, 0.1447, 0.0988, 0.0986],\n",
      "        [0.0687, 0.0217, 0.0520, 0.0605, 0.0686],\n",
      "        [0.0245, 0.0098, 0.0251, 0.0252, 0.0416],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0287, 0.0107, 0.0338, 0.0391, 0.0483],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0452, 0.0202, 0.0489, 0.0619, 0.0614],\n",
      "        [0.0701, 0.0243, 0.0561, 0.0833, 0.0674],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1037, 0.0506, 0.1005, 0.0966, 0.0628],\n",
      "        [0.0349, 0.0155, 0.0537, 0.0482, 0.0202],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0980, 0.0361, 0.1001, 0.0925, 0.1107],\n",
      "        [0.0674, 0.0350, 0.0733, 0.0983, 0.0711],\n",
      "        [0.1392, 0.0524, 0.1422, 0.1436, 0.1322],\n",
      "        [0.1489, 0.0655, 0.1664, 0.1551, 0.1325],\n",
      "        [0.0419, 0.0221, 0.0434, 0.0438, 0.0495],\n",
      "        [0.0819, 0.0692, 0.1086, 0.1092, 0.1050],\n",
      "        [0.0985, 0.0439, 0.0744, 0.0817, 0.1143],\n",
      "        [0.1557, 0.0733, 0.1517, 0.1755, 0.0965],\n",
      "        [0.1644, 0.0596, 0.1442, 0.1635, 0.1237],\n",
      "        [0.0725, 0.0202, 0.0423, 0.0543, 0.0461],\n",
      "        [0.0625, 0.0335, 0.0636, 0.0647, 0.0555],\n",
      "        [0.1439, 0.0426, 0.0793, 0.0846, 0.0702],\n",
      "        [0.1631, 0.0596, 0.1280, 0.1437, 0.1465],\n",
      "        [0.1955, 0.0870, 0.1747, 0.1588, 0.1427],\n",
      "        [0.1577, 0.0935, 0.1360, 0.1971, 0.1368],\n",
      "        [0.1720, 0.0916, 0.1406, 0.1235, 0.1260],\n",
      "        [0.1224, 0.0493, 0.1114, 0.1439, 0.1341],\n",
      "        [0.0836, 0.0290, 0.0868, 0.0928, 0.1017],\n",
      "        [0.0851, 0.0325, 0.1246, 0.0697, 0.0709],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1290, 0.0490, 0.1665, 0.0879, 0.0750],\n",
      "        [0.1097, 0.0693, 0.1422, 0.1455, 0.1727],\n",
      "        [0.1143, 0.0810, 0.1226, 0.0954, 0.1516],\n",
      "        [0.1492, 0.0809, 0.1677, 0.1964, 0.1288],\n",
      "        [0.1450, 0.0749, 0.1613, 0.2027, 0.1440],\n",
      "        [0.1480, 0.0731, 0.1512, 0.1060, 0.1216],\n",
      "        [0.1546, 0.0898, 0.1474, 0.1434, 0.1456],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1433, 0.0859, 0.1249, 0.1204, 0.1197],\n",
      "        [0.1439, 0.0803, 0.1673, 0.1430, 0.1451],\n",
      "        [0.1127, 0.0565, 0.1094, 0.1359, 0.1172],\n",
      "        [0.1724, 0.0639, 0.1292, 0.1732, 0.1386],\n",
      "        [0.0278, 0.0028, 0.0195, 0.0439, 0.0198],\n",
      "        [0.1177, 0.0372, 0.0861, 0.0505, 0.0501],\n",
      "        [0.1367, 0.0745, 0.1629, 0.1080, 0.1188],\n",
      "        [0.1389, 0.0749, 0.1208, 0.0991, 0.1407],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)], 'order': ['pd', 'nd', 'mod', 'dlt']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pd': {'accuracy': 0.3333333333333333,\n",
       "  'auc_micro': 0.8947577628133184,\n",
       "  'auc_mean': 0.585274565586227,\n",
       "  'auc_weighted': 0.605061871600574},\n",
       " 'nd': {'accuracy': 0.35877961700746513,\n",
       "  'auc_micro': 0.7628605200945627,\n",
       "  'auc_mean': 0.5110004655207351,\n",
       "  'auc_weighted': 0.5572855873517524},\n",
       " 'mod': {'accuracy': 0.35877961700746513,\n",
       "  'auc_micro': 0.7628605200945627,\n",
       "  'auc_mean': 0.5110004655207351,\n",
       "  'auc_weighted': 0.5572855873517524},\n",
       " 'dlts': {'accuracy': [0.9183673469387755,\n",
       "   0.9659863945578231,\n",
       "   0.9523809523809523,\n",
       "   0.9795918367346939,\n",
       "   0.9659863945578231],\n",
       "  'accuracy_mean': 0.9564625850340136,\n",
       "  'auc': [0.5814814814814815,\n",
       "   0.7563380281690141,\n",
       "   0.6275510204081632,\n",
       "   0.6296296296296295,\n",
       "   0.5521126760563381],\n",
       "  'auc_mean': 0.6294225671489253}}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmodel2 = train_state(model_args=t1_args,state=2,lr=.001,weights=[1,1,.1,.1],balanced=False)\n",
    "tmodel2[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "310c657c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I dont do balancing on the outputs because Idk how that would work\n",
      "epoch 0 train loss 2.861276149749756\n",
      "val loss 2.471322536468506\n",
      "______________\n",
      "epoch 1 train loss 2.4618568420410156\n",
      "val loss 2.1849172115325928\n",
      "______________\n",
      "epoch 2 train loss 2.19868803024292\n",
      "val loss 1.984065055847168\n",
      "______________\n",
      "epoch 3 train loss 2.0024421215057373\n",
      "val loss 1.8661103248596191\n",
      "______________\n",
      "epoch 4 train loss 1.8963170051574707\n",
      "val loss 1.8212862014770508\n",
      "______________\n",
      "epoch 5 train loss 1.8351136445999146\n",
      "val loss 1.8169653415679932\n",
      "______________\n",
      "epoch 6 train loss 1.8602060079574585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 1.8160605430603027\n",
      "______________\n",
      "epoch 7 train loss 1.8368017673492432\n",
      "val loss 1.798069715499878\n",
      "______________\n",
      "epoch 8 train loss 1.7876002788543701\n",
      "val loss 1.7632347345352173\n",
      "______________\n",
      "epoch 9 train loss 1.7738299369812012\n",
      "val loss 1.7206664085388184\n",
      "______________\n",
      "epoch 10 train loss 1.6916602849960327\n",
      "val loss 1.6817936897277832\n",
      "______________\n",
      "epoch 11 train loss 1.6516978740692139\n",
      "val loss 1.6531884670257568\n",
      "______________\n",
      "epoch 12 train loss 1.620025396347046\n",
      "val loss 1.6373862028121948\n",
      "______________\n",
      "epoch 13 train loss 1.568729281425476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 1.6329185962677002\n",
      "______________\n",
      "epoch 14 train loss 1.5323816537857056\n",
      "val loss 1.6361749172210693\n",
      "______________\n",
      "epoch 15 train loss 1.5067263841629028\n",
      "val loss 1.6424747705459595\n",
      "______________\n",
      "epoch 16 train loss 1.5120731592178345\n",
      "val loss 1.6479442119598389\n",
      "______________\n",
      "epoch 17 train loss 1.4783082008361816\n",
      "val loss 1.6511447429656982\n",
      "______________\n",
      "epoch 18 train loss 1.46162748336792\n",
      "val loss 1.6519887447357178\n",
      "______________\n",
      "epoch 19 train loss 1.4403913021087646\n",
      "val loss 1.650320053100586\n",
      "______________\n",
      "epoch 20 train loss 1.4107030630111694\n",
      "val loss 1.6479034423828125\n",
      "______________\n",
      "epoch 21 train loss 1.401190161705017\n",
      "val loss 1.6462373733520508\n",
      "______________\n",
      "epoch 22 train loss 1.3613038063049316\n",
      "val loss 1.647400140762329\n",
      "______________\n",
      "epoch 23 train loss 1.3293402194976807\n",
      "val loss 1.6518118381500244\n",
      "______________\n",
      "epoch 24 train loss 1.3320105075836182\n",
      "val loss 1.6575520038604736\n",
      "______________\n",
      "best loss 1.6329185962677002 {'Overall Survival (4 Years)': {'accuracy': 0.891156462585034, 'mse': 0.105073795, 'auc': 0.48377862595419846, 'precision': 0.891156462585034, 'recall': 1.0, 'f1': 0.9424460431654677}, 'FT': {'accuracy': 0.782312925170068, 'mse': 0.1599512, 'auc': 0.6741847826086957, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'Aspiration rate Post-therapy': {'accuracy': 0.8231292517006803, 'mse': 0.13265932, 'auc': 0.7380801017164653, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'LRC': {'accuracy': 0.8979591836734694, 'mse': 0.097782046, 'auc': 0.4954545454545455, 'precision': 0.8979591836734694, 'recall': 1.0, 'f1': 0.9462365591397849}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': tensor([[0.8209, 0.1362, 0.1776, 0.9080],\n",
      "        [0.9313, 0.2820, 0.1935, 0.9548],\n",
      "        [0.7513, 0.3953, 0.3927, 0.9224],\n",
      "        [0.7363, 0.3424, 0.2429, 0.7925],\n",
      "        [0.8327, 0.2316, 0.2512, 0.8395],\n",
      "        [0.9132, 0.0661, 0.0472, 0.9291],\n",
      "        [0.8301, 0.3578, 0.3271, 0.8129],\n",
      "        [0.8984, 0.0928, 0.0650, 0.9209],\n",
      "        [0.9594, 0.0237, 0.0261, 0.9675],\n",
      "        [0.8894, 0.0608, 0.0484, 0.9146],\n",
      "        [0.8177, 0.2080, 0.1550, 0.8810],\n",
      "        [0.8908, 0.2009, 0.2850, 0.9163],\n",
      "        [0.9194, 0.1286, 0.0739, 0.9537],\n",
      "        [0.9237, 0.0276, 0.0302, 0.9004],\n",
      "        [0.7915, 0.3464, 0.3247, 0.8547],\n",
      "        [0.9057, 0.0506, 0.0675, 0.8810],\n",
      "        [0.9453, 0.1136, 0.0711, 0.9743],\n",
      "        [0.8502, 0.2042, 0.1739, 0.8579],\n",
      "        [0.8862, 0.0958, 0.0793, 0.9221],\n",
      "        [0.8135, 0.2860, 0.1906, 0.8041],\n",
      "        [0.8325, 0.2984, 0.2494, 0.7883],\n",
      "        [0.8971, 0.0870, 0.0598, 0.8987],\n",
      "        [0.8101, 0.3504, 0.3419, 0.8637],\n",
      "        [0.7450, 0.4187, 0.2693, 0.8131],\n",
      "        [0.9919, 0.0039, 0.0039, 0.9961],\n",
      "        [0.8281, 0.1975, 0.2417, 0.8127],\n",
      "        [0.9846, 0.0117, 0.0091, 0.9920],\n",
      "        [0.9189, 0.2867, 0.2682, 0.9328],\n",
      "        [0.8536, 0.0939, 0.1035, 0.8750],\n",
      "        [0.9258, 0.0312, 0.0322, 0.9410],\n",
      "        [0.9190, 0.0633, 0.0499, 0.9277],\n",
      "        [0.8423, 0.2903, 0.2778, 0.8664],\n",
      "        [0.8808, 0.0854, 0.0587, 0.9086],\n",
      "        [0.7692, 0.3581, 0.3435, 0.8567],\n",
      "        [0.9327, 0.0322, 0.0336, 0.9504],\n",
      "        [0.8015, 0.3825, 0.4876, 0.8819],\n",
      "        [0.7516, 0.3771, 0.3973, 0.8037],\n",
      "        [0.8570, 0.1717, 0.1301, 0.7900],\n",
      "        [0.6782, 0.4837, 0.3393, 0.7625],\n",
      "        [0.9337, 0.0301, 0.0313, 0.9489],\n",
      "        [0.8388, 0.2124, 0.2126, 0.8911],\n",
      "        [0.8954, 0.2333, 0.2136, 0.9752],\n",
      "        [0.7743, 0.3802, 0.3133, 0.8432],\n",
      "        [0.8923, 0.2699, 0.3138, 0.8826],\n",
      "        [0.9125, 0.0696, 0.0390, 0.9302],\n",
      "        [0.9510, 0.0616, 0.0476, 0.9557],\n",
      "        [0.9151, 0.1413, 0.1238, 0.8936],\n",
      "        [0.8658, 0.0793, 0.0478, 0.9085],\n",
      "        [0.7647, 0.2651, 0.2205, 0.7564],\n",
      "        [0.8558, 0.1303, 0.1085, 0.8505],\n",
      "        [0.7796, 0.4515, 0.2591, 0.8102],\n",
      "        [0.7453, 0.3217, 0.2674, 0.8233],\n",
      "        [0.8971, 0.1260, 0.1769, 0.8859],\n",
      "        [0.8580, 0.1277, 0.0809, 0.8999],\n",
      "        [0.9157, 0.0591, 0.0460, 0.9369],\n",
      "        [0.7220, 0.3649, 0.3572, 0.7105],\n",
      "        [0.8989, 0.1264, 0.0924, 0.8693],\n",
      "        [0.8519, 0.2446, 0.1480, 0.8839],\n",
      "        [0.9414, 0.2470, 0.3269, 0.9682],\n",
      "        [0.8881, 0.1668, 0.1795, 0.8739],\n",
      "        [0.8580, 0.0900, 0.0533, 0.8296],\n",
      "        [0.8094, 0.3170, 0.2691, 0.8039],\n",
      "        [0.7855, 0.3417, 0.3288, 0.7643],\n",
      "        [0.9003, 0.0929, 0.0831, 0.9252],\n",
      "        [0.9707, 0.0225, 0.0109, 0.9921],\n",
      "        [0.9189, 0.0786, 0.0487, 0.9260],\n",
      "        [0.8577, 0.2310, 0.1960, 0.8608],\n",
      "        [0.7440, 0.3577, 0.3194, 0.8628],\n",
      "        [0.8784, 0.1389, 0.1160, 0.8643],\n",
      "        [0.8614, 0.1868, 0.1669, 0.8492],\n",
      "        [0.7148, 0.4861, 0.4856, 0.8197],\n",
      "        [0.8040, 0.2523, 0.2465, 0.7971],\n",
      "        [0.8436, 0.1367, 0.1089, 0.9070],\n",
      "        [0.9027, 0.0569, 0.0578, 0.9135],\n",
      "        [0.7869, 0.4242, 0.4359, 0.9098],\n",
      "        [0.8609, 0.2212, 0.1873, 0.8644],\n",
      "        [0.7221, 0.4967, 0.4028, 0.9123],\n",
      "        [0.7023, 0.4875, 0.3884, 0.7722],\n",
      "        [0.8013, 0.4171, 0.3724, 0.8869],\n",
      "        [0.8267, 0.3138, 0.2307, 0.8498],\n",
      "        [0.9176, 0.0536, 0.0519, 0.9464],\n",
      "        [0.8718, 0.1469, 0.1096, 0.8891],\n",
      "        [0.7346, 0.3745, 0.3876, 0.7825],\n",
      "        [0.8534, 0.1941, 0.1131, 0.8477],\n",
      "        [0.9474, 0.0867, 0.1498, 0.9680],\n",
      "        [0.9217, 0.0676, 0.0541, 0.8888],\n",
      "        [0.8536, 0.3022, 0.2420, 0.8712],\n",
      "        [0.8509, 0.2277, 0.0741, 0.8893],\n",
      "        [0.9285, 0.0232, 0.0221, 0.9473],\n",
      "        [0.8482, 0.2813, 0.3069, 0.9037],\n",
      "        [0.8899, 0.1617, 0.1100, 0.9019],\n",
      "        [0.9139, 0.0741, 0.0403, 0.9275],\n",
      "        [0.7356, 0.4208, 0.4051, 0.8528],\n",
      "        [0.9422, 0.0487, 0.0325, 0.9513],\n",
      "        [0.8818, 0.2218, 0.1858, 0.9432],\n",
      "        [0.7033, 0.4816, 0.4248, 0.8025],\n",
      "        [0.7581, 0.1780, 0.2221, 0.7826],\n",
      "        [0.8798, 0.4318, 0.2831, 0.9399],\n",
      "        [0.8841, 0.0495, 0.0530, 0.8607],\n",
      "        [0.6772, 0.4820, 0.3799, 0.8538],\n",
      "        [0.8411, 0.2824, 0.2079, 0.8626],\n",
      "        [0.8806, 0.0845, 0.0908, 0.8656],\n",
      "        [0.8231, 0.2316, 0.2164, 0.7812],\n",
      "        [0.8869, 0.2531, 0.2501, 0.9127],\n",
      "        [0.8748, 0.0444, 0.0427, 0.8944],\n",
      "        [0.8442, 0.1041, 0.1301, 0.8509],\n",
      "        [0.9107, 0.0726, 0.0968, 0.8568],\n",
      "        [0.9194, 0.0388, 0.0462, 0.8892],\n",
      "        [0.9198, 0.0635, 0.0507, 0.9147],\n",
      "        [0.8625, 0.1073, 0.0991, 0.8542],\n",
      "        [0.7311, 0.4010, 0.3661, 0.8592],\n",
      "        [0.8550, 0.1039, 0.0651, 0.8686],\n",
      "        [0.9018, 0.0922, 0.0631, 0.8851],\n",
      "        [0.8628, 0.3172, 0.3463, 0.9155],\n",
      "        [0.8845, 0.2408, 0.1584, 0.8867],\n",
      "        [0.8791, 0.1316, 0.1336, 0.8646],\n",
      "        [0.9136, 0.0562, 0.0449, 0.9156],\n",
      "        [0.8315, 0.1828, 0.1604, 0.8376],\n",
      "        [0.7963, 0.3500, 0.2064, 0.8151],\n",
      "        [0.6816, 0.4544, 0.4536, 0.7848],\n",
      "        [0.8316, 0.2558, 0.2797, 0.8684],\n",
      "        [0.8543, 0.2526, 0.2955, 0.8907],\n",
      "        [0.8428, 0.2087, 0.2398, 0.8019],\n",
      "        [0.8228, 0.1937, 0.1587, 0.8736],\n",
      "        [0.7648, 0.3159, 0.2638, 0.7439],\n",
      "        [0.9088, 0.0549, 0.0400, 0.9022],\n",
      "        [0.6415, 0.5120, 0.3157, 0.8428],\n",
      "        [0.9706, 0.0321, 0.0302, 0.9679],\n",
      "        [0.8165, 0.2584, 0.2644, 0.9148],\n",
      "        [0.8703, 0.0427, 0.0349, 0.9180],\n",
      "        [0.8550, 0.1472, 0.1681, 0.9071],\n",
      "        [0.9319, 0.0589, 0.0417, 0.9407],\n",
      "        [0.7844, 0.3546, 0.3675, 0.8099],\n",
      "        [0.7711, 0.2780, 0.3037, 0.7661],\n",
      "        [0.8895, 0.1302, 0.1054, 0.8363],\n",
      "        [0.8253, 0.3217, 0.3131, 0.8424],\n",
      "        [0.8203, 0.2597, 0.1479, 0.8049],\n",
      "        [0.8304, 0.1626, 0.2064, 0.8458],\n",
      "        [0.8778, 0.2791, 0.2564, 0.8370],\n",
      "        [0.8477, 0.1724, 0.1603, 0.8324],\n",
      "        [0.8668, 0.2801, 0.2258, 0.9064],\n",
      "        [0.7412, 0.3507, 0.2933, 0.8413],\n",
      "        [0.9709, 0.0621, 0.0973, 0.9728],\n",
      "        [0.8959, 0.1708, 0.2057, 0.8664],\n",
      "        [0.7764, 0.4029, 0.3298, 0.8602],\n",
      "        [0.8613, 0.2732, 0.2629, 0.8166],\n",
      "        [0.9796, 0.0251, 0.0177, 0.9849]], device='cuda:0',\n",
      "       grad_fn=<SigmoidBackward0>), '5%': tensor([[7.8785e-01, 9.8130e-02, 1.1015e-01, 8.7620e-01],\n",
      "        [9.1759e-01, 1.6811e-01, 1.1093e-01, 9.4980e-01],\n",
      "        [6.9998e-01, 3.0946e-01, 3.1398e-01, 8.5997e-01],\n",
      "        [6.7131e-01, 2.9316e-01, 1.9279e-01, 7.5597e-01],\n",
      "        [8.1091e-01, 1.6855e-01, 1.8952e-01, 8.2055e-01],\n",
      "        [8.7359e-01, 4.7937e-02, 2.6213e-02, 8.9586e-01],\n",
      "        [8.1260e-01, 3.1571e-01, 2.2756e-01, 7.9647e-01],\n",
      "        [8.5351e-01, 5.5956e-02, 3.7583e-02, 9.0192e-01],\n",
      "        [9.2468e-01, 1.1384e-02, 1.0938e-02, 9.4665e-01],\n",
      "        [8.6958e-01, 3.2686e-02, 3.1895e-02, 8.9017e-01],\n",
      "        [7.6765e-01, 1.3760e-01, 1.0278e-01, 8.5804e-01],\n",
      "        [8.3678e-01, 1.3582e-01, 2.2260e-01, 8.9074e-01],\n",
      "        [8.7332e-01, 8.5093e-02, 3.6912e-02, 9.1702e-01],\n",
      "        [8.6186e-01, 1.9558e-02, 1.9952e-02, 8.5530e-01],\n",
      "        [7.5300e-01, 2.6799e-01, 2.5454e-01, 8.3444e-01],\n",
      "        [8.7124e-01, 3.1604e-02, 4.6143e-02, 8.5447e-01],\n",
      "        [9.0281e-01, 3.8289e-02, 2.5560e-02, 9.6435e-01],\n",
      "        [8.1156e-01, 1.5692e-01, 1.2001e-01, 8.1286e-01],\n",
      "        [8.5089e-01, 7.4810e-02, 4.4208e-02, 9.0232e-01],\n",
      "        [7.8605e-01, 1.7625e-01, 1.2924e-01, 7.7253e-01],\n",
      "        [7.8586e-01, 2.2240e-01, 1.9195e-01, 7.7168e-01],\n",
      "        [8.4090e-01, 4.5323e-02, 4.4951e-02, 8.8657e-01],\n",
      "        [8.0088e-01, 3.1074e-01, 2.7515e-01, 8.2941e-01],\n",
      "        [7.2801e-01, 3.5559e-01, 1.9691e-01, 7.8679e-01],\n",
      "        [9.6874e-01, 7.3537e-04, 1.1809e-03, 9.9055e-01],\n",
      "        [7.6484e-01, 1.5962e-01, 1.9204e-01, 7.7257e-01],\n",
      "        [9.5874e-01, 4.2198e-03, 3.2062e-03, 9.6649e-01],\n",
      "        [8.6939e-01, 2.0144e-01, 1.8186e-01, 8.8494e-01],\n",
      "        [8.0192e-01, 6.0633e-02, 5.2549e-02, 8.2661e-01],\n",
      "        [8.9410e-01, 1.9661e-02, 1.6526e-02, 9.1183e-01],\n",
      "        [8.8862e-01, 3.4382e-02, 3.0294e-02, 9.1279e-01],\n",
      "        [8.0038e-01, 2.4509e-01, 2.3678e-01, 8.2575e-01],\n",
      "        [8.2507e-01, 4.8325e-02, 3.9115e-02, 8.9087e-01],\n",
      "        [7.4521e-01, 3.0358e-01, 2.8787e-01, 8.4504e-01],\n",
      "        [8.8992e-01, 2.2526e-02, 1.9911e-02, 9.0567e-01],\n",
      "        [7.7103e-01, 3.2296e-01, 4.2723e-01, 8.5708e-01],\n",
      "        [7.3734e-01, 3.1396e-01, 3.3307e-01, 7.7053e-01],\n",
      "        [8.0975e-01, 1.3691e-01, 9.2879e-02, 7.5409e-01],\n",
      "        [6.6697e-01, 4.2146e-01, 2.5955e-01, 7.1944e-01],\n",
      "        [9.1329e-01, 1.7574e-02, 2.0284e-02, 9.1961e-01],\n",
      "        [8.2094e-01, 1.6264e-01, 1.6381e-01, 8.6107e-01],\n",
      "        [8.3266e-01, 1.4263e-01, 1.3956e-01, 9.6208e-01],\n",
      "        [7.4653e-01, 3.3440e-01, 2.6924e-01, 8.3621e-01],\n",
      "        [8.7601e-01, 1.9987e-01, 2.4219e-01, 8.5911e-01],\n",
      "        [8.6092e-01, 3.7849e-02, 2.2777e-02, 9.1252e-01],\n",
      "        [9.1892e-01, 2.1486e-02, 2.8501e-02, 9.3042e-01],\n",
      "        [8.8115e-01, 9.9004e-02, 8.5844e-02, 8.8445e-01],\n",
      "        [8.6193e-01, 4.9669e-02, 2.1221e-02, 8.9406e-01],\n",
      "        [7.4036e-01, 2.1576e-01, 1.6919e-01, 7.0773e-01],\n",
      "        [8.3143e-01, 9.0028e-02, 6.1457e-02, 8.1553e-01],\n",
      "        [7.6683e-01, 3.5813e-01, 1.9926e-01, 7.7333e-01],\n",
      "        [7.1986e-01, 2.5541e-01, 2.2217e-01, 7.9125e-01],\n",
      "        [8.6254e-01, 7.3574e-02, 1.3068e-01, 8.3332e-01],\n",
      "        [8.1550e-01, 1.1764e-01, 5.8443e-02, 8.6001e-01],\n",
      "        [8.6772e-01, 3.7179e-02, 2.6375e-02, 8.9657e-01],\n",
      "        [7.2088e-01, 2.3081e-01, 2.6590e-01, 6.9849e-01],\n",
      "        [8.8140e-01, 7.2857e-02, 5.8083e-02, 8.3199e-01],\n",
      "        [7.8895e-01, 1.9005e-01, 1.0385e-01, 8.3814e-01],\n",
      "        [9.3028e-01, 1.0210e-01, 2.0173e-01, 9.6022e-01],\n",
      "        [8.5914e-01, 9.9838e-02, 1.2820e-01, 8.4698e-01],\n",
      "        [8.3065e-01, 6.0379e-02, 3.2839e-02, 8.1536e-01],\n",
      "        [7.8753e-01, 2.6345e-01, 2.1075e-01, 7.8547e-01],\n",
      "        [7.5930e-01, 2.8131e-01, 2.6628e-01, 7.5320e-01],\n",
      "        [8.3986e-01, 5.8430e-02, 6.8031e-02, 8.8418e-01],\n",
      "        [8.2007e-01, 9.4151e-03, 5.2668e-03, 9.4333e-01],\n",
      "        [8.7603e-01, 5.0983e-02, 2.8621e-02, 8.8590e-01],\n",
      "        [8.1807e-01, 1.7920e-01, 1.3740e-01, 8.1529e-01],\n",
      "        [7.1558e-01, 2.9887e-01, 2.3839e-01, 8.5236e-01],\n",
      "        [8.2296e-01, 8.9365e-02, 6.7656e-02, 8.2464e-01],\n",
      "        [8.0919e-01, 1.5393e-01, 1.2093e-01, 7.9806e-01],\n",
      "        [6.9187e-01, 4.3464e-01, 4.2019e-01, 7.9622e-01],\n",
      "        [7.6909e-01, 1.8525e-01, 2.0992e-01, 7.8202e-01],\n",
      "        [7.8598e-01, 9.6222e-02, 6.9082e-02, 8.8491e-01],\n",
      "        [8.5616e-01, 3.6585e-02, 2.9636e-02, 8.2656e-01],\n",
      "        [7.2985e-01, 3.4215e-01, 3.7608e-01, 8.8567e-01],\n",
      "        [8.4094e-01, 1.8529e-01, 1.1482e-01, 8.3542e-01],\n",
      "        [6.8757e-01, 3.8443e-01, 2.6945e-01, 8.7820e-01],\n",
      "        [6.6715e-01, 4.1577e-01, 3.2205e-01, 7.4989e-01],\n",
      "        [7.7437e-01, 3.7276e-01, 3.2964e-01, 8.6642e-01],\n",
      "        [8.0289e-01, 2.7966e-01, 1.7791e-01, 8.2347e-01],\n",
      "        [8.8023e-01, 3.4306e-02, 2.9493e-02, 9.2929e-01],\n",
      "        [8.3060e-01, 9.6289e-02, 6.9767e-02, 8.6955e-01],\n",
      "        [7.0596e-01, 3.1836e-01, 3.2653e-01, 7.2154e-01],\n",
      "        [7.9710e-01, 1.4107e-01, 6.5828e-02, 8.1057e-01],\n",
      "        [9.1676e-01, 3.4496e-02, 9.0468e-02, 9.4689e-01],\n",
      "        [8.6900e-01, 4.8154e-02, 3.2039e-02, 8.4105e-01],\n",
      "        [8.3121e-01, 2.0874e-01, 1.7410e-01, 8.5515e-01],\n",
      "        [8.2485e-01, 1.6200e-01, 5.3063e-02, 8.6994e-01],\n",
      "        [9.0393e-01, 1.6964e-02, 8.9300e-03, 9.0518e-01],\n",
      "        [8.3624e-01, 2.3282e-01, 2.0722e-01, 8.7946e-01],\n",
      "        [8.5455e-01, 1.1435e-01, 7.3962e-02, 8.4988e-01],\n",
      "        [8.5153e-01, 4.4844e-02, 2.9407e-02, 9.0408e-01],\n",
      "        [6.7469e-01, 3.4730e-01, 3.5045e-01, 8.2233e-01],\n",
      "        [9.0854e-01, 2.3961e-02, 1.8411e-02, 9.1154e-01],\n",
      "        [7.3527e-01, 1.5476e-01, 1.3425e-01, 8.7962e-01],\n",
      "        [6.6403e-01, 4.1405e-01, 3.5190e-01, 7.7961e-01],\n",
      "        [6.8302e-01, 1.3489e-01, 1.6548e-01, 7.5843e-01],\n",
      "        [8.3387e-01, 3.4163e-01, 1.9458e-01, 9.3537e-01],\n",
      "        [8.5968e-01, 3.4046e-02, 3.5399e-02, 8.0173e-01],\n",
      "        [6.3731e-01, 4.1783e-01, 3.1242e-01, 7.9593e-01],\n",
      "        [8.0565e-01, 2.1940e-01, 1.4222e-01, 8.5099e-01],\n",
      "        [8.2140e-01, 4.9461e-02, 5.6620e-02, 8.2270e-01],\n",
      "        [8.0777e-01, 1.9780e-01, 1.6128e-01, 7.1809e-01],\n",
      "        [8.4381e-01, 1.8183e-01, 1.6324e-01, 8.7631e-01],\n",
      "        [8.6748e-01, 1.8490e-02, 1.8605e-02, 8.7284e-01],\n",
      "        [7.9564e-01, 7.3853e-02, 8.1171e-02, 8.1419e-01],\n",
      "        [8.9780e-01, 4.8968e-02, 7.1269e-02, 8.3981e-01],\n",
      "        [8.8136e-01, 2.0806e-02, 2.8077e-02, 8.4973e-01],\n",
      "        [8.7987e-01, 3.8386e-02, 2.7439e-02, 8.8978e-01],\n",
      "        [8.4247e-01, 8.1925e-02, 6.0297e-02, 8.2502e-01],\n",
      "        [6.8323e-01, 3.2207e-01, 3.0960e-01, 8.1611e-01],\n",
      "        [7.8849e-01, 5.7530e-02, 3.9723e-02, 8.3143e-01],\n",
      "        [8.5682e-01, 7.3982e-02, 4.3663e-02, 8.6380e-01],\n",
      "        [8.4746e-01, 2.5954e-01, 2.4365e-01, 8.8617e-01],\n",
      "        [8.7907e-01, 1.8816e-01, 1.1613e-01, 8.5993e-01],\n",
      "        [8.3648e-01, 7.8789e-02, 8.3426e-02, 8.1837e-01],\n",
      "        [8.7312e-01, 3.4740e-02, 2.9537e-02, 8.5553e-01],\n",
      "        [7.9507e-01, 9.9055e-02, 1.1781e-01, 8.0830e-01],\n",
      "        [7.6293e-01, 2.7972e-01, 1.6028e-01, 7.8731e-01],\n",
      "        [6.4795e-01, 4.1405e-01, 3.9567e-01, 7.5311e-01],\n",
      "        [8.1000e-01, 1.9892e-01, 2.2895e-01, 8.2972e-01],\n",
      "        [8.0704e-01, 2.0632e-01, 2.7499e-01, 8.6129e-01],\n",
      "        [7.9372e-01, 1.7652e-01, 2.1280e-01, 7.6911e-01],\n",
      "        [7.8955e-01, 1.4023e-01, 1.1137e-01, 8.3390e-01],\n",
      "        [7.4911e-01, 2.7092e-01, 2.0783e-01, 7.1887e-01],\n",
      "        [8.7935e-01, 3.7334e-02, 2.1396e-02, 8.7279e-01],\n",
      "        [6.0544e-01, 4.3975e-01, 2.1742e-01, 8.1948e-01],\n",
      "        [9.4318e-01, 1.1216e-02, 1.2524e-02, 9.5055e-01],\n",
      "        [7.5687e-01, 2.0821e-01, 1.9097e-01, 8.9566e-01],\n",
      "        [8.2201e-01, 1.7723e-02, 2.4075e-02, 8.9154e-01],\n",
      "        [7.9291e-01, 1.1991e-01, 1.3285e-01, 8.8225e-01],\n",
      "        [8.8422e-01, 3.2312e-02, 2.3457e-02, 9.2298e-01],\n",
      "        [7.7114e-01, 2.9396e-01, 3.0010e-01, 7.8436e-01],\n",
      "        [7.3797e-01, 2.3657e-01, 2.3834e-01, 7.2250e-01],\n",
      "        [8.6272e-01, 8.6119e-02, 5.7947e-02, 7.8656e-01],\n",
      "        [7.9440e-01, 2.5614e-01, 2.5749e-01, 7.9550e-01],\n",
      "        [7.8407e-01, 1.9086e-01, 1.0232e-01, 7.4888e-01],\n",
      "        [8.0444e-01, 1.2422e-01, 1.4794e-01, 8.3099e-01],\n",
      "        [8.3472e-01, 2.2239e-01, 1.9310e-01, 7.9456e-01],\n",
      "        [8.1068e-01, 1.2995e-01, 9.7927e-02, 8.0018e-01],\n",
      "        [8.4618e-01, 2.2182e-01, 1.8768e-01, 8.8440e-01],\n",
      "        [6.9849e-01, 2.8035e-01, 2.1424e-01, 8.2055e-01],\n",
      "        [8.2615e-01, 2.3596e-02, 6.4031e-02, 7.4992e-01],\n",
      "        [8.2439e-01, 1.1851e-01, 1.4898e-01, 8.4266e-01],\n",
      "        [7.5503e-01, 3.1476e-01, 2.6051e-01, 8.1728e-01],\n",
      "        [8.1931e-01, 2.1258e-01, 2.1689e-01, 7.9055e-01],\n",
      "        [9.7212e-01, 5.3548e-03, 4.6556e-03, 9.7677e-01]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>), '95%': tensor([[0.8661, 0.1831, 0.2494, 0.9421],\n",
      "        [0.9622, 0.3810, 0.2715, 0.9840],\n",
      "        [0.8188, 0.4857, 0.4524, 0.9532],\n",
      "        [0.8046, 0.3708, 0.2849, 0.8464],\n",
      "        [0.8837, 0.2404, 0.2820, 0.8903],\n",
      "        [0.9526, 0.1113, 0.0706, 0.9638],\n",
      "        [0.8701, 0.3944, 0.3858, 0.8480],\n",
      "        [0.9478, 0.1170, 0.0872, 0.9553],\n",
      "        [0.9782, 0.0553, 0.0383, 0.9832],\n",
      "        [0.9466, 0.0949, 0.0742, 0.9484],\n",
      "        [0.8693, 0.2576, 0.2171, 0.9168],\n",
      "        [0.9243, 0.2354, 0.3546, 0.9421],\n",
      "        [0.9540, 0.1424, 0.0894, 0.9761],\n",
      "        [0.9599, 0.0460, 0.0642, 0.9432],\n",
      "        [0.8375, 0.3883, 0.3674, 0.8944],\n",
      "        [0.9420, 0.0922, 0.1126, 0.9153],\n",
      "        [0.9753, 0.1561, 0.0911, 0.9910],\n",
      "        [0.8892, 0.2539, 0.2097, 0.8974],\n",
      "        [0.9275, 0.1556, 0.1250, 0.9439],\n",
      "        [0.8730, 0.3350, 0.2406, 0.8602],\n",
      "        [0.8855, 0.3545, 0.3031, 0.8442],\n",
      "        [0.9347, 0.1214, 0.0935, 0.9441],\n",
      "        [0.8550, 0.4163, 0.3940, 0.8955],\n",
      "        [0.8018, 0.4800, 0.3288, 0.8643],\n",
      "        [0.9975, 0.0120, 0.0139, 0.9991],\n",
      "        [0.8881, 0.2435, 0.2724, 0.8789],\n",
      "        [0.9933, 0.0274, 0.0228, 0.9985],\n",
      "        [0.9512, 0.3094, 0.2838, 0.9550],\n",
      "        [0.9023, 0.1168, 0.1357, 0.9241],\n",
      "        [0.9549, 0.0544, 0.0507, 0.9582],\n",
      "        [0.9486, 0.1070, 0.0663, 0.9629],\n",
      "        [0.8851, 0.3196, 0.3366, 0.9033],\n",
      "        [0.9186, 0.1240, 0.0856, 0.9428],\n",
      "        [0.8127, 0.3841, 0.3920, 0.9031],\n",
      "        [0.9489, 0.0542, 0.0609, 0.9702],\n",
      "        [0.8647, 0.4526, 0.5431, 0.9296],\n",
      "        [0.7867, 0.4016, 0.4307, 0.8522],\n",
      "        [0.8850, 0.2009, 0.1638, 0.8336],\n",
      "        [0.7486, 0.5281, 0.3625, 0.8325],\n",
      "        [0.9619, 0.0440, 0.0492, 0.9642],\n",
      "        [0.8898, 0.2915, 0.2578, 0.9127],\n",
      "        [0.9425, 0.3165, 0.2986, 0.9866],\n",
      "        [0.8299, 0.4150, 0.3698, 0.8777],\n",
      "        [0.9455, 0.3191, 0.3494, 0.9296],\n",
      "        [0.9617, 0.0854, 0.0790, 0.9610],\n",
      "        [0.9759, 0.1044, 0.0759, 0.9778],\n",
      "        [0.9493, 0.1493, 0.1681, 0.9300],\n",
      "        [0.9152, 0.1169, 0.0718, 0.9382],\n",
      "        [0.8204, 0.2923, 0.2488, 0.7978],\n",
      "        [0.9048, 0.2011, 0.1490, 0.9031],\n",
      "        [0.8524, 0.4829, 0.2867, 0.8523],\n",
      "        [0.7983, 0.3441, 0.3214, 0.8723],\n",
      "        [0.9287, 0.1932, 0.2240, 0.9278],\n",
      "        [0.8951, 0.1609, 0.1132, 0.9244],\n",
      "        [0.9495, 0.1058, 0.0825, 0.9567],\n",
      "        [0.8078, 0.4051, 0.3949, 0.8009],\n",
      "        [0.9511, 0.1561, 0.1406, 0.9072],\n",
      "        [0.9051, 0.3169, 0.1792, 0.9212],\n",
      "        [0.9651, 0.2743, 0.4572, 0.9917],\n",
      "        [0.9314, 0.2172, 0.2345, 0.9232],\n",
      "        [0.9077, 0.1247, 0.0786, 0.8838],\n",
      "        [0.8659, 0.3459, 0.2899, 0.8552],\n",
      "        [0.8320, 0.3818, 0.3729, 0.8157],\n",
      "        [0.9243, 0.1436, 0.1358, 0.9446],\n",
      "        [0.9879, 0.1286, 0.0440, 0.9977],\n",
      "        [0.9381, 0.1108, 0.0694, 0.9478],\n",
      "        [0.8914, 0.2477, 0.2264, 0.8995],\n",
      "        [0.8162, 0.4158, 0.3700, 0.9012],\n",
      "        [0.9059, 0.1906, 0.1504, 0.9078],\n",
      "        [0.9042, 0.2309, 0.2175, 0.9079],\n",
      "        [0.7609, 0.5120, 0.4981, 0.8661],\n",
      "        [0.8456, 0.3016, 0.2773, 0.8526],\n",
      "        [0.8775, 0.2199, 0.1700, 0.9468],\n",
      "        [0.9286, 0.1100, 0.0854, 0.9430],\n",
      "        [0.8382, 0.4798, 0.4745, 0.9434],\n",
      "        [0.8915, 0.2603, 0.2288, 0.9150],\n",
      "        [0.8137, 0.5635, 0.4728, 0.9420],\n",
      "        [0.7416, 0.5333, 0.4444, 0.8271],\n",
      "        [0.8712, 0.4619, 0.4310, 0.9377],\n",
      "        [0.8680, 0.3501, 0.2619, 0.8890],\n",
      "        [0.9643, 0.0894, 0.0832, 0.9693],\n",
      "        [0.9185, 0.2042, 0.1695, 0.9408],\n",
      "        [0.7584, 0.4255, 0.4156, 0.8336],\n",
      "        [0.8959, 0.2358, 0.1436, 0.8899],\n",
      "        [0.9767, 0.1045, 0.1649, 0.9827],\n",
      "        [0.9567, 0.1066, 0.0914, 0.9341],\n",
      "        [0.9125, 0.3750, 0.2468, 0.9330],\n",
      "        [0.9077, 0.2914, 0.0875, 0.9252],\n",
      "        [0.9585, 0.0473, 0.0422, 0.9670],\n",
      "        [0.8843, 0.3222, 0.3811, 0.9308],\n",
      "        [0.9243, 0.2172, 0.1749, 0.9299],\n",
      "        [0.9515, 0.0831, 0.0579, 0.9488],\n",
      "        [0.7758, 0.4688, 0.4715, 0.9057],\n",
      "        [0.9642, 0.1326, 0.0474, 0.9681],\n",
      "        [0.9442, 0.4103, 0.3015, 0.9709],\n",
      "        [0.7507, 0.5060, 0.4218, 0.8453],\n",
      "        [0.8056, 0.2061, 0.2553, 0.8344],\n",
      "        [0.9243, 0.5415, 0.3080, 0.9693],\n",
      "        [0.9322, 0.0758, 0.0916, 0.9036],\n",
      "        [0.7307, 0.5584, 0.4229, 0.9013],\n",
      "        [0.8985, 0.3253, 0.2977, 0.9103],\n",
      "        [0.9201, 0.1130, 0.1220, 0.9167],\n",
      "        [0.8664, 0.2475, 0.2566, 0.8542],\n",
      "        [0.9222, 0.3252, 0.3046, 0.9480],\n",
      "        [0.9275, 0.0578, 0.0552, 0.9481],\n",
      "        [0.8819, 0.1262, 0.1787, 0.9012],\n",
      "        [0.9506, 0.1181, 0.1177, 0.9049],\n",
      "        [0.9450, 0.0582, 0.0712, 0.9386],\n",
      "        [0.9479, 0.0923, 0.0669, 0.9491],\n",
      "        [0.8974, 0.1497, 0.1249, 0.9046],\n",
      "        [0.7807, 0.4405, 0.3965, 0.8983],\n",
      "        [0.9127, 0.1286, 0.0809, 0.9091],\n",
      "        [0.9277, 0.1418, 0.1171, 0.9048],\n",
      "        [0.8898, 0.3516, 0.3836, 0.9442],\n",
      "        [0.9264, 0.2942, 0.2164, 0.9288],\n",
      "        [0.9170, 0.1677, 0.1540, 0.9060],\n",
      "        [0.9445, 0.1054, 0.0915, 0.9450],\n",
      "        [0.8741, 0.2360, 0.2114, 0.8838],\n",
      "        [0.8726, 0.3755, 0.2350, 0.8832],\n",
      "        [0.7170, 0.4755, 0.4885, 0.8198],\n",
      "        [0.8849, 0.3010, 0.3653, 0.9208],\n",
      "        [0.8937, 0.3039, 0.3668, 0.9283],\n",
      "        [0.8659, 0.2665, 0.2842, 0.8379],\n",
      "        [0.8824, 0.2118, 0.1960, 0.8911],\n",
      "        [0.8088, 0.3410, 0.2890, 0.7801],\n",
      "        [0.9436, 0.0816, 0.0591, 0.9396],\n",
      "        [0.7145, 0.5287, 0.3367, 0.8906],\n",
      "        [0.9866, 0.0628, 0.0495, 0.9787],\n",
      "        [0.8979, 0.3933, 0.3365, 0.9494],\n",
      "        [0.9255, 0.0663, 0.0469, 0.9473],\n",
      "        [0.8981, 0.1995, 0.2158, 0.9414],\n",
      "        [0.9528, 0.0836, 0.0566, 0.9748],\n",
      "        [0.8317, 0.4161, 0.4188, 0.8501],\n",
      "        [0.7925, 0.3061, 0.3177, 0.8001],\n",
      "        [0.9305, 0.1767, 0.1600, 0.8908],\n",
      "        [0.8620, 0.3441, 0.3398, 0.8811],\n",
      "        [0.8831, 0.2771, 0.1945, 0.8734],\n",
      "        [0.8882, 0.2254, 0.2214, 0.8994],\n",
      "        [0.9336, 0.3153, 0.3187, 0.9080],\n",
      "        [0.8922, 0.2085, 0.1852, 0.8669],\n",
      "        [0.9129, 0.3160, 0.3165, 0.9449],\n",
      "        [0.7780, 0.3704, 0.3559, 0.8736],\n",
      "        [0.9886, 0.3281, 0.3515, 0.9924],\n",
      "        [0.9416, 0.2164, 0.2500, 0.9288],\n",
      "        [0.8520, 0.4456, 0.3967, 0.9078],\n",
      "        [0.8770, 0.3332, 0.2951, 0.8672],\n",
      "        [0.9944, 0.0375, 0.0260, 0.9963]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Overall Survival (4 Years)': {'accuracy': 0.891156462585034,\n",
       "  'mse': 0.105073795,\n",
       "  'auc': 0.48377862595419846,\n",
       "  'precision': 0.891156462585034,\n",
       "  'recall': 1.0,\n",
       "  'f1': 0.9424460431654677},\n",
       " 'FT': {'accuracy': 0.782312925170068,\n",
       "  'mse': 0.1599512,\n",
       "  'auc': 0.6741847826086957,\n",
       "  'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1': 0.0},\n",
       " 'Aspiration rate Post-therapy': {'accuracy': 0.8231292517006803,\n",
       "  'mse': 0.13265932,\n",
       "  'auc': 0.7380801017164653,\n",
       "  'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1': 0.0},\n",
       " 'LRC': {'accuracy': 0.8979591836734694,\n",
       "  'mse': 0.097782046,\n",
       "  'auc': 0.4954545454545455,\n",
       "  'precision': 0.8979591836734694,\n",
       "  'recall': 1.0,\n",
       "  'f1': 0.9462365591397849}}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmodel3 = train_state(model_args=t1_args,state=3,lr=.001,weights=[1,1,1,1],use_smote=False)\n",
    "tmodel3[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "c2998f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/imblearn/over_sampling/_smote/base.py:345: FutureWarning: The parameter `n_jobs` has been deprecated in 0.10 and will be removed in 0.12. You can pass an nearest neighbors estimator where `n_jobs` is already set instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I dont do balancing on the outputs because Idk how that would work\n",
      "epoch 0 train loss 9.096540451049805\n",
      "val loss 6.9575395584106445\n",
      "______________\n",
      "epoch 1 train loss 7.4508280754089355\n",
      "val loss 6.033664226531982\n",
      "______________\n",
      "epoch 2 train loss 6.905258655548096\n",
      "val loss 5.752789497375488\n",
      "______________\n",
      "epoch 3 train loss 6.94904899597168\n",
      "val loss 5.625213146209717\n",
      "______________\n",
      "epoch 4 train loss 6.906423091888428\n",
      "val loss 5.48731803894043\n",
      "______________\n",
      "epoch 5 train loss 6.657055854797363\n",
      "val loss 5.3524699211120605\n",
      "______________\n",
      "epoch 6 train loss 6.363465785980225\n",
      "val loss 5.258492946624756\n",
      "______________\n",
      "epoch 7 train loss 6.142160892486572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 5.21937894821167\n",
      "______________\n",
      "epoch 8 train loss 5.937976837158203\n",
      "val loss 5.228362560272217\n",
      "______________\n",
      "epoch 9 train loss 5.860049247741699\n",
      "val loss 5.259349346160889\n",
      "______________\n",
      "epoch 10 train loss 5.783250331878662\n",
      "val loss 5.28120231628418\n",
      "______________\n",
      "epoch 11 train loss 5.726436614990234\n",
      "val loss 5.272973537445068\n",
      "______________\n",
      "epoch 12 train loss 5.666965961456299\n",
      "val loss 5.2347588539123535\n",
      "______________\n",
      "epoch 13 train loss 5.629974365234375\n",
      "val loss 5.181970119476318\n",
      "______________\n",
      "epoch 14 train loss 5.490771293640137\n",
      "val loss 5.133671760559082\n",
      "______________\n",
      "epoch 15 train loss 5.406649589538574\n",
      "val loss 5.10526180267334\n",
      "______________\n",
      "epoch 16 train loss 5.335970401763916\n",
      "val loss 5.102331638336182\n",
      "______________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17 train loss 5.260529518127441\n",
      "val loss 5.1225457191467285\n",
      "______________\n",
      "epoch 18 train loss 5.2259955406188965\n",
      "val loss 5.152615070343018\n",
      "______________\n",
      "epoch 19 train loss 5.126910209655762\n",
      "val loss 5.181323528289795\n",
      "______________\n",
      "epoch 20 train loss 5.053218841552734\n",
      "val loss 5.20449161529541\n",
      "______________\n",
      "epoch 21 train loss 5.010659694671631\n",
      "val loss 5.222507953643799\n",
      "______________\n",
      "epoch 22 train loss 4.898163318634033\n",
      "val loss 5.242443561553955\n",
      "______________\n",
      "epoch 23 train loss 4.846039772033691\n",
      "val loss 5.267419815063477\n",
      "______________\n",
      "epoch 24 train loss 4.792029857635498\n",
      "val loss 5.2996826171875\n",
      "______________\n",
      "epoch 25 train loss 4.725754737854004\n",
      "val loss 5.33983850479126\n",
      "______________\n",
      "epoch 26 train loss 4.659574508666992\n",
      "val loss 5.38545036315918\n",
      "______________\n",
      "epoch 27 train loss 4.556998252868652\n",
      "val loss 5.439597129821777\n",
      "______________\n",
      "best loss 5.102331638336182 {'Overall Survival (4 Years)': {'accuracy': 0.8843537414965986, 'mse': 0.105534956, 'auc': 0.5333969465648855, 'precision': 0.8904109589041096, 'recall': 0.9923664122137404, 'f1': 0.9386281588447654}, 'FT': {'accuracy': 0.782312925170068, 'mse': 0.17532517, 'auc': 0.7209239130434782, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'Aspiration rate Post-therapy': {'accuracy': 0.8231292517006803, 'mse': 0.13865589, 'auc': 0.748569612205976, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'LRC': {'accuracy': 0.8979591836734694, 'mse': 0.091692045, 'auc': 0.6499999999999999, 'precision': 0.8979591836734694, 'recall': 1.0, 'f1': 0.9462365591397849}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': tensor([[0.8421, 0.0813, 0.1027, 0.8640],\n",
      "        [0.9315, 0.0213, 0.0360, 0.9410],\n",
      "        [0.6280, 0.2570, 0.2654, 0.7650],\n",
      "        [0.6173, 0.2341, 0.2572, 0.6423],\n",
      "        [0.9012, 0.1070, 0.1101, 0.8665],\n",
      "        [0.9461, 0.0406, 0.0430, 0.9219],\n",
      "        [0.8497, 0.1185, 0.1537, 0.8606],\n",
      "        [0.9367, 0.0522, 0.0525, 0.9153],\n",
      "        [0.9895, 0.0109, 0.0127, 0.9766],\n",
      "        [0.9384, 0.0436, 0.0480, 0.8886],\n",
      "        [0.8917, 0.0927, 0.0986, 0.8838],\n",
      "        [0.8529, 0.1013, 0.1296, 0.8567],\n",
      "        [0.9597, 0.0426, 0.0396, 0.9461],\n",
      "        [0.9672, 0.0217, 0.0224, 0.9174],\n",
      "        [0.8346, 0.1594, 0.1665, 0.8495],\n",
      "        [0.9590, 0.0309, 0.0334, 0.8981],\n",
      "        [0.9404, 0.0248, 0.0303, 0.9664],\n",
      "        [0.9044, 0.0866, 0.0915, 0.8789],\n",
      "        [0.9188, 0.0654, 0.0710, 0.8987],\n",
      "        [0.8320, 0.1099, 0.1106, 0.8114],\n",
      "        [0.8718, 0.1375, 0.1436, 0.8535],\n",
      "        [0.9341, 0.0506, 0.0465, 0.8806],\n",
      "        [0.9272, 0.0956, 0.0991, 0.9126],\n",
      "        [0.5777, 0.2462, 0.2431, 0.6973],\n",
      "        [0.9972, 0.0018, 0.0021, 0.9927],\n",
      "        [0.7998, 0.1443, 0.1498, 0.7785],\n",
      "        [0.9785, 0.0101, 0.0104, 0.9714],\n",
      "        [0.9777, 0.0230, 0.0305, 0.9708],\n",
      "        [0.8749, 0.0899, 0.0962, 0.8403],\n",
      "        [0.9746, 0.0223, 0.0236, 0.9397],\n",
      "        [0.9483, 0.0405, 0.0421, 0.9264],\n",
      "        [0.9319, 0.0821, 0.0857, 0.9167],\n",
      "        [0.8822, 0.0746, 0.0770, 0.8621],\n",
      "        [0.8360, 0.1293, 0.1617, 0.8289],\n",
      "        [0.9639, 0.0283, 0.0297, 0.9381],\n",
      "        [0.7411, 0.1852, 0.2196, 0.7514],\n",
      "        [0.8272, 0.1574, 0.1826, 0.8330],\n",
      "        [0.7984, 0.1258, 0.1278, 0.7600],\n",
      "        [0.5367, 0.3311, 0.3352, 0.6666],\n",
      "        [0.9759, 0.0200, 0.0217, 0.9486],\n",
      "        [0.9113, 0.0753, 0.0863, 0.8952],\n",
      "        [0.8842, 0.0398, 0.0491, 0.9540],\n",
      "        [0.7888, 0.1791, 0.1839, 0.7982],\n",
      "        [0.7914, 0.1176, 0.1558, 0.7765],\n",
      "        [0.9426, 0.0422, 0.0393, 0.9054],\n",
      "        [0.9402, 0.0278, 0.0377, 0.9201],\n",
      "        [0.9491, 0.0556, 0.0522, 0.9312],\n",
      "        [0.9163, 0.0503, 0.0550, 0.8940],\n",
      "        [0.7861, 0.1557, 0.1601, 0.7575],\n",
      "        [0.8287, 0.1087, 0.1042, 0.7882],\n",
      "        [0.7474, 0.1780, 0.1945, 0.7632],\n",
      "        [0.8080, 0.1544, 0.1560, 0.7943],\n",
      "        [0.9547, 0.0459, 0.0619, 0.9245],\n",
      "        [0.8467, 0.0918, 0.0965, 0.8288],\n",
      "        [0.9467, 0.0383, 0.0421, 0.9183],\n",
      "        [0.5879, 0.2594, 0.2870, 0.6503],\n",
      "        [0.9132, 0.0607, 0.0711, 0.8807],\n",
      "        [0.8507, 0.1078, 0.1114, 0.8546],\n",
      "        [0.9571, 0.0263, 0.0507, 0.9581],\n",
      "        [0.9545, 0.0492, 0.0525, 0.9171],\n",
      "        [0.8426, 0.0818, 0.0815, 0.7730],\n",
      "        [0.8929, 0.1179, 0.1247, 0.8712],\n",
      "        [0.8044, 0.1517, 0.1787, 0.7749],\n",
      "        [0.9031, 0.0555, 0.0707, 0.8747],\n",
      "        [0.9415, 0.0198, 0.0244, 0.9544],\n",
      "        [0.9684, 0.0267, 0.0254, 0.9423],\n",
      "        [0.9335, 0.0606, 0.0604, 0.8839],\n",
      "        [0.7608, 0.1769, 0.1853, 0.7928],\n",
      "        [0.8771, 0.0924, 0.0963, 0.8481],\n",
      "        [0.8610, 0.1098, 0.1225, 0.8370],\n",
      "        [0.6542, 0.3110, 0.3210, 0.7260],\n",
      "        [0.8733, 0.1122, 0.1092, 0.8220],\n",
      "        [0.8819, 0.0928, 0.0986, 0.8781],\n",
      "        [0.9680, 0.0287, 0.0330, 0.9366],\n",
      "        [0.8452, 0.1779, 0.1682, 0.8770],\n",
      "        [0.9059, 0.0601, 0.0719, 0.9021],\n",
      "        [0.5959, 0.2956, 0.2875, 0.7384],\n",
      "        [0.5946, 0.2743, 0.2662, 0.6857],\n",
      "        [0.7785, 0.1757, 0.1910, 0.8338],\n",
      "        [0.8833, 0.1144, 0.1148, 0.8551],\n",
      "        [0.9702, 0.0271, 0.0285, 0.9453],\n",
      "        [0.9087, 0.0695, 0.0671, 0.8887],\n",
      "        [0.6288, 0.2852, 0.2904, 0.6905],\n",
      "        [0.8663, 0.0951, 0.0952, 0.8415],\n",
      "        [0.9797, 0.0132, 0.0242, 0.9730],\n",
      "        [0.9543, 0.0322, 0.0317, 0.9065],\n",
      "        [0.9010, 0.0923, 0.0932, 0.8864],\n",
      "        [0.8961, 0.0581, 0.0561, 0.8792],\n",
      "        [0.9719, 0.0232, 0.0235, 0.9369],\n",
      "        [0.8482, 0.1109, 0.1320, 0.8509],\n",
      "        [0.8957, 0.0782, 0.0898, 0.8759],\n",
      "        [0.9512, 0.0354, 0.0326, 0.9276],\n",
      "        [0.6466, 0.3037, 0.3147, 0.7479],\n",
      "        [0.9681, 0.0258, 0.0267, 0.9394],\n",
      "        [0.8975, 0.0582, 0.0844, 0.9065],\n",
      "        [0.5737, 0.3012, 0.3076, 0.6524],\n",
      "        [0.7510, 0.1509, 0.1730, 0.7369],\n",
      "        [0.7856, 0.1493, 0.1583, 0.8146],\n",
      "        [0.9395, 0.0384, 0.0403, 0.8739],\n",
      "        [0.5104, 0.2891, 0.2805, 0.6763],\n",
      "        [0.8504, 0.0861, 0.0950, 0.8529],\n",
      "        [0.9509, 0.0392, 0.0446, 0.9058],\n",
      "        [0.8336, 0.1386, 0.1435, 0.7849],\n",
      "        [0.7812, 0.1275, 0.1554, 0.7786],\n",
      "        [0.9287, 0.0518, 0.0508, 0.8744],\n",
      "        [0.9462, 0.0512, 0.0550, 0.9126],\n",
      "        [0.9728, 0.0249, 0.0285, 0.9290],\n",
      "        [0.9658, 0.0251, 0.0271, 0.9275],\n",
      "        [0.9323, 0.0304, 0.0471, 0.9047],\n",
      "        [0.8274, 0.1023, 0.1066, 0.7965],\n",
      "        [0.5662, 0.3108, 0.3037, 0.7216],\n",
      "        [0.8464, 0.0912, 0.0931, 0.8021],\n",
      "        [0.9162, 0.0603, 0.0595, 0.8531],\n",
      "        [0.9075, 0.1087, 0.1128, 0.9100],\n",
      "        [0.9497, 0.0504, 0.0516, 0.9290],\n",
      "        [0.8853, 0.0904, 0.1003, 0.8530],\n",
      "        [0.9326, 0.0465, 0.0481, 0.8914],\n",
      "        [0.8494, 0.1178, 0.1128, 0.8096],\n",
      "        [0.7638, 0.1623, 0.1801, 0.8038],\n",
      "        [0.5605, 0.3414, 0.3503, 0.6735],\n",
      "        [0.8072, 0.1351, 0.1518, 0.8312],\n",
      "        [0.9114, 0.1010, 0.1147, 0.9075],\n",
      "        [0.8974, 0.0879, 0.0918, 0.8410],\n",
      "        [0.8472, 0.1144, 0.1172, 0.8229],\n",
      "        [0.7550, 0.1790, 0.1802, 0.7380],\n",
      "        [0.9296, 0.0455, 0.0456, 0.8770],\n",
      "        [0.5249, 0.3133, 0.3186, 0.7385],\n",
      "        [0.9832, 0.0126, 0.0149, 0.9615],\n",
      "        [0.8046, 0.1161, 0.1460, 0.8586],\n",
      "        [0.8918, 0.0699, 0.0690, 0.8620],\n",
      "        [0.8577, 0.0879, 0.1076, 0.8674],\n",
      "        [0.9190, 0.0494, 0.0517, 0.8840],\n",
      "        [0.8473, 0.1590, 0.1725, 0.8459],\n",
      "        [0.7658, 0.1677, 0.1921, 0.7508],\n",
      "        [0.9044, 0.0670, 0.0712, 0.8358],\n",
      "        [0.9240, 0.0987, 0.1008, 0.8922],\n",
      "        [0.8456, 0.1307, 0.1248, 0.8572],\n",
      "        [0.9194, 0.0706, 0.0904, 0.8903],\n",
      "        [0.9592, 0.0530, 0.0535, 0.9157],\n",
      "        [0.8765, 0.1053, 0.1029, 0.8223],\n",
      "        [0.8730, 0.1086, 0.1224, 0.8712],\n",
      "        [0.7347, 0.2084, 0.2252, 0.7572],\n",
      "        [0.8120, 0.0520, 0.0835, 0.8411],\n",
      "        [0.9027, 0.0483, 0.0701, 0.8698],\n",
      "        [0.6169, 0.2012, 0.2513, 0.7302],\n",
      "        [0.9180, 0.0873, 0.0972, 0.8837],\n",
      "        [0.9965, 0.0015, 0.0017, 0.9932]], device='cuda:0',\n",
      "       grad_fn=<SigmoidBackward0>), '5%': tensor([[8.4104e-01, 6.0746e-02, 6.1764e-02, 8.5652e-01],\n",
      "        [9.0406e-01, 5.3509e-03, 1.0927e-02, 9.2201e-01],\n",
      "        [6.1301e-01, 1.6132e-01, 1.7734e-01, 7.3553e-01],\n",
      "        [5.9719e-01, 1.9175e-01, 2.2013e-01, 5.9094e-01],\n",
      "        [8.7176e-01, 7.9251e-02, 7.4776e-02, 8.3227e-01],\n",
      "        [9.2716e-01, 1.9785e-02, 2.1800e-02, 9.0946e-01],\n",
      "        [8.0170e-01, 8.9831e-02, 1.0519e-01, 8.0575e-01],\n",
      "        [9.1215e-01, 3.4375e-02, 3.4062e-02, 8.8098e-01],\n",
      "        [9.8663e-01, 5.0913e-03, 6.8880e-03, 9.6923e-01],\n",
      "        [9.1778e-01, 1.6128e-02, 2.3324e-02, 8.5950e-01],\n",
      "        [8.7577e-01, 4.7413e-02, 7.0873e-02, 8.5169e-01],\n",
      "        [8.2763e-01, 5.9704e-02, 8.8828e-02, 8.1165e-01],\n",
      "        [9.4367e-01, 2.3267e-02, 2.1453e-02, 9.2560e-01],\n",
      "        [9.4685e-01, 1.1271e-02, 1.2972e-02, 8.6406e-01],\n",
      "        [8.1544e-01, 1.1598e-01, 1.1706e-01, 8.2034e-01],\n",
      "        [9.3772e-01, 1.8343e-02, 2.4619e-02, 8.6758e-01],\n",
      "        [7.3908e-01, 1.0759e-02, 1.4425e-02, 8.4183e-01],\n",
      "        [8.4524e-01, 5.1581e-02, 6.1509e-02, 8.2768e-01],\n",
      "        [8.8139e-01, 3.7827e-02, 4.3220e-02, 8.4781e-01],\n",
      "        [8.0915e-01, 6.8851e-02, 8.0377e-02, 7.7055e-01],\n",
      "        [8.3194e-01, 8.2637e-02, 8.3553e-02, 7.9315e-01],\n",
      "        [9.1101e-01, 2.7840e-02, 2.4797e-02, 8.6905e-01],\n",
      "        [9.1482e-01, 6.2474e-02, 6.6311e-02, 8.9225e-01],\n",
      "        [5.6197e-01, 1.7203e-01, 1.7563e-01, 6.7001e-01],\n",
      "        [9.9098e-01, 3.9289e-04, 1.0024e-03, 9.5912e-01],\n",
      "        [7.8898e-01, 9.7215e-02, 9.3135e-02, 7.7239e-01],\n",
      "        [9.7274e-01, 3.4138e-03, 4.7757e-03, 9.4169e-01],\n",
      "        [9.5557e-01, 1.1232e-02, 1.0807e-02, 9.3860e-01],\n",
      "        [8.4760e-01, 4.9481e-02, 6.5015e-02, 8.3094e-01],\n",
      "        [9.5681e-01, 1.3572e-02, 1.4917e-02, 8.8151e-01],\n",
      "        [9.3322e-01, 2.0176e-02, 2.4562e-02, 8.9340e-01],\n",
      "        [9.0010e-01, 5.1008e-02, 4.3196e-02, 8.8279e-01],\n",
      "        [8.2876e-01, 4.3757e-02, 4.7931e-02, 8.0531e-01],\n",
      "        [7.6017e-01, 9.0322e-02, 1.1747e-01, 7.8044e-01],\n",
      "        [9.3420e-01, 1.7365e-02, 1.6236e-02, 8.7657e-01],\n",
      "        [7.0655e-01, 1.0971e-01, 1.5202e-01, 7.1678e-01],\n",
      "        [7.9021e-01, 1.1111e-01, 1.5246e-01, 7.8323e-01],\n",
      "        [7.2138e-01, 7.5577e-02, 9.5607e-02, 6.7419e-01],\n",
      "        [5.0531e-01, 3.0309e-01, 2.9282e-01, 6.1940e-01],\n",
      "        [9.5943e-01, 1.1777e-02, 1.1776e-02, 8.9666e-01],\n",
      "        [8.6724e-01, 5.0003e-02, 5.5644e-02, 8.6830e-01],\n",
      "        [8.5594e-01, 1.1067e-02, 1.9474e-02, 9.3193e-01],\n",
      "        [7.2967e-01, 1.1211e-01, 1.2448e-01, 7.4890e-01],\n",
      "        [7.4616e-01, 7.8145e-02, 1.0908e-01, 7.2152e-01],\n",
      "        [9.2351e-01, 1.8446e-02, 1.7084e-02, 8.6598e-01],\n",
      "        [9.1022e-01, 1.0953e-02, 1.9375e-02, 8.8848e-01],\n",
      "        [9.3492e-01, 3.0961e-02, 2.4932e-02, 9.1361e-01],\n",
      "        [8.8413e-01, 2.8727e-02, 3.3386e-02, 8.5322e-01],\n",
      "        [7.5886e-01, 1.0503e-01, 1.1095e-01, 7.1020e-01],\n",
      "        [8.1050e-01, 7.4409e-02, 7.2223e-02, 7.6490e-01],\n",
      "        [7.1783e-01, 1.4137e-01, 1.4200e-01, 7.2679e-01],\n",
      "        [7.6943e-01, 1.1769e-01, 1.1185e-01, 7.4001e-01],\n",
      "        [9.3131e-01, 3.0245e-02, 3.6816e-02, 8.8646e-01],\n",
      "        [8.2678e-01, 5.6638e-02, 6.4406e-02, 7.9686e-01],\n",
      "        [9.1176e-01, 2.1654e-02, 2.4387e-02, 8.7504e-01],\n",
      "        [5.4926e-01, 2.1479e-01, 2.1827e-01, 6.1432e-01],\n",
      "        [8.7871e-01, 3.7122e-02, 4.9090e-02, 8.5014e-01],\n",
      "        [8.1635e-01, 7.8992e-02, 7.6806e-02, 7.9462e-01],\n",
      "        [9.3320e-01, 1.0508e-02, 1.6039e-02, 9.3022e-01],\n",
      "        [9.5012e-01, 2.5543e-02, 3.2628e-02, 8.9225e-01],\n",
      "        [8.2404e-01, 4.7254e-02, 4.1598e-02, 7.4299e-01],\n",
      "        [8.4618e-01, 7.0174e-02, 8.3085e-02, 8.4701e-01],\n",
      "        [7.9096e-01, 1.1630e-01, 1.2711e-01, 7.5319e-01],\n",
      "        [8.7862e-01, 3.4909e-02, 3.8305e-02, 8.5937e-01],\n",
      "        [7.0111e-01, 9.0398e-03, 8.9071e-03, 7.9128e-01],\n",
      "        [9.5353e-01, 1.4509e-02, 1.1819e-02, 9.0538e-01],\n",
      "        [9.0834e-01, 3.3600e-02, 3.7900e-02, 8.4648e-01],\n",
      "        [7.3204e-01, 1.0532e-01, 1.2530e-01, 7.4524e-01],\n",
      "        [8.3101e-01, 7.1715e-02, 6.9177e-02, 7.8783e-01],\n",
      "        [8.1640e-01, 8.9940e-02, 9.0004e-02, 8.1164e-01],\n",
      "        [6.1951e-01, 2.6641e-01, 2.7776e-01, 6.9121e-01],\n",
      "        [8.3894e-01, 6.8098e-02, 7.2994e-02, 7.4876e-01],\n",
      "        [8.3852e-01, 6.7301e-02, 7.3048e-02, 8.4220e-01],\n",
      "        [9.5983e-01, 1.9498e-02, 2.0163e-02, 8.9288e-01],\n",
      "        [8.2276e-01, 1.1413e-01, 1.0953e-01, 8.6659e-01],\n",
      "        [8.7607e-01, 4.1715e-02, 3.8071e-02, 8.4642e-01],\n",
      "        [5.8339e-01, 2.3647e-01, 2.3926e-01, 7.2044e-01],\n",
      "        [5.2395e-01, 2.0042e-01, 2.1544e-01, 6.4981e-01],\n",
      "        [7.3053e-01, 1.2528e-01, 1.2675e-01, 8.0775e-01],\n",
      "        [8.5080e-01, 7.7650e-02, 7.2096e-02, 8.1309e-01],\n",
      "        [9.5504e-01, 1.1176e-02, 1.6209e-02, 9.2026e-01],\n",
      "        [8.9341e-01, 4.6629e-02, 3.1703e-02, 8.5626e-01],\n",
      "        [6.0580e-01, 2.4372e-01, 2.5827e-01, 6.5122e-01],\n",
      "        [8.1754e-01, 6.5454e-02, 6.1156e-02, 8.1302e-01],\n",
      "        [9.6933e-01, 5.9849e-03, 9.2920e-03, 9.5146e-01],\n",
      "        [9.1913e-01, 1.5308e-02, 1.6223e-02, 8.6674e-01],\n",
      "        [8.7512e-01, 4.5671e-02, 5.4097e-02, 8.6635e-01],\n",
      "        [8.7004e-01, 3.6012e-02, 3.6710e-02, 8.2885e-01],\n",
      "        [9.5392e-01, 1.0431e-02, 1.4172e-02, 9.0554e-01],\n",
      "        [8.3043e-01, 8.5263e-02, 8.3765e-02, 7.8297e-01],\n",
      "        [8.7658e-01, 4.3059e-02, 5.0980e-02, 8.4601e-01],\n",
      "        [9.2066e-01, 2.0311e-02, 1.5664e-02, 8.6670e-01],\n",
      "        [5.9866e-01, 2.4645e-01, 2.6127e-01, 7.1902e-01],\n",
      "        [9.4545e-01, 1.3052e-02, 1.4751e-02, 8.9237e-01],\n",
      "        [8.8153e-01, 2.7657e-02, 4.5380e-02, 8.8313e-01],\n",
      "        [5.5366e-01, 2.5171e-01, 2.5551e-01, 6.3238e-01],\n",
      "        [6.8748e-01, 1.1848e-01, 1.3947e-01, 6.7211e-01],\n",
      "        [7.4598e-01, 9.1700e-02, 9.5692e-02, 7.4918e-01],\n",
      "        [9.1216e-01, 3.1398e-02, 2.9853e-02, 8.3550e-01],\n",
      "        [4.8875e-01, 2.2765e-01, 2.3254e-01, 6.4838e-01],\n",
      "        [8.2397e-01, 4.7969e-02, 6.3439e-02, 8.2335e-01],\n",
      "        [9.3319e-01, 2.5979e-02, 2.5251e-02, 8.8522e-01],\n",
      "        [8.1327e-01, 9.6573e-02, 1.1049e-01, 7.5231e-01],\n",
      "        [5.8176e-01, 7.1925e-02, 9.0610e-02, 6.5871e-01],\n",
      "        [8.9373e-01, 3.3229e-02, 2.8028e-02, 7.9921e-01],\n",
      "        [9.3027e-01, 3.6138e-02, 3.7803e-02, 8.7793e-01],\n",
      "        [9.6113e-01, 1.5328e-02, 1.7734e-02, 8.9623e-01],\n",
      "        [9.4325e-01, 1.2213e-02, 1.6815e-02, 8.8667e-01],\n",
      "        [8.9644e-01, 1.7513e-02, 3.9613e-02, 8.5228e-01],\n",
      "        [8.0101e-01, 5.5334e-02, 5.4486e-02, 7.6622e-01],\n",
      "        [5.2998e-01, 2.6164e-01, 2.6532e-01, 6.7224e-01],\n",
      "        [8.2431e-01, 6.1326e-02, 6.8010e-02, 7.6006e-01],\n",
      "        [8.7504e-01, 3.8401e-02, 4.0246e-02, 7.8424e-01],\n",
      "        [8.8615e-01, 6.0331e-02, 6.9422e-02, 8.7371e-01],\n",
      "        [9.3433e-01, 2.2610e-02, 2.6309e-02, 8.9947e-01],\n",
      "        [8.6888e-01, 5.0635e-02, 6.4925e-02, 8.3781e-01],\n",
      "        [9.0512e-01, 2.8483e-02, 2.4793e-02, 8.4292e-01],\n",
      "        [7.9675e-01, 7.8848e-02, 7.8410e-02, 7.5873e-01],\n",
      "        [7.3071e-01, 1.0330e-01, 1.3760e-01, 7.7809e-01],\n",
      "        [5.4835e-01, 2.7578e-01, 2.9504e-01, 6.6585e-01],\n",
      "        [7.4927e-01, 9.4140e-02, 1.0725e-01, 7.9081e-01],\n",
      "        [8.9334e-01, 5.5361e-02, 7.4500e-02, 8.8621e-01],\n",
      "        [8.5326e-01, 5.4596e-02, 6.2711e-02, 8.0314e-01],\n",
      "        [8.4214e-01, 7.1297e-02, 7.2160e-02, 8.0872e-01],\n",
      "        [7.1968e-01, 1.1415e-01, 1.2501e-01, 7.0003e-01],\n",
      "        [9.1623e-01, 2.8282e-02, 2.2878e-02, 8.6735e-01],\n",
      "        [4.9135e-01, 2.5230e-01, 2.6831e-01, 6.9346e-01],\n",
      "        [9.7059e-01, 5.4542e-03, 8.3887e-03, 9.0305e-01],\n",
      "        [7.6655e-01, 7.6868e-02, 9.0969e-02, 8.1402e-01],\n",
      "        [8.5705e-01, 4.9932e-02, 3.7866e-02, 8.3076e-01],\n",
      "        [8.3043e-01, 5.8198e-02, 6.8724e-02, 8.4209e-01],\n",
      "        [9.0354e-01, 2.9818e-02, 3.6326e-02, 8.3470e-01],\n",
      "        [8.2777e-01, 1.1946e-01, 1.2859e-01, 8.2549e-01],\n",
      "        [7.4247e-01, 1.1295e-01, 1.3973e-01, 7.3069e-01],\n",
      "        [8.8464e-01, 4.0965e-02, 3.8923e-02, 7.9496e-01],\n",
      "        [8.5495e-01, 6.9081e-02, 7.2943e-02, 8.2259e-01],\n",
      "        [8.2800e-01, 9.3114e-02, 9.1489e-02, 8.2103e-01],\n",
      "        [9.0708e-01, 3.6619e-02, 5.4643e-02, 8.6247e-01],\n",
      "        [8.9355e-01, 3.0703e-02, 3.1971e-02, 8.6853e-01],\n",
      "        [8.4273e-01, 7.5889e-02, 7.5458e-02, 7.8892e-01],\n",
      "        [8.5383e-01, 5.8918e-02, 5.6345e-02, 8.5419e-01],\n",
      "        [6.9575e-01, 1.6570e-01, 1.7673e-01, 7.1339e-01],\n",
      "        [7.6425e-01, 2.4121e-02, 4.0097e-02, 8.1729e-01],\n",
      "        [7.1297e-01, 3.0306e-02, 4.7128e-02, 7.1544e-01],\n",
      "        [6.0423e-01, 1.3402e-01, 1.6177e-01, 6.7654e-01],\n",
      "        [8.9320e-01, 5.5341e-02, 6.7466e-02, 8.6872e-01],\n",
      "        [9.6443e-01, 4.9439e-04, 4.7476e-04, 9.0525e-01]], device='cuda:0'), '95%': tensor([[0.9051, 0.1261, 0.1320, 0.9173],\n",
      "        [0.9630, 0.0411, 0.0687, 0.9774],\n",
      "        [0.7133, 0.2720, 0.2644, 0.8392],\n",
      "        [0.6547, 0.2766, 0.2766, 0.6950],\n",
      "        [0.9212, 0.1275, 0.1325, 0.8885],\n",
      "        [0.9695, 0.0605, 0.0564, 0.9565],\n",
      "        [0.8906, 0.1538, 0.1908, 0.8909],\n",
      "        [0.9649, 0.0919, 0.0754, 0.9400],\n",
      "        [0.9945, 0.0170, 0.0203, 0.9867],\n",
      "        [0.9648, 0.0592, 0.0645, 0.9299],\n",
      "        [0.9264, 0.1147, 0.1140, 0.9276],\n",
      "        [0.8854, 0.1328, 0.1572, 0.8852],\n",
      "        [0.9782, 0.0570, 0.0520, 0.9673],\n",
      "        [0.9780, 0.0408, 0.0396, 0.9552],\n",
      "        [0.8851, 0.1801, 0.1832, 0.8832],\n",
      "        [0.9695, 0.0525, 0.0588, 0.9312],\n",
      "        [0.9756, 0.1891, 0.1916, 0.9874],\n",
      "        [0.9332, 0.1209, 0.1540, 0.9104],\n",
      "        [0.9523, 0.0818, 0.0991, 0.9246],\n",
      "        [0.8856, 0.1294, 0.1265, 0.8689],\n",
      "        [0.9137, 0.1772, 0.1953, 0.8893],\n",
      "        [0.9654, 0.0702, 0.0715, 0.9389],\n",
      "        [0.9554, 0.1095, 0.1255, 0.9513],\n",
      "        [0.6827, 0.2849, 0.2608, 0.7457],\n",
      "        [0.9988, 0.0067, 0.0078, 0.9977],\n",
      "        [0.8576, 0.1434, 0.1641, 0.8501],\n",
      "        [0.9886, 0.0117, 0.0137, 0.9869],\n",
      "        [0.9901, 0.0413, 0.0803, 0.9896],\n",
      "        [0.9158, 0.1078, 0.1094, 0.8930],\n",
      "        [0.9815, 0.0348, 0.0425, 0.9593],\n",
      "        [0.9699, 0.0688, 0.0493, 0.9626],\n",
      "        [0.9566, 0.1333, 0.1260, 0.9531],\n",
      "        [0.9166, 0.1110, 0.1072, 0.8986],\n",
      "        [0.8761, 0.1557, 0.1900, 0.8666],\n",
      "        [0.9798, 0.0560, 0.0588, 0.9616],\n",
      "        [0.7966, 0.2255, 0.2365, 0.8150],\n",
      "        [0.8513, 0.1966, 0.2370, 0.8822],\n",
      "        [0.8407, 0.1984, 0.1861, 0.8287],\n",
      "        [0.5746, 0.3606, 0.3746, 0.7084],\n",
      "        [0.9834, 0.0313, 0.0468, 0.9688],\n",
      "        [0.9394, 0.1009, 0.1186, 0.9330],\n",
      "        [0.9622, 0.0542, 0.0613, 0.9860],\n",
      "        [0.8421, 0.2421, 0.2175, 0.8530],\n",
      "        [0.8416, 0.1477, 0.1640, 0.8439],\n",
      "        [0.9648, 0.0517, 0.0410, 0.9225],\n",
      "        [0.9643, 0.0417, 0.0459, 0.9577],\n",
      "        [0.9685, 0.0715, 0.0578, 0.9640],\n",
      "        [0.9520, 0.0776, 0.0786, 0.9361],\n",
      "        [0.8341, 0.2018, 0.2020, 0.8050],\n",
      "        [0.8789, 0.1347, 0.1170, 0.8461],\n",
      "        [0.8172, 0.1976, 0.2201, 0.7999],\n",
      "        [0.8493, 0.2084, 0.1924, 0.8233],\n",
      "        [0.9726, 0.0641, 0.1027, 0.9408],\n",
      "        [0.8832, 0.0912, 0.1091, 0.8744],\n",
      "        [0.9668, 0.0798, 0.0733, 0.9544],\n",
      "        [0.6775, 0.2759, 0.3163, 0.7258],\n",
      "        [0.9310, 0.0936, 0.1126, 0.9113],\n",
      "        [0.8967, 0.1425, 0.1428, 0.9023],\n",
      "        [0.9788, 0.0388, 0.0632, 0.9801],\n",
      "        [0.9691, 0.0704, 0.0669, 0.9539],\n",
      "        [0.8857, 0.0990, 0.1132, 0.8668],\n",
      "        [0.9242, 0.1472, 0.1646, 0.9108],\n",
      "        [0.8645, 0.1776, 0.2116, 0.8261],\n",
      "        [0.9311, 0.0719, 0.1006, 0.9114],\n",
      "        [0.9692, 0.1995, 0.1682, 0.9821],\n",
      "        [0.9813, 0.0501, 0.0441, 0.9699],\n",
      "        [0.9611, 0.0948, 0.0826, 0.9278],\n",
      "        [0.8210, 0.2063, 0.2164, 0.8511],\n",
      "        [0.9055, 0.1389, 0.1183, 0.8879],\n",
      "        [0.8937, 0.1689, 0.1660, 0.8804],\n",
      "        [0.7124, 0.3422, 0.3410, 0.7768],\n",
      "        [0.9070, 0.1705, 0.1408, 0.9010],\n",
      "        [0.9081, 0.1222, 0.1159, 0.9074],\n",
      "        [0.9801, 0.0491, 0.0478, 0.9636],\n",
      "        [0.9027, 0.1958, 0.1892, 0.9266],\n",
      "        [0.9353, 0.0834, 0.0958, 0.9445],\n",
      "        [0.6404, 0.3145, 0.3263, 0.8012],\n",
      "        [0.6785, 0.3298, 0.3040, 0.7184],\n",
      "        [0.8533, 0.1842, 0.2053, 0.9029],\n",
      "        [0.9195, 0.1391, 0.1385, 0.8993],\n",
      "        [0.9857, 0.0466, 0.0438, 0.9679],\n",
      "        [0.9377, 0.0933, 0.0805, 0.9327],\n",
      "        [0.6886, 0.2947, 0.3155, 0.7335],\n",
      "        [0.9101, 0.1069, 0.1222, 0.8966],\n",
      "        [0.9903, 0.0258, 0.0488, 0.9885],\n",
      "        [0.9747, 0.0506, 0.0385, 0.9424],\n",
      "        [0.9503, 0.0994, 0.1118, 0.9518],\n",
      "        [0.9267, 0.0850, 0.0769, 0.9199],\n",
      "        [0.9863, 0.0427, 0.0318, 0.9573],\n",
      "        [0.8978, 0.1209, 0.1447, 0.8913],\n",
      "        [0.9317, 0.0867, 0.1168, 0.9315],\n",
      "        [0.9680, 0.0578, 0.0479, 0.9509],\n",
      "        [0.7326, 0.3210, 0.3340, 0.8297],\n",
      "        [0.9824, 0.0432, 0.0477, 0.9597],\n",
      "        [0.9323, 0.0726, 0.1180, 0.9466],\n",
      "        [0.6234, 0.3192, 0.3238, 0.7231],\n",
      "        [0.7920, 0.2094, 0.2324, 0.7858],\n",
      "        [0.8621, 0.2179, 0.1992, 0.8893],\n",
      "        [0.9551, 0.0566, 0.0677, 0.9116],\n",
      "        [0.5512, 0.3249, 0.3174, 0.7266],\n",
      "        [0.8798, 0.1143, 0.1238, 0.8986],\n",
      "        [0.9669, 0.0708, 0.0753, 0.9467],\n",
      "        [0.8742, 0.2001, 0.1792, 0.8262],\n",
      "        [0.8617, 0.2056, 0.2582, 0.8727],\n",
      "        [0.9534, 0.0746, 0.0849, 0.9234],\n",
      "        [0.9640, 0.0638, 0.0693, 0.9482],\n",
      "        [0.9797, 0.0431, 0.0386, 0.9477],\n",
      "        [0.9807, 0.0361, 0.0441, 0.9693],\n",
      "        [0.9452, 0.0614, 0.0803, 0.9461],\n",
      "        [0.9007, 0.1421, 0.1380, 0.8404],\n",
      "        [0.6064, 0.3491, 0.3419, 0.7485],\n",
      "        [0.8933, 0.1276, 0.1054, 0.8605],\n",
      "        [0.9438, 0.0855, 0.0845, 0.9074],\n",
      "        [0.9389, 0.1211, 0.1327, 0.9399],\n",
      "        [0.9611, 0.0684, 0.0874, 0.9491],\n",
      "        [0.9218, 0.1245, 0.1305, 0.9000],\n",
      "        [0.9473, 0.0736, 0.0782, 0.9130],\n",
      "        [0.8850, 0.1568, 0.1431, 0.8666],\n",
      "        [0.8290, 0.1949, 0.2284, 0.8666],\n",
      "        [0.6260, 0.3482, 0.3604, 0.7213],\n",
      "        [0.8620, 0.1584, 0.1771, 0.8760],\n",
      "        [0.9393, 0.1501, 0.1502, 0.9476],\n",
      "        [0.9352, 0.1282, 0.1326, 0.8941],\n",
      "        [0.8985, 0.1277, 0.1398, 0.8688],\n",
      "        [0.8014, 0.2169, 0.2102, 0.7937],\n",
      "        [0.9536, 0.0695, 0.0597, 0.9063],\n",
      "        [0.5759, 0.3450, 0.3573, 0.7656],\n",
      "        [0.9899, 0.0317, 0.0294, 0.9839],\n",
      "        [0.8811, 0.1652, 0.2236, 0.9225],\n",
      "        [0.9333, 0.0957, 0.0989, 0.8950],\n",
      "        [0.9063, 0.1063, 0.1420, 0.9172],\n",
      "        [0.9412, 0.0805, 0.0850, 0.9152],\n",
      "        [0.8878, 0.1895, 0.2190, 0.8993],\n",
      "        [0.8188, 0.1792, 0.2066, 0.8313],\n",
      "        [0.9421, 0.0788, 0.0851, 0.8952],\n",
      "        [0.9557, 0.1575, 0.1354, 0.9290],\n",
      "        [0.8900, 0.1544, 0.1310, 0.8916],\n",
      "        [0.9512, 0.0813, 0.1002, 0.9269],\n",
      "        [0.9764, 0.1053, 0.1176, 0.9543],\n",
      "        [0.9019, 0.1287, 0.1242, 0.9025],\n",
      "        [0.9383, 0.1373, 0.1564, 0.9278],\n",
      "        [0.8000, 0.2294, 0.2587, 0.8023],\n",
      "        [0.8813, 0.0742, 0.1122, 0.9006],\n",
      "        [0.9357, 0.1536, 0.1836, 0.9281],\n",
      "        [0.7152, 0.2559, 0.2619, 0.7833],\n",
      "        [0.9419, 0.1216, 0.1327, 0.9257],\n",
      "        [0.9993, 0.0314, 0.0339, 0.9979]], device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Overall Survival (4 Years)': {'accuracy': 0.8843537414965986,\n",
       "  'mse': 0.105534956,\n",
       "  'auc': 0.5333969465648855,\n",
       "  'precision': 0.8904109589041096,\n",
       "  'recall': 0.9923664122137404,\n",
       "  'f1': 0.9386281588447654},\n",
       " 'FT': {'accuracy': 0.782312925170068,\n",
       "  'mse': 0.17532517,\n",
       "  'auc': 0.7209239130434782,\n",
       "  'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1': 0.0},\n",
       " 'Aspiration rate Post-therapy': {'accuracy': 0.8231292517006803,\n",
       "  'mse': 0.13865589,\n",
       "  'auc': 0.748569612205976,\n",
       "  'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1': 0.0},\n",
       " 'LRC': {'accuracy': 0.8979591836734694,\n",
       "  'mse': 0.091692045,\n",
       "  'auc': 0.6499999999999999,\n",
       "  'precision': 0.8979591836734694,\n",
       "  'recall': 1.0,\n",
       "  'f1': 0.9462365591397849}}"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmodel3_smote = train_state(model_args=t1_args,state=3,lr=.001,weights=[10,1,1,1],use_smote=True)\n",
    "tmodel3_smote[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9a2d06a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_ensemble(model_arg_list,state,\n",
    "                         n_bags=20,\n",
    "                         smote_options=[True,False],\n",
    "                         smote_baseline_options=[True,False],\n",
    "                         weight_options=[None],\n",
    "                         weight_baseline_options=[None],\n",
    "                         **kwargs):\n",
    "    resampled_models =[]\n",
    "    base_models = []\n",
    "    base_metrics = []\n",
    "    resample_metrics = []\n",
    "    base_losses = []\n",
    "    resample_losses = []\n",
    "    n_errors = 0\n",
    "    for margs in model_arg_list:\n",
    "        for s in smote_baseline_options:\n",
    "            for w in weight_baseline_options:\n",
    "                [base_model,blosses,bmetrics] = train_state(model_args=margs,state=state,\n",
    "                                         resample_training=False,\n",
    "                                         use_smote=smote_options,\n",
    "                                         weights=w,\n",
    "                                         verbose=False,**kwargs)\n",
    "                base_models.append(base_model)\n",
    "                base_metrics.append(bmetrics)\n",
    "                base_losses.append(blosses)\n",
    "        for n in range(n_bags):\n",
    "            for s in smote_options:\n",
    "                for w in weight_options:\n",
    "                    #this can fail if I resample a bad distribution with no minority classes\n",
    "                    done = False\n",
    "                    while not done:\n",
    "                        try:\n",
    "                            [model,loss,metrics] = train_state(model_args=margs,\n",
    "                                                state=state,resample_training=True,\n",
    "                                                use_smote=s,\n",
    "                                                weights=w,\n",
    "                                                verbose=False,**kwargs)\n",
    "                            resampled_models.append(model)\n",
    "                            resample_metrics.append(metrics)\n",
    "                            resample_losses.append(loss)\n",
    "                            done=True\n",
    "                            print('model_done',len(base_models)+len(resampled_models))\n",
    "                        except Exception as e:\n",
    "                            print('error training model')\n",
    "                            print(e)\n",
    "                            n_errors += 1\n",
    "    print('done with',n_errors,'errors')\n",
    "    return base_models,resampled_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673e9b5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# t1_arglist = [\n",
    "#      {'hidden_layers': [100], 'dropout': 0.1, 'input_dropout': 0.1},\n",
    "# ]\n",
    "# m1_list, m1_ensemble = train_model_ensemble(t1_arglist,1,\n",
    "#                                             n_bags=10,\n",
    "#                                            )\n",
    "# from Models import *\n",
    "# emodel1 = TransitionEnsemble(m1_list,m1_ensemble)\n",
    "# test2 = emodel1(transition_sample(1)[0][0].view(1,-1).to(emodel1.get_device()))\n",
    "# test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc239f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t2_arglist = [\n",
    "#     {'hidden_layers': [100], 'dropout': 0.1, 'input_dropout': 0.1},\n",
    "# ]\n",
    "# m2_list, m2_ensemble = train_model_ensemble(t2_arglist,2,\n",
    "#                                             n_bags=10)\n",
    "# emodel2 = TransitionEnsemble(m2_list,m2_ensemble)\n",
    "# emodel2(transition_sample(2)[0][0].view(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b788327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t3_arglist = [\n",
    "#     {'hidden_layers': [100], 'dropout': 0.5, 'input_dropout': 0.1},\n",
    "# ]\n",
    "# m3_list, m3_ensemble = train_model_ensemble(t3_arglist,3,n_bags=10,\n",
    "#                                            )\n",
    "# emodel3 = EndpointEnsemble(m3_list,m3_ensemble)\n",
    "# emodel3(transition_sample(3)[0][0].view(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9536fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gridsearch_attention_transition_models(state=1,attentions=[False]):\n",
    "    hiddenlayers = [[10],[50],[100],[400],[1000,100],[10,10],[50,50],[100,100],[400,400]]\n",
    "    model_arglist = [{'hidden_layers': h} for h in hiddenlayers]\n",
    "    best_loss = 100000000000\n",
    "    best_metrics = {}\n",
    "    best_args = {}\n",
    "    best_model = None\n",
    "    k = 0\n",
    "    records = []\n",
    "    for margs in model_arglist:\n",
    "        args = {k:v for k,v in margs.items()}\n",
    "        for use_smote in [True,False]:\n",
    "            for dropout in [0,.1,.25,.5,.9]:\n",
    "                args['dropout'] = dropout\n",
    "                for input_dropout in [0,.1,.25,.5]:\n",
    "                    args['input_dropout'] = input_dropout\n",
    "                    model,m_loss,m_metrics = train_state(\n",
    "                        model_args=args,\n",
    "                        state=state,\n",
    "                        lr=.001,\n",
    "                        use_smote=use_smote,\n",
    "                        use_attention=False,\n",
    "                        verbose=False)\n",
    "                    newargs = {k:copy.deepcopy(v) for k,v in args.items()}\n",
    "                    newargs['use_smote'] = use_smote\n",
    "                    record = {'loss': m_loss,'metrics':m_metrics,'args':newargs,'model': model}\n",
    "                    records.append(record)\n",
    "                    print('done',k,m_loss)\n",
    "                    k+=1\n",
    "                    if m_loss < best_loss:\n",
    "                        best_loss = m_loss\n",
    "                        best_metrics  = m_metrics\n",
    "                        best_model = model\n",
    "                        best_args = {k:v for k,v in args.items()}\n",
    "                        best_args['smote'] = use_smote\n",
    "                        print('_++++++++++New Best++++____')\n",
    "                        print(best_loss)\n",
    "                        print(best_metrics)\n",
    "                        print(best_args)\n",
    "                        print(use_smote)\n",
    "                        print('___________')\n",
    "                        print('++++++++')\n",
    "                        print()\n",
    "    print('_________')\n",
    "    print('+++++++++++')\n",
    "    print('best stuff',best_loss)\n",
    "    print(best_metrics)\n",
    "    print(best_args)\n",
    "    return best_model, records\n",
    "# model1_gs = gridsearch_attention_transition_models(1)\n",
    "# model1_gs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cdf2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2_gs = gridsearch_attention_transition_models(2)\n",
    "# model2_gs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33d908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model3_gs= gridsearch_attention_transition_models(3)\n",
    "# model3_gs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46494c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [(m['args'],m['loss']) for m in sorted(model1_gs[1], key = lambda x: x['loss'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abef27dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [(m['args'],m['loss']) for m in sorted(model2_gs[1], key = lambda x: x['loss'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdab783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [(m['args'],m['loss']) for m in sorted(model3_gs[1], key = lambda x: x['loss'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2937b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_trained_models():\n",
    "#     files = Const.tuned_transition_models\n",
    "#     decision_file = Const.tuned_decision_model\n",
    "#     [model1,model2,model3] = [torch.load(file) for file in files]\n",
    "#     decision_model = torch.load(decision_file)\n",
    "#     return decision_model, model1,model2,model3\n",
    "# _, model1, model2, model3 =load_trained_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b29d259b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 1., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 1., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 1., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 1., 1.],\n",
       "         [0., 1., 1.],\n",
       "         [0., 1., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 1., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 1., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 1., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [0., 1., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 1.],\n",
       "         [0., 1., 0.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [0., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [0., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 1., 1.],\n",
       "         [0., 1., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [0., 1., 0.],\n",
       "         [1., 1., 1.],\n",
       "         [0., 1., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 1., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 1., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [0., 1., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 1., 0.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [0., 1., 1.],\n",
       "         [1., 1., 0.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [0., 1., 0.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [0., 1., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 1., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [0., 1., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 0.]]),\n",
       " {'pd1': tensor([[0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [9.7671e-01, 2.3288e-02, 1.2315e-07],\n",
       "          [6.8678e-05, 9.9993e-01, 2.1180e-06],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [5.0123e-01, 4.9863e-01, 1.4223e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [9.2803e-01, 7.1680e-02, 2.9083e-04],\n",
       "          [7.1105e-01, 2.8884e-01, 1.0555e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [7.4937e-01, 2.5024e-01, 3.8235e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [3.2806e-02, 9.6713e-01, 6.0705e-05],\n",
       "          [2.2574e-01, 7.7401e-01, 2.5104e-04],\n",
       "          [6.2735e-03, 9.9373e-01, 4.8260e-08],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [9.9304e-01, 6.9351e-03, 2.6636e-05],\n",
       "          [9.9684e-01, 3.1485e-03, 1.3001e-05],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [6.6458e-01, 3.3504e-01, 3.8545e-04],\n",
       "          [8.9062e-03, 9.9109e-01, 1.4129e-08],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [1.9360e-01, 8.0586e-01, 5.3959e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [9.3596e-01, 6.3988e-02, 5.1438e-05],\n",
       "          [8.8812e-01, 1.1175e-01, 1.3146e-04],\n",
       "          [1.8332e-15, 1.0000e+00, 0.0000e+00],\n",
       "          [1.8546e-01, 8.1433e-01, 2.1481e-04],\n",
       "          [1.9327e-01, 8.0630e-01, 4.3552e-04],\n",
       "          [8.4073e-01, 1.5926e-01, 9.2433e-06],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [7.1128e-02, 9.2887e-01, 1.8130e-07],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [9.9110e-01, 8.7079e-03, 1.8889e-04],\n",
       "          [4.0430e-01, 5.9530e-01, 3.9659e-04],\n",
       "          [7.7585e-01, 2.2414e-01, 7.0740e-06],\n",
       "          [3.7474e-01, 6.2451e-01, 7.4628e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [6.3623e-01, 3.6322e-01, 5.4362e-04],\n",
       "          [6.1190e-01, 3.8803e-01, 7.8405e-05],\n",
       "          [5.6712e-03, 9.9418e-01, 1.4405e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [7.3749e-01, 2.6251e-01, 2.9839e-08],\n",
       "          [8.2794e-01, 1.7141e-01, 6.5719e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [9.5936e-01, 4.0418e-02, 2.1933e-04],\n",
       "          [7.2471e-01, 2.7507e-01, 2.1663e-04],\n",
       "          [9.6308e-01, 3.6729e-02, 1.8955e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [6.9191e-01, 3.0793e-01, 1.5570e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [9.9240e-01, 7.4959e-03, 1.0144e-04],\n",
       "          [9.0650e-01, 9.3046e-02, 4.5030e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [5.1246e-01, 4.8672e-01, 8.1192e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [8.6080e-01, 1.3912e-01, 7.5200e-05],\n",
       "          [1.7097e-01, 8.2898e-01, 5.1602e-05],\n",
       "          [2.3551e-01, 7.6421e-01, 2.8154e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [2.1408e-02, 9.7841e-01, 1.8343e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [4.4550e-01, 5.5428e-01, 2.2531e-04],\n",
       "          [7.3381e-01, 2.6565e-01, 5.4345e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [9.9644e-01, 3.5517e-03, 4.8195e-06],\n",
       "          [9.9652e-01, 3.4760e-03, 2.7907e-06],\n",
       "          [9.4141e-01, 5.8406e-02, 1.8484e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [9.9131e-01, 8.6765e-03, 1.7011e-05],\n",
       "          [1.0000e+00, 6.9255e-12, 0.0000e+00],\n",
       "          [8.8238e-01, 1.1744e-01, 1.7296e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [3.3510e-01, 6.6411e-01, 7.9254e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [1.2820e-02, 9.8718e-01, 2.8746e-09],\n",
       "          [4.0213e-02, 9.5979e-01, 1.2453e-06],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [9.4254e-01, 5.7227e-02, 2.3541e-04],\n",
       "          [5.3561e-04, 9.9946e-01, 5.9224e-06],\n",
       "          [2.8712e-01, 7.1243e-01, 4.4945e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [9.5603e-01, 4.3517e-02, 4.5118e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [9.1894e-01, 8.0549e-02, 5.1535e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [7.3361e-01, 2.6524e-01, 1.1505e-03],\n",
       "          [9.9000e-01, 9.9509e-03, 4.4758e-05],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [5.3622e-01, 4.6355e-01, 2.3267e-04],\n",
       "          [9.4998e-01, 4.9981e-02, 3.6026e-05],\n",
       "          [6.3808e-01, 3.6178e-01, 1.4274e-04],\n",
       "          [2.6897e-01, 7.3090e-01, 1.3722e-04],\n",
       "          [9.7795e-01, 2.1914e-02, 1.4072e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [8.6465e-01, 1.3469e-01, 6.6395e-04],\n",
       "          [3.1242e-02, 9.6866e-01, 9.8101e-05],\n",
       "          [9.1651e-01, 8.3226e-02, 2.6764e-04],\n",
       "          [2.2402e-01, 7.7564e-01, 3.4052e-04],\n",
       "          [9.3094e-01, 6.8760e-02, 3.0387e-04],\n",
       "          [4.7145e-01, 5.2817e-01, 3.7785e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [9.9481e-01, 5.1791e-03, 8.7418e-06],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [9.4047e-01, 5.9297e-02, 2.2881e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [9.8570e-01, 1.4121e-02, 1.7472e-04],\n",
       "          [5.3092e-01, 4.6892e-01, 1.6107e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [6.8864e-01, 3.1058e-01, 7.7936e-04],\n",
       "          [9.0451e-01, 9.4603e-02, 8.8492e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [8.2906e-01, 1.7083e-01, 1.0926e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [6.6828e-01, 3.3077e-01, 9.5110e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [8.3955e-03, 9.9111e-01, 4.9347e-04],\n",
       "          [1.9560e-01, 8.0373e-01, 6.7256e-04],\n",
       "          [4.3859e-01, 5.6110e-01, 3.1009e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [5.3242e-01, 4.6641e-01, 1.1664e-03],\n",
       "          [8.1731e-01, 1.8220e-01, 4.8963e-04]], grad_fn=<CopySlices>),\n",
       "  'nd1': tensor([[0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [2.2321e-12, 1.0000e+00, 2.2321e-12],\n",
       "          [6.1717e-06, 9.9999e-01, 6.1621e-06],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [1.9008e-06, 1.0000e+00, 1.9009e-06],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [2.2068e-06, 1.0000e+00, 2.2065e-06],\n",
       "          [3.2563e-07, 1.0000e+00, 3.2549e-07],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [1.0517e-06, 1.0000e+00, 1.0515e-06],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [1.2362e-06, 1.0000e+00, 1.2361e-06],\n",
       "          [3.9708e-06, 9.9999e-01, 3.9678e-06],\n",
       "          [8.2154e-11, 1.0000e+00, 8.2154e-11],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [2.0869e-06, 1.0000e+00, 2.0876e-06],\n",
       "          [1.7082e-06, 1.0000e+00, 1.7075e-06],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [3.1995e-06, 9.9999e-01, 3.1978e-06],\n",
       "          [2.1925e-11, 1.0000e+00, 2.1925e-11],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [4.3325e-06, 9.9999e-01, 4.3336e-06],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [2.1721e-06, 1.0000e+00, 2.1708e-06],\n",
       "          [4.4877e-07, 1.0000e+00, 4.4871e-07],\n",
       "          [0.0000e+00, 1.0000e+00, 0.0000e+00],\n",
       "          [5.5520e-06, 9.9999e-01, 5.5499e-06],\n",
       "          [3.7961e-06, 9.9999e-01, 3.7941e-06],\n",
       "          [1.1981e-07, 1.0000e+00, 1.1979e-07],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [2.7590e-11, 1.0000e+00, 2.7591e-11],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [3.8306e-05, 9.9992e-01, 3.8191e-05],\n",
       "          [1.9393e-06, 1.0000e+00, 1.9382e-06],\n",
       "          [6.5175e-08, 1.0000e+00, 6.5173e-08],\n",
       "          [2.6148e-05, 9.9995e-01, 2.6122e-05],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [6.5073e-06, 9.9999e-01, 6.5025e-06],\n",
       "          [4.3778e-07, 1.0000e+00, 4.3771e-07],\n",
       "          [1.5528e-06, 1.0000e+00, 1.5544e-06],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [3.5040e-13, 1.0000e+00, 3.5040e-13],\n",
       "          [2.7168e-05, 9.9995e-01, 2.7155e-05],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [5.9119e-06, 9.9999e-01, 5.9215e-06],\n",
       "          [2.1282e-06, 1.0000e+00, 2.1211e-06],\n",
       "          [1.9078e-06, 1.0000e+00, 1.9072e-06],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [4.3065e-07, 1.0000e+00, 4.3085e-07],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [2.0887e-05, 9.9996e-01, 2.0876e-05],\n",
       "          [1.7575e-05, 9.9996e-01, 1.7532e-05],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [4.3043e-05, 9.9991e-01, 4.2942e-05],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [9.8349e-07, 1.0000e+00, 9.8252e-07],\n",
       "          [6.7336e-07, 1.0000e+00, 6.7327e-07],\n",
       "          [2.4544e-06, 1.0000e+00, 2.4540e-06],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [2.2866e-05, 9.9995e-01, 2.2755e-05],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [4.1199e-06, 9.9999e-01, 4.1126e-06],\n",
       "          [2.1293e-06, 1.0000e+00, 2.1306e-06],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [3.7989e-07, 1.0000e+00, 3.7979e-07],\n",
       "          [7.7974e-08, 1.0000e+00, 7.7959e-08],\n",
       "          [6.1441e-07, 1.0000e+00, 6.1421e-07],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [7.2701e-07, 1.0000e+00, 7.2721e-07],\n",
       "          [0.0000e+00, 1.0000e+00, 0.0000e+00],\n",
       "          [2.8916e-06, 9.9999e-01, 2.8967e-06],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [1.6079e-05, 9.9997e-01, 1.6094e-05],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [1.3716e-11, 1.0000e+00, 1.3716e-11],\n",
       "          [8.2636e-09, 1.0000e+00, 8.2637e-09],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [5.8975e-06, 9.9999e-01, 5.8816e-06],\n",
       "          [6.5776e-07, 1.0000e+00, 6.5779e-07],\n",
       "          [1.8957e-06, 1.0000e+00, 1.8960e-06],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [1.2415e-05, 9.9998e-01, 1.2398e-05],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [1.1716e-05, 9.9998e-01, 1.1707e-05],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [7.6813e-06, 9.9998e-01, 7.6942e-06],\n",
       "          [7.2664e-07, 1.0000e+00, 7.2759e-07],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [1.2681e-06, 1.0000e+00, 1.2685e-06],\n",
       "          [6.7946e-07, 1.0000e+00, 6.7896e-07],\n",
       "          [2.0180e-06, 1.0000e+00, 2.0165e-06],\n",
       "          [2.4011e-06, 1.0000e+00, 2.3982e-06],\n",
       "          [1.1188e-05, 9.9998e-01, 1.1189e-05],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [1.1008e-05, 9.9998e-01, 1.0990e-05],\n",
       "          [2.8023e-06, 9.9999e-01, 2.8006e-06],\n",
       "          [9.1173e-06, 9.9998e-01, 9.1129e-06],\n",
       "          [6.0161e-06, 9.9999e-01, 6.0094e-06],\n",
       "          [1.7644e-05, 9.9996e-01, 1.7522e-05],\n",
       "          [1.4612e-05, 9.9997e-01, 1.4558e-05],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [3.5024e-07, 1.0000e+00, 3.5013e-07],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [7.6643e-06, 9.9998e-01, 7.6091e-06],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [5.8002e-06, 9.9999e-01, 5.7855e-06],\n",
       "          [1.8366e-06, 1.0000e+00, 1.8348e-06],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [5.7884e-05, 9.9988e-01, 5.7496e-05],\n",
       "          [3.9560e-05, 9.9992e-01, 3.9544e-05],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [2.1750e-06, 1.0000e+00, 2.1719e-06],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [7.4430e-05, 9.9985e-01, 7.4217e-05],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [5.7237e-06, 9.9999e-01, 5.7281e-06],\n",
       "          [2.1012e-05, 9.9996e-01, 2.1021e-05],\n",
       "          [1.0478e-06, 1.0000e+00, 1.0474e-06],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.3333e-01],\n",
       "          [2.7268e-05, 9.9995e-01, 2.7226e-05],\n",
       "          [6.6813e-06, 9.9999e-01, 6.6709e-06]], grad_fn=<CopySlices>),\n",
       "  'nd2': tensor([[9.3550e-01, 6.1732e-02, 2.7674e-03],\n",
       "          [9.3676e-01, 6.3148e-02, 9.2248e-05],\n",
       "          [9.5005e-01, 4.9215e-02, 7.3409e-04],\n",
       "          [9.0337e-01, 9.3442e-02, 3.1897e-03],\n",
       "          [7.4876e-01, 2.3419e-01, 1.7046e-02],\n",
       "          [6.9524e-01, 2.7908e-01, 2.5681e-02],\n",
       "          [5.7174e-01, 4.2674e-01, 1.5287e-03],\n",
       "          [8.2914e-01, 1.6504e-01, 5.8195e-03],\n",
       "          [6.8173e-01, 3.1235e-01, 5.9267e-03],\n",
       "          [5.0494e-01, 4.7414e-01, 2.0921e-02],\n",
       "          [4.3067e-01, 5.4839e-01, 2.0935e-02],\n",
       "          [9.4806e-01, 5.1513e-02, 4.2508e-04],\n",
       "          [7.5448e-01, 2.2214e-01, 2.3375e-02],\n",
       "          [4.0414e-01, 5.9103e-01, 4.8299e-03],\n",
       "          [7.7533e-01, 2.1706e-01, 7.6062e-03],\n",
       "          [6.1507e-01, 3.6632e-01, 1.8614e-02],\n",
       "          [9.1485e-01, 8.4976e-02, 1.7329e-04],\n",
       "          [6.2587e-01, 3.4808e-01, 2.6049e-02],\n",
       "          [3.3619e-01, 6.3952e-01, 2.4289e-02],\n",
       "          [5.6368e-01, 4.3106e-01, 5.2511e-03],\n",
       "          [8.6510e-01, 1.2758e-01, 7.3250e-03],\n",
       "          [6.1984e-01, 3.6881e-01, 1.1355e-02],\n",
       "          [6.1563e-01, 3.6122e-01, 2.3152e-02],\n",
       "          [8.8209e-01, 1.1596e-01, 1.9489e-03],\n",
       "          [8.8624e-01, 1.1376e-01, 5.5714e-07],\n",
       "          [7.4153e-01, 2.5087e-01, 7.6043e-03],\n",
       "          [6.0871e-01, 3.9129e-01, 1.8235e-06],\n",
       "          [4.2328e-01, 5.7327e-01, 3.4443e-03],\n",
       "          [4.8979e-01, 5.0235e-01, 7.8621e-03],\n",
       "          [7.8093e-01, 2.1306e-01, 6.0048e-03],\n",
       "          [4.9394e-01, 5.0174e-01, 4.3226e-03],\n",
       "          [6.3194e-01, 3.4858e-01, 1.9481e-02],\n",
       "          [5.8661e-01, 4.0015e-01, 1.3244e-02],\n",
       "          [6.4818e-01, 3.3812e-01, 1.3694e-02],\n",
       "          [5.9037e-01, 4.0103e-01, 8.5962e-03],\n",
       "          [8.9232e-01, 1.0442e-01, 3.2590e-03],\n",
       "          [7.4366e-01, 2.4789e-01, 8.4514e-03],\n",
       "          [7.8753e-01, 2.0599e-01, 6.4856e-03],\n",
       "          [7.0493e-01, 2.8751e-01, 7.5576e-03],\n",
       "          [6.0978e-01, 3.7983e-01, 1.0383e-02],\n",
       "          [7.2223e-01, 2.6703e-01, 1.0745e-02],\n",
       "          [9.4855e-01, 5.1420e-02, 3.0805e-05],\n",
       "          [9.4503e-01, 5.2835e-02, 2.1384e-03],\n",
       "          [7.8131e-01, 2.1530e-01, 3.3935e-03],\n",
       "          [3.9883e-01, 5.8271e-01, 1.8461e-02],\n",
       "          [8.3353e-01, 1.6596e-01, 5.0211e-04],\n",
       "          [7.6960e-01, 2.2124e-01, 9.1634e-03],\n",
       "          [6.5681e-01, 3.3628e-01, 6.9118e-03],\n",
       "          [8.0438e-01, 1.7877e-01, 1.6853e-02],\n",
       "          [6.3997e-01, 3.3939e-01, 2.0639e-02],\n",
       "          [8.5922e-01, 1.3925e-01, 1.5315e-03],\n",
       "          [6.4361e-01, 3.4191e-01, 1.4481e-02],\n",
       "          [7.3358e-01, 2.5629e-01, 1.0130e-02],\n",
       "          [7.1941e-01, 2.6674e-01, 1.3844e-02],\n",
       "          [6.7675e-01, 3.0059e-01, 2.2656e-02],\n",
       "          [4.6954e-01, 5.1768e-01, 1.2784e-02],\n",
       "          [7.9304e-01, 1.9803e-01, 8.9263e-03],\n",
       "          [5.7256e-01, 4.1788e-01, 9.5536e-03],\n",
       "          [9.9685e-01, 3.1421e-03, 4.3089e-06],\n",
       "          [6.0928e-01, 3.7353e-01, 1.7196e-02],\n",
       "          [2.2794e-01, 7.5772e-01, 1.4344e-02],\n",
       "          [7.8933e-01, 1.9291e-01, 1.7764e-02],\n",
       "          [5.6286e-01, 4.1808e-01, 1.9057e-02],\n",
       "          [8.0774e-01, 1.8507e-01, 7.1920e-03],\n",
       "          [8.4384e-01, 1.5615e-01, 5.0163e-06],\n",
       "          [5.6676e-01, 4.1517e-01, 1.8070e-02],\n",
       "          [5.8624e-01, 3.9899e-01, 1.4767e-02],\n",
       "          [8.7777e-01, 1.1471e-01, 7.5181e-03],\n",
       "          [8.3202e-01, 1.4949e-01, 1.8487e-02],\n",
       "          [6.7100e-01, 3.0605e-01, 2.2946e-02],\n",
       "          [7.7405e-01, 2.1307e-01, 1.2876e-02],\n",
       "          [6.5374e-01, 3.2027e-01, 2.5990e-02],\n",
       "          [5.2809e-01, 4.6792e-01, 3.9942e-03],\n",
       "          [6.7546e-01, 3.1412e-01, 1.0419e-02],\n",
       "          [5.9144e-01, 4.0291e-01, 5.6508e-03],\n",
       "          [8.3707e-01, 1.5986e-01, 3.0740e-03],\n",
       "          [4.9539e-01, 5.0046e-01, 4.1495e-03],\n",
       "          [7.3494e-01, 2.4927e-01, 1.5790e-02],\n",
       "          [8.8374e-01, 1.1168e-01, 4.5789e-03],\n",
       "          [8.3792e-01, 1.5904e-01, 3.0404e-03],\n",
       "          [7.5755e-01, 2.2259e-01, 1.9854e-02],\n",
       "          [8.7160e-01, 1.2295e-01, 5.4501e-03],\n",
       "          [7.5823e-01, 2.3089e-01, 1.0878e-02],\n",
       "          [6.6821e-01, 3.2579e-01, 5.9932e-03],\n",
       "          [9.4101e-01, 5.8370e-02, 6.2118e-04],\n",
       "          [7.1963e-01, 2.7598e-01, 4.3889e-03],\n",
       "          [6.1004e-01, 3.8236e-01, 7.6077e-03],\n",
       "          [7.7339e-01, 2.2240e-01, 4.2069e-03],\n",
       "          [7.3862e-01, 2.5100e-01, 1.0387e-02],\n",
       "          [9.7658e-01, 2.0788e-02, 2.6356e-03],\n",
       "          [7.3417e-01, 2.5564e-01, 1.0189e-02],\n",
       "          [4.7572e-01, 5.0482e-01, 1.9460e-02],\n",
       "          [6.2045e-01, 3.5260e-01, 2.6953e-02],\n",
       "          [4.3832e-01, 5.4947e-01, 1.2206e-02],\n",
       "          [7.9525e-01, 2.0460e-01, 1.5377e-04],\n",
       "          [7.9770e-01, 2.0088e-01, 1.4196e-03],\n",
       "          [6.6016e-01, 3.3062e-01, 9.2157e-03],\n",
       "          [8.4795e-01, 1.5158e-01, 4.6811e-04],\n",
       "          [7.0156e-01, 2.8296e-01, 1.5487e-02],\n",
       "          [7.9360e-01, 1.9032e-01, 1.6086e-02],\n",
       "          [7.3482e-01, 2.5939e-01, 5.7853e-03],\n",
       "          [8.0260e-01, 1.8554e-01, 1.1865e-02],\n",
       "          [8.5245e-01, 1.3549e-01, 1.2053e-02],\n",
       "          [9.2738e-01, 7.2325e-02, 2.9725e-04],\n",
       "          [7.4811e-01, 2.4114e-01, 1.0754e-02],\n",
       "          [6.3146e-01, 3.4856e-01, 1.9984e-02],\n",
       "          [7.8590e-01, 2.0056e-01, 1.3535e-02],\n",
       "          [7.5766e-01, 2.3649e-01, 5.8484e-03],\n",
       "          [9.0191e-01, 9.5050e-02, 3.0430e-03],\n",
       "          [6.9218e-01, 2.8680e-01, 2.1015e-02],\n",
       "          [7.5679e-01, 2.3752e-01, 5.6963e-03],\n",
       "          [4.2186e-01, 5.6084e-01, 1.7296e-02],\n",
       "          [6.2140e-01, 3.5795e-01, 2.0643e-02],\n",
       "          [9.4824e-01, 4.9625e-02, 2.1337e-03],\n",
       "          [6.8142e-01, 3.0437e-01, 1.4217e-02],\n",
       "          [5.4754e-01, 4.4162e-01, 1.0834e-02],\n",
       "          [3.6453e-01, 6.2691e-01, 8.5577e-03],\n",
       "          [4.1308e-01, 5.7920e-01, 7.7198e-03],\n",
       "          [9.0658e-01, 8.9079e-02, 4.3441e-03],\n",
       "          [6.3024e-01, 3.6466e-01, 5.1043e-03],\n",
       "          [8.4432e-01, 1.4778e-01, 7.9042e-03],\n",
       "          [6.9272e-01, 2.9590e-01, 1.1386e-02],\n",
       "          [7.4498e-01, 2.3276e-01, 2.2263e-02],\n",
       "          [7.3104e-01, 2.3222e-01, 3.6736e-02],\n",
       "          [6.2370e-01, 3.4734e-01, 2.8965e-02],\n",
       "          [6.0114e-01, 3.8031e-01, 1.8554e-02],\n",
       "          [8.2753e-01, 1.7007e-01, 2.3955e-03],\n",
       "          [9.3796e-01, 6.1571e-02, 4.6697e-04],\n",
       "          [8.5198e-01, 1.4627e-01, 1.7418e-03],\n",
       "          [4.8763e-01, 4.9430e-01, 1.8068e-02],\n",
       "          [8.1450e-01, 1.6586e-01, 1.9637e-02],\n",
       "          [5.7502e-01, 4.1762e-01, 7.3673e-03],\n",
       "          [6.9731e-01, 2.9609e-01, 6.6019e-03],\n",
       "          [6.6147e-01, 3.1775e-01, 2.0777e-02],\n",
       "          [6.4901e-01, 3.4296e-01, 8.0266e-03],\n",
       "          [6.3451e-01, 3.5811e-01, 7.3766e-03],\n",
       "          [8.6777e-01, 1.0824e-01, 2.3989e-02],\n",
       "          [6.6162e-01, 3.3237e-01, 6.0073e-03],\n",
       "          [5.8680e-01, 3.8711e-01, 2.6090e-02],\n",
       "          [6.9307e-01, 2.9085e-01, 1.6071e-02],\n",
       "          [6.8387e-01, 3.0843e-01, 7.7021e-03],\n",
       "          [7.1893e-01, 2.6380e-01, 1.7277e-02],\n",
       "          [7.8275e-01, 2.1725e-01, 8.1562e-06],\n",
       "          [9.5628e-01, 4.3384e-02, 3.3610e-04],\n",
       "          [7.6234e-01, 2.3228e-01, 5.3868e-03],\n",
       "          [8.0365e-01, 1.8356e-01, 1.2786e-02],\n",
       "          [8.4563e-01, 1.4859e-01, 5.7783e-03]], grad_fn=<CopySlices>),\n",
       "  'pd2': tensor([[9.9954e-01, 2.3313e-04, 2.3067e-04],\n",
       "          [1.0000e+00, 1.5190e-08, 1.5081e-08],\n",
       "          [9.9990e-01, 5.0326e-05, 4.8273e-05],\n",
       "          [9.9880e-01, 6.1387e-04, 5.8831e-04],\n",
       "          [9.9684e-01, 1.6028e-03, 1.5557e-03],\n",
       "          [9.9672e-01, 1.6572e-03, 1.6222e-03],\n",
       "          [9.9991e-01, 4.3284e-05, 4.2757e-05],\n",
       "          [9.9969e-01, 1.5515e-04, 1.5465e-04],\n",
       "          [9.9956e-01, 2.2272e-04, 2.2201e-04],\n",
       "          [9.9749e-01, 1.2676e-03, 1.2429e-03],\n",
       "          [9.9832e-01, 8.4166e-04, 8.4052e-04],\n",
       "          [9.9996e-01, 2.0072e-05, 2.0054e-05],\n",
       "          [9.9054e-01, 4.9182e-03, 4.5369e-03],\n",
       "          [9.9922e-01, 3.9209e-04, 3.8551e-04],\n",
       "          [9.9664e-01, 1.7181e-03, 1.6418e-03],\n",
       "          [9.9521e-01, 2.4553e-03, 2.3318e-03],\n",
       "          [1.0000e+00, 7.2452e-07, 7.2351e-07],\n",
       "          [9.9599e-01, 2.0778e-03, 1.9365e-03],\n",
       "          [9.9480e-01, 2.6981e-03, 2.5006e-03],\n",
       "          [9.9952e-01, 2.4212e-04, 2.4081e-04],\n",
       "          [9.9878e-01, 6.2545e-04, 5.9208e-04],\n",
       "          [9.9916e-01, 4.2474e-04, 4.1244e-04],\n",
       "          [9.9100e-01, 4.5931e-03, 4.4069e-03],\n",
       "          [9.9981e-01, 9.8094e-05, 9.6097e-05],\n",
       "          [1.0000e+00, 2.2776e-10, 2.2772e-10],\n",
       "          [9.9853e-01, 7.6735e-04, 6.9853e-04],\n",
       "          [1.0000e+00, 8.5842e-09, 8.5821e-09],\n",
       "          [9.9991e-01, 4.6748e-05, 4.5998e-05],\n",
       "          [9.9911e-01, 4.5266e-04, 4.4021e-04],\n",
       "          [9.9980e-01, 1.0383e-04, 9.9681e-05],\n",
       "          [9.9966e-01, 1.7235e-04, 1.6965e-04],\n",
       "          [9.9749e-01, 1.2742e-03, 1.2314e-03],\n",
       "          [9.9831e-01, 8.5096e-04, 8.3691e-04],\n",
       "          [9.9593e-01, 2.0871e-03, 1.9805e-03],\n",
       "          [9.9931e-01, 3.4755e-04, 3.4028e-04],\n",
       "          [9.9946e-01, 2.7207e-04, 2.6414e-04],\n",
       "          [9.9851e-01, 7.5151e-04, 7.3911e-04],\n",
       "          [9.9858e-01, 7.4124e-04, 6.7659e-04],\n",
       "          [9.9928e-01, 3.6523e-04, 3.5254e-04],\n",
       "          [9.9894e-01, 5.3002e-04, 5.2634e-04],\n",
       "          [9.9754e-01, 1.2467e-03, 1.2148e-03],\n",
       "          [1.0000e+00, 3.6518e-08, 3.6479e-08],\n",
       "          [9.9965e-01, 1.7935e-04, 1.6786e-04],\n",
       "          [9.9894e-01, 5.3034e-04, 5.2852e-04],\n",
       "          [9.9933e-01, 3.4067e-04, 3.2829e-04],\n",
       "          [9.9999e-01, 4.0223e-06, 4.0119e-06],\n",
       "          [9.9671e-01, 1.6735e-03, 1.6197e-03],\n",
       "          [9.9935e-01, 3.3021e-04, 3.2067e-04],\n",
       "          [9.9587e-01, 2.0662e-03, 2.0686e-03],\n",
       "          [9.9504e-01, 2.5585e-03, 2.4047e-03],\n",
       "          [1.0000e+00, 2.1005e-06, 2.0214e-06],\n",
       "          [9.9565e-01, 2.1898e-03, 2.1562e-03],\n",
       "          [9.9799e-01, 1.0222e-03, 9.8550e-04],\n",
       "          [9.9610e-01, 1.9850e-03, 1.9167e-03],\n",
       "          [9.9749e-01, 1.2912e-03, 1.2211e-03],\n",
       "          [9.9735e-01, 1.3663e-03, 1.2845e-03],\n",
       "          [9.9835e-01, 8.3201e-04, 8.1566e-04],\n",
       "          [9.9834e-01, 8.5251e-04, 8.0782e-04],\n",
       "          [1.0000e+00, 7.3335e-09, 7.2557e-09],\n",
       "          [9.9515e-01, 2.4544e-03, 2.3934e-03],\n",
       "          [9.9878e-01, 6.3780e-04, 5.8526e-04],\n",
       "          [9.9617e-01, 1.9368e-03, 1.8946e-03],\n",
       "          [9.9475e-01, 2.6996e-03, 2.5535e-03],\n",
       "          [9.9972e-01, 1.3935e-04, 1.3804e-04],\n",
       "          [1.0000e+00, 3.4688e-10, 3.4659e-10],\n",
       "          [9.9761e-01, 1.2011e-03, 1.1865e-03],\n",
       "          [9.9852e-01, 7.7085e-04, 7.0700e-04],\n",
       "          [9.9891e-01, 5.5131e-04, 5.3779e-04],\n",
       "          [9.9715e-01, 1.4615e-03, 1.3857e-03],\n",
       "          [9.9620e-01, 1.9216e-03, 1.8816e-03],\n",
       "          [9.9665e-01, 1.6900e-03, 1.6640e-03],\n",
       "          [9.9141e-01, 4.5023e-03, 4.0838e-03],\n",
       "          [9.9932e-01, 3.4071e-04, 3.3595e-04],\n",
       "          [9.9856e-01, 7.2619e-04, 7.0963e-04],\n",
       "          [9.9913e-01, 4.3674e-04, 4.3339e-04],\n",
       "          [9.9989e-01, 5.6649e-05, 5.4896e-05],\n",
       "          [9.9989e-01, 5.3121e-05, 5.2395e-05],\n",
       "          [9.9450e-01, 2.8307e-03, 2.6714e-03],\n",
       "          [9.9962e-01, 1.9260e-04, 1.8865e-04],\n",
       "          [9.9920e-01, 4.0488e-04, 3.9272e-04],\n",
       "          [9.9563e-01, 2.2523e-03, 2.1183e-03],\n",
       "          [9.9932e-01, 3.4375e-04, 3.3781e-04],\n",
       "          [9.9876e-01, 6.3461e-04, 6.0702e-04],\n",
       "          [9.9972e-01, 1.4305e-04, 1.3773e-04],\n",
       "          [9.9996e-01, 1.9477e-05, 1.9462e-05],\n",
       "          [9.9928e-01, 3.6192e-04, 3.5941e-04],\n",
       "          [9.9936e-01, 3.2163e-04, 3.2002e-04],\n",
       "          [9.9985e-01, 7.8634e-05, 7.5847e-05],\n",
       "          [9.9913e-01, 4.4340e-04, 4.2445e-04],\n",
       "          [9.9952e-01, 2.4641e-04, 2.3653e-04],\n",
       "          [9.9796e-01, 1.0315e-03, 1.0111e-03],\n",
       "          [9.9854e-01, 7.2529e-04, 7.3293e-04],\n",
       "          [9.9459e-01, 2.7602e-03, 2.6467e-03],\n",
       "          [9.9963e-01, 1.8581e-04, 1.8503e-04],\n",
       "          [1.0000e+00, 1.5578e-06, 1.5584e-06],\n",
       "          [9.9994e-01, 3.2462e-05, 3.2090e-05],\n",
       "          [9.9839e-01, 8.1975e-04, 7.9373e-04],\n",
       "          [9.9998e-01, 1.0735e-05, 1.0437e-05],\n",
       "          [9.9795e-01, 1.0332e-03, 1.0127e-03],\n",
       "          [9.9915e-01, 4.2911e-04, 4.1722e-04],\n",
       "          [9.9956e-01, 2.2227e-04, 2.1694e-04],\n",
       "          [9.9900e-01, 5.0763e-04, 4.9617e-04],\n",
       "          [9.9587e-01, 2.0794e-03, 2.0500e-03],\n",
       "          [9.9998e-01, 8.1460e-06, 8.1312e-06],\n",
       "          [9.9924e-01, 3.9107e-04, 3.7164e-04],\n",
       "          [9.9690e-01, 1.5835e-03, 1.5163e-03],\n",
       "          [9.9790e-01, 1.0664e-03, 1.0383e-03],\n",
       "          [9.9856e-01, 7.2764e-04, 7.1363e-04],\n",
       "          [9.9984e-01, 8.1171e-05, 8.0460e-05],\n",
       "          [9.9702e-01, 1.5448e-03, 1.4321e-03],\n",
       "          [9.9930e-01, 3.5421e-04, 3.4327e-04],\n",
       "          [9.9578e-01, 2.1167e-03, 2.1014e-03],\n",
       "          [9.9660e-01, 1.7484e-03, 1.6547e-03],\n",
       "          [9.9926e-01, 3.7371e-04, 3.6944e-04],\n",
       "          [9.9435e-01, 2.8624e-03, 2.7893e-03],\n",
       "          [9.9851e-01, 7.4859e-04, 7.3979e-04],\n",
       "          [9.9835e-01, 8.3410e-04, 8.1108e-04],\n",
       "          [9.9966e-01, 1.7138e-04, 1.6961e-04],\n",
       "          [9.9956e-01, 2.2882e-04, 2.1283e-04],\n",
       "          [9.9934e-01, 3.2936e-04, 3.3227e-04],\n",
       "          [9.9930e-01, 3.6643e-04, 3.3615e-04],\n",
       "          [9.9906e-01, 4.9902e-04, 4.4558e-04],\n",
       "          [9.9551e-01, 2.2473e-03, 2.2422e-03],\n",
       "          [9.9072e-01, 4.7442e-03, 4.5405e-03],\n",
       "          [9.9837e-01, 8.2031e-04, 8.1226e-04],\n",
       "          [9.9821e-01, 9.1358e-04, 8.7963e-04],\n",
       "          [9.9988e-01, 5.9244e-05, 5.8410e-05],\n",
       "          [9.9994e-01, 2.9119e-05, 2.9025e-05],\n",
       "          [9.9988e-01, 6.1929e-05, 6.1535e-05],\n",
       "          [9.9732e-01, 1.3551e-03, 1.3276e-03],\n",
       "          [9.9218e-01, 3.9770e-03, 3.8426e-03],\n",
       "          [9.9949e-01, 2.5902e-04, 2.5451e-04],\n",
       "          [9.9824e-01, 8.9307e-04, 8.6565e-04],\n",
       "          [9.9621e-01, 1.9250e-03, 1.8624e-03],\n",
       "          [9.9824e-01, 9.0274e-04, 8.6045e-04],\n",
       "          [9.9716e-01, 1.4268e-03, 1.4139e-03],\n",
       "          [9.8725e-01, 6.4600e-03, 6.2882e-03],\n",
       "          [9.9795e-01, 1.0340e-03, 1.0170e-03],\n",
       "          [9.9415e-01, 3.0664e-03, 2.7824e-03],\n",
       "          [9.9523e-01, 2.4577e-03, 2.3117e-03],\n",
       "          [9.9882e-01, 6.0617e-04, 5.7755e-04],\n",
       "          [9.9393e-01, 3.1515e-03, 2.9216e-03],\n",
       "          [1.0000e+00, 2.1264e-08, 2.1263e-08],\n",
       "          [9.9999e-01, 4.9271e-06, 4.8823e-06],\n",
       "          [9.9946e-01, 2.7398e-04, 2.6650e-04],\n",
       "          [9.9763e-01, 1.2067e-03, 1.1598e-03],\n",
       "          [9.9898e-01, 5.2156e-04, 4.9665e-04]], grad_fn=<CopySlices>),\n",
       "  'mod': tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.0000e+00, 4.4783e-08, 4.4783e-08, 4.4784e-08, 4.4783e-08, 4.4783e-08],\n",
       "          [9.9932e-01, 1.3582e-04, 1.3576e-04, 1.3629e-04, 1.3596e-04, 1.3707e-04],\n",
       "          [1.0000e+00, 7.9010e-34, 7.9010e-34, 7.9010e-34, 7.9010e-34, 7.9010e-34],\n",
       "          [1.0000e+00, 7.9192e-35, 7.9192e-35, 7.9192e-35, 7.9192e-35, 7.9192e-35],\n",
       "          [9.9959e-01, 8.1127e-05, 8.1313e-05, 8.1117e-05, 8.1560e-05, 8.1098e-05],\n",
       "          [1.0000e+00, 1.1438e-39, 1.1438e-39, 1.1438e-39, 1.1438e-39, 1.1438e-39],\n",
       "          [1.0000e+00, 7.9051e-40, 7.9051e-40, 7.9051e-40, 7.9051e-40, 7.9051e-40],\n",
       "          [9.9941e-01, 1.1731e-04, 1.1798e-04, 1.1744e-04, 1.1815e-04, 1.1738e-04],\n",
       "          [9.9918e-01, 1.6436e-04, 1.6438e-04, 1.6482e-04, 1.6435e-04, 1.6457e-04],\n",
       "          [1.0000e+00, 1.7871e-36, 1.7871e-36, 1.7871e-36, 1.7871e-36, 1.7871e-36],\n",
       "          [1.0000e+00, 2.2882e-37, 2.2882e-37, 2.2882e-37, 2.2882e-37, 2.2882e-37],\n",
       "          [9.9892e-01, 2.1691e-04, 2.1694e-04, 2.1707e-04, 2.1691e-04, 2.1691e-04],\n",
       "          [9.9971e-01, 5.8338e-05, 5.8338e-05, 5.8338e-05, 5.8338e-05, 5.8338e-05],\n",
       "          [9.9776e-01, 4.4784e-04, 4.4823e-04, 4.4833e-04, 4.4836e-04, 4.4992e-04],\n",
       "          [9.9869e-01, 2.6216e-04, 2.6282e-04, 2.6221e-04, 2.6212e-04, 2.6380e-04],\n",
       "          [1.0000e+00, 2.5092e-07, 2.5094e-07, 2.5092e-07, 2.5092e-07, 2.5093e-07],\n",
       "          [1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [9.9880e-01, 2.4059e-04, 2.4028e-04, 2.4107e-04, 2.4076e-04, 2.4074e-04],\n",
       "          [9.9983e-01, 3.4815e-05, 3.4755e-05, 3.4926e-05, 3.4728e-05, 3.4790e-05],\n",
       "          [1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.0000e+00, 2.1720e-43, 2.1720e-43, 2.1720e-43, 2.1720e-43, 2.1720e-43],\n",
       "          [9.9615e-01, 7.6803e-04, 7.6866e-04, 7.6734e-04, 7.6733e-04, 7.7714e-04],\n",
       "          [1.0000e+00, 3.5101e-08, 3.5101e-08, 3.5103e-08, 3.5101e-08, 3.5101e-08],\n",
       "          [1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [9.9224e-01, 1.5454e-03, 1.5430e-03, 1.5658e-03, 1.5421e-03, 1.5636e-03],\n",
       "          [1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.0000e+00, 6.8231e-40, 6.8231e-40, 6.8231e-40, 6.8231e-40, 6.8231e-40],\n",
       "          [9.9891e-01, 2.1869e-04, 2.1869e-04, 2.1869e-04, 2.1869e-04, 2.1869e-04],\n",
       "          [1.0000e+00, 3.2013e-37, 3.2013e-37, 3.2013e-37, 3.2013e-37, 3.2013e-37],\n",
       "          [1.0000e+00, 4.0538e-41, 4.0538e-41, 4.0538e-41, 4.0538e-41, 4.0538e-41],\n",
       "          [1.0000e+00, 4.3580e-43, 4.3580e-43, 4.3580e-43, 4.3580e-43, 4.3580e-43],\n",
       "          [1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [9.9877e-01, 2.4468e-04, 2.4483e-04, 2.4536e-04, 2.4482e-04, 2.4613e-04],\n",
       "          [9.9927e-01, 1.4503e-04, 1.4503e-04, 1.4521e-04, 1.4518e-04, 1.4535e-04],\n",
       "          [1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [9.9872e-01, 2.5489e-04, 2.5470e-04, 2.5639e-04, 2.5471e-04, 2.5509e-04],\n",
       "          [9.9893e-01, 2.1364e-04, 2.1333e-04, 2.1364e-04, 2.1378e-04, 2.1367e-04],\n",
       "          [9.9990e-01, 2.0654e-05, 2.0651e-05, 2.0658e-05, 2.0679e-05, 2.0661e-05],\n",
       "          [9.9947e-01, 1.0650e-04, 1.0650e-04, 1.0650e-04, 1.0650e-04, 1.0650e-04],\n",
       "          [1.0000e+00, 3.8442e-37, 3.8442e-37, 3.8442e-37, 3.8442e-37, 3.8442e-37],\n",
       "          [1.0000e+00, 1.6008e-07, 1.6008e-07, 1.6009e-07, 1.6009e-07, 1.6009e-07],\n",
       "          [1.0000e+00, 1.4051e-39, 1.4051e-39, 1.4051e-39, 1.4051e-39, 1.4051e-39],\n",
       "          [1.0000e+00, 2.2570e-31, 2.2570e-31, 2.2570e-31, 2.2570e-31, 2.2570e-31],\n",
       "          [1.0000e+00, 2.5525e-40, 2.5525e-40, 2.5525e-40, 2.5525e-40, 2.5525e-40],\n",
       "          [1.0000e+00, 1.0021e-36, 1.0021e-36, 1.0021e-36, 1.0021e-36, 1.0021e-36],\n",
       "          [1.0000e+00, 1.5016e-39, 1.5016e-39, 1.5016e-39, 1.5016e-39, 1.5016e-39],\n",
       "          [1.0000e+00, 2.5161e-36, 2.5161e-36, 2.5161e-36, 2.5161e-36, 2.5161e-36],\n",
       "          [9.9180e-01, 1.6491e-03, 1.6277e-03, 1.6426e-03, 1.6300e-03, 1.6495e-03],\n",
       "          [9.9830e-01, 3.3952e-04, 3.3925e-04, 3.3979e-04, 3.3946e-04, 3.3910e-04],\n",
       "          [9.9995e-01, 1.0935e-05, 1.0932e-05, 1.0937e-05, 1.0932e-05, 1.0934e-05],\n",
       "          [9.9631e-01, 7.3167e-04, 7.3340e-04, 7.5281e-04, 7.3687e-04, 7.3812e-04],\n",
       "          [9.9770e-01, 4.5971e-04, 4.5971e-04, 4.5971e-04, 4.5971e-04, 4.5971e-04],\n",
       "          [9.9628e-01, 7.4063e-04, 7.4111e-04, 7.5409e-04, 7.4000e-04, 7.4293e-04],\n",
       "          [9.9932e-01, 1.3514e-04, 1.3510e-04, 1.3548e-04, 1.3520e-04, 1.3540e-04],\n",
       "          [9.9937e-01, 1.2618e-04, 1.2694e-04, 1.2625e-04, 1.2617e-04, 1.2663e-04],\n",
       "          [1.0000e+00, 1.2861e-39, 1.2861e-39, 1.2861e-39, 1.2861e-39, 1.2861e-39],\n",
       "          [1.0000e+00, 3.6334e-36, 3.6334e-36, 3.6334e-36, 3.6334e-36, 3.6334e-36],\n",
       "          [1.0000e+00, 1.3784e-08, 1.3784e-08, 1.3784e-08, 1.3784e-08, 1.3784e-08],\n",
       "          [9.9436e-01, 1.1296e-03, 1.1157e-03, 1.1448e-03, 1.1187e-03, 1.1289e-03],\n",
       "          [1.0000e+00, 1.1286e-39, 1.1286e-39, 1.1286e-39, 1.1286e-39, 1.1286e-39],\n",
       "          [9.9853e-01, 2.9329e-04, 2.9154e-04, 2.9445e-04, 2.9456e-04, 2.9436e-04],\n",
       "          [9.9964e-01, 7.1791e-05, 7.1832e-05, 7.1895e-05, 7.1646e-05, 7.1648e-05],\n",
       "          [9.9867e-01, 2.6517e-04, 2.6486e-04, 2.7318e-04, 2.6552e-04, 2.6512e-04],\n",
       "          [1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [9.9975e-01, 5.0242e-05, 5.0177e-05, 5.0244e-05, 5.0183e-05, 5.0262e-05],\n",
       "          [1.0000e+00, 4.3920e-38, 4.3920e-38, 4.3920e-38, 4.3920e-38, 4.3920e-38],\n",
       "          [1.0000e+00, 9.5077e-41, 9.5077e-41, 9.5077e-41, 9.5077e-41, 9.5077e-41],\n",
       "          [9.9492e-01, 1.0170e-03, 1.0146e-03, 1.0163e-03, 1.0147e-03, 1.0155e-03],\n",
       "          [9.9529e-01, 9.3911e-04, 9.4030e-04, 9.4321e-04, 9.3874e-04, 9.4423e-04],\n",
       "          [1.0000e+00, 4.3638e-33, 4.3638e-33, 4.3638e-33, 4.3638e-33, 4.3638e-33],\n",
       "          [9.9443e-01, 1.1146e-03, 1.1079e-03, 1.1104e-03, 1.1131e-03, 1.1232e-03],\n",
       "          [1.0000e+00, 2.2561e-43, 2.2561e-43, 2.2561e-43, 2.2561e-43, 2.2561e-43],\n",
       "          [9.9933e-01, 1.3355e-04, 1.3344e-04, 1.3385e-04, 1.3335e-04, 1.3408e-04],\n",
       "          [9.9989e-01, 2.1146e-05, 2.1087e-05, 2.1109e-05, 2.1083e-05, 2.1195e-05],\n",
       "          [9.9833e-01, 3.3255e-04, 3.3310e-04, 3.3378e-04, 3.3352e-04, 3.3598e-04],\n",
       "          [1.0000e+00, 1.5933e-42, 1.5933e-42, 1.5933e-42, 1.5933e-42, 1.5933e-42],\n",
       "          [9.9778e-01, 4.4318e-04, 4.4413e-04, 4.4537e-04, 4.4298e-04, 4.4455e-04],\n",
       "          [1.0000e+00, 5.0543e-41, 5.0543e-41, 5.0543e-41, 5.0543e-41, 5.0543e-41],\n",
       "          [1.0000e+00, 5.3052e-33, 5.3052e-33, 5.3052e-33, 5.3052e-33, 5.3052e-33],\n",
       "          [9.9951e-01, 9.7195e-05, 9.7245e-05, 9.7296e-05, 9.7213e-05, 9.7292e-05],\n",
       "          [9.9951e-01, 9.7895e-05, 9.8054e-05, 9.8187e-05, 9.7797e-05, 9.8361e-05],\n",
       "          [1.0000e+00, 6.6262e-37, 6.6262e-37, 6.6262e-37, 6.6262e-37, 6.6262e-37],\n",
       "          [9.9994e-01, 1.2478e-05, 1.2454e-05, 1.2455e-05, 1.2476e-05, 1.2463e-05],\n",
       "          [9.9988e-01, 2.3688e-05, 2.3682e-05, 2.3707e-05, 2.3696e-05, 2.3695e-05],\n",
       "          [9.9963e-01, 7.4719e-05, 7.4759e-05, 7.5372e-05, 7.4829e-05, 7.5154e-05],\n",
       "          [1.0000e+00, 7.0819e-35, 7.0819e-35, 7.0819e-35, 7.0819e-35, 7.0819e-35],\n",
       "          [1.0000e+00, 2.8278e-42, 2.8278e-42, 2.8278e-42, 2.8278e-42, 2.8278e-42],\n",
       "          [9.9961e-01, 7.8256e-05, 7.8071e-05, 7.8123e-05, 7.8080e-05, 7.8262e-05],\n",
       "          [1.0000e+00, 9.8091e-45, 9.8091e-45, 9.8091e-45, 9.8091e-45, 9.8091e-45],\n",
       "          [9.9571e-01, 8.5639e-04, 8.5640e-04, 8.6082e-04, 8.5641e-04, 8.5888e-04],\n",
       "          [1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [9.8798e-01, 2.4157e-03, 2.3944e-03, 2.4143e-03, 2.3892e-03, 2.4066e-03],\n",
       "          [1.0000e+00, 7.0910e-40, 7.0910e-40, 7.0910e-40, 7.0910e-40, 7.0910e-40],\n",
       "          [1.0000e+00, 3.0412e-08, 3.0412e-08, 3.0413e-08, 3.0412e-08, 3.0414e-08],\n",
       "          [9.9997e-01, 6.7373e-06, 6.7369e-06, 6.7374e-06, 6.7364e-06, 6.7485e-06],\n",
       "          [1.0000e+00, 3.5599e-39, 3.5599e-39, 3.5599e-39, 3.5599e-39, 3.5599e-39],\n",
       "          [1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [9.9873e-01, 2.5341e-04, 2.5217e-04, 2.5288e-04, 2.5437e-04, 2.5273e-04],\n",
       "          [9.9975e-01, 5.0420e-05, 5.0527e-05, 5.0437e-05, 5.0422e-05, 5.0455e-05],\n",
       "          [9.9861e-01, 2.7683e-04, 2.7659e-04, 2.7771e-04, 2.7754e-04, 2.7827e-04],\n",
       "          [1.0000e+00, 1.1839e-30, 1.1839e-30, 1.1839e-30, 1.1839e-30, 1.1839e-30],\n",
       "          [9.9742e-01, 5.1749e-04, 5.1481e-04, 5.1778e-04, 5.1216e-04, 5.1772e-04],\n",
       "          [1.0000e+00, 7.6416e-41, 7.6416e-41, 7.6416e-41, 7.6416e-41, 7.6416e-41],\n",
       "          [1.0000e+00, 1.7893e-35, 1.7893e-35, 1.7893e-35, 1.7893e-35, 1.7893e-35],\n",
       "          [9.9622e-01, 7.5300e-04, 7.5516e-04, 7.5755e-04, 7.5580e-04, 7.6147e-04],\n",
       "          [1.0000e+00, 4.4955e-36, 4.4955e-36, 4.4955e-36, 4.4955e-36, 4.4955e-36],\n",
       "          [9.9167e-01, 1.6659e-03, 1.6632e-03, 1.6736e-03, 1.6630e-03, 1.6688e-03],\n",
       "          [9.9965e-01, 7.0610e-05, 7.0576e-05, 7.0689e-05, 7.0426e-05, 7.0416e-05],\n",
       "          [1.0000e+00, 2.3822e-43, 2.3822e-43, 2.3822e-43, 2.3822e-43, 2.3822e-43],\n",
       "          [9.9959e-01, 8.1299e-05, 8.1252e-05, 8.1767e-05, 8.1288e-05, 8.1762e-05],\n",
       "          [9.9950e-01, 1.0012e-04, 1.0000e-04, 9.9962e-05, 1.0001e-04, 1.0003e-04],\n",
       "          [9.9968e-01, 6.3906e-05, 6.4008e-05, 6.4162e-05, 6.3922e-05, 6.4217e-05],\n",
       "          [9.9868e-01, 2.6659e-04, 2.6345e-04, 2.6366e-04, 2.6331e-04, 2.6548e-04],\n",
       "          [9.9782e-01, 4.3669e-04, 4.3595e-04, 4.3687e-04, 4.3718e-04, 4.3680e-04],\n",
       "          [1.0000e+00, 1.4522e-38, 1.4522e-38, 1.4522e-38, 1.4522e-38, 1.4522e-38],\n",
       "          [9.9938e-01, 1.2364e-04, 1.2364e-04, 1.2364e-04, 1.2364e-04, 1.2364e-04],\n",
       "          [1.0000e+00, 5.0617e-37, 5.0617e-37, 5.0617e-37, 5.0617e-37, 5.0617e-37],\n",
       "          [9.9675e-01, 6.5178e-04, 6.5232e-04, 6.4747e-04, 6.4777e-04, 6.5245e-04],\n",
       "          [9.9892e-01, 2.1698e-04, 2.1658e-04, 2.1617e-04, 2.1624e-04, 2.1686e-04],\n",
       "          [9.9798e-01, 4.0313e-04, 4.0288e-04, 4.0349e-04, 4.0463e-04, 4.0487e-04],\n",
       "          [9.9842e-01, 3.1546e-04, 3.1591e-04, 3.1593e-04, 3.1766e-04, 3.1848e-04],\n",
       "          [9.9826e-01, 3.4667e-04, 3.4729e-04, 3.4645e-04, 3.4726e-04, 3.5150e-04],\n",
       "          [9.9676e-01, 6.4805e-04, 6.4431e-04, 6.5402e-04, 6.4479e-04, 6.5017e-04],\n",
       "          [1.0000e+00, 4.5688e-36, 4.5688e-36, 4.5688e-36, 4.5688e-36, 4.5688e-36],\n",
       "          [9.9954e-01, 9.0942e-05, 9.0861e-05, 9.1069e-05, 9.0948e-05, 9.1199e-05],\n",
       "          [9.9644e-01, 7.1218e-04, 7.1218e-04, 7.1218e-04, 7.1218e-04, 7.1218e-04],\n",
       "          [9.9843e-01, 3.1486e-04, 3.1304e-04, 3.1426e-04, 3.1314e-04, 3.1320e-04],\n",
       "          [1.0000e+00, 5.6184e-35, 5.6184e-35, 5.6184e-35, 5.6184e-35, 5.6184e-35],\n",
       "          [9.9904e-01, 1.9226e-04, 1.9227e-04, 1.9313e-04, 1.9216e-04, 1.9225e-04],\n",
       "          [9.9962e-01, 7.5636e-05, 7.5713e-05, 7.5909e-05, 7.5683e-05, 7.5897e-05],\n",
       "          [1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [9.9365e-01, 1.2652e-03, 1.2666e-03, 1.2666e-03, 1.2656e-03, 1.2832e-03],\n",
       "          [9.9602e-01, 7.9428e-04, 8.1051e-04, 7.8864e-04, 7.9119e-04, 7.9445e-04],\n",
       "          [1.0000e+00, 1.4013e-45, 1.4013e-45, 1.4013e-45, 1.4013e-45, 1.4013e-45],\n",
       "          [9.9919e-01, 1.6214e-04, 1.6209e-04, 1.6285e-04, 1.6242e-04, 1.6403e-04],\n",
       "          [1.0000e+00, 1.8600e-36, 1.8600e-36, 1.8600e-36, 1.8600e-36, 1.8600e-36],\n",
       "          [9.9330e-01, 1.3349e-03, 1.3337e-03, 1.3426e-03, 1.3400e-03, 1.3516e-03],\n",
       "          [1.0000e+00, 1.0297e-36, 1.0297e-36, 1.0297e-36, 1.0297e-36, 1.0297e-36],\n",
       "          [9.9235e-01, 1.5225e-03, 1.5228e-03, 1.5379e-03, 1.5246e-03, 1.5471e-03],\n",
       "          [9.9614e-01, 7.7175e-04, 7.7089e-04, 7.7145e-04, 7.7042e-04, 7.7878e-04],\n",
       "          [9.9671e-01, 6.5691e-04, 6.5685e-04, 6.5813e-04, 6.5636e-04, 6.5787e-04],\n",
       "          [1.0000e+00, 5.1376e-40, 5.1376e-40, 5.1376e-40, 5.1376e-40, 5.1376e-40],\n",
       "          [9.9890e-01, 2.2072e-04, 2.2072e-04, 2.2072e-04, 2.2072e-04, 2.2072e-04],\n",
       "          [9.8446e-01, 3.1079e-03, 3.0989e-03, 3.1175e-03, 3.1034e-03, 3.1081e-03],\n",
       "          [9.9654e-01, 6.9086e-04, 6.9256e-04, 6.9044e-04, 6.9113e-04, 6.9588e-04]],\n",
       "         grad_fn=<CopySlices>),\n",
       "  'cc': tensor([[6.0183e-02, 8.4881e-01, 4.7359e-02, 4.3644e-02],\n",
       "          [1.0126e-02, 9.8590e-01, 2.0740e-03, 1.8962e-03],\n",
       "          [2.8941e-01, 6.1555e-01, 4.9764e-02, 4.5280e-02],\n",
       "          [8.0246e-02, 8.5667e-01, 3.3025e-02, 3.0057e-02],\n",
       "          [2.1940e-01, 5.8813e-01, 1.0020e-01, 9.2272e-02],\n",
       "          [1.3554e-01, 7.4857e-01, 6.1451e-02, 5.4437e-02],\n",
       "          [6.9123e-02, 8.7128e-01, 3.1495e-02, 2.8100e-02],\n",
       "          [7.7186e-02, 8.6127e-01, 3.3783e-02, 2.7756e-02],\n",
       "          [9.6566e-02, 8.3656e-01, 3.6153e-02, 3.0725e-02],\n",
       "          [1.2613e-01, 7.9727e-01, 4.3087e-02, 3.3514e-02],\n",
       "          [1.5778e-01, 7.1142e-01, 7.3947e-02, 5.6853e-02],\n",
       "          [1.4109e-01, 8.0493e-01, 2.7512e-02, 2.6463e-02],\n",
       "          [2.9008e-01, 4.9747e-01, 1.2051e-01, 9.1948e-02],\n",
       "          [1.3635e-01, 7.5924e-01, 5.8779e-02, 4.5632e-02],\n",
       "          [2.3596e-01, 5.5797e-01, 1.0896e-01, 9.7105e-02],\n",
       "          [2.7070e-01, 4.8172e-01, 1.3198e-01, 1.1561e-01],\n",
       "          [2.5040e-02, 9.5192e-01, 1.2424e-02, 1.0616e-02],\n",
       "          [1.4827e-01, 7.2507e-01, 6.9952e-02, 5.6707e-02],\n",
       "          [1.4277e-01, 6.5590e-01, 1.1298e-01, 8.8350e-02],\n",
       "          [1.2790e-01, 7.4969e-01, 6.5417e-02, 5.6990e-02],\n",
       "          [1.1152e-01, 7.8000e-01, 5.7735e-02, 5.0738e-02],\n",
       "          [1.2070e-01, 7.6882e-01, 5.8826e-02, 5.1654e-02],\n",
       "          [2.1944e-01, 5.4295e-01, 1.2415e-01, 1.1346e-01],\n",
       "          [1.4085e-01, 7.4192e-01, 6.0641e-02, 5.6592e-02],\n",
       "          [1.3171e-03, 9.9812e-01, 2.8329e-04, 2.8105e-04],\n",
       "          [1.5889e-01, 7.2837e-01, 6.0211e-02, 5.2528e-02],\n",
       "          [2.8928e-03, 9.9389e-01, 1.6481e-03, 1.5706e-03],\n",
       "          [5.1177e-02, 8.7750e-01, 3.7003e-02, 3.4322e-02],\n",
       "          [1.1262e-01, 7.7464e-01, 6.1814e-02, 5.0918e-02],\n",
       "          [2.0544e-01, 6.7005e-01, 7.0242e-02, 5.4277e-02],\n",
       "          [1.4673e-01, 7.4236e-01, 6.3243e-02, 4.7667e-02],\n",
       "          [1.0379e-01, 7.8921e-01, 5.6423e-02, 5.0578e-02],\n",
       "          [1.6694e-01, 6.7360e-01, 8.4512e-02, 7.4948e-02],\n",
       "          [1.6358e-01, 6.8708e-01, 8.5949e-02, 6.3388e-02],\n",
       "          [1.6232e-01, 7.3154e-01, 5.6297e-02, 4.9843e-02],\n",
       "          [1.7459e-01, 6.8957e-01, 6.8476e-02, 6.7370e-02],\n",
       "          [1.5788e-01, 7.1574e-01, 7.1377e-02, 5.5003e-02],\n",
       "          [1.3276e-01, 7.3495e-01, 7.7067e-02, 5.5216e-02],\n",
       "          [7.3049e-02, 8.2520e-01, 5.7707e-02, 4.4043e-02],\n",
       "          [1.4334e-01, 7.6251e-01, 4.9045e-02, 4.5101e-02],\n",
       "          [1.7614e-01, 6.3926e-01, 1.0874e-01, 7.5867e-02],\n",
       "          [1.1592e-01, 8.6438e-01, 1.0224e-02, 9.4741e-03],\n",
       "          [6.8510e-02, 8.3774e-01, 4.9311e-02, 4.4441e-02],\n",
       "          [1.0618e-01, 7.9424e-01, 5.3446e-02, 4.6138e-02],\n",
       "          [2.1193e-01, 6.2209e-01, 9.9784e-02, 6.6192e-02],\n",
       "          [1.6002e-02, 9.6815e-01, 8.3899e-03, 7.4573e-03],\n",
       "          [1.7645e-01, 6.7790e-01, 7.9357e-02, 6.6298e-02],\n",
       "          [8.6700e-02, 8.0926e-01, 5.4747e-02, 4.9293e-02],\n",
       "          [1.7613e-01, 6.6687e-01, 8.6407e-02, 7.0597e-02],\n",
       "          [2.4949e-01, 5.3855e-01, 1.2080e-01, 9.1164e-02],\n",
       "          [3.2188e-02, 9.4959e-01, 9.9364e-03, 8.2876e-03],\n",
       "          [1.7555e-01, 6.6640e-01, 8.8818e-02, 6.9234e-02],\n",
       "          [1.3636e-01, 6.9405e-01, 9.3450e-02, 7.6142e-02],\n",
       "          [2.5307e-01, 5.4143e-01, 1.1018e-01, 9.5322e-02],\n",
       "          [2.2630e-01, 6.0907e-01, 8.6274e-02, 7.8352e-02],\n",
       "          [1.0558e-01, 7.8501e-01, 5.8661e-02, 5.0750e-02],\n",
       "          [1.9493e-01, 6.3317e-01, 9.6862e-02, 7.5037e-02],\n",
       "          [1.7901e-01, 6.8942e-01, 7.0307e-02, 6.1262e-02],\n",
       "          [1.1107e-03, 9.9771e-01, 5.9875e-04, 5.7836e-04],\n",
       "          [2.1968e-01, 6.3824e-01, 7.4342e-02, 6.7738e-02],\n",
       "          [2.1171e-01, 6.1013e-01, 1.0095e-01, 7.7210e-02],\n",
       "          [1.3101e-01, 7.4209e-01, 6.6971e-02, 5.9923e-02],\n",
       "          [1.7710e-01, 6.1224e-01, 1.2086e-01, 8.9802e-02],\n",
       "          [1.4000e-01, 7.9683e-01, 3.4659e-02, 2.8512e-02],\n",
       "          [1.8033e-02, 9.7420e-01, 3.8914e-03, 3.8775e-03],\n",
       "          [1.7614e-01, 6.6814e-01, 8.4118e-02, 7.1596e-02],\n",
       "          [2.7283e-01, 5.1591e-01, 1.1749e-01, 9.3762e-02],\n",
       "          [1.1587e-01, 8.1005e-01, 3.9730e-02, 3.4343e-02],\n",
       "          [9.2969e-02, 8.0634e-01, 5.3199e-02, 4.7493e-02],\n",
       "          [8.3807e-02, 8.3896e-01, 4.4818e-02, 3.2417e-02],\n",
       "          [8.1832e-02, 8.2774e-01, 4.9442e-02, 4.0983e-02],\n",
       "          [1.8481e-01, 6.4831e-01, 9.3451e-02, 7.3430e-02],\n",
       "          [9.9200e-02, 7.9817e-01, 5.4868e-02, 4.7759e-02],\n",
       "          [2.7871e-01, 5.7098e-01, 8.0040e-02, 7.0272e-02],\n",
       "          [1.8418e-01, 6.5663e-01, 8.7999e-02, 7.1192e-02],\n",
       "          [3.5346e-02, 9.2341e-01, 2.2057e-02, 1.9182e-02],\n",
       "          [4.2733e-02, 9.2261e-01, 1.7744e-02, 1.6914e-02],\n",
       "          [2.2795e-01, 5.5263e-01, 1.2071e-01, 9.8708e-02],\n",
       "          [5.4975e-02, 8.7863e-01, 3.4474e-02, 3.1916e-02],\n",
       "          [8.4124e-02, 8.3213e-01, 4.7287e-02, 3.6463e-02],\n",
       "          [2.0921e-01, 6.5409e-01, 7.3009e-02, 6.3692e-02],\n",
       "          [1.4786e-01, 7.4695e-01, 6.2942e-02, 4.2244e-02],\n",
       "          [1.3002e-01, 7.7713e-01, 5.1155e-02, 4.1698e-02],\n",
       "          [8.0634e-02, 8.1221e-01, 5.6616e-02, 5.0543e-02],\n",
       "          [2.5767e-02, 9.5093e-01, 1.2476e-02, 1.0823e-02],\n",
       "          [1.5035e-01, 7.3304e-01, 6.5596e-02, 5.1015e-02],\n",
       "          [1.0829e-01, 8.3144e-01, 3.2265e-02, 2.7998e-02],\n",
       "          [5.0732e-02, 8.9118e-01, 3.0126e-02, 2.7959e-02],\n",
       "          [1.5947e-01, 6.9302e-01, 8.0854e-02, 6.6660e-02],\n",
       "          [1.1389e-01, 7.8157e-01, 5.5249e-02, 4.9292e-02],\n",
       "          [1.6333e-01, 7.4139e-01, 4.9887e-02, 4.5394e-02],\n",
       "          [2.4127e-01, 5.0361e-01, 1.3896e-01, 1.1615e-01],\n",
       "          [1.8695e-01, 6.6060e-01, 7.9962e-02, 7.2488e-02],\n",
       "          [1.7056e-01, 6.7280e-01, 8.2143e-02, 7.4502e-02],\n",
       "          [3.3336e-02, 9.5432e-01, 6.2631e-03, 6.0828e-03],\n",
       "          [8.2101e-02, 8.3893e-01, 4.3276e-02, 3.5696e-02],\n",
       "          [7.6142e-02, 8.3343e-01, 4.6653e-02, 4.3780e-02],\n",
       "          [3.4477e-02, 9.3310e-01, 1.6909e-02, 1.5513e-02],\n",
       "          [2.0201e-01, 6.1397e-01, 9.9623e-02, 8.4400e-02],\n",
       "          [9.9546e-02, 8.2027e-01, 4.3640e-02, 3.6542e-02],\n",
       "          [4.9163e-02, 8.9372e-01, 3.0096e-02, 2.7020e-02],\n",
       "          [2.1448e-01, 6.7216e-01, 6.2633e-02, 5.0725e-02],\n",
       "          [1.7074e-01, 7.0289e-01, 6.6508e-02, 5.9864e-02],\n",
       "          [4.2253e-02, 9.2683e-01, 1.7314e-02, 1.3601e-02],\n",
       "          [1.5641e-01, 7.4310e-01, 5.5812e-02, 4.4673e-02],\n",
       "          [1.6040e-01, 7.1673e-01, 6.4262e-02, 5.8616e-02],\n",
       "          [2.3253e-01, 6.2178e-01, 7.4941e-02, 7.0743e-02],\n",
       "          [1.9635e-01, 6.4541e-01, 8.4821e-02, 7.3418e-02],\n",
       "          [1.4982e-01, 7.8400e-01, 3.5467e-02, 3.0714e-02],\n",
       "          [1.5917e-01, 7.3617e-01, 5.5964e-02, 4.8701e-02],\n",
       "          [1.4915e-01, 7.5560e-01, 5.2904e-02, 4.2349e-02],\n",
       "          [1.9676e-01, 6.6237e-01, 7.4061e-02, 6.6809e-02],\n",
       "          [2.2288e-01, 5.9434e-01, 1.0084e-01, 8.1943e-02],\n",
       "          [1.1443e-01, 7.6421e-01, 6.3485e-02, 5.7869e-02],\n",
       "          [2.2130e-01, 6.2828e-01, 8.2437e-02, 6.7985e-02],\n",
       "          [1.3399e-01, 7.2438e-01, 7.8247e-02, 6.3382e-02],\n",
       "          [1.2045e-01, 8.0619e-01, 3.8977e-02, 3.4382e-02],\n",
       "          [5.0762e-02, 9.0076e-01, 2.5787e-02, 2.2690e-02],\n",
       "          [1.1940e-01, 7.8360e-01, 5.0420e-02, 4.6580e-02],\n",
       "          [7.3775e-02, 8.5691e-01, 3.6543e-02, 3.2770e-02],\n",
       "          [1.5937e-01, 7.4116e-01, 5.0859e-02, 4.8614e-02],\n",
       "          [1.1955e-01, 7.7188e-01, 5.8997e-02, 4.9579e-02],\n",
       "          [1.6907e-01, 6.7902e-01, 8.6075e-02, 6.5840e-02],\n",
       "          [2.0543e-01, 6.4677e-01, 7.9505e-02, 6.8293e-02],\n",
       "          [8.1372e-02, 8.3003e-01, 4.7935e-02, 4.0660e-02],\n",
       "          [2.3766e-01, 6.6216e-01, 5.3887e-02, 4.6301e-02],\n",
       "          [6.6372e-02, 8.7604e-01, 3.2864e-02, 2.4721e-02],\n",
       "          [6.6307e-02, 8.9014e-01, 2.2168e-02, 2.1385e-02],\n",
       "          [2.9541e-02, 9.3484e-01, 1.8944e-02, 1.6672e-02],\n",
       "          [1.7218e-01, 6.6459e-01, 9.3212e-02, 7.0012e-02],\n",
       "          [2.3944e-01, 5.4207e-01, 1.1374e-01, 1.0475e-01],\n",
       "          [1.3253e-01, 7.2023e-01, 8.4074e-02, 6.3168e-02],\n",
       "          [7.4755e-02, 8.2755e-01, 5.1269e-02, 4.6423e-02],\n",
       "          [1.3087e-01, 7.2602e-01, 7.9894e-02, 6.3219e-02],\n",
       "          [1.0748e-01, 7.6299e-01, 6.9747e-02, 5.9781e-02],\n",
       "          [1.2134e-01, 7.4283e-01, 7.2827e-02, 6.3006e-02],\n",
       "          [1.7066e-01, 6.5430e-01, 9.6175e-02, 7.8867e-02],\n",
       "          [1.7297e-01, 6.5043e-01, 9.5223e-02, 8.1380e-02],\n",
       "          [2.7180e-01, 5.0972e-01, 1.1992e-01, 9.8558e-02],\n",
       "          [1.7667e-01, 6.7076e-01, 8.3714e-02, 6.8859e-02],\n",
       "          [1.3377e-01, 7.6675e-01, 5.3982e-02, 4.5500e-02],\n",
       "          [2.0161e-01, 6.4371e-01, 8.2269e-02, 7.2412e-02],\n",
       "          [7.0775e-02, 9.0441e-01, 1.2559e-02, 1.2260e-02],\n",
       "          [5.5917e-02, 9.1909e-01, 1.2613e-02, 1.2379e-02],\n",
       "          [1.7256e-01, 7.3845e-01, 4.5713e-02, 4.3273e-02],\n",
       "          [1.2217e-01, 7.4741e-01, 6.9744e-02, 6.0675e-02],\n",
       "          [2.2491e-01, 6.4797e-01, 6.5220e-02, 6.1906e-02]],\n",
       "         grad_fn=<CopySlices>),\n",
       "  'dlt1': tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [5.3135e-02, 1.2767e-02, 4.2875e-03, 4.8439e-03, 8.0217e-03],\n",
       "          [7.3635e-02, 2.2275e-01, 6.2539e-02, 1.2302e-02, 1.2036e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [6.5603e-02, 1.2277e-01, 6.1825e-02, 2.5863e-02, 5.9208e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [3.9295e-01, 3.2265e-01, 8.6575e-02, 2.0683e-02, 9.9831e-02],\n",
       "          [1.8174e-01, 1.3862e-01, 6.8754e-02, 7.9620e-03, 4.0245e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [6.0038e-02, 2.4474e-01, 3.5720e-02, 1.8767e-02, 2.3903e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.0504e-01, 1.8176e-01, 4.8048e-02, 7.1321e-03, 7.2826e-02],\n",
       "          [1.6585e-01, 2.5268e-01, 4.8961e-02, 1.6469e-02, 1.9488e-01],\n",
       "          [9.6349e-03, 5.2285e-02, 1.9356e-04, 1.2516e-04, 8.2124e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.2270e-01, 1.3136e-01, 1.6384e-01, 2.0642e-02, 1.4147e-01],\n",
       "          [2.2345e-01, 4.2972e-02, 1.5934e-01, 1.1730e-02, 2.6927e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.2948e-01, 1.9164e-01, 7.6355e-02, 4.5466e-02, 6.5223e-02],\n",
       "          [1.6182e-01, 7.2780e-02, 1.3719e-02, 1.1859e-04, 9.4028e-03],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.7026e-01, 2.0096e-01, 1.6286e-01, 1.7511e-02, 4.3292e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [9.6977e-02, 1.0467e-01, 6.6593e-02, 3.7946e-02, 1.9070e-01],\n",
       "          [1.8406e-01, 1.7700e-01, 3.2478e-02, 1.1771e-02, 3.1068e-02],\n",
       "          [1.2461e-12, 2.1996e-16, 6.7516e-24, 5.5202e-23, 4.9024e-13],\n",
       "          [1.2338e-01, 6.9289e-02, 5.5269e-02, 3.6226e-02, 1.0603e-01],\n",
       "          [2.3468e-01, 1.6757e-01, 3.6548e-02, 1.3308e-02, 5.2845e-02],\n",
       "          [6.9329e-03, 6.6813e-02, 8.8254e-03, 1.4226e-02, 5.4997e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [9.5354e-02, 9.8280e-03, 2.7590e-03, 4.8436e-04, 6.4408e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [4.0306e-01, 1.6148e-01, 5.5038e-02, 3.3831e-02, 1.6402e-01],\n",
       "          [2.0133e-01, 2.7413e-01, 4.7455e-02, 2.5603e-02, 1.2088e-01],\n",
       "          [4.7545e-02, 6.3203e-02, 4.6159e-03, 1.2884e-02, 1.3538e-02],\n",
       "          [1.1305e-01, 1.9733e-01, 9.1616e-02, 3.9583e-02, 7.2625e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.7919e-01, 2.2417e-01, 1.3225e-01, 3.0787e-02, 1.3910e-01],\n",
       "          [2.1292e-01, 2.4663e-01, 2.0523e-02, 1.5879e-03, 7.1511e-02],\n",
       "          [1.6703e-01, 1.2759e-01, 1.1725e-01, 1.0381e-02, 4.3139e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.2429e-02, 8.8737e-02, 1.9190e-04, 2.9498e-04, 1.0527e-03],\n",
       "          [1.9694e-01, 1.5063e-01, 3.5591e-02, 4.4541e-02, 4.8978e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.6798e-01, 2.4065e-01, 2.3716e-02, 1.8047e-02, 2.0526e-01],\n",
       "          [1.1836e-01, 1.4383e-01, 1.1733e-02, 3.2227e-02, 2.3527e-02],\n",
       "          [2.0433e-01, 1.0321e-01, 1.1902e-01, 4.6549e-02, 5.8577e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [6.9612e-02, 2.0203e-01, 5.5364e-02, 3.1474e-02, 3.9449e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.6273e-01, 2.1137e-01, 2.6541e-02, 3.9495e-02, 7.9457e-02],\n",
       "          [2.0201e-01, 1.4183e-01, 1.7099e-01, 1.5152e-02, 1.1136e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.8401e-01, 2.1434e-01, 6.2606e-02, 5.5928e-02, 1.3990e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [4.4081e-02, 1.0559e-01, 1.0683e-01, 6.6808e-03, 4.4325e-02],\n",
       "          [2.4154e-01, 1.3572e-01, 1.7628e-02, 9.9583e-03, 8.5769e-02],\n",
       "          [8.7133e-02, 8.8647e-02, 3.2418e-02, 6.8071e-03, 1.0537e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.3511e-01, 2.8320e-01, 3.5203e-02, 3.0926e-02, 7.2040e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.1232e-01, 1.2935e-01, 7.6069e-02, 1.3405e-02, 8.2204e-02],\n",
       "          [4.3495e-01, 2.4350e-01, 4.8668e-02, 8.1005e-03, 3.3602e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.5058e-01, 1.5918e-01, 6.1211e-03, 1.3699e-02, 2.2557e-02],\n",
       "          [2.6229e-02, 1.2277e-01, 1.7450e-02, 9.2457e-03, 1.0479e-01],\n",
       "          [1.0410e-01, 4.4507e-02, 1.7823e-02, 1.0786e-02, 2.5974e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [9.2059e-02, 3.6395e-01, 2.9439e-02, 3.1289e-02, 2.1609e-02],\n",
       "          [1.2682e-13, 3.5374e-09, 3.1189e-14, 1.0097e-26, 3.0514e-22],\n",
       "          [1.7411e-01, 6.8874e-02, 5.0648e-02, 3.1127e-02, 7.5689e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.1755e-01, 2.2835e-01, 1.0715e-01, 2.5005e-02, 2.1877e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [4.5527e-02, 6.3448e-03, 6.5418e-03, 1.8003e-04, 8.9092e-03],\n",
       "          [1.1409e-01, 1.7215e-01, 1.8823e-02, 2.0459e-03, 4.5023e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [5.3090e-02, 2.0657e-01, 7.3371e-02, 4.3006e-02, 4.6220e-02],\n",
       "          [1.0161e-01, 2.6739e-01, 5.6129e-03, 2.7997e-02, 4.6321e-02],\n",
       "          [3.6950e-01, 1.7181e-01, 2.7899e-02, 1.4343e-02, 3.0464e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [3.2547e-01, 1.7629e-01, 5.6484e-02, 1.0404e-01, 2.2204e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [3.6504e-01, 2.1032e-01, 1.0079e-01, 1.9134e-02, 1.1911e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [3.5251e-01, 1.5388e-01, 9.6058e-02, 1.7058e-02, 8.4107e-02],\n",
       "          [2.1863e-01, 8.2953e-02, 2.4156e-02, 3.9746e-03, 8.2008e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [3.6794e-01, 1.0617e-01, 5.2006e-02, 9.2933e-03, 4.4713e-02],\n",
       "          [1.1869e-01, 4.7889e-01, 1.1515e-01, 1.8592e-02, 1.9589e-01],\n",
       "          [1.6506e-01, 1.7505e-01, 6.0748e-02, 1.1704e-02, 9.1569e-02],\n",
       "          [1.1797e-01, 4.2669e-02, 1.2043e-02, 6.6970e-03, 1.0981e-01],\n",
       "          [1.2382e-01, 1.3548e-01, 1.2052e-02, 7.8985e-02, 3.0985e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.3218e-01, 2.5086e-01, 9.0465e-02, 2.7253e-02, 5.4371e-02],\n",
       "          [5.4755e-01, 2.0277e-01, 1.8217e-01, 5.1926e-03, 7.6174e-02],\n",
       "          [7.2446e-02, 1.3621e-01, 2.7870e-02, 2.7914e-02, 1.6254e-01],\n",
       "          [1.7112e-01, 3.1553e-01, 7.1994e-02, 1.5411e-02, 2.1838e-02],\n",
       "          [1.6356e-01, 1.3142e-01, 1.2040e-01, 5.6649e-02, 1.3550e-01],\n",
       "          [1.9732e-01, 6.0263e-02, 1.0897e-01, 1.2225e-02, 1.2054e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.2745e-01, 2.4837e-01, 7.1744e-02, 7.6913e-03, 5.9564e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.6454e-01, 9.5782e-02, 1.0134e-01, 2.5074e-02, 8.3993e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.4014e-01, 3.1961e-01, 1.7291e-02, 3.9445e-02, 9.4441e-02],\n",
       "          [4.4467e-02, 1.0527e-01, 8.5995e-02, 1.0450e-02, 3.5408e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [3.8704e-01, 2.2697e-01, 2.3594e-02, 5.8411e-02, 1.6853e-01],\n",
       "          [1.9065e-01, 3.7881e-01, 1.4344e-01, 7.5211e-02, 5.0692e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.9568e-01, 1.0130e-01, 6.0727e-02, 8.4386e-03, 8.2753e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.7120e-01, 2.0953e-01, 1.3735e-01, 8.4345e-02, 1.6594e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.7410e-01, 5.2557e-01, 5.5489e-02, 1.7003e-02, 8.2266e-02],\n",
       "          [1.9441e-01, 2.5646e-01, 2.7511e-02, 3.1135e-02, 1.6946e-01],\n",
       "          [1.1798e-01, 1.1196e-01, 5.4967e-02, 1.0986e-02, 4.2171e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.1782e-01, 5.1762e-01, 1.0024e-01, 7.2531e-02, 1.1378e-01],\n",
       "          [7.1356e-02, 2.8176e-01, 9.9896e-02, 5.8165e-02, 3.7033e-01]],\n",
       "         grad_fn=<CopySlices>),\n",
       "  'dlt2': tensor([[0.0700, 0.0206, 0.0263, 0.0296, 0.0272],\n",
       "          [0.0049, 0.0277, 0.0011, 0.0004, 0.0016],\n",
       "          [0.0293, 0.0353, 0.0438, 0.0353, 0.0564],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0661, 0.1888, 0.0617, 0.1170, 0.1031],\n",
       "          [0.0806, 0.0796, 0.0458, 0.1052, 0.1046],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0528, 0.0388, 0.0357, 0.0441, 0.0751],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0727, 0.0916, 0.0499, 0.1026, 0.0387],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0157, 0.0212, 0.0137, 0.0080, 0.0322],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0008, 0.0406, 0.0035, 0.0159, 0.0080],\n",
       "          [0.1317, 0.1538, 0.0731, 0.1006, 0.1009],\n",
       "          [0.1162, 0.1729, 0.1011, 0.0710, 0.0692],\n",
       "          [0.0353, 0.0581, 0.0206, 0.0635, 0.0448],\n",
       "          [0.0870, 0.2472, 0.0501, 0.0635, 0.0793],\n",
       "          [0.0703, 0.0870, 0.0348, 0.1283, 0.1318],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0242, 0.0339, 0.1040, 0.0536, 0.0133],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0019, 0.0078, 0.0004, 0.0014, 0.0013],\n",
       "          [0.0244, 0.0797, 0.0534, 0.0689, 0.0246],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0626, 0.0727, 0.0811, 0.0997, 0.0487],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0583, 0.1382, 0.0433, 0.0997, 0.0969],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0571, 0.0646, 0.0448, 0.1073, 0.0196],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0673, 0.1359, 0.0512, 0.0717, 0.1259],\n",
       "          [0.0045, 0.0195, 0.0004, 0.0098, 0.0049],\n",
       "          [0.0288, 0.0495, 0.0796, 0.0536, 0.0438],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0749, 0.1833, 0.0165, 0.0255, 0.0466],\n",
       "          [0.0145, 0.0213, 0.0169, 0.0085, 0.0127],\n",
       "          [0.1481, 0.1219, 0.0630, 0.0534, 0.0476],\n",
       "          [0.1310, 0.0955, 0.0294, 0.0851, 0.0390],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.1322, 0.1496, 0.0915, 0.0785, 0.0663],\n",
       "          [0.0464, 0.1586, 0.1453, 0.1687, 0.1398],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0507, 0.1405, 0.0755, 0.1499, 0.0828],\n",
       "          [0.1213, 0.2176, 0.0421, 0.0830, 0.1213],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0368, 0.0278, 0.0007, 0.0033, 0.0005],\n",
       "          [0.1670, 0.1501, 0.0279, 0.1656, 0.1036],\n",
       "          [0.0437, 0.0915, 0.0423, 0.1241, 0.0485],\n",
       "          [0.1013, 0.1555, 0.1271, 0.0834, 0.1155],\n",
       "          [0.1565, 0.2479, 0.1173, 0.0422, 0.1664],\n",
       "          [0.0640, 0.0683, 0.0162, 0.0495, 0.0658],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0972, 0.1381, 0.0383, 0.0504, 0.0784],\n",
       "          [0.0494, 0.0593, 0.0339, 0.0497, 0.0397],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0649, 0.1210, 0.0998, 0.1270, 0.0932],\n",
       "          [0.0375, 0.1341, 0.0896, 0.0551, 0.0751],\n",
       "          [0.1359, 0.1898, 0.0813, 0.2510, 0.1429],\n",
       "          [0.0747, 0.0484, 0.0563, 0.0488, 0.0710],\n",
       "          [0.0415, 0.0563, 0.0439, 0.0758, 0.0960],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0252, 0.0508, 0.0766, 0.0366, 0.0611],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0695, 0.1073, 0.0319, 0.0312, 0.0716],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0324, 0.0848, 0.0346, 0.1058, 0.1502],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0448, 0.0434, 0.0109, 0.0203, 0.0182],\n",
       "          [0.0489, 0.0805, 0.0611, 0.0467, 0.0316],\n",
       "          [0.1387, 0.0780, 0.0489, 0.0454, 0.0623],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0783, 0.1049, 0.0993, 0.1053, 0.0713],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0133, 0.0286, 0.0045, 0.0197, 0.0048],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0733, 0.1511, 0.1224, 0.0910, 0.0965],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0935, 0.0597, 0.0491, 0.0758, 0.1201],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.1546, 0.1647, 0.0445, 0.1360, 0.0904],\n",
       "          [0.0240, 0.0430, 0.0227, 0.0874, 0.0212],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0877, 0.1310, 0.0513, 0.0683, 0.0592],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.1392, 0.1031, 0.1292, 0.0675, 0.0857],\n",
       "          [0.0752, 0.0815, 0.0575, 0.1013, 0.0868],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0137, 0.0271, 0.0202, 0.0251, 0.0386],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0488, 0.0417, 0.0191, 0.0870, 0.0501],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0851, 0.1341, 0.1090, 0.0577, 0.0671],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0960, 0.1519, 0.0831, 0.0912, 0.1269],\n",
       "          [0.1146, 0.0799, 0.1091, 0.1256, 0.0780],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0197, 0.0145, 0.0185, 0.0063, 0.0188],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], grad_fn=<CopySlices>)})"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_optimal_decisions(dataset,ids,m1,m2,m3,weights=[-1,1,1,-1],outcome_loss_func=None,get_transitions=True):\n",
    "    m1.eval()\n",
    "    m2.eval()\n",
    "    m3.eval()\n",
    "    device = m1.get_device()\n",
    "    data = dataset.processed_df.copy().loc[ids]\n",
    "    \n",
    "    def get_dlt(state):\n",
    "        if state == 2:\n",
    "            return data[Const.dlt2].copy()\n",
    "        d = data[Const.dlt1].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_pd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.primary_disease_states2].copy()\n",
    "        d = data[Const.primary_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_nd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.nodal_disease_states2].copy()\n",
    "        d = data[Const.nodal_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_cc(state):\n",
    "        res = data[Const.ccs].copy()\n",
    "        if state == 1:\n",
    "            res.values[:,:] = np.zeros(res.values.shape)\n",
    "        return res\n",
    "    \n",
    "    def get_mod(state):\n",
    "        res = data[Const.modifications].copy()\n",
    "        #this should have an ic condition but we don't use it anumore anywa\n",
    "        return res\n",
    "        \n",
    "    def formatdf(d):\n",
    "        d = df_to_torch(d).to(device)\n",
    "        return d\n",
    "    \n",
    "    \n",
    "    outcomedf = data[Const.outcomes]\n",
    "    baseline = dataset.get_state('baseline').loc[ids]\n",
    "    baseline_input = formatdf(baseline)\n",
    "\n",
    "    \n",
    "#     def outcome_loss(ypred):\n",
    "#         l = torch.mul((-1*(ypred[:,0] - 1)),-weights[0])\n",
    "#         for i,weight in enumerate(weights[1:]):\n",
    "#             #weights with negative values will invert the outcome so e.g. Regional control becomes no regional control\n",
    "#             #so the penaly is correct\n",
    "#             if weight > 0:\n",
    "#                 newloss = torch.mul(ypred[:,i],weight)\n",
    "#             else:\n",
    "#                 newloss = torch.mul(-1*(ypred[:,i] - 1),-weights[i])\n",
    "#             l = torch.add(l,newloss)\n",
    "#         print(l)\n",
    "#         return l\n",
    "    \n",
    "    def outcome_loss(ypred):\n",
    "        l = torch.mul(ypred[:,0],weights[0])\n",
    "        for i,weight in enumerate(weights[1:]):\n",
    "            #weights with negative values will invert the outcome so e.g. Regional control becomes no regional control\n",
    "            #so the penaly is correct\n",
    "            newloss = torch.mul(ypred[:,i],weight)\n",
    "            l = torch.add(l,newloss)\n",
    "        return l\n",
    "    \n",
    "    if outcome_loss_func is None:\n",
    "        outcome_loss_func = outcome_loss\n",
    "    \n",
    "    cat = lambda x: torch.cat([xx.to(device) for xx in x],axis=1).to(device)\n",
    "    format_transition = lambda x: x.to(device)\n",
    "    \n",
    "    def get_outcome(d1,d2,d3):\n",
    "        d1 = torch.full((len(ids),1),d1).type(torch.FloatTensor)\n",
    "        d2 = torch.full((len(ids),1),d2).type(torch.FloatTensor)\n",
    "        d3 = torch.full((len(ids),1),d3).type(torch.FloatTensor)\n",
    "        \n",
    "        tinput1 = cat([baseline_input,d1])\n",
    "        ytransition = m1(tinput1)\n",
    "        [ypd1,ynd1,ymod,ydlt1] = [format_transition(xx) for xx in ytransition['predictions']]\n",
    "        d1_thresh = torch.gt(d1,.5).view(-1,1).to(device)\n",
    "        ypd1[:,0:2] = ypd1[:,0:2]*d1_thresh\n",
    "        ynd1[:,0:2] = ynd1[:,0:2]*d1_thresh\n",
    "        \n",
    "        tinput2 = cat([baseline_input,ypd1,ynd1,ymod,ydlt1,d1,d2])\n",
    "        ytransition2 = m2(tinput2)\n",
    "        [ypd2,ynd2,ycc,ydlt2] = [format_transition(xx) for xx in ytransition2['predictions']]\n",
    "        \n",
    "        input3 = cat([baseline_input, ypd2, ynd2, ycc, ydlt2, d1, d2,d3])\n",
    "        outcome = m3(input3)['predictions']\n",
    "        transitions = {\n",
    "            'pd1': ypd1,\n",
    "            'nd1': ynd1,\n",
    "            'nd2': ynd2,\n",
    "            'pd2': ypd2,\n",
    "            'mod': ymod,\n",
    "            'cc': ycc,\n",
    "            'dlt1': ydlt1,\n",
    "            'dlt2': ydlt2,\n",
    "        }\n",
    "        return outcome, transitions\n",
    "\n",
    "    losses = []\n",
    "    loss_order = []\n",
    "    transitions = {}\n",
    "    for d1 in [0,1]:\n",
    "        for d2 in [0,1]:\n",
    "            for d3 in [0,1]:\n",
    "                outcomes, transition_entry = get_outcome(d1,d2,d3)\n",
    "                loss = outcome_loss_func(outcomes)\n",
    "                losses.append(loss)\n",
    "                loss_order.append([d1,d2,d3])\n",
    "                transitions[str(d1)+str(d2)+str(d3)] = transition_entry\n",
    "    losses = torch.stack(losses,axis=1)\n",
    "    optimal_decisions = [loss_order[i] for i in torch.argmin(losses,axis=1)]\n",
    "    result = torch.tensor(optimal_decisions).type(torch.FloatTensor)\n",
    "    if get_transitions:\n",
    "        opt_transitions = {k: torch.zeros(v.shape).type(torch.FloatTensor) for k,v in transitions['000'].items()}\n",
    "        for i,od in enumerate(optimal_decisions):\n",
    "            key = ''.join([str(o) for o in od])\n",
    "            entry = transitions[key]\n",
    "            for kk,vv in entry.items():\n",
    "                opt_transitions[kk][i,:] = vv[i,:]\n",
    "        return result, opt_transitions\n",
    "    return result\n",
    "\n",
    "test, testtest = get_tt_split()\n",
    "calc_optimal_decisions(DTDataset(),testtest,tmodel1[0],tmodel2[0],tmodel3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "122ee514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 536, 87])\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 0 _____\n",
      "val reward 1.0138548612594604\n",
      "imitation reward 1.367457628250122\n",
      "distance losses 2.6842362880706787 1.172102928161621\n",
      "distributions [0.9150996804237366, 0.0431910939514637, 0.9642633199691772]\n",
      "[{'decision': 0, 'optimal_auc': 0.7940206185567011, 'imitation_auc': 0.5887404580152672, 'optimal_acc': 0.6598639455782312, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.5492957746478873, 'imitation_auc': 0.5880434782608696, 'optimal_acc': 0.9659863945578231, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.6417673235855055, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 1 _____\n",
      "val reward 0.7711609601974487\n",
      "imitation reward 1.3454434871673584\n",
      "distance losses 2.8743433952331543 1.0386924743652344\n",
      "distributions [0.5290834307670593, 0.008858423680067062, 0.9927453398704529]\n",
      "[{'decision': 0, 'optimal_auc': 0.7657731958762887, 'imitation_auc': 0.5458015267175572, 'optimal_acc': 0.6666666666666666, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.4211267605633803, 'imitation_auc': 0.6657608695652174, 'optimal_acc': 0.9659863945578231, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7237762237762237, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 2 _____\n",
      "val reward 0.7420130372047424\n",
      "imitation reward 1.128311276435852\n",
      "distance losses 2.8472089767456055 1.288922905921936\n",
      "distributions [0.7314165830612183, 0.007183977402746677, 0.9955416321754456]\n",
      "[{'decision': 0, 'optimal_auc': 0.8024742268041236, 'imitation_auc': 0.47614503816793896, 'optimal_acc': 0.6666666666666666, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.46197183098591554, 'imitation_auc': 0.6627717391304347, 'optimal_acc': 0.9659863945578231, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7587412587412588, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 3 _____\n",
      "val reward 0.730701208114624\n",
      "imitation reward 1.1455446481704712\n",
      "distance losses 2.81013822555542 1.235464096069336\n",
      "distributions [0.652661919593811, 0.007462457753717899, 0.9969691038131714]\n",
      "[{'decision': 0, 'optimal_auc': 0.8030927835051546, 'imitation_auc': 0.48473282442748095, 'optimal_acc': 0.7414965986394558, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.2549295774647887, 'imitation_auc': 0.66875, 'optimal_acc': 0.9659863945578231, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7116973935155754, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 4 _____\n",
      "val reward 0.7228432297706604\n",
      "imitation reward 1.1639771461486816\n",
      "distance losses 2.76829195022583 1.1527711153030396\n",
      "distributions [0.6369143128395081, 0.00745014613494277, 0.9985082745552063]\n",
      "[{'decision': 0, 'optimal_auc': 0.8406185567010309, 'imitation_auc': 0.51956106870229, 'optimal_acc': 0.7482993197278912, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.3126760563380282, 'imitation_auc': 0.6483695652173913, 'optimal_acc': 0.9659863945578231, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7304513668150032, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 5 _____\n",
      "val reward 0.703008234500885\n",
      "imitation reward 1.1281986236572266\n",
      "distance losses 2.6801257133483887 1.1947346925735474\n",
      "distributions [0.6671197414398193, 0.009096908383071423, 0.998984694480896]\n",
      "[{'decision': 0, 'optimal_auc': 0.8468041237113402, 'imitation_auc': 0.5572519083969466, 'optimal_acc': 0.7210884353741497, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.5366197183098592, 'imitation_auc': 0.6605978260869565, 'optimal_acc': 0.9659863945578231, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7428480610298791, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 6 _____\n",
      "val reward 0.6796457767486572\n",
      "imitation reward 1.112625002861023\n",
      "distance losses 2.673551321029663 1.2518473863601685\n",
      "distributions [0.6183621287345886, 0.012306817807257175, 0.999170184135437]\n",
      "[{'decision': 0, 'optimal_auc': 0.8455670103092784, 'imitation_auc': 0.5505725190839694, 'optimal_acc': 0.8231292517006803, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.7563380281690141, 'imitation_auc': 0.6820652173913043, 'optimal_acc': 0.9659863945578231, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7428480610298793, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 7 _____\n",
      "val reward 0.6626547574996948\n",
      "imitation reward 1.1090291738510132\n",
      "distance losses 2.7080342769622803 1.1211806535720825\n",
      "distributions [0.6337401270866394, 0.016453392803668976, 0.9995251893997192]\n",
      "[{'decision': 0, 'optimal_auc': 0.8490721649484535, 'imitation_auc': 0.5572519083969466, 'optimal_acc': 0.8435374149659864, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.7873239436619718, 'imitation_auc': 0.7008152173913044, 'optimal_acc': 0.9659863945578231, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7472981563890655, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 8 _____\n",
      "val reward 0.6363298892974854\n",
      "imitation reward 1.0954725742340088\n",
      "distance losses 2.814058780670166 1.1821324825286865\n",
      "distributions [0.7294032573699951, 0.02256953716278076, 0.9997382164001465]\n",
      "[{'decision': 0, 'optimal_auc': 0.8488659793814433, 'imitation_auc': 0.5515267175572519, 'optimal_acc': 0.7619047619047619, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.8014084507042254, 'imitation_auc': 0.7092391304347826, 'optimal_acc': 0.9659863945578231, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7536554354736172, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 9 _____\n",
      "val reward 0.6340816020965576\n",
      "imitation reward 1.1142154932022095\n",
      "distance losses 2.7168891429901123 1.1064578294754028\n",
      "distributions [0.5471775531768799, 0.03855937346816063, 0.9997102618217468]\n",
      "[{'decision': 0, 'optimal_auc': 0.8527835051546391, 'imitation_auc': 0.5663167938931297, 'optimal_acc': 0.8231292517006803, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.8098591549295775, 'imitation_auc': 0.722554347826087, 'optimal_acc': 0.9659863945578231, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7584233947870311, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 10 _____\n",
      "val reward 0.5979086756706238\n",
      "imitation reward 1.0831514596939087\n",
      "distance losses 2.755784034729004 1.059696078300476\n",
      "distributions [0.6160004138946533, 0.04825638607144356, 0.9997106790542603]\n",
      "[{'decision': 0, 'optimal_auc': 0.8602061855670102, 'imitation_auc': 0.5567748091603053, 'optimal_acc': 0.8707482993197279, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.8056338028169014, 'imitation_auc': 0.7247282608695651, 'optimal_acc': 0.9659863945578231, 'imitation_acc': 0.7687074829931972}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7584233947870311, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 11 _____\n",
      "val reward 0.6228361129760742\n",
      "imitation reward 1.09210205078125\n",
      "distance losses 2.7035036087036133 0.9775488972663879\n",
      "distributions [0.7220135927200317, 0.043768979609012604, 0.9998129606246948]\n",
      "[{'decision': 0, 'optimal_auc': 0.8657731958762886, 'imitation_auc': 0.5791984732824428, 'optimal_acc': 0.7619047619047619, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.8028169014084507, 'imitation_auc': 0.6991847826086957, 'optimal_acc': 0.9659863945578231, 'imitation_acc': 0.7687074829931972}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7848061029879212, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 12 _____\n",
      "val reward 0.6307811141014099\n",
      "imitation reward 1.1225640773773193\n",
      "distance losses 2.6268270015716553 1.0813636779785156\n",
      "distributions [0.5353437662124634, 0.0409589558839798, 0.9998502135276794]\n",
      "[{'decision': 0, 'optimal_auc': 0.8676288659793815, 'imitation_auc': 0.6178435114503817, 'optimal_acc': 0.8639455782312925, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.8084507042253521, 'imitation_auc': 0.70625, 'optimal_acc': 0.9659863945578231, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.789574062301335, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 13 _____\n",
      "val reward 0.5631035566329956\n",
      "imitation reward 1.0728569030761719\n",
      "distance losses 2.9632394313812256 1.0449520349502563\n",
      "distributions [0.5953181982040405, 0.03416825085878372, 0.9998794794082642]\n",
      "[{'decision': 0, 'optimal_auc': 0.867422680412371, 'imitation_auc': 0.6049618320610687, 'optimal_acc': 0.8775510204081632, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.8098591549295775, 'imitation_auc': 0.7328804347826087, 'optimal_acc': 0.9659863945578231, 'imitation_acc': 0.7687074829931972}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7739987285441832, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 14 _____\n",
      "val reward 0.5606288313865662\n",
      "imitation reward 1.0660990476608276\n",
      "distance losses 2.8934028148651123 1.0226012468338013\n",
      "distributions [0.7623752355575562, 0.025521202012896538, 0.9999325275421143]\n",
      "[{'decision': 0, 'optimal_auc': 0.880618556701031, 'imitation_auc': 0.6020992366412213, 'optimal_acc': 0.8231292517006803, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.8084507042253521, 'imitation_auc': 0.7366847826086957, 'optimal_acc': 0.9659863945578231, 'imitation_acc': 0.7687074829931972}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7644628099173554, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 15 _____\n",
      "val reward 0.5433476567268372\n",
      "imitation reward 1.133915901184082\n",
      "distance losses 2.7446606159210205 1.1566897630691528\n",
      "distributions [0.6150827407836914, 0.024052180349826813, 0.99994957447052]\n",
      "[{'decision': 0, 'optimal_auc': 0.8818556701030927, 'imitation_auc': 0.607824427480916, 'optimal_acc': 0.8979591836734694, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.8070422535211268, 'imitation_auc': 0.7529891304347825, 'optimal_acc': 0.9659863945578231, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7854418308963764, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 16 _____\n",
      "val reward 0.5554196238517761\n",
      "imitation reward 1.093103289604187\n",
      "distance losses 2.6832492351531982 1.0267407894134521\n",
      "distributions [0.5821611881256104, 0.02623583748936653, 0.9999303221702576]\n",
      "[{'decision': 0, 'optimal_auc': 0.8857731958762887, 'imitation_auc': 0.588263358778626, 'optimal_acc': 0.8707482993197279, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.8, 'imitation_auc': 0.7551630434782608, 'optimal_acc': 0.9659863945578231, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7873490146217419, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 17 _____\n",
      "val reward 0.5346265435218811\n",
      "imitation reward 1.0704151391983032\n",
      "distance losses 2.7129924297332764 1.0545799732208252\n",
      "distributions [0.7247467041015625, 0.03009304776787758, 0.9999039769172668]\n",
      "[{'decision': 0, 'optimal_auc': 0.8851546391752577, 'imitation_auc': 0.5448473282442748, 'optimal_acc': 0.8775510204081632, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.8028169014084507, 'imitation_auc': 0.7665760869565217, 'optimal_acc': 0.9659863945578231, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7596948506039416, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 18 _____\n",
      "val reward 0.5712530612945557\n",
      "imitation reward 1.091237187385559\n",
      "distance losses 2.8204290866851807 1.184962511062622\n",
      "distributions [0.593208372592926, 0.041605833917856216, 0.9998847842216492]\n",
      "[{'decision': 0, 'optimal_auc': 0.865360824742268, 'imitation_auc': 0.5257633587786259, 'optimal_acc': 0.891156462585034, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.7985915492957746, 'imitation_auc': 0.7720108695652175, 'optimal_acc': 0.9659863945578231, 'imitation_acc': 0.7619047619047619}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7447552447552448, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 19 _____\n",
      "val reward 0.5338156819343567\n",
      "imitation reward 1.1140165328979492\n",
      "distance losses 2.9575536251068115 1.1823253631591797\n",
      "distributions [0.6878799796104431, 0.04984473064541817, 0.9998922348022461]\n",
      "[{'decision': 0, 'optimal_auc': 0.8802061855670104, 'imitation_auc': 0.5400763358778626, 'optimal_acc': 0.9115646258503401, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.7704225352112676, 'imitation_auc': 0.7652173913043478, 'optimal_acc': 0.9659863945578231, 'imitation_acc': 0.7687074829931972}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7498410680228862, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 20 _____\n",
      "val reward 0.5111414194107056\n",
      "imitation reward 1.1079883575439453\n",
      "distance losses 2.967400074005127 0.9657952189445496\n",
      "distributions [0.7218243479728699, 0.04801642522215843, 0.9998777508735657]\n",
      "[{'decision': 0, 'optimal_auc': 0.891340206185567, 'imitation_auc': 0.564885496183206, 'optimal_acc': 0.891156462585034, 'imitation_acc': 0.8843537414965986}, {'decision': 1, 'optimal_auc': 0.7830985915492958, 'imitation_auc': 0.7676630434782609, 'optimal_acc': 0.9659863945578231, 'imitation_acc': 0.7687074829931972}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7565162110616657, 'optimal_acc': 1.0, 'imitation_acc': 0.8095238095238095}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 21 _____\n",
      "val reward 0.4998634159564972\n",
      "imitation reward 1.0924254655838013\n",
      "distance losses 2.6447737216949463 0.9524986743927002\n",
      "distributions [0.6020340919494629, 0.044592104852199554, 0.9998079538345337]\n",
      "[{'decision': 0, 'optimal_auc': 0.8921649484536083, 'imitation_auc': 0.5720419847328244, 'optimal_acc': 0.8843537414965986, 'imitation_acc': 0.8843537414965986}, {'decision': 1, 'optimal_auc': 0.795774647887324, 'imitation_auc': 0.7733695652173913, 'optimal_acc': 0.9659863945578231, 'imitation_acc': 0.7619047619047619}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7619198982835347, 'optimal_acc': 1.0, 'imitation_acc': 0.8163265306122449}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 22 _____\n",
      "val reward 0.48438674211502075\n",
      "imitation reward 1.13831627368927\n",
      "distance losses 2.7706077098846436 0.9178046584129333\n",
      "distributions [0.5980581641197205, 0.03729317709803581, 0.999854564666748]\n",
      "[{'decision': 0, 'optimal_auc': 0.8981443298969072, 'imitation_auc': 0.5882633587786259, 'optimal_acc': 0.8707482993197279, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.7929577464788733, 'imitation_auc': 0.7657608695652174, 'optimal_acc': 0.9659863945578231, 'imitation_acc': 0.7687074829931972}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.757787666878576, 'optimal_acc': 1.0, 'imitation_acc': 0.8299319727891157}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 23 _____\n",
      "val reward 0.5268952250480652\n",
      "imitation reward 1.1150099039077759\n",
      "distance losses 2.931448459625244 0.8761224150657654\n",
      "distributions [0.5337097644805908, 0.03286178037524223, 0.9997989535331726]\n",
      "[{'decision': 0, 'optimal_auc': 0.8929896907216495, 'imitation_auc': 0.5844465648854962, 'optimal_acc': 0.8639455782312925, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.7887323943661971, 'imitation_auc': 0.7529891304347825, 'optimal_acc': 0.9659863945578231, 'imitation_acc': 0.7619047619047619}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7558804831532104, 'optimal_acc': 1.0, 'imitation_acc': 0.8299319727891157}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 24 _____\n",
      "val reward 0.5311831831932068\n",
      "imitation reward 1.0878771543502808\n",
      "distance losses 2.73832106590271 0.8382863402366638\n",
      "distributions [0.5330559611320496, 0.02952786535024643, 0.9996986389160156]\n",
      "[{'decision': 0, 'optimal_auc': 0.8915463917525774, 'imitation_auc': 0.5562977099236641, 'optimal_acc': 0.8231292517006803, 'imitation_acc': 0.8775510204081632}, {'decision': 1, 'optimal_auc': 0.7816901408450704, 'imitation_auc': 0.7388586956521739, 'optimal_acc': 0.9659863945578231, 'imitation_acc': 0.7687074829931972}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7479338842975207, 'optimal_acc': 1.0, 'imitation_acc': 0.8095238095238095}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 25 _____\n",
      "val reward 0.48664093017578125\n",
      "imitation reward 1.1220142841339111\n",
      "distance losses 2.548377275466919 0.9883819222450256\n",
      "distributions [0.7234770655632019, 0.027425609529018402, 0.9996680617332458]\n",
      "[{'decision': 0, 'optimal_auc': 0.8956701030927836, 'imitation_auc': 0.5577290076335878, 'optimal_acc': 0.891156462585034, 'imitation_acc': 0.8775510204081632}, {'decision': 1, 'optimal_auc': 0.7704225352112677, 'imitation_auc': 0.7285326086956522, 'optimal_acc': 0.9659863945578231, 'imitation_acc': 0.7619047619047619}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7434837889383343, 'optimal_acc': 1.0, 'imitation_acc': 0.7959183673469388}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 26 _____\n",
      "val reward 0.4791781008243561\n",
      "imitation reward 1.2004903554916382\n",
      "distance losses 2.8955793380737305 0.8544273972511292\n",
      "distributions [0.6688808798789978, 0.034958209842443466, 0.9997738003730774]\n",
      "[{'decision': 0, 'optimal_auc': 0.8977319587628866, 'imitation_auc': 0.5725190839694656, 'optimal_acc': 0.891156462585034, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.7661971830985915, 'imitation_auc': 0.6758152173913045, 'optimal_acc': 0.9659863945578231, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7269548633184997, 'optimal_acc': 1.0, 'imitation_acc': 0.8299319727891157}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 27 _____\n",
      "val reward 0.5588788986206055\n",
      "imitation reward 1.1922110319137573\n",
      "distance losses 2.7064144611358643 1.1197454929351807\n",
      "distributions [0.5343142747879028, 0.042940277606248856, 0.9995990991592407]\n",
      "[{'decision': 0, 'optimal_auc': 0.8927835051546392, 'imitation_auc': 0.5896946564885496, 'optimal_acc': 0.8299319727891157, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.7690140845070422, 'imitation_auc': 0.714945652173913, 'optimal_acc': 0.9659863945578231, 'imitation_acc': 0.7687074829931972}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7390336935791482, 'optimal_acc': 1.0, 'imitation_acc': 0.8299319727891157}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 28 _____\n",
      "val reward 0.4873581528663635\n",
      "imitation reward 1.0980268716812134\n",
      "distance losses 2.7918317317962646 1.1246182918548584\n",
      "distributions [0.6983725428581238, 0.04157226160168648, 0.9994581341743469]\n",
      "[{'decision': 0, 'optimal_auc': 0.9092783505154638, 'imitation_auc': 0.5715648854961831, 'optimal_acc': 0.8503401360544217, 'imitation_acc': 0.8707482993197279}, {'decision': 1, 'optimal_auc': 0.7521126760563379, 'imitation_auc': 0.7625000000000001, 'optimal_acc': 0.9659863945578231, 'imitation_acc': 0.7551020408163265}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7476160203432931, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 29 _____\n",
      "val reward 0.4725745618343353\n",
      "imitation reward 1.1097452640533447\n",
      "distance losses 2.7670109272003174 1.2028414011001587\n",
      "distributions [0.6836457252502441, 0.037241533398628235, 0.9995524287223816]\n",
      "[{'decision': 0, 'optimal_auc': 0.9111340206185568, 'imitation_auc': 0.5381679389312977, 'optimal_acc': 0.8775510204081632, 'imitation_acc': 0.8775510204081632}, {'decision': 1, 'optimal_auc': 0.7507042253521126, 'imitation_auc': 0.7733695652173913, 'optimal_acc': 0.9659863945578231, 'imitation_acc': 0.7619047619047619}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7406230133502861, 'optimal_acc': 1.0, 'imitation_acc': 0.8095238095238095}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 30 _____\n",
      "val reward 0.7255977392196655\n",
      "imitation reward 1.2088111639022827\n",
      "distance losses 2.7705495357513428 1.1027982234954834\n",
      "distributions [0.38901078701019287, 0.03339160233736038, 0.9996574521064758]\n",
      "[{'decision': 0, 'optimal_auc': 0.8975257731958762, 'imitation_auc': 0.5529580152671756, 'optimal_acc': 0.6326530612244898, 'imitation_acc': 0.8843537414965986}, {'decision': 1, 'optimal_auc': 0.7577464788732395, 'imitation_auc': 0.7665760869565217, 'optimal_acc': 0.9659863945578231, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.753973299427845, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 31 _____\n",
      "val reward 0.48018568754196167\n",
      "imitation reward 1.14479660987854\n",
      "distance losses 2.836857795715332 1.1480143070220947\n",
      "distributions [0.5911685824394226, 0.03371873125433922, 0.9996974468231201]\n",
      "[{'decision': 0, 'optimal_auc': 0.9103092783505154, 'imitation_auc': 0.5696564885496183, 'optimal_acc': 0.8707482993197279, 'imitation_acc': 0.8843537414965986}, {'decision': 1, 'optimal_auc': 0.7718309859154929, 'imitation_auc': 0.7684782608695653, 'optimal_acc': 0.9795918367346939, 'imitation_acc': 0.7551020408163265}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7536554354736174, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 32 _____\n",
      "val reward 0.5047796964645386\n",
      "imitation reward 1.0964670181274414\n",
      "distance losses 2.834609270095825 1.0765564441680908\n",
      "distributions [0.7809039354324341, 0.032678477466106415, 0.9997107982635498]\n",
      "[{'decision': 0, 'optimal_auc': 0.9255670103092783, 'imitation_auc': 0.5772900763358778, 'optimal_acc': 0.8707482993197279, 'imitation_acc': 0.8775510204081632}, {'decision': 1, 'optimal_auc': 0.7746478873239436, 'imitation_auc': 0.7755434782608696, 'optimal_acc': 0.9795918367346939, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7438016528925621, 'optimal_acc': 1.0, 'imitation_acc': 0.7891156462585034}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 33 _____\n",
      "val reward 0.43710076808929443\n",
      "imitation reward 1.146770715713501\n",
      "distance losses 2.948286294937134 1.1748102903366089\n",
      "distributions [0.6782158017158508, 0.03031155839562416, 0.9996963739395142]\n",
      "[{'decision': 0, 'optimal_auc': 0.9228865979381443, 'imitation_auc': 0.5987595419847328, 'optimal_acc': 0.891156462585034, 'imitation_acc': 0.8843537414965986}, {'decision': 1, 'optimal_auc': 0.7647887323943662, 'imitation_auc': 0.7619565217391305, 'optimal_acc': 0.9659863945578231, 'imitation_acc': 0.7687074829931972}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7447552447552448, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 34 _____\n",
      "val reward 0.7222157716751099\n",
      "imitation reward 1.2015621662139893\n",
      "distance losses 2.683100461959839 1.3059896230697632\n",
      "distributions [0.37792620062828064, 0.024571865797042847, 0.9995079040527344]\n",
      "[{'decision': 0, 'optimal_auc': 0.9101030927835051, 'imitation_auc': 0.6073473282442748, 'optimal_acc': 0.673469387755102, 'imitation_acc': 0.8843537414965986}, {'decision': 1, 'optimal_auc': 0.7661971830985915, 'imitation_auc': 0.7491847826086957, 'optimal_acc': 0.9659863945578231, 'imitation_acc': 0.7687074829931972}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7415766052129689, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 35 _____\n",
      "val reward 0.5505148768424988\n",
      "imitation reward 1.1178938150405884\n",
      "distance losses 2.6618189811706543 1.3529016971588135\n",
      "distributions [0.5151513814926147, 0.028875645250082016, 0.999267578125]\n",
      "[{'decision': 0, 'optimal_auc': 0.9049484536082474, 'imitation_auc': 0.6111641221374046, 'optimal_acc': 0.8095238095238095, 'imitation_acc': 0.8843537414965986}, {'decision': 1, 'optimal_auc': 0.76056338028169, 'imitation_auc': 0.748641304347826, 'optimal_acc': 0.9659863945578231, 'imitation_acc': 0.7687074829931972}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7358550540368722, 'optimal_acc': 1.0, 'imitation_acc': 0.8299319727891157}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 36 _____\n",
      "val reward 0.47502005100250244\n",
      "imitation reward 1.1104350090026855\n",
      "distance losses 2.966858386993408 1.2761683464050293\n",
      "distributions [0.7018141150474548, 0.03865094855427742, 0.9992745518684387]\n",
      "[{'decision': 0, 'optimal_auc': 0.9068041237113402, 'imitation_auc': 0.6259541984732824, 'optimal_acc': 0.891156462585034, 'imitation_acc': 0.8775510204081632}, {'decision': 1, 'optimal_auc': 0.7535211267605634, 'imitation_auc': 0.7497282608695652, 'optimal_acc': 0.9727891156462585, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7447552447552448, 'optimal_acc': 1.0, 'imitation_acc': 0.7891156462585034}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 37 _____\n",
      "val reward 0.49508312344551086\n",
      "imitation reward 1.166259765625\n",
      "distance losses 2.716876268386841 1.1703444719314575\n",
      "distributions [0.7463921904563904, 0.04271317273378372, 0.9996083378791809]\n",
      "[{'decision': 0, 'optimal_auc': 0.9068041237113402, 'imitation_auc': 0.6383587786259541, 'optimal_acc': 0.8639455782312925, 'imitation_acc': 0.8843537414965986}, {'decision': 1, 'optimal_auc': 0.7507042253521127, 'imitation_auc': 0.7415760869565218, 'optimal_acc': 0.9727891156462585, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7479338842975206, 'optimal_acc': 1.0, 'imitation_acc': 0.8163265306122449}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 38 _____\n",
      "val reward 0.46971386671066284\n",
      "imitation reward 1.2393786907196045\n",
      "distance losses 2.8407492637634277 1.3879728317260742\n",
      "distributions [0.6749589443206787, 0.04029736667871475, 0.999659538269043]\n",
      "[{'decision': 0, 'optimal_auc': 0.9041237113402062, 'imitation_auc': 0.6397900763358779, 'optimal_acc': 0.891156462585034, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.7422535211267606, 'imitation_auc': 0.7459239130434783, 'optimal_acc': 0.9727891156462585, 'imitation_acc': 0.7687074829931972}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7565162110616656, 'optimal_acc': 1.0, 'imitation_acc': 0.8299319727891157}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 39 _____\n",
      "val reward 0.5314595699310303\n",
      "imitation reward 1.1600887775421143\n",
      "distance losses 2.894319534301758 1.2675676345825195\n",
      "distributions [0.5361701250076294, 0.0386396124958992, 0.9994712471961975]\n",
      "[{'decision': 0, 'optimal_auc': 0.9016494845360824, 'imitation_auc': 0.6216603053435115, 'optimal_acc': 0.8367346938775511, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.7253521126760565, 'imitation_auc': 0.7472826086956522, 'optimal_acc': 0.9727891156462585, 'imitation_acc': 0.7891156462585034}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7584233947870311, 'optimal_acc': 1.0, 'imitation_acc': 0.8163265306122449}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 40 _____\n",
      "val reward 0.5803452730178833\n",
      "imitation reward 1.1286219358444214\n",
      "distance losses 3.0446553230285645 1.3672176599502563\n",
      "distributions [0.48744145035743713, 0.033157411962747574, 0.9992289543151855]\n",
      "[{'decision': 0, 'optimal_auc': 0.8977319587628866, 'imitation_auc': 0.5911259541984732, 'optimal_acc': 0.782312925170068, 'imitation_acc': 0.8435374149659864}, {'decision': 1, 'optimal_auc': 0.7126760563380281, 'imitation_auc': 0.7540760869565217, 'optimal_acc': 0.9727891156462585, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7590591226954863, 'optimal_acc': 1.0, 'imitation_acc': 0.782312925170068}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 41 _____\n",
      "val reward 0.5459736585617065\n",
      "imitation reward 1.1122387647628784\n",
      "distance losses 3.0576467514038086 1.1986414194107056\n",
      "distributions [0.515129029750824, 0.024335529655218124, 0.9994208812713623]\n",
      "[{'decision': 0, 'optimal_auc': 0.9047422680412371, 'imitation_auc': 0.6040076335877862, 'optimal_acc': 0.8231292517006803, 'imitation_acc': 0.8775510204081632}, {'decision': 1, 'optimal_auc': 0.7169014084507043, 'imitation_auc': 0.7516304347826087, 'optimal_acc': 0.9727891156462585, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7689129052765417, 'optimal_acc': 1.0, 'imitation_acc': 0.7687074829931972}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 42 _____\n",
      "val reward 0.4652687907218933\n",
      "imitation reward 1.1509697437286377\n",
      "distance losses 2.7355189323425293 1.1512137651443481\n",
      "distributions [0.6769227385520935, 0.015996119007468224, 0.9996930360794067]\n",
      "[{'decision': 0, 'optimal_auc': 0.9101030927835051, 'imitation_auc': 0.6278625954198473, 'optimal_acc': 0.9047619047619048, 'imitation_acc': 0.8843537414965986}, {'decision': 1, 'optimal_auc': 0.7183098591549296, 'imitation_auc': 0.7456521739130435, 'optimal_acc': 0.9795918367346939, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7612841703750794, 'optimal_acc': 1.0, 'imitation_acc': 0.8027210884353742}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 43 _____\n",
      "val reward 0.47071605920791626\n",
      "imitation reward 1.1462814807891846\n",
      "distance losses 2.8232996463775635 1.2756167650222778\n",
      "distributions [0.7191587686538696, 0.015832021832466125, 0.9997008442878723]\n",
      "[{'decision': 0, 'optimal_auc': 0.9189690721649485, 'imitation_auc': 0.6106870229007634, 'optimal_acc': 0.891156462585034, 'imitation_acc': 0.8843537414965986}, {'decision': 1, 'optimal_auc': 0.6971830985915494, 'imitation_auc': 0.7497282608695652, 'optimal_acc': 0.9795918367346939, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.770184361093452, 'optimal_acc': 1.0, 'imitation_acc': 0.7891156462585034}]\n",
      "tensor([0.6599, 0.0340, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 44 _____\n",
      "val reward 0.4577677249908447\n",
      "imitation reward 1.1507723331451416\n",
      "distance losses 2.7783539295196533 1.171411395072937\n",
      "distributions [0.6769285202026367, 0.02183353900909424, 0.9996235370635986]\n",
      "[{'decision': 0, 'optimal_auc': 0.9144329896907217, 'imitation_auc': 0.5906488549618321, 'optimal_acc': 0.8979591836734694, 'imitation_acc': 0.8843537414965986}, {'decision': 1, 'optimal_auc': 0.6718309859154931, 'imitation_auc': 0.7554347826086957, 'optimal_acc': 0.9795918367346939, 'imitation_acc': 0.7959183673469388}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7720915448188175, 'optimal_acc': 1.0, 'imitation_acc': 0.7891156462585034}]\n",
      "++++++++++Final+++++++++++\n",
      "best tensor(1.3046, grad_fn=<AddBackward0>)\n",
      "[{'decision': 0, 'optimal_auc': 0.9228865979381443, 'imitation_auc': 0.5987595419847328, 'optimal_acc': 0.891156462585034, 'imitation_acc': 0.8843537414965986}, {'decision': 1, 'optimal_auc': 0.7647887323943662, 'imitation_auc': 0.7619565217391305, 'optimal_acc': 0.9659863945578231, 'imitation_acc': 0.7687074829931972}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7447552447552448, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionAttentionModel(\n",
       "  (input_dropout): Dropout(p=0.25, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=100, out_features=500, bias=True)\n",
       "  )\n",
       "  (batchnorm): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       "  (relu): Softplus(beta=1, threshold=20)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (softmax): Softmax(dim=1)\n",
       "  (final_opt_layer): Linear(in_features=500, out_features=100, bias=True)\n",
       "  (final_imitation_layer): Linear(in_features=500, out_features=100, bias=True)\n",
       "  (final_layer): Linear(in_features=500, out_features=6, bias=True)\n",
       "  (resize_layer): Linear(in_features=91, out_features=100, bias=True)\n",
       "  (attentions): ModuleList(\n",
       "    (0): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (norms): ModuleList(\n",
       "    (0): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (activation): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def torch_apply_along_axis(function, x, axis: int = 0):\n",
    "    return torch.stack([\n",
    "        function(x_i) for x_i in torch.unbind(x, dim=axis)\n",
    "    ], dim=axis)\n",
    "\n",
    "def get_unique_sequence(array):\n",
    "    #converts a row of boolean values to a unique number e.g. [1,1,0] => 11, [0,0,1] => 100\n",
    "    uniqueify = lambda r: torch.sum(torch.stack([i*(10**ii) for ii,i in enumerate(r)]))\n",
    "    return torch_apply_along_axis(uniqueify,array)\n",
    "\n",
    "def train_decision_model_triplet(\n",
    "    tmodel1,\n",
    "    tmodel2,\n",
    "    tmodel3,\n",
    "    use_default_split=True,\n",
    "    use_bagging_split=False,\n",
    "    use_attention=True,\n",
    "    lr=.001,\n",
    "    epochs=10000,\n",
    "    patience=5,\n",
    "    weights=[-1,1,1,-1], #realtive weight of survival, feeding tube, aspiration, andl lrc\n",
    "    opt_weights=[1,1,1],\n",
    "    imitation_weights=[.5,1,1],#weights of imitation decisions, because ic overtrains too quickly\n",
    "    imitation_weight=1,\n",
    "    reward_weight=1,\n",
    "    imitation_triplet_weight=2,\n",
    "    reward_triplet_weight = 2,\n",
    "    shufflecol_chance = 0.1,\n",
    "    split=.7,\n",
    "    resample_training=False,\n",
    "    save_path='../data/models/',\n",
    "    file_suffix='',\n",
    "    verbose=True,\n",
    "    use_gpu=False,\n",
    "    **model_kwargs,\n",
    "):\n",
    "    tmodel1.eval()\n",
    "    tmodel2.eval()\n",
    "    tmodel3.eval()\n",
    "\n",
    "    train_ids, test_ids = get_tt_split(use_default_split=use_default_split,use_bagging_split=use_bagging_split,resample_training=resample_training)\n",
    "    true_ids = train_ids + test_ids #for saving memory without upsampling\n",
    "\n",
    "    dataset = DTDataset()\n",
    "    data = dataset.processed_df.copy()\n",
    "    \n",
    "    def get_dlt(state):\n",
    "        if state == 2:\n",
    "            return data[Const.dlt2].copy()\n",
    "        d = data[Const.dlt1].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_pd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.primary_disease_states2].copy()\n",
    "        d = data[Const.primary_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_nd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.nodal_disease_states2].copy()\n",
    "        d = data[Const.nodal_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_cc(state):\n",
    "        res = data[Const.ccs].copy()\n",
    "        if state == 1:\n",
    "            res.values[:,:] = np.zeros(res.values.shape)\n",
    "        return res\n",
    "    \n",
    "    def get_mod(state):\n",
    "        res = data[Const.modifications].copy()\n",
    "        #this should have an ic condition but we don't use it anumore anywa\n",
    "        return res\n",
    "        \n",
    "    outcomedf = data[Const.outcomes]\n",
    "    baseline = dataset.get_state('baseline')\n",
    "    \n",
    "    def formatdf(d,dids=train_ids):\n",
    "        d = df_to_torch(d.loc[dids]).to(model.get_device())\n",
    "        return d\n",
    "    \n",
    "    def makegrad(v):\n",
    "        if not v.requires_grad:\n",
    "            v.requires_grad=True\n",
    "        return v\n",
    "    \n",
    "    if use_attention:\n",
    "        model = DecisionAttentionModel(baseline.shape[1],**model_kwargs)\n",
    "    else:\n",
    "        model_kwargs = {k:v for k,v in model_kwargs.items() if 'attention' not in k and 'embed' not in k}\n",
    "        model = DecisionModel(baseline.shape[1],**model_kwargs)\n",
    "        \n",
    "    device = 'cpu'\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "        \n",
    "    model.set_device(device)\n",
    "\n",
    "    tmodel1.set_device(device)\n",
    "    tmodel2.set_device(device)\n",
    "    tmodel3.set_device(device)\n",
    "    hashcode = str(hash(','.join([str(i) for i in train_ids])))\n",
    "    \n",
    "    save_file = save_path + 'model_' + model.identifier +'_hash' + hashcode + file_suffix + '.tar'\n",
    "    model.fit_normalizer(df_to_torch(baseline.loc[train_ids]))\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "    \n",
    "    \n",
    "    optimal_train,transitions_train = calc_optimal_decisions(dataset,train_ids,tmodel1,tmodel2,tmodel3,\n",
    "                                           weights=weights,\n",
    "                                          )\n",
    "    optimal_test,transitions_test = calc_optimal_decisions(dataset,test_ids,tmodel1,tmodel2,tmodel3,\n",
    "                                          weights=weights,\n",
    "                                         )\n",
    "    optimal_train = optimal_train.to(model.get_device())\n",
    "    optimal_test = optimal_test.to(model.get_device())\n",
    "    mse = torch.nn.MSELoss()\n",
    "    bce = torch.nn.BCELoss()\n",
    "    \n",
    "    def compare_decisions(d1,d2,d3,ids):\n",
    "#         ypred = np.concatenate([dd.cpu().detach().numpy().reshape(-1,1) for dd in [d1,d2,d3]],axis=1)\n",
    "        ytrue = df_to_torch(outcomedf.loc[ids])\n",
    "        dloss = bce(d1.view(-1),ytrue[:,0])\n",
    "        dloss += bce(d2.view(-1),ytrue[:,1])\n",
    "        dloss += bce(d3,view(-1),ytrue[:,2])\n",
    "        return dloss\n",
    "        \n",
    "    def remove_decisions(df):\n",
    "        cols = [c for c in df.columns if c not in Const.decisions ]\n",
    "        ddf = df[cols]\n",
    "        return ddf\n",
    "    \n",
    "    makeinput = lambda step,dids: df_to_torch(remove_decisions(dataset.get_input_state(step=step,ids=dids)))\n",
    "    threshold = lambda x: torch.gt(x,torch.rand(x.shape[0])).type(torch.FloatTensor)\n",
    "\n",
    "    randchoice = lambda x: x[torch.randint(len(x),(1,))[0]]\n",
    "    tloss_func = torch.nn.TripletMarginLoss()\n",
    "    def get_tloss(row,step,yt,x,imitation=True):\n",
    "        if yt[:,step].std() < .001:\n",
    "            return torch.tensor([0]).to(device)\n",
    "        positive_idx= torch.nonzero(yt[:,step] == yt[row,step])\n",
    "        if len(positive_idx) <= 1:\n",
    "            print('no losses','n positive',len(positive_idx),'yt',yt[row,step],'row',row,'step',step,'imitation',imitation,end='\\r')\n",
    "            return torch.tensor([0]).to(device)\n",
    "        positive_idx = torch.stack([ii for ii in positive_idx if ii != row]).view(-1)\n",
    "        negative_idx = torch.tensor([ii for ii in range(x.shape[0]) if ii not in positive_idx and ii != row])\n",
    "        if len(positive_idx) < 1 or len(negative_idx) < 1:\n",
    "            print('no losses','n positive',len(positive_idx),'n negative',len(negative_idx),'yt',yt[row,step],'row',row,'step',step,'imitation',imitation,end='\\r')\n",
    "            return torch.tensor([0]).to(device)\n",
    "        positive = x[randchoice(positive_idx)]\n",
    "        negative = x[randchoice(negative_idx)]\n",
    "        anchor = x[row]\n",
    "        if use_attention:\n",
    "            [anchor_embedding,pos_embedding,neg_embedding] = [model.get_embedding(xx.view(1,-1),position=step,use_saved_memory=True) for xx in [anchor,positive,negative]]\n",
    "        else:    \n",
    "            [anchor_embedding,pos_embedding,neg_embedding] = [model.get_embedding(xx.view(1,-1),position=step,concatenate=False)[int(imitation)] for xx in [anchor,positive,negative]]\n",
    "        tloss = tloss_func(anchor_embedding,pos_embedding,neg_embedding)\n",
    "        return tloss\n",
    "    #save the inputs from the whole dataset for future reference\n",
    "    if use_attention:\n",
    "        full_data = []\n",
    "        for mstep in [0,1,2]:\n",
    "            full_data_step = [baseline, get_dlt(min(mstep,1)),\n",
    "                         get_dlt(mstep),get_pd(mstep),get_nd(mstep),get_cc(mstep),get_mod(mstep)]\n",
    "            full_data_step = torch.cat([formatdf(fd,true_ids) for fd in full_data_step],axis=1)\n",
    "            full_data.append(full_data_step)\n",
    "        full_data = torch.stack(full_data)\n",
    "        model.save_memory(full_data)\n",
    "        print(full_data.shape)\n",
    "        \n",
    "    def step(train=True):\n",
    "        if train:\n",
    "            model.train(True)\n",
    "            tmodel1.train(True)\n",
    "            tmodel2.train(True)\n",
    "            tmodel3.train(True)\n",
    "            optimizer.zero_grad()\n",
    "            ids = train_ids\n",
    "            y_opt = makegrad(optimal_train)\n",
    "            transition_dict = {k: torch.clone(v).detach() for k,v in transitions_train.items()}\n",
    "        else:\n",
    "            ids = test_ids\n",
    "            model.eval()\n",
    "            tmodel1.eval()\n",
    "            tmodel2.eval()\n",
    "            tmodel3.eval()\n",
    "            y_opt = makegrad(optimal_test)\n",
    "            print(y_opt.mean(axis=0))\n",
    "            transition_dict = {k: torch.clone(v).detach() for k,v in transitions_test.items()}\n",
    "        model.set_device(device)\n",
    "        ytrain = df_to_torch(outcomedf.loc[ids]).to(device)\n",
    "        #imitation losses and decision 1\n",
    "        xxtrained = [baseline, get_dlt(0),get_dlt(0),get_pd(0),get_nd(0),get_cc(0),get_mod(0)]\n",
    "        xxtrain = [formatdf(xx,ids) for xx in xxtrained]\n",
    "        xxtrain = torch.cat(xxtrain,axis=1).to(device)\n",
    "        o1 = model(xxtrain,position=0,use_saved_memory= (not train))\n",
    "        decision1_imitation = o1[:,3]\n",
    "        decision1_opt = o1[:,0]\n",
    "    \n",
    "        imitation_loss1 = bce(decision1_imitation,ytrain[:,0])\n",
    "        imitation_loss1 = torch.mul(imitation_loss1,imitation_weights[0])\n",
    "        opt_loss1 = bce(decision1_opt,y_opt[:,0])\n",
    "        opt_loss1 = torch.mul(opt_loss1,opt_weights[0])\n",
    "        \n",
    "        x1_imitation = [baseline, get_dlt(1),get_dlt(0),get_pd(1),get_nd(1),get_cc(1),get_mod(1)]\n",
    "        x1_imitation = [formatdf(xx1,ids) for xx1 in x1_imitation]\n",
    "        x1_imitation = torch.cat(x1_imitation,axis=1).to(device)\n",
    "        \n",
    "        o2 = model(x1_imitation,position=1,use_saved_memory= (not train))\n",
    "            \n",
    "        decision2_imitation = o2[:,4]\n",
    "            \n",
    "        imitation_loss2 =  bce(decision2_imitation,ytrain[:,1])\n",
    "        imitation_loss2 = torch.mul(imitation_loss2,imitation_weights[1])\n",
    "        \n",
    "        \n",
    "        x2_imitation = [baseline, get_dlt(1),get_dlt(2),get_pd(2),get_nd(2),get_cc(2),get_mod(2)]\n",
    "        \n",
    "        x2_imitation = [formatdf(xx2,ids) for xx2 in x2_imitation]\n",
    "        x2_imitation = torch.cat(x2_imitation,axis=1).to(device)\n",
    "        \n",
    "        \n",
    "        o3 = model(x2_imitation,position=2,use_saved_memory= (not train))\n",
    "            \n",
    "        decision3_imitation = o3[:,5]\n",
    "        \n",
    "        imitation_loss3 = bce(decision3_imitation,ytrain[:,2])\n",
    "        imitation_loss3 = torch.mul(imitation_loss3,imitation_weights[2])\n",
    "        \n",
    "        opt_input2 = [\n",
    "            formatdf(baseline,ids), \n",
    "            transition_dict['dlt1'],\n",
    "            formatdf(get_dlt(0),ids),\n",
    "            transition_dict['pd1'],\n",
    "            transition_dict['nd1'], \n",
    "            formatdf(get_cc(0),ids),\n",
    "            transition_dict['mod']\n",
    "                 ]\n",
    "        opt_input2 = [o.to(device) for o in opt_input2]\n",
    "\n",
    "        opt_input2 = torch.cat(opt_input2,axis=1).to(device)\n",
    "        decision2_opt = model(opt_input2,position=1,use_saved_memory= (not train))[:,1]\n",
    "        \n",
    "        opt_loss2 = bce(decision2_opt,y_opt[:,1])\n",
    "        opt_loss2 = torch.mul(opt_loss2,opt_weights[1])\n",
    "        \n",
    "        opt_input3 = [\n",
    "            formatdf(baseline,ids),\n",
    "            transition_dict['dlt1'],\n",
    "            transition_dict['dlt2'],\n",
    "            transition_dict['pd2'],\n",
    "            transition_dict['nd2'],\n",
    "            transition_dict['cc'],\n",
    "            transition_dict['mod'],\n",
    "        ]\n",
    "        opt_input3 = [o.to(device) for o in opt_input3]\n",
    "        opt_input3 = torch.cat(opt_input3,axis=1).to(device)\n",
    "        decision3_opt = model(opt_input3,position=2,use_saved_memory= (not train))[:,2]\n",
    "        \n",
    "        opt_loss3 = bce(decision3_opt,y_opt[:,2])\n",
    "        opt_loss3 = torch.mul(opt_loss3,opt_weights[2])\n",
    "        \n",
    "        iloss = torch.add(torch.add(imitation_loss1,imitation_loss2),imitation_loss3)\n",
    "        iloss = torch.mul(iloss,imitation_weight)\n",
    "        \n",
    "        reward_loss = torch.add(torch.add(opt_loss1,opt_loss2),opt_loss3)\n",
    "        reward_loss =torch.mul(reward_loss,reward_weight)\n",
    "        \n",
    "        loss = torch.add(iloss,reward_loss)\n",
    "        \n",
    "        imitation_tloss = torch.FloatTensor([0]).to(device)\n",
    "        opt_tloss = torch.FloatTensor([0]).to(device)\n",
    "        n_rows = xxtrain.shape[0]\n",
    "        if reward_triplet_weight + imitation_triplet_weight > 0.0001:\n",
    "            for i in range(n_rows):\n",
    "                \n",
    "                if imitation_triplet_weight > .0001:\n",
    "                    imitation_tloss += get_tloss(i,0,ytrain,xxtrain,True)\n",
    "                    imitation_tloss += get_tloss(i,1,ytrain,x1_imitation,True)\n",
    "                    imitation_tloss += get_tloss(i,2,ytrain,x2_imitation,True)\n",
    "                if reward_triplet_weight > .0001:\n",
    "                    opt_tloss += get_tloss(i,0,y_opt,xxtrain,False)\n",
    "                    opt_tloss += get_tloss(i,1,y_opt,opt_input2,False)\n",
    "                    opt_tloss += get_tloss(i,2,y_opt,opt_input3,False)\n",
    "            loss += torch.mul(imitation_tloss[0],imitation_triplet_weight/n_rows)\n",
    "            loss += torch.mul(opt_tloss[0],reward_triplet_weight/n_rows)\n",
    "        losses = [iloss,reward_loss,imitation_tloss*imitation_triplet_weight/n_rows,opt_tloss*reward_triplet_weight/n_rows]\n",
    "        \n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            return losses\n",
    "        else:\n",
    "            scores = []\n",
    "            distributions = [decision1_opt.mean().item(),decision2_opt.mean().item(),decision3_opt.mean().item()]\n",
    "            imitation = [decision1_imitation,decision2_imitation,decision3_imitation]\n",
    "            optimal = [decision1_opt,decision2_opt,decision3_opt]\n",
    "            for i,decision_im in enumerate(imitation):\n",
    "                deci = decision_im.cpu().detach().numpy()\n",
    "                deci0 = (deci > .5).astype(int)\n",
    "                iout = ytrain[:,i].cpu().detach().numpy()\n",
    "                acci = accuracy_score(iout,deci0)\n",
    "                try:\n",
    "                    auci = roc_auc_score(iout,deci)\n",
    "                except:\n",
    "                    auci = -1\n",
    "                \n",
    "                deco = optimal[i].cpu().detach().numpy()\n",
    "                deci0 = (deco > .5).astype(int)\n",
    "                oout = y_opt[:,i].cpu().detach().numpy()\n",
    "                acco = accuracy_score(oout,deci0)\n",
    "                try:\n",
    "                    auco = roc_auc_score(oout,deco)\n",
    "                except:\n",
    "                    auco=-1\n",
    "                scores.append({'decision': i,'optimal_auc': auco,'imitation_auc': auci,'optimal_acc': acco,'imitation_acc': acci})\n",
    "            return losses, scores, distributions\n",
    "        \n",
    "    best_val_loss = torch.tensor(1000000000.0)\n",
    "    steps_since_improvement = 0\n",
    "    best_val_score = {}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        losses = step(True)\n",
    "        val_losses,val_metrics,val_distributions = step(False)\n",
    "        vl = val_losses[0] + val_losses[1]\n",
    "        for vm in val_metrics:\n",
    "            vl += (-((vm['optimal_auc']*reward_weight) + (vm['imitation_auc']*imitation_weight)))/10\n",
    "        if verbose:\n",
    "            print('______epoch',str(epoch),'_____')\n",
    "            print('val reward',val_losses[1].item())\n",
    "            print('imitation reward', val_losses[0].item())\n",
    "            print('distance losses',val_losses[2].item(),val_losses[-1].item())\n",
    "            print('distributions',val_distributions)\n",
    "            print(val_metrics)\n",
    "        if vl < best_val_loss:\n",
    "            best_val_loss = vl\n",
    "            best_val_score = val_metrics\n",
    "            best_val_distributions = val_distributions\n",
    "            steps_since_improvement = 0\n",
    "            torch.save(model.state_dict(),save_file)\n",
    "        else:\n",
    "            steps_since_improvement += 1\n",
    "        if steps_since_improvement > patience:\n",
    "            break\n",
    "    print('++++++++++Final+++++++++++')\n",
    "    print('best',best_val_loss)\n",
    "    print(best_val_score)\n",
    "    model.load_state_dict(torch.load(save_file))\n",
    "    model.eval()\n",
    "    return model, best_val_score, best_val_loss, best_val_distributions\n",
    "\n",
    "from Models import *\n",
    "args = {\n",
    "    'hidden_layers': [500], \n",
    "    'opt_layer_size': 20, \n",
    "    'imitation_layer_size': 20, \n",
    "    'dropout': 0.25, \n",
    "    'input_dropout': 0.25, \n",
    "    'shufflecol_chance': 0.5\n",
    "}\n",
    "\n",
    "#1.8424\n",
    "decision_model, decision_score, decision_loss, _ = train_decision_model_triplet(\n",
    "    tmodel1[0],tmodel2[0],tmodel3_smote[0],\n",
    "    lr=.01,\n",
    "    imitation_weight=1,\n",
    "    reward_weight=1,\n",
    "    patience=10,\n",
    "    imitation_triplet_weight=1,\n",
    "    reward_triplet_weight =1,\n",
    "    verbose=True,\n",
    "    use_attention=True,\n",
    "    **args)\n",
    "decision_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd8f70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 536, 87])\n",
      "tensor([0.1429, 0.7483, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 0 _____\n",
      "val reward 1.5617798566818237\n",
      "imitation reward 1.3217936754226685\n",
      "distance losses 2.798015594482422 2.0553805828094482\n",
      "distributions [0.3763808310031891, 0.46246063709259033, 0.7454255223274231]\n",
      "[{'decision': 0, 'optimal_auc': 0.502267573696145, 'imitation_auc': 0.5009541984732824, 'optimal_acc': 0.8231292517006803, 'imitation_acc': 0.8843537414965986}, {'decision': 1, 'optimal_auc': 0.7049140049140049, 'imitation_auc': 0.6342391304347826, 'optimal_acc': 0.46938775510204084, 'imitation_acc': 0.7619047619047619}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.6271455816910362, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.1429, 0.7483, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 1 _____\n",
      "val reward 1.3305341005325317\n",
      "imitation reward 1.2153525352478027\n",
      "distance losses 2.7947072982788086 2.264432668685913\n",
      "distributions [0.30152395367622375, 0.5316998958587646, 0.8098813891410828]\n",
      "[{'decision': 0, 'optimal_auc': 0.5071806500377929, 'imitation_auc': 0.5081106870229007, 'optimal_acc': 0.8571428571428571, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.7481572481572482, 'imitation_auc': 0.6459239130434783, 'optimal_acc': 0.7142857142857143, 'imitation_acc': 0.7891156462585034}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.6325492689129053, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.1429, 0.7483, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 2 _____\n",
      "val reward 1.1739943027496338\n",
      "imitation reward 1.1567264795303345\n",
      "distance losses 2.734595775604248 2.3179378509521484\n",
      "distributions [0.2406661957502365, 0.595185399055481, 0.8594604134559631]\n",
      "[{'decision': 0, 'optimal_auc': 0.5052910052910052, 'imitation_auc': 0.523854961832061, 'optimal_acc': 0.8571428571428571, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.7791154791154791, 'imitation_auc': 0.654891304347826, 'optimal_acc': 0.7619047619047619, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.6484424666242848, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.1429, 0.7483, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 3 _____\n",
      "val reward 1.0763205289840698\n",
      "imitation reward 1.1330904960632324\n",
      "distance losses 2.6128017902374268 2.4546127319335938\n",
      "distributions [0.1934346854686737, 0.6488136649131775, 0.8955382108688354]\n",
      "[{'decision': 0, 'optimal_auc': 0.5037792894935752, 'imitation_auc': 0.5248091603053435, 'optimal_acc': 0.8571428571428571, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.7975429975429975, 'imitation_auc': 0.6608695652173914, 'optimal_acc': 0.7414965986394558, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.666878575969485, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.1429, 0.7483, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 4 _____\n",
      "val reward 1.017424464225769\n",
      "imitation reward 1.130314588546753\n",
      "distance losses 2.9153659343719482 2.2638607025146484\n",
      "distributions [0.15963566303253174, 0.686792254447937, 0.9220851063728333]\n",
      "[{'decision': 0, 'optimal_auc': 0.4988662131519274, 'imitation_auc': 0.5314885496183206, 'optimal_acc': 0.8571428571428571, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.8167076167076166, 'imitation_auc': 0.6646739130434783, 'optimal_acc': 0.7482993197278912, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.6783216783216783, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n",
      "tensor([0.1429, 0.7483, 1.0000], grad_fn=<MeanBackward1>)\n",
      "______epoch 5 _____\n",
      "val reward 0.9815435409545898\n",
      "imitation reward 1.1369000673294067\n",
      "distance losses 2.668236494064331 2.317408323287964\n",
      "distributions [0.13599073886871338, 0.7127640247344971, 0.9410397410392761]\n",
      "[{'decision': 0, 'optimal_auc': 0.49281934996220705, 'imitation_auc': 0.5381679389312977, 'optimal_acc': 0.8571428571428571, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.8346437346437348, 'imitation_auc': 0.6679347826086957, 'optimal_acc': 0.7482993197278912, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.6894469167196441, 'optimal_acc': 1.0, 'imitation_acc': 0.8231292517006803}]\n"
     ]
    }
   ],
   "source": [
    "from Utils import *\n",
    "from Models import *\n",
    "[t1_alt, t2_alt, t3_alt] = load_sklearn_transition_models()\n",
    "decision_model_alt, decision_score_alt, decision_loss_alt, _ = train_decision_model_triplet(\n",
    "    t1_alt,t2_alt,t3_alt,\n",
    "    lr=.001,\n",
    "    imitation_weight=1,\n",
    "    reward_weight=1,\n",
    "    patience=10,\n",
    "    imitation_triplet_weight=1,\n",
    "    reward_triplet_weight =1,\n",
    "    verbose=True,\n",
    "    use_attention=True,\n",
    "    **args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "7364bc2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.array([1,3,54,3.2,23,4])\n",
    "np.isnan(test.astype(np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "26a7faad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 536, 87])\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8700, 0.2070, 0.1854, 0.8912], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 0 _____\n",
      "val reward 0.02157173864543438\n",
      "imitation reward 1.4515347480773926\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.3741496503353119, 0.31972789764404297, 0.12925170361995697]\n",
      "[{'decision': 0, 'optimal_auc': 0.35917525773195874, 'imitation_auc': 0.34685114503816794, 'optimal_acc': 0.3333333333333333, 'imitation_acc': 0.8367346938775511}, {'decision': 1, 'optimal_auc': 0.4380281690140845, 'imitation_auc': 0.5959239130434782, 'optimal_acc': 0.6598639455782312, 'imitation_acc': 0.6938775510204082}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.4561347743165925, 'optimal_acc': 0.1292517006802721, 'imitation_acc': 0.8163265306122449}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8705, 0.2041, 0.1835, 0.8892], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 1 _____\n",
      "val reward 0.020592903718352318\n",
      "imitation reward 1.261374831199646\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.35374149680137634, 0.25170066952705383, 0.15646257996559143]\n",
      "[{'decision': 0, 'optimal_auc': 0.34371134020618554, 'imitation_auc': 0.3449427480916031, 'optimal_acc': 0.3129251700680272, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.47323943661971835, 'imitation_auc': 0.6293478260869565, 'optimal_acc': 0.7278911564625851, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.5133502860775588, 'optimal_acc': 0.1564625850340136, 'imitation_acc': 0.8231292517006803}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8712, 0.2032, 0.1826, 0.8887], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 2 _____\n",
      "val reward 0.020565271377563477\n",
      "imitation reward 1.183445692062378\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.3401360511779785, 0.21088434755802155, 0.16326530277729034]\n",
      "[{'decision': 0, 'optimal_auc': 0.3182474226804124, 'imitation_auc': 0.3635496183206107, 'optimal_acc': 0.2857142857142857, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.39084507042253525, 'imitation_auc': 0.6540760869565218, 'optimal_acc': 0.7551020408163265, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.5921805467260013, 'optimal_acc': 0.16326530612244897, 'imitation_acc': 0.8231292517006803}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8706, 0.2019, 0.1818, 0.8866], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 3 _____\n",
      "val reward 0.020161649212241173\n",
      "imitation reward 1.171202540397644\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.31292515993118286, 0.20408162474632263, 0.18367347121238708]\n",
      "[{'decision': 0, 'optimal_auc': 0.35824742268041243, 'imitation_auc': 0.37595419847328243, 'optimal_acc': 0.3129251700680272, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.3943661971830986, 'imitation_auc': 0.6845108695652175, 'optimal_acc': 0.7619047619047619, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.6595677050222504, 'optimal_acc': 0.1836734693877551, 'imitation_acc': 0.8231292517006803}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8704, 0.1996, 0.1803, 0.8843], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 4 _____\n",
      "val reward 0.019230984151363373\n",
      "imitation reward 1.1859394311904907\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.31292515993118286, 0.14965985715389252, 0.21768707036972046]\n",
      "[{'decision': 0, 'optimal_auc': 0.35824742268041243, 'imitation_auc': 0.39646946564885494, 'optimal_acc': 0.3129251700680272, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.4225352112676056, 'imitation_auc': 0.6953804347826087, 'optimal_acc': 0.8163265306122449, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7034329307056579, 'optimal_acc': 0.21768707482993196, 'imitation_acc': 0.8231292517006803}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8691, 0.1983, 0.1799, 0.8804], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 5 _____\n",
      "val reward 0.018451528623700142\n",
      "imitation reward 1.2016726732254028\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.31972789764404297, 0.14965985715389252, 0.25170066952705383]\n",
      "[{'decision': 0, 'optimal_auc': 0.36340206185567014, 'imitation_auc': 0.4122137404580153, 'optimal_acc': 0.3197278911564626, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.4225352112676056, 'imitation_auc': 0.7051630434782609, 'optimal_acc': 0.8163265306122449, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7263191354100444, 'optimal_acc': 0.25170068027210885, 'imitation_acc': 0.8231292517006803}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8685, 0.1974, 0.1793, 0.8784], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 6 _____\n",
      "val reward 0.018165702000260353\n",
      "imitation reward 1.206160306930542\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.30612245202064514, 0.13605442643165588, 0.27210885286331177]\n",
      "[{'decision': 0, 'optimal_auc': 0.3530927835051546, 'imitation_auc': 0.43988549618320616, 'optimal_acc': 0.30612244897959184, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.4295774647887324, 'imitation_auc': 0.7127717391304348, 'optimal_acc': 0.8299319727891157, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.733630006357279, 'optimal_acc': 0.272108843537415, 'imitation_acc': 0.8231292517006803}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8671, 0.1967, 0.1793, 0.8753], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 7 _____\n",
      "val reward 0.01743903011083603\n",
      "imitation reward 1.19655442237854\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.29931971430778503, 0.14965985715389252, 0.29931971430778503]\n",
      "[{'decision': 0, 'optimal_auc': 0.3479381443298969, 'imitation_auc': 0.4565839694656489, 'optimal_acc': 0.29931972789115646, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.4225352112676056, 'imitation_auc': 0.7127717391304348, 'optimal_acc': 0.8163265306122449, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7383979656706928, 'optimal_acc': 0.29931972789115646, 'imitation_acc': 0.8231292517006803}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8663, 0.1970, 0.1795, 0.8732], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 8 _____\n",
      "val reward 0.017472773790359497\n",
      "imitation reward 1.1755050420761108\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.2925170063972473, 0.16326530277729034, 0.31292515993118286]\n",
      "[{'decision': 0, 'optimal_auc': 0.3427835051546392, 'imitation_auc': 0.46851145038167946, 'optimal_acc': 0.2925170068027211, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.4154929577464789, 'imitation_auc': 0.7154891304347827, 'optimal_acc': 0.8027210884353742, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7368086458995551, 'optimal_acc': 0.3129251700680272, 'imitation_acc': 0.8231292517006803}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8652, 0.1980, 0.1802, 0.8715], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 9 _____\n",
      "val reward 0.01776515319943428\n",
      "imitation reward 1.1493709087371826\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.27210885286331177, 0.19727890193462372, 0.31972789764404297]\n",
      "[{'decision': 0, 'optimal_auc': 0.34247422680412376, 'imitation_auc': 0.48854961832061067, 'optimal_acc': 0.2857142857142857, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.397887323943662, 'imitation_auc': 0.7171195652173913, 'optimal_acc': 0.7687074829931972, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7396694214876032, 'optimal_acc': 0.3197278911564626, 'imitation_acc': 0.8163265306122449}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8647, 0.1996, 0.1809, 0.8712], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 10 _____\n",
      "val reward 0.01866799034178257\n",
      "imitation reward 1.1242432594299316\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.22448979318141937, 0.2448979616165161, 0.31292515993118286]\n",
      "[{'decision': 0, 'optimal_auc': 0.3518556701030928, 'imitation_auc': 0.49904580152671757, 'optimal_acc': 0.2789115646258503, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.3732394366197183, 'imitation_auc': 0.713858695652174, 'optimal_acc': 0.7210884353741497, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7425301970756516, 'optimal_acc': 0.3129251700680272, 'imitation_acc': 0.8163265306122449}]\n",
      "True True True\n",
      "True True True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False False False\n",
      "False False True\n",
      "tensor([0.8639, 0.2005, 0.1815, 0.8704], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 11 _____\n",
      "val reward 0.01902039721608162\n",
      "imitation reward 1.1054091453552246\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.20408162474632263, 0.2925170063972473, 0.31292515993118286]\n",
      "[{'decision': 0, 'optimal_auc': 0.3818556701030928, 'imitation_auc': 0.5014312977099237, 'optimal_acc': 0.29931972789115646, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.3485915492957746, 'imitation_auc': 0.7089673913043477, 'optimal_acc': 0.673469387755102, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7463445645263828, 'optimal_acc': 0.3129251700680272, 'imitation_acc': 0.7959183673469388}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8627, 0.2023, 0.1826, 0.8698], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 12 _____\n",
      "val reward 0.019665595144033432\n",
      "imitation reward 1.0927815437316895\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.17006802558898926, 0.36054420471191406, 0.30612245202064514]\n",
      "[{'decision': 0, 'optimal_auc': 0.3560824742268041, 'imitation_auc': 0.4942748091603053, 'optimal_acc': 0.2653061224489796, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.5204225352112676, 'imitation_auc': 0.7054347826086957, 'optimal_acc': 0.6326530612244898, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7485696122059758, 'optimal_acc': 0.30612244897959184, 'imitation_acc': 0.7891156462585034}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8624, 0.2023, 0.1827, 0.8696], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 13 _____\n",
      "val reward 0.01960563473403454\n",
      "imitation reward 1.0843348503112793\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.13605442643165588, 0.3741496503353119, 0.30612245202064514]\n",
      "[{'decision': 0, 'optimal_auc': 0.3909278350515464, 'imitation_auc': 0.48425572519083965, 'optimal_acc': 0.2857142857142857, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.5133802816901408, 'imitation_auc': 0.70625, 'optimal_acc': 0.6190476190476191, 'imitation_acc': 0.7891156462585034}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7479338842975207, 'optimal_acc': 0.30612244897959184, 'imitation_acc': 0.7891156462585034}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8624, 0.2038, 0.1836, 0.8704], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 14 _____\n",
      "val reward 0.02019094116985798\n",
      "imitation reward 1.0792629718780518\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.11564625799655914, 0.40816324949264526, 0.2925170063972473]\n",
      "[{'decision': 0, 'optimal_auc': 0.4057731958762887, 'imitation_auc': 0.47709923664122145, 'optimal_acc': 0.2925170068027211, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.4957746478873239, 'imitation_auc': 0.7051630434782608, 'optimal_acc': 0.5850340136054422, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7530197075651621, 'optimal_acc': 0.2925170068027211, 'imitation_acc': 0.7891156462585034}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8629, 0.2045, 0.1840, 0.8719], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 15 _____\n",
      "val reward 0.02049684338271618\n",
      "imitation reward 1.077471375465393\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.12925170361995697, 0.40816324949264526, 0.2789115607738495]\n",
      "[{'decision': 0, 'optimal_auc': 0.4160824742268041, 'imitation_auc': 0.4756679389312977, 'optimal_acc': 0.30612244897959184, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.4957746478873239, 'imitation_auc': 0.7016304347826087, 'optimal_acc': 0.5850340136054422, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7609663064208518, 'optimal_acc': 0.2789115646258503, 'imitation_acc': 0.7891156462585034}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8623, 0.2055, 0.1849, 0.8716], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 16 _____\n",
      "val reward 0.020585397258400917\n",
      "imitation reward 1.079971432685852\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.13605442643165588, 0.4285714328289032, 0.2789115607738495]\n",
      "[{'decision': 0, 'optimal_auc': 0.4212371134020618, 'imitation_auc': 0.47853053435114506, 'optimal_acc': 0.3129251700680272, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.48521126760563377, 'imitation_auc': 0.7000000000000001, 'optimal_acc': 0.564625850340136, 'imitation_acc': 0.7891156462585034}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7676414494596313, 'optimal_acc': 0.2789115646258503, 'imitation_acc': 0.7891156462585034}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8624, 0.2073, 0.1861, 0.8733], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 17 _____\n",
      "val reward 0.02119806408882141\n",
      "imitation reward 1.0865939855575562\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.16326530277729034, 0.45578229427337646, 0.25850340723991394]\n",
      "[{'decision': 0, 'optimal_auc': 0.41154639175257735, 'imitation_auc': 0.4818702290076336, 'optimal_acc': 0.3129251700680272, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.4711267605633802, 'imitation_auc': 0.6978260869565217, 'optimal_acc': 0.5374149659863946, 'imitation_acc': 0.7891156462585034}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7724094087730452, 'optimal_acc': 0.2585034013605442, 'imitation_acc': 0.7891156462585034}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8635, 0.2094, 0.1871, 0.8763], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 18 _____\n",
      "val reward 0.022302918136119843\n",
      "imitation reward 1.097334861755371\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.16326530277729034, 0.4761904776096344, 0.21768707036972046]\n",
      "[{'decision': 0, 'optimal_auc': 0.41154639175257735, 'imitation_auc': 0.4880725190839694, 'optimal_acc': 0.3129251700680272, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.4605633802816901, 'imitation_auc': 0.6986413043478261, 'optimal_acc': 0.5170068027210885, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7784488239033694, 'optimal_acc': 0.21768707482993196, 'imitation_acc': 0.7959183673469388}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8632, 0.2092, 0.1870, 0.8755], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 19 _____\n",
      "val reward 0.0221717469394207\n",
      "imitation reward 1.109074354171753\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.16326530277729034, 0.4761904776096344, 0.22448979318141937]\n",
      "[{'decision': 0, 'optimal_auc': 0.41154639175257735, 'imitation_auc': 0.5009541984732824, 'optimal_acc': 0.3129251700680272, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.4605633802816901, 'imitation_auc': 0.7008152173913044, 'optimal_acc': 0.5170068027210885, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7832167832167832, 'optimal_acc': 0.22448979591836735, 'imitation_acc': 0.8027210884353742}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8638, 0.2099, 0.1872, 0.8773], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 20 _____\n",
      "val reward 0.02270682342350483\n",
      "imitation reward 1.1192874908447266\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.16326530277729034, 0.4693877398967743, 0.21088434755802155]\n",
      "[{'decision': 0, 'optimal_auc': 0.41154639175257735, 'imitation_auc': 0.5100190839694657, 'optimal_acc': 0.3129251700680272, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.46408450704225346, 'imitation_auc': 0.6983695652173914, 'optimal_acc': 0.5238095238095238, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7851239669421488, 'optimal_acc': 0.2108843537414966, 'imitation_acc': 0.8027210884353742}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8635, 0.2104, 0.1879, 0.8770], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 21 _____\n",
      "val reward 0.022520942613482475\n",
      "imitation reward 1.1277852058410645\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.16326530277729034, 0.4829931855201721, 0.21768707036972046]\n",
      "[{'decision': 0, 'optimal_auc': 0.41154639175257735, 'imitation_auc': 0.5190839694656488, 'optimal_acc': 0.3129251700680272, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.4570422535211267, 'imitation_auc': 0.6991847826086957, 'optimal_acc': 0.5102040816326531, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7848061029879211, 'optimal_acc': 0.21768707482993196, 'imitation_acc': 0.7959183673469388}]\n",
      "True True True\n",
      "True True True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False False False\n",
      "False False True\n",
      "tensor([0.8634, 0.2107, 0.1881, 0.8769], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 22 _____\n",
      "val reward 0.02256741188466549\n",
      "imitation reward 1.1337333917617798\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.16326530277729034, 0.4897959232330322, 0.21768707036972046]\n",
      "[{'decision': 0, 'optimal_auc': 0.41154639175257735, 'imitation_auc': 0.5324427480916031, 'optimal_acc': 0.3129251700680272, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.45352112676056333, 'imitation_auc': 0.6983695652173914, 'optimal_acc': 0.5034013605442177, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7860775588048314, 'optimal_acc': 0.21768707482993196, 'imitation_acc': 0.7959183673469388}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8631, 0.2106, 0.1881, 0.8762], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 23 _____\n",
      "val reward 0.02248716913163662\n",
      "imitation reward 1.136338710784912\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.16326530277729034, 0.4897959232330322, 0.2312925159931183]\n",
      "[{'decision': 0, 'optimal_auc': 0.41154639175257735, 'imitation_auc': 0.5438931297709924, 'optimal_acc': 0.3129251700680272, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.45352112676056333, 'imitation_auc': 0.7005434782608696, 'optimal_acc': 0.5034013605442177, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7867132867132867, 'optimal_acc': 0.23129251700680273, 'imitation_acc': 0.782312925170068}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8627, 0.2109, 0.1884, 0.8757], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 24 _____\n",
      "val reward 0.02257257141172886\n",
      "imitation reward 1.1374646425247192\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.16326530277729034, 0.5034013390541077, 0.2380952388048172]\n",
      "[{'decision': 0, 'optimal_auc': 0.41154639175257735, 'imitation_auc': 0.5548664122137404, 'optimal_acc': 0.3129251700680272, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.4464788732394366, 'imitation_auc': 0.7019021739130435, 'optimal_acc': 0.4897959183673469, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7870311506675144, 'optimal_acc': 0.23809523809523808, 'imitation_acc': 0.782312925170068}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8631, 0.2108, 0.1882, 0.8765], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 25 _____\n",
      "val reward 0.02259897254407406\n",
      "imitation reward 1.1378569602966309\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.14965985715389252, 0.5034013390541077, 0.2312925159931183]\n",
      "[{'decision': 0, 'optimal_auc': 0.43154639175257736, 'imitation_auc': 0.5672709923664123, 'optimal_acc': 0.32653061224489793, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.4464788732394366, 'imitation_auc': 0.7032608695652174, 'optimal_acc': 0.4897959183673469, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7854418308963763, 'optimal_acc': 0.23129251700680273, 'imitation_acc': 0.7755102040816326}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8633, 0.2105, 0.1881, 0.8767], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 26 _____\n",
      "val reward 0.022349072620272636\n",
      "imitation reward 1.1370834112167358\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.12925170361995697, 0.49659863114356995, 0.2312925159931183]\n",
      "[{'decision': 0, 'optimal_auc': 0.44639175257731967, 'imitation_auc': 0.5782442748091603, 'optimal_acc': 0.3333333333333333, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.44999999999999996, 'imitation_auc': 0.7040760869565218, 'optimal_acc': 0.4965986394557823, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7870311506675144, 'optimal_acc': 0.23129251700680273, 'imitation_acc': 0.7755102040816326}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8633, 0.2106, 0.1882, 0.8767], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 27 _____\n",
      "val reward 0.022376224398612976\n",
      "imitation reward 1.1352425813674927\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.13605442643165588, 0.49659863114356995, 0.2312925159931183]\n",
      "[{'decision': 0, 'optimal_auc': 0.4515463917525774, 'imitation_auc': 0.5868320610687023, 'optimal_acc': 0.3401360544217687, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.44999999999999996, 'imitation_auc': 0.7057065217391304, 'optimal_acc': 0.4965986394557823, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7879847425301971, 'optimal_acc': 0.23129251700680273, 'imitation_acc': 0.7687074829931972}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8633, 0.2111, 0.1886, 0.8769], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 28 _____\n",
      "val reward 0.02243761718273163\n",
      "imitation reward 1.1324846744537354\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.13605442643165588, 0.5102040767669678, 0.22448979318141937]\n",
      "[{'decision': 0, 'optimal_auc': 0.4515463917525774, 'imitation_auc': 0.5916030534351145, 'optimal_acc': 0.3401360544217687, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.4429577464788732, 'imitation_auc': 0.7065217391304348, 'optimal_acc': 0.48299319727891155, 'imitation_acc': 0.7619047619047619}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7892561983471075, 'optimal_acc': 0.22448979591836735, 'imitation_acc': 0.7687074829931972}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8634, 0.2109, 0.1885, 0.8770], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 29 _____\n",
      "val reward 0.022428758442401886\n",
      "imitation reward 1.1302917003631592\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.13605442643165588, 0.5034013390541077, 0.22448979318141937]\n",
      "[{'decision': 0, 'optimal_auc': 0.4667010309278351, 'imitation_auc': 0.5978053435114503, 'optimal_acc': 0.35374149659863946, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.4464788732394366, 'imitation_auc': 0.7065217391304348, 'optimal_acc': 0.4897959183673469, 'imitation_acc': 0.7619047619047619}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7949777495232041, 'optimal_acc': 0.22448979591836735, 'imitation_acc': 0.7755102040816326}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8641, 0.2111, 0.1885, 0.8781], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 30 _____\n",
      "val reward 0.02255488932132721\n",
      "imitation reward 1.1298328638076782\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.1428571343421936, 0.49659863114356995, 0.21088434755802155]\n",
      "[{'decision': 0, 'optimal_auc': 0.4567010309278351, 'imitation_auc': 0.6001908396946565, 'optimal_acc': 0.3469387755102041, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.44999999999999996, 'imitation_auc': 0.7076086956521739, 'optimal_acc': 0.4965986394557823, 'imitation_acc': 0.7619047619047619}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7978385251112524, 'optimal_acc': 0.2108843537414966, 'imitation_acc': 0.782312925170068}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8643, 0.2104, 0.1881, 0.8783], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 31 _____\n",
      "val reward 0.02232428267598152\n",
      "imitation reward 1.1310592889785767\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.12925170361995697, 0.4829931855201721, 0.21088434755802155]\n",
      "[{'decision': 0, 'optimal_auc': 0.47670103092783506, 'imitation_auc': 0.603530534351145, 'optimal_acc': 0.36054421768707484, 'imitation_acc': 0.8843537414965986}, {'decision': 1, 'optimal_auc': 0.4570422535211267, 'imitation_auc': 0.7076086956521739, 'optimal_acc': 0.5102040816326531, 'imitation_acc': 0.7619047619047619}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8016528925619835, 'optimal_acc': 0.2108843537414966, 'imitation_acc': 0.782312925170068}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8642, 0.2102, 0.1881, 0.8780], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 32 _____\n",
      "val reward 0.022178085520863533\n",
      "imitation reward 1.132243275642395\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.12244898080825806, 0.4829931855201721, 0.21768707036972046]\n",
      "[{'decision': 0, 'optimal_auc': 0.48670103092783507, 'imitation_auc': 0.6059160305343512, 'optimal_acc': 0.3673469387755102, 'imitation_acc': 0.8775510204081632}, {'decision': 1, 'optimal_auc': 0.4570422535211267, 'imitation_auc': 0.7067934782608696, 'optimal_acc': 0.5102040816326531, 'imitation_acc': 0.7619047619047619}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8045136681500318, 'optimal_acc': 0.21768707482993196, 'imitation_acc': 0.782312925170068}]\n",
      "True True True\n",
      "True True True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False False False\n",
      "False False True\n",
      "tensor([0.8650, 0.2098, 0.1877, 0.8794], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 33 _____\n",
      "val reward 0.02206243947148323\n",
      "imitation reward 1.1322009563446045\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.10884353518486023, 0.4693877398967743, 0.21088434755802155]\n",
      "[{'decision': 0, 'optimal_auc': 0.506701030927835, 'imitation_auc': 0.6025763358778626, 'optimal_acc': 0.38095238095238093, 'imitation_acc': 0.8639455782312925}, {'decision': 1, 'optimal_auc': 0.46408450704225346, 'imitation_auc': 0.7070652173913043, 'optimal_acc': 0.5238095238095238, 'imitation_acc': 0.7551020408163265}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8057851239669422, 'optimal_acc': 0.2108843537414966, 'imitation_acc': 0.782312925170068}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8649, 0.2096, 0.1875, 0.8791], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 34 _____\n",
      "val reward 0.022101139649748802\n",
      "imitation reward 1.1295347213745117\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.10884353518486023, 0.4693877398967743, 0.21768707036972046]\n",
      "[{'decision': 0, 'optimal_auc': 0.506701030927835, 'imitation_auc': 0.5987595419847328, 'optimal_acc': 0.38095238095238093, 'imitation_acc': 0.8571428571428571}, {'decision': 1, 'optimal_auc': 0.46408450704225346, 'imitation_auc': 0.7089673913043478, 'optimal_acc': 0.5238095238095238, 'imitation_acc': 0.7551020408163265}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8045136681500318, 'optimal_acc': 0.21768707482993196, 'imitation_acc': 0.782312925170068}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8649, 0.2096, 0.1875, 0.8791], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 35 _____\n",
      "val reward 0.022101139649748802\n",
      "imitation reward 1.1262177228927612\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.10884353518486023, 0.4693877398967743, 0.21768707036972046]\n",
      "[{'decision': 0, 'optimal_auc': 0.506701030927835, 'imitation_auc': 0.5944656488549618, 'optimal_acc': 0.38095238095238093, 'imitation_acc': 0.8503401360544217}, {'decision': 1, 'optimal_auc': 0.46408450704225346, 'imitation_auc': 0.7119565217391305, 'optimal_acc': 0.5238095238095238, 'imitation_acc': 0.7551020408163265}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8064208518753972, 'optimal_acc': 0.21768707482993196, 'imitation_acc': 0.782312925170068}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8646, 0.2092, 0.1873, 0.8781], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 36 _____\n",
      "val reward 0.021967589855194092\n",
      "imitation reward 1.123811960220337\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.10204081237316132, 0.4693877398967743, 0.2312925159931183]\n",
      "[{'decision': 0, 'optimal_auc': 0.516701030927835, 'imitation_auc': 0.5844465648854963, 'optimal_acc': 0.3877551020408163, 'imitation_acc': 0.8299319727891157}, {'decision': 1, 'optimal_auc': 0.46408450704225346, 'imitation_auc': 0.7114130434782608, 'optimal_acc': 0.5238095238095238, 'imitation_acc': 0.7551020408163265}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8019707565162111, 'optimal_acc': 0.23129251700680273, 'imitation_acc': 0.7891156462585034}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8650, 0.2081, 0.1865, 0.8785], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 37 _____\n",
      "val reward 0.021655846387147903\n",
      "imitation reward 1.1226565837860107\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.10204081237316132, 0.44897958636283875, 0.2312925159931183]\n",
      "[{'decision': 0, 'optimal_auc': 0.516701030927835, 'imitation_auc': 0.583969465648855, 'optimal_acc': 0.3877551020408163, 'imitation_acc': 0.8163265306122449}, {'decision': 1, 'optimal_auc': 0.4746478873239436, 'imitation_auc': 0.7130434782608696, 'optimal_acc': 0.54421768707483, 'imitation_acc': 0.7619047619047619}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8019707565162111, 'optimal_acc': 0.23129251700680273, 'imitation_acc': 0.782312925170068}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8648, 0.2075, 0.1861, 0.8777], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 38 _____\n",
      "val reward 0.021373556926846504\n",
      "imitation reward 1.1238067150115967\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.0952380895614624, 0.44217684864997864, 0.2448979616165161]\n",
      "[{'decision': 0, 'optimal_auc': 0.5115463917525773, 'imitation_auc': 0.5839694656488549, 'optimal_acc': 0.38095238095238093, 'imitation_acc': 0.8163265306122449}, {'decision': 1, 'optimal_auc': 0.47816901408450696, 'imitation_auc': 0.7146739130434783, 'optimal_acc': 0.5510204081632653, 'imitation_acc': 0.7687074829931972}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8010171646535282, 'optimal_acc': 0.24489795918367346, 'imitation_acc': 0.7755102040816326}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8651, 0.2072, 0.1859, 0.8778], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 39 _____\n",
      "val reward 0.021352672949433327\n",
      "imitation reward 1.1256059408187866\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.0952380895614624, 0.4353741407394409, 0.2448979616165161]\n",
      "[{'decision': 0, 'optimal_auc': 0.5115463917525773, 'imitation_auc': 0.583969465648855, 'optimal_acc': 0.38095238095238093, 'imitation_acc': 0.8163265306122449}, {'decision': 1, 'optimal_auc': 0.4816901408450704, 'imitation_auc': 0.7157608695652173, 'optimal_acc': 0.5578231292517006, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8003814367450731, 'optimal_acc': 0.24489795918367346, 'imitation_acc': 0.782312925170068}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8641, 0.2064, 0.1857, 0.8753], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 40 _____\n",
      "val reward 0.020720170810818672\n",
      "imitation reward 1.128347396850586\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.0952380895614624, 0.4353741407394409, 0.2789115607738495]\n",
      "[{'decision': 0, 'optimal_auc': 0.5115463917525773, 'imitation_auc': 0.5849236641221374, 'optimal_acc': 0.38095238095238093, 'imitation_acc': 0.8299319727891157}, {'decision': 1, 'optimal_auc': 0.4816901408450704, 'imitation_auc': 0.7152173913043478, 'optimal_acc': 0.5578231292517006, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8003814367450731, 'optimal_acc': 0.2789115646258503, 'imitation_acc': 0.782312925170068}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8634, 0.2060, 0.1854, 0.8735], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 41 _____\n",
      "val reward 0.020562129095196724\n",
      "imitation reward 1.1320033073425293\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.10204081237316132, 0.4353741407394409, 0.29931971430778503]\n",
      "[{'decision': 0, 'optimal_auc': 0.516701030927835, 'imitation_auc': 0.5896946564885497, 'optimal_acc': 0.3877551020408163, 'imitation_acc': 0.8299319727891157}, {'decision': 1, 'optimal_auc': 0.4816901408450704, 'imitation_auc': 0.7152173913043478, 'optimal_acc': 0.5578231292517006, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8003814367450731, 'optimal_acc': 0.29931972789115646, 'imitation_acc': 0.782312925170068}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8628, 0.2054, 0.1852, 0.8721], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 42 _____\n",
      "val reward 0.020196057856082916\n",
      "imitation reward 1.1370224952697754\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.11564625799655914, 0.4353741407394409, 0.3265306055545807]\n",
      "[{'decision': 0, 'optimal_auc': 0.4967010309278351, 'imitation_auc': 0.5949427480916031, 'optimal_acc': 0.3741496598639456, 'imitation_acc': 0.8299319727891157}, {'decision': 1, 'optimal_auc': 0.4816901408450704, 'imitation_auc': 0.716304347826087, 'optimal_acc': 0.5578231292517006, 'imitation_acc': 0.7891156462585034}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8019707565162111, 'optimal_acc': 0.32653061224489793, 'imitation_acc': 0.7891156462585034}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8629, 0.2053, 0.1851, 0.8722], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 43 _____\n",
      "val reward 0.020146341994404793\n",
      "imitation reward 1.142024278640747\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.11564625799655914, 0.4285714328289032, 0.3265306055545807]\n",
      "[{'decision': 0, 'optimal_auc': 0.4967010309278351, 'imitation_auc': 0.5935114503816794, 'optimal_acc': 0.3741496598639456, 'imitation_acc': 0.8299319727891157}, {'decision': 1, 'optimal_auc': 0.48521126760563377, 'imitation_auc': 0.7152173913043478, 'optimal_acc': 0.564625850340136, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8019707565162111, 'optimal_acc': 0.32653061224489793, 'imitation_acc': 0.782312925170068}]\n",
      "True True True\n",
      "True True True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False False False\n",
      "False False True\n",
      "tensor([0.8629, 0.2049, 0.1850, 0.8717], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 44 _____\n",
      "val reward 0.019963644444942474\n",
      "imitation reward 1.1441617012023926\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.12244898080825806, 0.4217686951160431, 0.3333333134651184]\n",
      "[{'decision': 0, 'optimal_auc': 0.48670103092783507, 'imitation_auc': 0.5963740458015268, 'optimal_acc': 0.3673469387755102, 'imitation_acc': 0.8299319727891157}, {'decision': 1, 'optimal_auc': 0.48873239436619714, 'imitation_auc': 0.716304347826087, 'optimal_acc': 0.5714285714285714, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.803560076287349, 'optimal_acc': 0.3333333333333333, 'imitation_acc': 0.782312925170068}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8634, 0.2043, 0.1844, 0.8721], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 45 _____\n",
      "val reward 0.019866345450282097\n",
      "imitation reward 1.1426279544830322\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.12244898080825806, 0.40136054158210754, 0.3333333134651184]\n",
      "[{'decision': 0, 'optimal_auc': 0.47154639175257734, 'imitation_auc': 0.5954198473282444, 'optimal_acc': 0.35374149659863946, 'imitation_acc': 0.8367346938775511}, {'decision': 1, 'optimal_auc': 0.49929577464788727, 'imitation_auc': 0.7165760869565218, 'optimal_acc': 0.5918367346938775, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8016528925619835, 'optimal_acc': 0.3333333333333333, 'imitation_acc': 0.782312925170068}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8638, 0.2035, 0.1838, 0.8727], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 46 _____\n",
      "val reward 0.019738884642720222\n",
      "imitation reward 1.1400792598724365\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.12925170361995697, 0.3741496503353119, 0.3333333134651184]\n",
      "[{'decision': 0, 'optimal_auc': 0.4615463917525774, 'imitation_auc': 0.5958969465648856, 'optimal_acc': 0.3469387755102041, 'imitation_acc': 0.8367346938775511}, {'decision': 1, 'optimal_auc': 0.5133802816901408, 'imitation_auc': 0.7171195652173913, 'optimal_acc': 0.6190476190476191, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7997457088366179, 'optimal_acc': 0.3333333333333333, 'imitation_acc': 0.782312925170068}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8640, 0.2034, 0.1837, 0.8730], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 47 _____\n",
      "val reward 0.019708989188075066\n",
      "imitation reward 1.1360682249069214\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.13605442643165588, 0.36734694242477417, 0.3333333134651184]\n",
      "[{'decision': 0, 'optimal_auc': 0.4667010309278351, 'imitation_auc': 0.5887404580152672, 'optimal_acc': 0.35374149659863946, 'imitation_acc': 0.8367346938775511}, {'decision': 1, 'optimal_auc': 0.5169014084507042, 'imitation_auc': 0.7184782608695652, 'optimal_acc': 0.6258503401360545, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7981563890654799, 'optimal_acc': 0.3333333333333333, 'imitation_acc': 0.7755102040816326}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8643, 0.2025, 0.1831, 0.8730], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 48 _____\n",
      "val reward 0.019489388912916183\n",
      "imitation reward 1.1317695379257202\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.12244898080825806, 0.34693875908851624, 0.3333333134651184]\n",
      "[{'decision': 0, 'optimal_auc': 0.47154639175257734, 'imitation_auc': 0.586354961832061, 'optimal_acc': 0.35374149659863946, 'imitation_acc': 0.8367346938775511}, {'decision': 1, 'optimal_auc': 0.5274647887323943, 'imitation_auc': 0.7192934782608695, 'optimal_acc': 0.6462585034013606, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7978385251112523, 'optimal_acc': 0.3333333333333333, 'imitation_acc': 0.782312925170068}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8645, 0.2022, 0.1828, 0.8734], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 49 _____\n",
      "val reward 0.019471295177936554\n",
      "imitation reward 1.1303867101669312\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.13605442643165588, 0.3333333134651184, 0.3333333134651184]\n",
      "[{'decision': 0, 'optimal_auc': 0.4667010309278351, 'imitation_auc': 0.5839694656488549, 'optimal_acc': 0.35374149659863946, 'imitation_acc': 0.8367346938775511}, {'decision': 1, 'optimal_auc': 0.4309859154929577, 'imitation_auc': 0.7209239130434782, 'optimal_acc': 0.6462585034013606, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7940241576605214, 'optimal_acc': 0.3333333333333333, 'imitation_acc': 0.7755102040816326}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8642, 0.2014, 0.1823, 0.8714], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 50 _____\n",
      "val reward 0.019050126895308495\n",
      "imitation reward 1.1303120851516724\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.12925170361995697, 0.3265306055545807, 0.34693875908851624]\n",
      "[{'decision': 0, 'optimal_auc': 0.47670103092783506, 'imitation_auc': 0.5801526717557252, 'optimal_acc': 0.36054421768707484, 'imitation_acc': 0.8367346938775511}, {'decision': 1, 'optimal_auc': 0.4345070422535211, 'imitation_auc': 0.7214673913043478, 'optimal_acc': 0.6530612244897959, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7933884297520661, 'optimal_acc': 0.3469387755102041, 'imitation_acc': 0.7755102040816326}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8639, 0.2013, 0.1824, 0.8707], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 51 _____\n",
      "val reward 0.018895797431468964\n",
      "imitation reward 1.1326197385787964\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.13605442643165588, 0.3265306055545807, 0.35374149680137634]\n",
      "[{'decision': 0, 'optimal_auc': 0.48185567010309277, 'imitation_auc': 0.5815839694656488, 'optimal_acc': 0.3673469387755102, 'imitation_acc': 0.8367346938775511}, {'decision': 1, 'optimal_auc': 0.4345070422535211, 'imitation_auc': 0.7209239130434782, 'optimal_acc': 0.6530612244897959, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7924348378893834, 'optimal_acc': 0.35374149659863946, 'imitation_acc': 0.7755102040816326}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8630, 0.1999, 0.1820, 0.8688], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 52 _____\n",
      "val reward 0.017875777557492256\n",
      "imitation reward 1.1383904218673706\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.14965985715389252, 0.31292515993118286, 0.3741496503353119]\n",
      "[{'decision': 0, 'optimal_auc': 0.4921649484536082, 'imitation_auc': 0.5811068702290076, 'optimal_acc': 0.38095238095238093, 'imitation_acc': 0.8367346938775511}, {'decision': 1, 'optimal_auc': 0.44154929577464785, 'imitation_auc': 0.7222826086956522, 'optimal_acc': 0.6666666666666666, 'imitation_acc': 0.7891156462585034}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.791163382072473, 'optimal_acc': 0.3741496598639456, 'imitation_acc': 0.7755102040816326}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8627, 0.1997, 0.1819, 0.8683], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 53 _____\n",
      "val reward 0.017761440947651863\n",
      "imitation reward 1.1474403142929077\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.16326530277729034, 0.31292515993118286, 0.3809523582458496]\n",
      "[{'decision': 0, 'optimal_auc': 0.5024742268041237, 'imitation_auc': 0.5811068702290078, 'optimal_acc': 0.3945578231292517, 'imitation_acc': 0.8367346938775511}, {'decision': 1, 'optimal_auc': 0.44154929577464785, 'imitation_auc': 0.7198369565217391, 'optimal_acc': 0.6666666666666666, 'imitation_acc': 0.7891156462585034}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7914812460267006, 'optimal_acc': 0.38095238095238093, 'imitation_acc': 0.7755102040816326}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8627, 0.1997, 0.1819, 0.8683], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 54 _____\n",
      "val reward 0.017761440947651863\n",
      "imitation reward 1.157792091369629\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.16326530277729034, 0.31292515993118286, 0.3809523582458496]\n",
      "[{'decision': 0, 'optimal_auc': 0.5024742268041237, 'imitation_auc': 0.5806297709923663, 'optimal_acc': 0.3945578231292517, 'imitation_acc': 0.8367346938775511}, {'decision': 1, 'optimal_auc': 0.44154929577464785, 'imitation_auc': 0.7192934782608695, 'optimal_acc': 0.6666666666666666, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7911633820724729, 'optimal_acc': 0.38095238095238093, 'imitation_acc': 0.7755102040816326}]\n",
      "True True True\n",
      "True True True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False False False\n",
      "False False True\n",
      "tensor([0.8620, 0.1987, 0.1816, 0.8655], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 55 _____\n",
      "val reward 0.017101341858506203\n",
      "imitation reward 1.1675934791564941\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.17687074840068817, 0.30612245202064514, 0.40136054158210754]\n",
      "[{'decision': 0, 'optimal_auc': 0.4976288659793815, 'imitation_auc': 0.579675572519084, 'optimal_acc': 0.3945578231292517, 'imitation_acc': 0.8367346938775511}, {'decision': 1, 'optimal_auc': 0.44507042253521123, 'imitation_auc': 0.7195652173913044, 'optimal_acc': 0.673469387755102, 'imitation_acc': 0.7891156462585034}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7917991099809282, 'optimal_acc': 0.4013605442176871, 'imitation_acc': 0.7755102040816326}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8618, 0.1985, 0.1816, 0.8647], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 56 _____\n",
      "val reward 0.016905713826417923\n",
      "imitation reward 1.17449152469635\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.1904761791229248, 0.29931971430778503, 0.40816324949264526]\n",
      "[{'decision': 0, 'optimal_auc': 0.4927835051546392, 'imitation_auc': 0.5787213740458016, 'optimal_acc': 0.3945578231292517, 'imitation_acc': 0.8299319727891157}, {'decision': 1, 'optimal_auc': 0.4485915492957746, 'imitation_auc': 0.7206521739130434, 'optimal_acc': 0.6802721088435374, 'imitation_acc': 0.7891156462585034}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7959313413858868, 'optimal_acc': 0.40816326530612246, 'imitation_acc': 0.782312925170068}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8620, 0.1984, 0.1814, 0.8653], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 57 _____\n",
      "val reward 0.01696840487420559\n",
      "imitation reward 1.179534673690796\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.1904761791229248, 0.2925170063972473, 0.40816324949264526]\n",
      "[{'decision': 0, 'optimal_auc': 0.4927835051546392, 'imitation_auc': 0.5772900763358778, 'optimal_acc': 0.3945578231292517, 'imitation_acc': 0.8231292517006803}, {'decision': 1, 'optimal_auc': 0.452112676056338, 'imitation_auc': 0.7217391304347827, 'optimal_acc': 0.6870748299319728, 'imitation_acc': 0.7891156462585034}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7981563890654799, 'optimal_acc': 0.40816326530612246, 'imitation_acc': 0.782312925170068}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8616, 0.1985, 0.1814, 0.8647], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 58 _____\n",
      "val reward 0.017110062763094902\n",
      "imitation reward 1.182849645614624\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.1904761791229248, 0.29931971430778503, 0.40816324949264526]\n",
      "[{'decision': 0, 'optimal_auc': 0.4927835051546392, 'imitation_auc': 0.5758587786259542, 'optimal_acc': 0.3945578231292517, 'imitation_acc': 0.8231292517006803}, {'decision': 1, 'optimal_auc': 0.4485915492957746, 'imitation_auc': 0.7239130434782608, 'optimal_acc': 0.6802721088435374, 'imitation_acc': 0.7891156462585034}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7987921169739352, 'optimal_acc': 0.40816326530612246, 'imitation_acc': 0.7891156462585034}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8617, 0.1983, 0.1813, 0.8647], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 59 _____\n",
      "val reward 0.017006974667310715\n",
      "imitation reward 1.1836286783218384\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.18367347121238708, 0.29931971430778503, 0.40816324949264526]\n",
      "[{'decision': 0, 'optimal_auc': 0.5027835051546392, 'imitation_auc': 0.5782442748091603, 'optimal_acc': 0.4013605442176871, 'imitation_acc': 0.8231292517006803}, {'decision': 1, 'optimal_auc': 0.4485915492957746, 'imitation_auc': 0.7252717391304347, 'optimal_acc': 0.6802721088435374, 'imitation_acc': 0.7891156462585034}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8000635727908456, 'optimal_acc': 0.40816326530612246, 'imitation_acc': 0.7891156462585034}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8618, 0.1984, 0.1813, 0.8646], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 60 _____\n",
      "val reward 0.017026454210281372\n",
      "imitation reward 1.1849639415740967\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.17687074840068817, 0.29931971430778503, 0.40816324949264526]\n",
      "[{'decision': 0, 'optimal_auc': 0.5127835051546392, 'imitation_auc': 0.5791984732824427, 'optimal_acc': 0.40816326530612246, 'imitation_acc': 0.8231292517006803}, {'decision': 1, 'optimal_auc': 0.34507042253521125, 'imitation_auc': 0.7271739130434783, 'optimal_acc': 0.6666666666666666, 'imitation_acc': 0.7891156462585034}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8003814367450731, 'optimal_acc': 0.40816326530612246, 'imitation_acc': 0.7755102040816326}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8623, 0.1987, 0.1815, 0.8660], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 61 _____\n",
      "val reward 0.01715182512998581\n",
      "imitation reward 1.1874454021453857\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.17687074840068817, 0.29931971430778503, 0.39455780386924744]\n",
      "[{'decision': 0, 'optimal_auc': 0.5127835051546392, 'imitation_auc': 0.5791984732824427, 'optimal_acc': 0.40816326530612246, 'imitation_acc': 0.8231292517006803}, {'decision': 1, 'optimal_auc': 0.34507042253521125, 'imitation_auc': 0.7279891304347826, 'optimal_acc': 0.6666666666666666, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7984742530197075, 'optimal_acc': 0.3945578231292517, 'imitation_acc': 0.7755102040816326}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8621, 0.1985, 0.1812, 0.8650], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 62 _____\n",
      "val reward 0.017241330817341805\n",
      "imitation reward 1.1895297765731812\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.17687074840068817, 0.29931971430778503, 0.40816324949264526]\n",
      "[{'decision': 0, 'optimal_auc': 0.5127835051546392, 'imitation_auc': 0.5791984732824428, 'optimal_acc': 0.40816326530612246, 'imitation_acc': 0.8231292517006803}, {'decision': 1, 'optimal_auc': 0.34507042253521125, 'imitation_auc': 0.7298913043478261, 'optimal_acc': 0.6666666666666666, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7975206611570248, 'optimal_acc': 0.40816326530612246, 'imitation_acc': 0.782312925170068}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8623, 0.1986, 0.1813, 0.8653], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 63 _____\n",
      "val reward 0.01738331839442253\n",
      "imitation reward 1.1894890069961548\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.17006802558898926, 0.29931971430778503, 0.40136054158210754]\n",
      "[{'decision': 0, 'optimal_auc': 0.5076288659793814, 'imitation_auc': 0.5782442748091603, 'optimal_acc': 0.4013605442176871, 'imitation_acc': 0.8231292517006803}, {'decision': 1, 'optimal_auc': 0.34507042253521125, 'imitation_auc': 0.7298913043478261, 'optimal_acc': 0.6666666666666666, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7987921169739352, 'optimal_acc': 0.4013605442176871, 'imitation_acc': 0.782312925170068}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8637, 0.1977, 0.1803, 0.8672], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 64 _____\n",
      "val reward 0.017385099083185196\n",
      "imitation reward 1.190802812576294\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.17006802558898926, 0.25170066952705383, 0.39455780386924744]\n",
      "[{'decision': 0, 'optimal_auc': 0.5076288659793814, 'imitation_auc': 0.5801526717557253, 'optimal_acc': 0.4013605442176871, 'imitation_acc': 0.8231292517006803}, {'decision': 1, 'optimal_auc': 0.36971830985915494, 'imitation_auc': 0.73125, 'optimal_acc': 0.7142857142857143, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.800381436745073, 'optimal_acc': 0.3945578231292517, 'imitation_acc': 0.782312925170068}]\n",
      "True True True\n",
      "True True True\n",
      "False False False\n",
      "False False True\n",
      "tensor([0.8643, 0.1967, 0.1795, 0.8674], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 65 _____\n",
      "val reward 0.017150744795799255\n",
      "imitation reward 1.1937087774276733\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.16326530277729034, 0.22448979318141937, 0.39455780386924744]\n",
      "[{'decision': 0, 'optimal_auc': 0.5176288659793815, 'imitation_auc': 0.58206106870229, 'optimal_acc': 0.40816326530612246, 'imitation_acc': 0.8231292517006803}, {'decision': 1, 'optimal_auc': 0.38380281690140844, 'imitation_auc': 0.73125, 'optimal_acc': 0.7414965986394558, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8006993006993006, 'optimal_acc': 0.3945578231292517, 'imitation_acc': 0.782312925170068}]\n",
      "True True True\n",
      "True True True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False False False\n",
      "False False True\n",
      "tensor([0.8646, 0.1965, 0.1795, 0.8679], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "______epoch 66 _____\n",
      "val reward 0.016907403245568275\n",
      "imitation reward 1.1985191106796265\n",
      "distance losses 0.0 0.0\n",
      "distributions [0.1904761791229248, 0.20408162474632263, 0.40136054158210754]\n",
      "[{'decision': 0, 'optimal_auc': 0.5079381443298969, 'imitation_auc': 0.583969465648855, 'optimal_acc': 0.40816326530612246, 'imitation_acc': 0.8299319727891157}, {'decision': 1, 'optimal_auc': 0.3943661971830986, 'imitation_auc': 0.7298913043478261, 'optimal_acc': 0.7619047619047619, 'imitation_acc': 0.782312925170068}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.8041958041958042, 'optimal_acc': 0.4013605442176871, 'imitation_acc': 0.7891156462585034}]\n",
      "++++++++++Final+++++++++++\n",
      "best tensor(0.9218, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "[{'decision': 0, 'optimal_auc': 0.4160824742268041, 'imitation_auc': 0.4756679389312977, 'optimal_acc': 0.30612244897959184, 'imitation_acc': 0.891156462585034}, {'decision': 1, 'optimal_auc': 0.4957746478873239, 'imitation_auc': 0.7016304347826087, 'optimal_acc': 0.5850340136054422, 'imitation_acc': 0.7755102040816326}, {'decision': 2, 'optimal_auc': -1, 'imitation_auc': 0.7609663064208518, 'optimal_acc': 0.2789115646258503, 'imitation_acc': 0.7891156462585034}]\n"
     ]
    }
   ],
   "source": [
    "def train_decision_model(\n",
    "    tmodel1,\n",
    "    tmodel2,\n",
    "    tmodel3,\n",
    "    use_default_split=True,\n",
    "    use_bagging_split=False,\n",
    "    lr=.0001,\n",
    "    epochs=10000,\n",
    "    patience=50,\n",
    "    weights=[-1,1,1,-1], #realtive weight of survival, feeding tube, aspiration, andl lrc\n",
    "    imitation_weights=[.5,1,1],#weights of imitation decisions, because ic overtrains too quickly\n",
    "    imitation_weight=0.1,\n",
    "    shufflecol_chance = 0.1,\n",
    "    reward_weight=1,\n",
    "    imitation_triplet_weight=0,\n",
    "    reward_triplet_weight = 0,\n",
    "    split=.7,\n",
    "    resample_training=False,\n",
    "    save_path='../data/models/',\n",
    "    file_suffix='',\n",
    "    use_gpu=True,\n",
    "    use_attention=True,\n",
    "    verbose=True,\n",
    "    threshold_decisions=True,#convert decisiosn to binary in simulation, usually breaks it\n",
    "    use_smote=False,\n",
    "    validate_with_memory=True,\n",
    "    **model_kwargs):\n",
    "    \n",
    "    tmodel1.eval()\n",
    "    tmodel2.eval()\n",
    "    tmodel3.eval()\n",
    "\n",
    "    train_ids, test_ids = get_tt_split(use_default_split=use_default_split,use_bagging_split=use_bagging_split,resample_training=resample_training)\n",
    "    true_ids = train_ids + test_ids #for saving memory without upsampling\n",
    "    if use_smote:\n",
    "        dataset = DTDataset(use_smote=True,smote_ids = train_ids)\n",
    "        train_ids = [i for i in dataset.processed_df.index.values if i not in test_ids]\n",
    "    else:\n",
    "        dataset = DTDataset()\n",
    "    data = dataset.processed_df.copy()\n",
    "    \n",
    "    def get_dlt(state):\n",
    "        if state == 2:\n",
    "            return data[Const.dlt2].copy()\n",
    "        d = data[Const.dlt1].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_pd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.primary_disease_states2].copy()\n",
    "        d = data[Const.primary_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_nd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.nodal_disease_states2].copy()\n",
    "        d = data[Const.nodal_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_cc(state):\n",
    "        res = data[Const.ccs].copy()\n",
    "        if state == 1:\n",
    "            res.values[:,:] = np.zeros(res.values.shape)\n",
    "        return res\n",
    "    \n",
    "    def get_mod(state):\n",
    "        res = data[Const.modifications].copy()\n",
    "        #this should have an ic condition but we don't use it anumore anywa\n",
    "        return res\n",
    "        \n",
    "    outcomedf = data[Const.outcomes]\n",
    "    baseline = dataset.get_state('baseline')\n",
    "    \n",
    "    def formatdf(d,dids=train_ids):\n",
    "        d = df_to_torch(d.loc[dids]).to(model.get_device())\n",
    "        return d\n",
    "    \n",
    "    def makegrad(v):\n",
    "        if not v.requires_grad:\n",
    "            v.requires_grad=True\n",
    "        return v\n",
    "    \n",
    "    if use_attention:\n",
    "        model = DecisionAttentionModel(baseline.shape[1],**model_kwargs)\n",
    "    else:\n",
    "        model_kwargs = {k:v for k,v in model_kwargs.items() if 'attention' not in k and 'embed' not in k}\n",
    "        model = DecisionModel(baseline.shape[1],**model_kwargs)\n",
    "\n",
    "    device = 'cpu'\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "        \n",
    "    model.set_device(device)\n",
    "\n",
    "    tmodel1.set_device(device)\n",
    "    tmodel2.set_device(device)\n",
    "    tmodel3.set_device(device)\n",
    "    \n",
    "    hashcode = str(hash(','.join([str(i) for i in train_ids])))\n",
    "    \n",
    "    save_file = save_path + 'model_' + model.identifier +'_hash' + hashcode + file_suffix + '.tar'\n",
    "    model.fit_normalizer(df_to_torch(baseline.loc[train_ids]).to(model.get_device()))\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "\n",
    "    def outcome_loss(ypred):\n",
    "        l = torch.mul(ypred[:,0],weights[0])\n",
    "        for i,weight in enumerate(weights[1:]):\n",
    "            #weights with negative values will invert the outcome so e.g. Regional control becomes no regional control\n",
    "            #so the penaly is correct\n",
    "            newloss = torch.mul(ypred[:,i],weight)\n",
    "            l = torch.add(l,newloss)\n",
    "        return l\n",
    "    \n",
    "    mse = torch.nn.MSELoss()\n",
    "    bce = torch.nn.BCELoss()\n",
    "    device = model.get_device()\n",
    "    def compare_decisions(d1,d2,d3,ids):\n",
    "#         ypred = np.concatenate([dd.cpu().detach().numpy().reshape(-1,1) for dd in [d1,d2,d3]],axis=1)\n",
    "        ytrue = df_to_torch(outcomedf.loc[ids])\n",
    "        dloss = bce(d1.view(-1),ytrue[:,0])\n",
    "        dloss += bce(d2.view(-1),ytrue[:,1])\n",
    "        dloss += bce(d3,view(-1),ytrue[:,2])\n",
    "        return dloss\n",
    "        \n",
    "    def remove_decisions(df):\n",
    "        cols = [c for c in df.columns if c not in Const.decisions ]\n",
    "        ddf = df[cols]\n",
    "        return ddf\n",
    "    \n",
    "    makeinput = lambda step,dids: df_to_torch(remove_decisions(dataset.get_input_state(step=step,ids=dids))).to(device)\n",
    "    thresh = lambda x: torch.sigmoid(100000000*(x - .5))\n",
    "\n",
    "    optimal_train,transitions_train = calc_optimal_decisions(dataset,train_ids,tmodel1,tmodel2,tmodel3,\n",
    "                                                weights=weights,\n",
    "                                                outcome_loss_func = outcome_loss,\n",
    "                                          )\n",
    "    optimal_test,transitions_test = calc_optimal_decisions(dataset,test_ids,tmodel1,tmodel2,tmodel3,\n",
    "                                                           weights=weights,\n",
    "                                                           outcome_loss_func = outcome_loss,\n",
    "                                         )\n",
    "    optimal_train = optimal_train.to(model.get_device())\n",
    "    optimal_test = optimal_test.to(model.get_device())\n",
    "    \n",
    "    #save the inputs from the whole dataset for future reference\n",
    "    if use_attention:\n",
    "        full_data = []\n",
    "        for mstep in [0,1,2]:\n",
    "            full_data_step = [baseline, get_dlt(min(mstep,1)),\n",
    "                         get_dlt(mstep),get_pd(mstep),get_nd(mstep),get_cc(mstep),get_mod(mstep)]\n",
    "            full_data_step = torch.cat([formatdf(fd,true_ids) for fd in full_data_step],axis=1)\n",
    "            full_data.append(full_data_step)\n",
    "        full_data = torch.stack(full_data)\n",
    "        model.save_memory(full_data.to(device))\n",
    "        print(full_data.shape)\n",
    "        \n",
    "    randchoice = lambda x: x[torch.randint(len(x),(1,))[0]]\n",
    "    tloss_func = torch.nn.TripletMarginLoss()\n",
    "    def get_tloss(row,step,yt,x,imitation=True):\n",
    "        if yt[:,step].std() < .001:\n",
    "            return torch.tensor([0]).to(device)\n",
    "        positive_idx= torch.nonzero(yt[:,step] == yt[row,step])\n",
    "        positive_idx = torch.stack([ii for ii in positive_idx if ii != row]).view(-1)\n",
    "        negative_idx = torch.tensor([ii for ii in range(x.shape[0]) if ii not in positive_idx and ii != row])\n",
    "        if len(positive_idx) < 1 or len(negative_idx) < 1:\n",
    "            print('no losses','n positive',len(positive_idx),'n negative',len(negative_idx),'yt',yt[row,step],'row',row,'step',step,'imitation',imitation,end='\\r')\n",
    "            return torch.tensor([0]).to(device)\n",
    "        positive = x[randchoice(positive_idx)]\n",
    "        negative = x[randchoice(negative_idx)]\n",
    "        anchor = x[row]\n",
    "        if use_attention:\n",
    "            [anchor_embedding,pos_embedding,neg_embedding] = [model.get_embedding(xx.view(1,-1),position=step,use_saved_memory=True) for xx in [anchor,positive,negative]]\n",
    "        else:    \n",
    "            [anchor_embedding,pos_embedding,neg_embedding] = [model.get_embedding(xx.view(1,-1),position=step,concatenate=False)[int(imitation)] for xx in [anchor,positive,negative]]\n",
    "        tloss = tloss_func(anchor_embedding,pos_embedding,neg_embedding)\n",
    "        return tloss\n",
    "    \n",
    "    def step(train=True):\n",
    "        if train:\n",
    "            model.train(True)\n",
    "            tmodel1.train(True)\n",
    "            tmodel2.train(True)\n",
    "            tmodel3.train(True)\n",
    "            optimizer.zero_grad()\n",
    "            ids = train_ids\n",
    "            y_opt = makegrad(optimal_train)\n",
    "        else:\n",
    "            ids = test_ids\n",
    "            model.eval()\n",
    "            tmodel1.eval()\n",
    "            tmodel2.eval()\n",
    "            tmodel3.eval()\n",
    "            y_opt = makegrad(optimal_test)\n",
    "            \n",
    "            \n",
    "        ytrain = df_to_torch(outcomedf.loc[ids]).to(device)\n",
    "        #imitation losses and decision 1\n",
    "        xxtrained = [baseline, get_dlt(0),get_dlt(0),get_pd(0),get_nd(0),get_cc(0),get_mod(0)]\n",
    "        xxtrain = torch.cat([formatdf(xx,ids) for xx in xxtrained],axis=1).to(device)\n",
    "        \n",
    "        use_memory = (not train) and validate_with_memory\n",
    "\n",
    "        o1 = model(xxtrain,position=0,use_saved_memory = use_memory)\n",
    "\n",
    "        decision1_imitation = o1[:,3]\n",
    "        \n",
    "        decision1_opt = o1[:,0]\n",
    "        if threshold_decisions:\n",
    "            decision1_opt = thresh(decision1_opt)\n",
    "\n",
    "        imitation_loss1 = bce(decision1_imitation,ytrain[:,0])\n",
    "        imitation_loss1 = torch.mul(imitation_loss1,imitation_weights[0])\n",
    "        \n",
    "        x1_imitation = [baseline, get_dlt(1),get_dlt(0),get_pd(1),get_nd(1),get_cc(1),get_mod(1)]\n",
    "        x1_imitation = [formatdf(xx1,ids) for xx1 in x1_imitation]\n",
    "        x1_imitation = torch.cat(x1_imitation,axis=1).to(device)\n",
    "        decision2_imitation = model(x1_imitation,position=1,use_saved_memory = use_memory)[:,4]\n",
    "\n",
    "        imitation_loss2 =  bce(decision2_imitation,ytrain[:,1])\n",
    "        imitation_loss2 = torch.mul(imitation_loss2,imitation_weights[1])\n",
    "        \n",
    "        x2_imitation = [baseline, get_dlt(1),get_dlt(2),get_pd(2),get_nd(2),get_cc(2),get_mod(2)]\n",
    "        \n",
    "        x2_imitation = [formatdf(xx2,ids) for xx2 in x2_imitation]\n",
    "        x2_imitation = torch.cat(x2_imitation,axis=1)\n",
    "        decision3_imitation = model(x2_imitation,position=2,use_saved_memory = use_memory)[:,5]\n",
    "\n",
    "        imitation_loss3 = bce(decision3_imitation,ytrain[:,2])\n",
    "        imitation_loss3 = torch.mul(imitation_loss3,imitation_weights[2])\n",
    "        \n",
    "        #reward decisions\n",
    "        xx1 = makeinput(1,ids)\n",
    "        xx2 = makeinput(2,ids)\n",
    "        xx3 = makeinput(3,ids)\n",
    "\n",
    "        xx1 = makegrad(xx1)\n",
    "        xx2 = makegrad(xx2)\n",
    "        xx3 = makegrad(xx3)\n",
    "        baseline_train_base = formatdf(baseline,ids)\n",
    "            \n",
    "        baseline_train = torch.clone(baseline_train_base)\n",
    "\n",
    "        \n",
    "        xi1 = torch.cat([xx1,decision1_opt.view(-1,1)],axis=1)\n",
    "        print(train,tmodel1.training,tmodel1.dropout.training)\n",
    "        [ypd1, ynd1, ymod, ydlt1] = tmodel1(xi1)['predictions']\n",
    "        print(train,tmodel1.training,tmodel1.dropout.training)\n",
    "        d1_thresh = torch.gt(decision1_opt.view(-1,1),.5).to(ypd1.device)\n",
    "        d1_scale = torch.cat([d1_thresh,d1_thresh,torch.ones(d1_thresh.view(-1,1).shape).to(ypd1.device)],dim=1)\n",
    "        ypd1= torch.mul(ypd1,d1_scale)\n",
    "        ynd1= torch.mul(ynd1,d1_scale)\n",
    "        \n",
    "        x1 = [baseline_train,ydlt1,formatdf(get_dlt(0),ids),ypd1,ynd1,formatdf(get_cc(1),ids),ymod]\n",
    "        x1= torch.cat([xx1.to(model.get_device()) for xx1 in x1],axis=1)\n",
    "        \n",
    "        decision2_opt = model(x1,position=1,use_saved_memory = use_memory)[:,1] \n",
    "        if threshold_decisions:\n",
    "            decision2_opt = thresh(decision2_opt)\n",
    "            \n",
    "        xi2 = torch.cat([xx2,decision1_opt.view(-1,1),decision2_opt.view(-1,1)],axis=1)\n",
    "        [ypd2,ynd2,ycc,ydlt2] = tmodel2(xi2)['predictions']\n",
    "\n",
    "        x2 = [baseline_train,ydlt1,ydlt2,ypd2,ynd2,ycc,ymod]\n",
    "        x2 = torch.cat([xx2.to(model.get_device()) for xx2 in x2],axis=1)\n",
    "        decision3_opt = model(x2,position=2,use_saved_memory = use_memory)[:,2]\n",
    "        \n",
    "        if threshold_decisions:\n",
    "            decision3_opt = thresh(decision3_opt)\n",
    "            \n",
    "        xi3 = torch.cat([xx3,decision1_opt.view(-1,1),decision2_opt.view(-1,1),decision3_opt.view(-1,1)],axis=1)\n",
    "        \n",
    "        outcomes = tmodel3(xi3)['predictions']\n",
    "\n",
    "        if not train and verbose:\n",
    "            print(torch.mean(outcomes,dim=0))\n",
    "            \n",
    "        reward_loss = torch.mean(outcome_loss(outcomes))\n",
    "        loss = torch.add(imitation_loss1,imitation_loss2)\n",
    "        loss = torch.add(loss,imitation_loss3)\n",
    "        loss = torch.mul(loss,imitation_weight/3)\n",
    "        loss = torch.add(loss,torch.mul(reward_loss,reward_weight))\n",
    "        \n",
    "        imitation_tloss = torch.FloatTensor([0]).to(device)\n",
    "        opt_tloss = torch.FloatTensor([0]).to(device)\n",
    "        n_rows = x1.shape[0]\n",
    "        if reward_triplet_weight + imitation_triplet_weight > 0.0001:\n",
    "            for i in range(n_rows):\n",
    "                #skip if we're using an attention model idk\n",
    "                if not use_attention and imitation_triplet_weight > .0001:\n",
    "                    imitation_tloss += get_tloss(i,0,ytrain,xxtrain,True)\n",
    "                    imitation_tloss += get_tloss(i,1,ytrain,x1_imitation,True)\n",
    "                    imitation_tloss += get_tloss(i,2,ytrain,x2_imitation,True)\n",
    "                if reward_triplet_weight > .0001:\n",
    "                    opt_tloss += get_tloss(i,0,y_opt,xxtrain,False)\n",
    "                    opt_tloss += get_tloss(i,1,y_opt,x1,False)\n",
    "                    opt_tloss += get_tloss(i,2,y_opt,x2,False)\n",
    "            loss += torch.mul(imitation_tloss[0],imitation_triplet_weight/n_rows)\n",
    "            loss += torch.mul(opt_tloss[0],reward_triplet_weight/n_rows)\n",
    "        \n",
    "        losses = [imitation_loss1+imitation_loss2+imitation_loss3,reward_loss,imitation_tloss,opt_tloss]\n",
    "\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            return losses\n",
    "        else:\n",
    "            scores = []\n",
    "            distributions = [decision1_opt.mean().item(),decision2_opt.mean().item(),decision3_opt.mean().item()]\n",
    "            imitation = [decision1_imitation,decision2_imitation,decision3_imitation]\n",
    "            optimal = [decision1_opt,decision2_opt,decision3_opt]\n",
    "            for i,decision_im in enumerate(imitation):\n",
    "                deci = decision_im.cpu().detach().numpy()\n",
    "                deci0 = (deci > .5).astype(int)\n",
    "                iout = ytrain[:,i].cpu().detach().numpy()\n",
    "                acci = accuracy_score(iout,deci0)\n",
    "                try:\n",
    "                    auci = roc_auc_score(iout,deci)\n",
    "                except:\n",
    "                    auci = -1\n",
    "                \n",
    "                deco = optimal[i].cpu().detach().numpy()\n",
    "                deci0 = (deco > .5).astype(int)\n",
    "                oout = y_opt[:,i].cpu().detach().numpy()\n",
    "                acco = accuracy_score(oout,deci0)\n",
    "                try:\n",
    "                    auco = roc_auc_score(oout,deco)\n",
    "                except:\n",
    "                    auco=-1\n",
    "                scores.append({'decision': i,'optimal_auc': auco,'imitation_auc': auci,'optimal_acc': acco,'imitation_acc': acci})\n",
    "            return losses, scores, distributions\n",
    "        \n",
    "    best_val_loss = torch.tensor(1000000000.0)\n",
    "    steps_since_improvement = 0\n",
    "    best_val_score = {}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        losses = step(True)\n",
    "        val_losses,val_metrics,val_distributions = step(False)\n",
    "        vl = val_losses[0] + val_losses[1]\n",
    "        for vm in val_metrics:\n",
    "            vl += (-((vm['optimal_auc']*reward_weight) + (vm['imitation_auc']*imitation_weight)))/10\n",
    "        if verbose:\n",
    "            print('______epoch',str(epoch),'_____')\n",
    "            print('val reward',val_losses[1].item())\n",
    "            print('imitation reward', val_losses[0].item())\n",
    "            if len(val_losses) > 2:\n",
    "                print('distance losses',val_losses[2].item(),val_losses[-1].item())\n",
    "            print('distributions',val_distributions)\n",
    "            print(val_metrics)\n",
    "        if vl < best_val_loss:\n",
    "            best_val_loss = vl\n",
    "            best_val_score = val_metrics\n",
    "            best_val_distributions = val_distributions\n",
    "            steps_since_improvement = 0\n",
    "            torch.save(model.state_dict(),save_file)\n",
    "        else:\n",
    "            steps_since_improvement += 1\n",
    "        if steps_since_improvement > patience:\n",
    "            break\n",
    "    print('++++++++++Final+++++++++++')\n",
    "    print('best',best_val_loss)\n",
    "    print(best_val_score)\n",
    "    model.load_state_dict(torch.load(save_file))\n",
    "    model.eval()\n",
    "    return model, best_val_score, best_val_loss, best_val_distributions\n",
    "\n",
    "from Models import *\n",
    "# args = {\n",
    "#     'hidden_layers': [50,50], \n",
    "#     'attention_heads': [2,2],\n",
    "#     'embed_size': 120, \n",
    "#     'dropout': 0.5, \n",
    "#     'input_dropout': 0.2, \n",
    "#     'shufflecol_chance':  0.2,\n",
    "# }\n",
    "args = {\n",
    "    'hidden_layers': [500], \n",
    "    'opt_layer_size': 20, \n",
    "    'imitation_layer_size': 20, \n",
    "    'dropout': 0.25, \n",
    "    'input_dropout': 0.25, \n",
    "    'shufflecol_chance': 0.5\n",
    "}\n",
    "# from Models import *\n",
    "# decision_model, _, _, _ = train_decision_model(\n",
    "#     tmodel1[0],tmodel2[0],tmodel3_smote[0],\n",
    "#     lr=.001,\n",
    "#     use_attention=True,\n",
    "#     imitation_weight=1,\n",
    "#     imitation_triplet_weight=0,\n",
    "#     reward_triplet_weight=0,\n",
    "#     reward_weight=2,\n",
    "#     validate_with_memory=True,\n",
    "#     use_smote=False,\n",
    "#     **args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf95052",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sigmoid(1000000000000*(torch.tensor([.4]) - .5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062ec356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch_triplet_model(m1,m2,m3,weights=[-1,1,1,-1]):\n",
    "    hidden_layers = [[50],[100],[200],[500],[1000]]\n",
    "    extra_layers = [20,50,100,200]\n",
    "    best_loss = 100000000000\n",
    "    best_metrics = {}\n",
    "    best_args = {}\n",
    "    best_model = None\n",
    "    best_record = {}\n",
    "    k = 0\n",
    "    records = []\n",
    "    for hl in hidden_layers:\n",
    "        for el in extra_layers:\n",
    "            args = {'hidden_layers': hl, 'opt_layer_size': el, 'imitation_layer_size': el}\n",
    "            for dropout in [.1,.25,.5,.75]:\n",
    "                args['dropout'] = dropout\n",
    "                for input_dropout in [0.1,.25,.5]:\n",
    "                    args['input_dropout'] = input_dropout\n",
    "                    for shufflecol_chance in [.1,.5]:\n",
    "                        try:\n",
    "                            args['shufflecol_chance'] = shufflecol_chance\n",
    "                            model,m_metrics,m_loss,m_distribution = train_decision_model_triplet(m1,m2,m3,\n",
    "                                                                        lr=.01,\n",
    "                                                                        weights=weights,\n",
    "                                                                        verbose=False,\n",
    "                                                                        **args)\n",
    "                            entry = {\n",
    "                                'metrics': m_metrics,\n",
    "                                'loss': m_loss,\n",
    "                                'args': args,\n",
    "                                'distribution': m_distribution,\n",
    "                            }\n",
    "                            records.append(entry)\n",
    "                            print('done',k,m_loss)\n",
    "                            print('curr best',best_loss)\n",
    "                            k+=1\n",
    "                            if m_loss < best_loss:\n",
    "                                best_loss = m_loss\n",
    "                                best_metrics  = m_metrics\n",
    "                                best_model = model\n",
    "                                best_args = args\n",
    "                                best_record = entry\n",
    "                                print('_++++++++++New Best++++____')\n",
    "                                print(best_loss)\n",
    "                                print(best_metrics)\n",
    "                                print(best_args)\n",
    "                                print('___________')\n",
    "                                print('++++++++')\n",
    "                                print()\n",
    "                        except Exception as e:\n",
    "                            print('error',e,args)\n",
    "    print('_________')\n",
    "    print('+++++++++++')\n",
    "    print('best stuff',best_loss)\n",
    "    print(best_metrics)\n",
    "    print(best_args)\n",
    "    return best_model, records, best_record\n",
    "\n",
    "# decision_model_best,records, bestr = gridsearch_triplet_model(emodel1,emodel2,emodel3,weights=[-1,1,1,-1])\n",
    "# records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebdf76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91af46a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model1,'../data/models/final_transition1_model_' + emodel1.identifier + '.pt')\n",
    "# torch.save(model2,'../data/models/final_transition2_model_' + emodel2.identifier + '.pt')\n",
    "# torch.save(model3,'../data/models/final_outcome_model_' + emodel3.identifier + '.pt')\n",
    "# print('../data/models/final_transition1_model_' + emodel1.identifier + '.pt')\n",
    "# print('../data/models/final_transition2_model_' + emodel2.identifier + '.pt')\n",
    "# print('../data/models/final_outcome_model_' + emodel3.identifier + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "30998ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_model.set_device('cpu')\n",
    "tmodel1[0].set_device('cpu')\n",
    "tmodel2[0].set_device('cpu')\n",
    "tmodel3[0].set_device('cpu')\n",
    "torch.save(decision_model,'../resources/decision_model.pt')\n",
    "torch.save(tmodel1[0],'../resources/transition1_model.pt')\n",
    "torch.save(tmodel2[0],'../resources/transition2_model.pt')\n",
    "torch.save(tmodel3_smote[0],'../resources/outcome_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "a59220fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_model_alt.set_device('cpu')\n",
    "torch.save(decision_model_alt,'../resources/decision_model_alt.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321249c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_predictions(dm):\n",
    "    res  = []\n",
    "    for state in [0,1,2]:\n",
    "        mem = dm.memory[state]\n",
    "        mem = torch.median(mem,dim=0)[0].type(torch.FloatTensor)\n",
    "        val = dm(mem.reshape(1,-1),position=state)\n",
    "        res.append(val.cpu().detach().numpy())\n",
    "    return np.stack(res)\n",
    "get_default_predictions(decision_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c01c02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
